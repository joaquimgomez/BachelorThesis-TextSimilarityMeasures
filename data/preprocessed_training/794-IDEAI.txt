ieee transact imag processing,2016 1 antipod invari metric fast regression-bas super-resolut eduardo perez-pellitero, jordi salvador, javier ruiz-hidalgo, bodo rosenhahn abstractdictionary-bas super-resolut algorithm usu- alli select dictionari atom base distanc similar metrics. optim select nearest neighbor central import methods, impact proper metric super-resolut (sr) overlook literature, mainli vast usag euclidean distance. paper present fast regression-bas algorithm build dens popul anchor neighborhood sublinear search structures. perform studi natur featur commonli sr, observ featur usual lie unitari hypersphere, point diametr opposit one, i.e. antipode, modul angle, opposit direction. valid benefit antipod invari metrics, binari split us euclidean distance, handl antipod optimally. order benefit worlds, propos simpl effect antipod invari transform (ait) easili includ euclidean distanc calculation. modifi origin spheric hash algorithm metric antipod invari spheric hash scheme, obtain perform pure antipod invari metric. round contribut novel featur transform obtain better coars approxim input imag thank iter projection. perform method, name antipod invari super- resolut (ais), improv qualiti (psnr) faster state-of-the-art method. index termssuper-resolution, regression, antipodes, spher- ical hashing. i. introduct super-resolut (sr) techniqu aim extend theresolut signal surpass limit origin captur device. increas resolut imag inform deepli ill-pos problem, there- fore, need address certain prior knowledge. sr rel young research field, prior proposed. simplest earliest imag sr method base piecewis linear smooth prior (i.e. bi- linear bicub interpolation, respectively), result fast interpolation-bas algorithms. tsai huang [1] show possibl reconstruct higher-resolut imag regist fuse multipl images, pioneer vast approach multi-imag sr, call e. perez-pellitero technicolor r&i hannov tnt lab, leibniz universitat hannover, germany. j. salvador technicolor r&i hannover. j. ruiz-hidalgo imag process group, universitat politecnica catalunya, spain. b. rosenhahn tnt lab, leibniz universitat hannover. manuscript receiv march 20, 2015. slower <--- run time (s) ---> faster 10-1100101 p s n r ( d b ) 31.7 31.8 31.9 32 32.1 32.2 32.3 32.4 32.5 srcnn zeyd et al. ne+ll 9-1-5 9-5-5 anr ai 7 sphere 1 sphere a+ fig. 1: propos ai (antipod invari super- resolution) achiev best qualiti (psnr) fastest speed (s) comparison recent state-of-the- art methods. algorithm run set14 2 upscal factor, tabl iii information. reconstruction-bas sr. idea refined, others, introduct iter back-project improv registr irani peleg [2], fur- ther analysi baker kanad [3] lin shum [4] show fundament limit type sr, mainli condit registr accuracy. learning-bas sr, known example-based, overcam aforemen- tion limit avoid necess registr process build prior imag statistics. origin work freeman et al. [5] aim learn patch- feature-bas exampl produc effect magnif practic limit multi-imag sr. example-bas sr approach dictionari usu- alli divid categories: intern extern dictionary-bas sr. exploit strong self- similar prior. prior learnt directli rela- tionship imag patch differ scale input image. open work subcategori introduc glasner et al. [6], present power framework fuse reconstruction-bas example-bas sr. re- search categori freedman fattal [7] introduc mechan high-frequ transfer base exampl small area patch, better local cross-scal self-similar prior spatial neighborhood. recent work yang et al. [8] develop idea local cross-scal self-similar prior arriv ruben pocul texto escrito mquina ruben pocul texto escrito mquina ruben pocul texto escrito mquina 2016 ieee. person us materi permitted. permiss ieee obtain uses, current futur media, includ reprinting/republish materi advertis promot purposes, creat new collect works, resal redistribut server lists, reus copyright compon work works. doi 10.1109/tip.2016.2549362 ruben pocul texto escrito mquina ruben pocul texto escrito mquina ruben pocul texto escrito mquina 2 ieee transact imag processing,2016 in-plac prior, i.e. best match scale locat exactli posit scale similar enough. extern dictionary-bas sr us imag build dictionaries. repres wide approach base spars decomposition. main idea approach decomposit patch input imag combin spars subset entri compact dictionary. work yang et al. [9] us extern databas compos relat low high-resolut patch jointli learn compact dictionari pair. testing, imag patch decompos spars linear combin entri low-resolut (lr) dictionari weight gener high-resolut (hr) patch linear combin hr entries. dictionari train test costli l1 regular term enforc sparsity. work zeyd et al. [10] extend spars sr propos algorithm speed-up improv performance. however, bottleneck sparsiti method remain spars decomposition. neighbor embed algorithm gao et al. [11] propos method build neighborhood linear embed unifi featur subspac span lrhr imag patch origin lr featur space alone. recently, regression-bas sr receiv great deal attent research community. case, goal learn certain map manifold lr patch hr patches, follow manifold assumpt earlier work chang et al. [12]. map manifold assum local linear linear regressor anchor manifold piecewis linear [13]. kei observ perform neighbor embed predefin set anchor point allow preprocess train time, lessen substanti test time complexity. recent work [14] put light properli creat neighborhood methods, obtain sizabl benefit term reconstruct quality. addit that, explor sublinear search structur regressor nearest neighbor search, take signific quota run time process sr pipeline. paper follow research direct introduc follow contributions: 1) studi provid insight behavior distanc metric regress process. detect import antipod invariance. 2) provid suitabl effici spheric hash framework exploit fast regressor search. order benefit antipod invariance, introduc new simpl effect transform make euclidean distanc (and spheric hashing) antipod invari- ant. 3) present novel featur transform base gradient iter project improv qualiti upscal imag thank better coars approxim confluenc mention contribut yield state-of-the-art speed qualiti results. propos approxim regressor search fastest (about 411 time faster) greatli improv qualiti (about 0.2db higher) compar predecessor a+. addition, perform exhaust search obtain better psnr result (up 0.32db higher). rest paper organ follows. section ii defin problem review state-of-the-art relat work. section iii propos algorithm. section iv discuss assess perform present contribut separately. experiment result comparison state-of-the-art method provid section v. conclud paper section vi. ii. relat work upcom section introduc sr problem example-bas approach tackl it, follow review relev method state art. a. problem statement super-resolut aim upscal imag un- satisfactori pixel resolut preserv visual sharpness, formal x = (y ) s.t. x y, (1) y input image, x output upscal image, () upsampl oper calligraph font denot spectrum image. literatur transform usual mod- el backward restor origin imag suffer certain degrad [9] y = (b(x)), (2) b() blur filter () downsampl operator. problem usual address patch level, denot lower case (e.g. y, x) extract respect uppercas imag (e.g. y, x). example-bas sr famili tackl super-resolut problem find meaning exampl hr counterpart known, coupl dictionar- i dl dh. low resolut patch y decompos linear combin atom lr dictionari dl. min y dl 2 2 + p , (3) select weight element dictionari weight possibl lp-norm regular term. linear combin atom obtained, high resolut patch x obtain appli weight hr dictionari dh x = dh. lp-norm select dictionary-build process depend chosen prior defin sr algorithm. perez-pellitero et al.: antipod invari sr 3 b. state art regression-bas sr method prolif sr famili recent years. section re- view relev public take advantag regress strategies, special emphasi on, limit to, piecewis linear regress schemes. regression-bas sr object train given regressor r obtain certain map function lr hr patches. lr patch form input manifold m dimens m hr patch form target manifold n dimens n. formally, train pair (yf , x) yf m x n , like infer map : m rm n rn. work kim et al. [15] example-bas learn build framework freeman et al. [5] aim recov hf content x non-linear regress stage, ad bicub interpolation. regress consist kernel ridg regress (krr) cost function reads: o( { f 1 , . . . , f n } ) = n i=1 ( 1 2 l j=1 (f (yfj)xij) 2 + 1 2 f i2), (4) f express gaussian kernel: f i(yfj) = l j=1 aij exp( yfj xij2 /k), = 1, . . . , n. (5) solut minim problem (4) certain basi set lb element smaller train set size (lb l), prohibit time complex- ity. basi point select train datapoint increment approach base kernel match pursuit gradient descent, have necess appli addit simplif reduc complexity. train time report author dai desktop pc. krr stage, number candid final pixel combin follow certain weight learnt training, contrast usual overlap weight sr algorithms. output stage present ring artifacts, modifi natur imag prior base tappen et al. [16] appli belief propag (bp). recent work timoft et al. [13] especi re- markabl low-complex natur achiev order magnitud speed-up have competit qualiti result compar state-of-the-art. propos re- laxat l1-norm regular commonli neighbor embed (ne) spars code (sc) approaches, reformul problem squar (ls) l2-norm regular regression, known ridg regression. solv l1-norm constrain minim problem computation demanding, relax l2-norm, closed-form solut used. propos minim problem read min yf nl 2 2 + 2 , (6) nl lr neighborhood chosen solv problem yf featur extract lr patch. algebra solut = (ntl nl + i) 1ntl yf . (7) coeffici appli correspond hr neighborhood nh reconstruct hr patch, i.e. x = nh. written matrix multipl x = ryf , project matrix (i.e. regressor) calcul as: r = nh(n t l nl + i) 1ntl (8) comput offline, move minimiza- tion problem test train time. propos us spars dictionari ds atom size, train k-svd algorithm [17]. regressor rj anchor atom dj dl, neighborhood nl equat (8) select k-nn subset dl: nlj = knn(dj , dl). (9) sr problem address find nn atom dj input patch featur yfi appli associ rj it. specif case neighborhood size k = ds, gener regressor obtain neighborhood compris atom dictionari consequ requir nn search. case refer origin paper global regress (gr). advanc famili algorithm includ naiv bay sr forest [18], us ensembl bimod tree order benefit antipod invariance. highli competit speed thank naiv bay select procedur appli singl regressor patch, differ forest approaches, e.g. sr forest schulter et al. [19], appli regressor patch number tree ensemble. recent follow-up anr multipl linear map (mlm) sr zhang et al. [20], train regressor set orthonorm basi repres correspond subspac (i.e. subdictionary) later perform post- process stage base non-loc mean regularization. iii. propos algorithm section present algorithm fast effici regression-bas sr, divid subsect main pillar contributions. overview complet algorithm figur 2. a. linear regress framework seen section ii, select non-linear regress scheme (e.g. krr [15]) result prohibit train complexity, usual mitig reduc train set certain assumptions. test time, non- linear regress sr method reason fast, compar modestli fast state art method [13], [14], [21]. linear regress simpl regress scheme, i.e. output variabl perform linear weight sum 4 ieee transact imag processing,2016 ibp gradient filter patch extract + pca compress normal antipod invari transform spheric hash linear regress overlapping-patch reconstruct fig. 2: pipelin sr algorithm. firstly, extract featur vector gradient imag upscal ibp. secondly, best-fit regressor r spheric hash normal featur propos ait. appli respect regressor featur vector reconstruct final imag overlap patches. input variables. oversimplif upscal problem consid linear regressor. instead, linear regressor anchor differ point manifold, obtain finer piecewis linear regress model. strategies, data split train time test time proper regressor selected. sr problem, regress appli input featur aim recov certain compon patch, e.g. miss high frequency. model linear regress framework gener wai as: x = x+ri yf , s.t. ri = argmin ri{rk} (ri, yf ), (10) x coars approxim hr patch x, () metric evalu dissimilar input featur ith regressor cluster anchor point {rk} ensembl train regressors. choic obtain x affect content learnt, regress output aim recov x x. discuss import section iii-b featur space select transform yf . b. featur space coars approxim sr algorithm usual perform featur space raw lumin pixel values. literature, common rule featur transform enforc mid high frequenc lr patches, observ similar lr hr patch structur improv predict easier. hr featur space (i.e. output featur space), principl enforc high frequenc applies, case assumpt high frequenc band condition independ lower frequenc bands, suppress low-frequ band hr featur space collaps train data possibl low-frequ valu valu [5]. differ input lr featur space, hr featur space need abl revers featur pixel-bas valu final imag reconstruction. featur proposed: earli work freeman et al. [5] simpl high-pass filter consist subtract low-pass filter. direction, [12] [9] concaten first- second- order gradients, inexpens solut high- pass filter approximation. type featur refin zeyd et al. [10] introduc pca compress order reduc featur dimension memori usage. import remark featur transform comput coars approximation, i.e. upscal imag x . observ effect approxim unnot literature, bicub interpol patch-mean valu com- mon practice, lr hr featur space. paper propos new featur transform take advantag better coars approxim obtain lr input features, denot yf . main idea obtain imag approxim x better obtain bicub interpol certain low-complex boundaries. present fea- ture transform base simplifi iter project (ibp) algorithm [2], unidimension 1-st 2-nd order gradients. refer novel featur transform gradient ibp (gibp). start initi guess x(0) hr image, ibp simul imag process obtain lr imag y (0) correspond observ input imag y . differ imag e(0) = (y y (0)) comput improv initi guess back-project error valu correspond field x(0), x(1) = x(0)+e(0). process repeat iteratively: x(n) = x(0) + n j=1 e(j1) (11) origin work irani peleg [2], imag process model convolut given point spread function back-project kernel. low-complex approach, model function simpl effect bicub downscal upscal kernel. n = 2 iter coars approxim improv greatli compar bicubic. filter upscal imag x 1-st 2-nd order unidimension gradient filter (two vertic horizontal). point overlap patch extract gradient images, 4 gradient patch correspond patch posit concaten featur vector yf . note dimension featur perez-pellitero et al.: antipod invari sr 5 -1 0 1 -1 0 1 (a) -1 0 1 -1 0 1 (b) -1 0 1 0 -1 1 (c) -1 0 1 -1 0 1 (d) fig. 3: normal degre 3 polynomi manifold illustr propos approach compar [13]. (a) bidimension manifold samples. (b) manifold (blue) spars represent obtain k-svd algorithm (green) 8 atoms. (c) linear regressor (red) train neighborhood (k = 1) obtain spars dictionary, [13]. (d) linear regressor (red) obtain propos approach: neighborhood obtain sampl manifold (k = 10). time patch size. eventu memori problem, pca compress appli bare inform loss [10]. section iv-a assess perform propos featur transform. hr featur space, consist us ibp, non-revers gradient step pca compression. training, form hr featur simpli subtract coars approxim ground-truth patch x x(2), regress stage special correct error ibp introducing. testing, hr featur transform requir substitut x x(2) equat 10. c. cluster train previous seen, recent regression-bas sr us linear regressor easili comput close form appli matrix multiplication. however, map highli complex non-linear [22]. model non-linear natur mapping, ensembl regressor {rk} trained, repres local linear parametr , assumpt mani- fold m n similar local geometry. analyz effect distribut regressor man- ifold (i.e. anchor points) import properli choos nl equat (8), conclud new train approach. work timoft et al. [13], overcomplet spars represent obtain initi lr train patch k-svd [17]. atom new reduc dictionari dl anchor point manifold datapoint regress training. gr, uniqu regressor rg train element dictionary, accept higher regress error singl linear manifold. order achiev accur regress propos anchor neighbor- hood regress (anr). anr us regressor anchor spars dictionari atom. build atom neighborhood k-nn compos atom spars dictionari dl, final train ridg regressor correspond neighborhood. perform spars decomposit high number patch effici compress data smaller dictio- nary, yield atom repres train dataset, i.e. manifold. reason suitabl anchor points, sub-optim neighborhood embedding. sub-optim necessari local condit linear assumpt like violated. atom spars dictionari set centroid repres train set and, consequence, like similar them. import variat spars atom expected, unappropri local linear embedding. follow observation, [14] propos differ approach train linear regressor sr: spars represent anchor point manifold, form neighborhood raw manifold sampl (e.g. features, patches). so, closer nearest neighbor and, therefore, fulfil better local condition. additionally, higher number local independ measur avail (e.g. mean distanc 1000 neighbor raw-patch approach compar 40 atom neighborhood spars approach) control number k-nn selected, i.e. upper-bound dictionari size. low-dimension exampl propos train scheme figur 3. build dens compact cluster methodolog kei contribut boost perform complex cost. parallel time, timoft et al. present similar idea a+ algorithm [23]. present work optim train stage review nearest neighbor search strategy, follow section iii-d, propos usag differ metric allow better local conditions. d. search strategy: antipod invari spheric hash section studi appropri metric evalu similar featur anchor points, present insight behavior antipod point import properli deal them. present novel search strategi speed regres- sor select introduc modif spheric 6 ieee transact imag processing,2016 0 0.1 0.2 number queri t im e ( s ) parallel (gpu) parallel (cpu) singl thread 103 104 105 106102 fig. 4: run time measur comput 6-bit hash code (6 hyperspheres) increas number queri (in logarithm axi re-ranking) single-thread (cpu) parallel (cpu gpu) implementations. hash algorithm [24] order benefit antipod invariance. sr algorithm requir perform comparison vector order guid algorithm decisions, e.g. features, anchor points, dictionari atoms. comparison perform follow given metric function (a, b)a, b rm typic design quantifi similar dissimilar compar vectors. recurr literatur usag euclidean distanc purpose. characterist featur propos algorithm (which share sr algorithm [10], [13], [23], [14]), euclidean distanc suboptim fail manag antipod point (i.e. point surfac sphere antipod diametr opposite). insight ambigu variat defin metric ignor look scalar matrix multipl linear regression, i.e. given scalar linear regress read x = r(y). regressor r associ linear oper modifi scale operation, present side equality. normal featur vector compar some- time necessari dictionari optim algorithm (a case k-svd) enforc normal avoid 0 solution. necessary, normal good practic collaps train exampl simi- lar structur differ scalar norms. testing, normal featur best-suit regressor, non-norm featur necessari final matrix multiplication. however, vector normal handl case negative, norm strictli posit compens r. case antipod point (i.e. sign change). train assign differ regressor antipo- dal point increas perform better specialization, sign chang side equal regressor associ linear oper ident antipod point (i.e. x = r(y) x = r(y)). regressor associ anchor point, describ certain mode structur patches, regardless structur posit neg chang (e.g. posit neg chang gradient), describ sign normal vector. metric util select best regressor abl associ antipod point anchor point, have antipod invariance. way, build neighborhood training, observ applies. defin antipod invari metric as: (a, b) = (a, b) = (a, b) = (a, b). (12) propos usag absolut valu cosin similar (avcs) better altern compar featur vector anchor points. metric ||(a, b) = |a b| an- tipod invari (respect equat (12) ), furthermore, requir oper common euclidean distanc calculation. training, us time constrains. refer section iv assess improv antipod invari metric train testing. test time, best-fit regressor select ensembl train regressor. state- of-th art regression-bas sr push forward speed regard dictionary-bas sr [10], [9], find right regressor patch take consider quota process time. work [13], encod time (i.e. time left subtract share process time, includ bicub interpolations, patch extractions, etc.) spent task (i.e. 96% time). order reduc search complexity, binari split (e.g. tree hash functions) train form hierarch structur compos split nodes. split node divid space wai children leav improv inform gain. face challeng adapt- ing binari search structur algorithm: outcom train stage set anchor point associ respect regressors. dictionari obtain independ ahead search structure, hierarch structur neighborhoods, interpret clusters, share element them. furthermore, antipod invari import aspect search strategi discuss previously. search structur built, therefore, adapt scenario. choos hash techniqu tree-bas method main reasons: hash scheme provid low memori usag (the number split function hashing-bas structur o(log2(n)) tree-bas structur o(n), n repres number clusters) highli parallelizable. binari hash techniqu aim emb high-dimension point binari codes, provid compact represent high-dimension data. vast rang applications, effici similar search, includ approxim nearest neighbor retrieval, hash code preserv rel distances. recent activ research data-depend hash function oppos hash- ing method [25] data-independent. data- depend method intend better fit hash function data distribut [26], [27] off-lin train stage. data-depend state-of-the-art methods, select spheric hash algorithm heo et al. [24], perez-pellitero et al.: antipod invari sr 7 -1 0 1 -1 0 1 b c d (a) -1 0 1 -1 0 1 - d d c b- c (b) fig. 5: antipod invari transform. (a) c antipod point (c = a) b d close antipod (d b). similar features, euclidean distanc fail proper nearest neighbor, handl characterist antipod points. (b) propos antipod invari transform (ait), forbid given half-spac manifold (in case neg y axis, i.e. yf ey < 0) project featur lai correspond antipod point. transformation, euclidean distanc properli deal antipod points. abl defin close region rm split function. hash framework us model invers search scheme enabl benefit substanti speed-up reduc nn search appli precomput function, conveni scale parallel implementations, shown figur 4. spheric hash differ previou approach set- ting hyperspher defin hash function behalf previous hyperplanes. given hash function h(yf ) = (h1(yf ), . . . , hc(yf )) map point rm base 2 nc, i.e. {0, 1}c. hash function hk(yf ) indic point yf insid kth hypersphere, model purpos pivot pk rm distanc threshold (i.e. radiu hypersphere) tk r+ as: hk(yf ) = { 0 (pk, yf ) > tk 1 (pk, yf ) tk , (13) (pk, yf ) denot distanc metric point rm (euclidean distanc work heo et al.[24]). advantag hyperspher instead hyperplan abil defin close tighter sub-spac rm intersect hyperspheres. iter optim train process propos [24] obtain set {pk, tk}, aim balanc partit train data independ be- tween hash functions. perform mention iter hashing-funct optim set input patch featur train images, h(yf ) adapt natur imag distribut featur space. propos spheric hash search scheme symmetr figur 6, i.e. imag anchor point label binari codes. intuit understood creat nn subspac group (we refer bins), label regressor appli hash function anchor points. relat hash code regressor train time. search return k-nn anchor point, ensur input imag patch relat regressor (i.e. patch k-nn anchor points). solut proposed: (a) us gener regressor patch k-nn anchor point, propos [13] (b) us regressor closest label hash code calcul spheric ham distance, defin [24] dsh(a, b) = (ab) (ab) , xor bit oper bit operation. note guaranteed, rare happen patch k-nn regressor (e.g. select paramet 6 hyperspher occurs). observ signific differ performance, select (a) lowest complex solution. similar way, invers search assign regressor singl patch. common literatur re-rank strategi deal issu [28]. antipod invari sph. open section discuss good properli deal antipodes. however, binari splits, func- tion spheric hash unabl recogn antipod vector bin. happen caus threshold mechanism, base direction- euclidean distanc center hyperspher featur point. seen previously, antipod patch us regressor linear oper appli them, uniqu differ input featur sign. order benefit speed-up binari split design us euclidean distance, propos novel trans- format enabl benefit antipod invari euclidean space. describ borsuk-ulam theorem [29], con- tinuou function m dimension unit sphere sm euclidean m-space rm map antipod point singl point. goal transform similar: design function map antipod point singl point. however, situat address paper mismatch sphere space euclidean space, deal function map sm1 rm. idea build design continu hyperplan cross sm1 pass origin coordinates, map point singl one. order that, enforc forbidden space region, correspond neg half-spac qth dimension, i.e. featur yf eq r+, eq qth standard basi euclidean m space. propos antipod invari transform (ait), featur ly outsid space project antipodes: yfait = yf , yf eq < 0. (14) low-dimension illustr exampl shown fig- ur 5. dimens q balanc posit 8 ieee transact imag processing,2016 {rn}r1 r2 r3 r4 r5 r6 r7 0 1 0 1 1 1 0 0 1 0 1 0 0 1 ... 0 1 0 0 1 1 0 0 1 fig. 6: spheric hash adapt regressor search sr. certain hash function optim featur patch statist creat set hyperspher intersect directli label hash code. train time, regressor intersect (i.e. bins) test time hash function appli patch, directli map regressor. neg valu transform effective. train featur vector (around 500k examples), dimens highli similarli balanced, decid select dimens variat (pca analysis). transform point want us antipod invari metric oper euclidean space, equat (14) appli vector euclidean distanc calculation: ait (pk, yf ) = m (pkait , yfait ) 2, (15) distanc metric (pk, yf ) equat (13). thank obtain antipod invari spher- ical hash (aisph) optim sr regress problem problem share featur characteristics. figur 7 advantag propos ait. figur 7(a), confirm neighborhood creat ait featur euclidean distanc metric lower averag distanc transformation, obtain better local condit have higher number sampl avail given maximum distance. figur 7(b), ass resili antipod varianc ait: averag angular distanc obtain ait neighborhood (equat 15) approach creat pure antipod invari metric (i.e. avcs). valid result shown tabl ii, ait avc obtain similar psnr performance. iv. configur section assess prove optim perform contribut paper separately. us baselin configur propos algorithm, 1 hyperspher ait, train 91 train imag [8], singl scale (about 500k train samples) spars dictionari 1024 atom specifi otherwise. size neighborhood set 3000 sampl weight regular term = 0.12. select patch size 3 3 extract overlap patch set5 set14 psnr time psnr time bic. gradient 32.55 0.216 23.27 0.407 gibp 32.65 0.222 32.33 0.419 tabl i: averag perform term psnr (db) time (s) bicub gradient featur gibp features, run set14 set5 2 magnif factor. lr refer imag (i.e. number patch extract upper scale lr refer scale, patch size overlap ratio proportion adapted). a. featur space compar perform propos gibp featur featur propos zeyd et al. base gradient bicub interpolation. compress featur pca order reduc dimensionality, featur compact, special regressors. tabl propos featur consist improv qualiti (i.e. 0.06db 0.10db) respect previous features. ass that, expected, comput time sr algorithm increas gibp requir comput bicub interpol (three interpol singl one). increas run time low incid respect sr pipelin (i.e. 3% total sr time). b. search strategi order confirm superior antipod invari metrics, test euclidean distanc avc propos ait+euclidean distance. want evalu- at incid differ metrics, exclud experiment setup effect approxim search introduc aisph (a comparison explicitli shown result section). perform experi differ metric train test separately. result clearli advantag absolut invari metric, improv greatli psnr (+0.18 db). prove invari perez-pellitero et al.: antipod invari sr 9 neighborhood size 0 200 400 600 800 1000 1200 1400 1600 1800 2000 d ta n ce 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 euclidean distanc euclidean distanc ait (a) cluster size 0 200 400 600 800 1000 1200 1400 1600 1800 2000 n g u la r d ta n ce 0.25 0.3 0.35 0.4 0.45 0.5 euclidean neighborhood ait neighborhood avc neighborhood (b) fig. 7: averag distanc neighborhood anchor point increas neighborhood sizes. (a) show euclidean distanc neighborhood creat ait featur (b) show angular distanc (i.e. distanc deriv cosin similarity) neighborhood obtain differ metrics: euclidean distance, ait euclidean distanc avcs. (a) thank ait cluster tighter euclidean space. (b) assess improv invari antipod respect euclidean distance, close curv obtain pure antipod invari metric (i.e. avcs). test train cosin euclidean ait cosin 32.33 32.21 32.33 euclidean 32.27 32.15 32.26 tabl ii: perform differ metric train test run set14 2 magnif factor. ait qualiti obtain equival avcs. observ train test improv perform separ handl properli antipod (which +0.12db testing, +0.06db training). c. dictionari sphere ratio present algorithm size spars dictionari necessarili associ number hyperspher test time. hash scheme defin hash code buckets, regressor label train time. case have regressor bucket, rerank strategi follow best-suit regressor obtain bucket candidates. ratio number spars atom number bucket (2 s number hyperspheres) give averag number regressor hash code. section algorithm scale term qualiti increas size spars dictionary, therefore, worth increas size adapt number hyperspher obtain desir qualiti speed trade-off. figur 9 algorithm scale better increas dictionari size a+, improv smaller tend satur earlier. obtain maximum qualiti set dictionari size 8192 element and, afterwards, fix number hyperspher give desir speed. aim obtain speed origin work [14] 1024 atom 6 hyperspheres. obtain similar time figur (while obtain substanti improv psnr quality) 7 sphere 8192 atoms. note particular configur differ exhaust search approxim search non- existent, special upscal factor 4. like remark comparison point point figur 9 assess speed-up thank parallel implementation, algorithm exhaust search algorithm complex same. figur show method consist obtain better psnr valu (about 0.2db higher) differ dictionari sizes, 1024 perform better a+ 8192 atoms. v. result section experiment result propos method compar perform term qualiti execut time state-of-the-art recent methods. experi run intel xeon w3690 3.47ghz equip 12gb ram memory. method includ benchmark are: bicub baseline, k-svd method zeyd et al. [10] (denot sparse), sr method base deep learn dong et al. [21], open work srcnn [21] (refer dl 9-1-5) recent public [30] (refer dl 9-5-5), anr method timoft et al. [13] improv version a+ [23]. dataset experiment setup follow [8], [9], [21], [14], addit kodak dataset 2 magnif factor. code obtain respect author website. case dl, implement provid slower publication, make time comparison problematic. however, report time dl 9-5-5 slower a+ benchmark [30]. method train train dataset [9], contain imag crop high frequenc except dl 9-5-5 us imagenet (in order thousand images). neighborhood embedding, ai a+ us 10 ieee transact imag processing,2016 ground truth bicub anr [13] a+ [23] ais, s = 1 ais, s = 7 fig. 8: close-up result visual qualit assess 2 (three rows) 3 (last rows) magnif factor dataset benchmark. best-view zoom in. perez-pellitero et al.: antipod invari sr 11 time (s) 0 0.5 1 1.5 2 2.5 3 3.5 p s n r ( d b ) 36.5 36.55 36.6 36.65 36.7 36.75 36.8 36.85 36.9 propos algorithm a+ (a) set5 time (s) 0 1 2 3 4 5 p s n r ( d b ) 32.25 32.3 32.35 32.4 32.45 32.5 propos algorithm a+ (b) set14 fig. 9: psnr vs time valu dictionari size 1024, 4096 8192 atom (from left right lines) propos method exhaust search a+ . tabl iii: perform 2, 3 4 magnif term averag psnr (db) averag execut time (s) dataset set5, set14 kodak. best result bold. bicub spars [10] anr [13] dl 9-1-5 [21] dl 9-5-5 [30] a+ [23] ais, s = 1 ais, s = 7 mf psnr time psnr time psnr time psnr time psnr time psnr time psnr time psnr time set5 2 33.66 0.002 35.78 3.181 35.83 0.712 36.34 3.953 36.66 4.434 36.55 0.761 36.87 1.167 36.80 0.105 3 30.39 0.002 31.90 1.474 31.92 0.449 32.39 3.916 32.75 4.950 32.59 0.467 32.79 0.583 32.75 0.080 4 28.42 0.002 29.69 0.916 29.69 0.348 30.09 4.031 30.48 10.284 30.29 0.346 30.46 0.382 30.45 0.075 set14 2 30.23 0.002 31.81 6.506 31.80 1.717 32.18 7.695 32.45 8.204 32.28 1.739 32.48 2.223 32.44 0.205 3 27.54 0.002 28.67 3.003 28.65 0.933 29.00 7.646 29.29 8.098 29.13 0.963 29.26 1.075 29.23 0.153 4 26.00 0.002 26.88 1.862 26.85 0.696 27.20 7.944 27.50 8.305 27.32 0.714 27.45 0.716 27.42 0.155 kodak 2 30.85 0.003 32.19 11.286 32.24 2.938 32.63 13.121 32.81 14.367 32.71 3.161 32.89 3.943 32.84 0.339 3 28.43 0.003 29.22 5.250 29.21 1.615 29.43 12.805 29.65 15.026 29.57 1.678 29.68 1.771 29.65 0.246 4 27.23 0.003 27.83 3.228 27.80 1.199 27.94 13.315 28.17 14.069 28.10 1.226 28.17 1.186 28.15 0.245 tabl iv: perform 2, 3 4 magnif term averag ifc averag ssim dataset set5, set14 kodak. best result bold. bicub spars [10] anr [13] srcnn[21] srcnn [30] a+ [23] ais, s = 1 ais, s = 7 mf ifc ssim ifc ssim ifc ssim ifc ssim ifc ssim ifc ssim ifc ssim ifc ssim set5 2 6.083 0.9299 7.856 0.9493 8.090 0.9499 7.524 0.9521 8.036 0.9542 8.477 0.9544 8.683 0.9560 8.628 0.9557 3 3.579 0.8682 4.483 0.8968 4.606 0.8968 4.313 0.9033 4.658 0.9090 4.929 0.9088 5.060 0.9121 5.022 0.9111 4 2.329 0.8104 2.935 0.8428 3.005 0.8419 2.844 0.8530 2.991 0.8628 3.249 0.8603 3.348 0.8655 3.319 0.8643 set14 2 6.105 0.8687 7.663 0.8988 7.846 0.9004 7.237 0.9039 7.785 0.9067 8.140 0.9056 8.292 0.9081 8.261 0.9076 3 3.473 0.7736 4.218 0.8075 4.317 0.8093 4.026 0.8145 4.338 0.8215 4.535 0.8188 4.643 0.8227 4.609 0.8218 4 2.237 0.7019 2.725 0.7342 2.792 0.7353 2.614 0.7413 2.751 0.7513 2.961 0.7491 3.034 0.7537 3.009 0.7526 kodak 2 5.711 0.8694 7.025 0.8993 7.187 0.9013 6.746 0.9050 7.150 0.9073 7.381 0.9075 7.493 0.9102 7.471 0.9096 3 3.214 0.7781 3.827 0.8064 3.906 0.8083 3.656 0.8116 3.895 0.8177 4.053 0.8174 4.132 0.8208 4.109 0.8198 4 2.026 0.7186 2.431 0.7430 2.472 0.7438 2.330 0.7454 2.430 0.7540 2.593 0.7539 2.650 0.7572 2.632 0.7563 pyramid multipl scale obtain train sampl (12 scales). upscal imag magnif factor 2, 3 4 author recommend configur measur psnr, time, structur similar (ssim) [31] inform fidel criterion (ifc) [32], highest correl perceptu score sr evalu [33]. propos ai parallel implementation, run cpu platform methods. us lr patch size 3 3 lr overlapping. us k-svd spars dictionari 8192 element chosen neighborhood size 4250 k-nn, regular = 0.12. a+ us dictionari 1024 atom neighborhood size 2048 atoms, set 4250 degrad qualiti results. section iv-c discuss spars dictionari size number hyperspher select ai relat a+. present configur algorithm: 1 hyperspher (i.e. exhaust search) set upper qualiti limit 7 hyperspher optim configur term qualiti vs speed trade-off. show configur evalu effect approxim search qualiti drop time speed-up, show time potenti antipod search gibp features. object evalu method tabl iii (psnr) tabl iv (ifc ssim). all, psnr obtain ai consist 0.2db higher a+, relat compar method. 12 ieee transact imag processing,2016 tabl v: perform 2, 3 4 magnif term psnr (db) execut time (s) set5 dataset. best result bold. set5 bicub spars [10] anr [13] srcnn [21] srcnn [30] a+ [23] ais, s = 1 ais, = 7 imag mf psnr time psnr time psnr time psnr time psnr time psnr time psnr time psnr time babi 2 37.1 0.003 38.2 7.537 38.4 1.571 38.5 8.563 38.5 8.860 38.5 1.617 38.6 2.644 38.6 0.234 bird 2 36.8 0.002 39.9 2.251 40.0 0.457 40.6 2.943 40.9 3.462 41.1 0.549 41.7 0.887 41.6 0.079 butterfli 2 27.4 0.002 30.6 1.793 30.5 0.447 32.2 2.555 32.8 3.088 32.0 0.476 32.7 0.678 32.5 0.065 head 2 34.9 0.002 35.6 2.170 35.7 0.546 35.6 2.831 35.7 3.437 35.8 0.576 35.8 0.831 35.8 0.073 woman 2 32.1 0.002 34.5 2.151 34.5 0.540 34.9 2.870 35.4 3.321 35.3 0.588 35.6 0.795 35.5 0.072 averag 2 33.66 0.002 35.78 3.181 35.83 0.712 36.34 3.953 36.66 4.434 36.55 0.761 36.87 1.167 36.80 0.105 babi 3 33.9 0.003 35.1 3.471 35.1 1.061 35.0 8.226 35.2 9.077 35.2 1.101 35.3 1.240 35.3 0.174 bird 3 32.6 0.002 34.6 1.084 34.6 0.329 34.9 2.931 35.5 4.067 35.5 0.338 35.9 0.404 35.8 0.065 butterfli 3 24.0 0.002 25.9 0.854 25.9 0.246 27.6 2.626 28.0 3.622 27.2 0.256 27.6 0.383 27.5 0.050 head 3 32.9 0.002 33.6 0.973 33.6 0.305 33.5 2.896 33.7 4.060 33.8 0.321 33.9 0.444 33.8 0.057 woman 3 28.6 0.002 30.4 0.987 30.3 0.305 30.9 2.902 31.4 3.926 31.2 0.319 31.4 0.445 31.3 0.057 averag 3 30.39 0.002 31.90 1.474 31.92 0.449 32.39 3.916 32.75 4.950 32.59 0.467 32.79 0.583 32.75 0.080 babi 4 31.8 0.003 33.1 2.213 33.0 0.844 33.0 8.531 33.1 9.550 33.3 0.826 33.4 0.824 33.3 0.162 bird 4 30.2 0.002 31.7 0.650 31.8 0.251 32.0 3.065 32.5 10.200 32.5 0.245 32.8 0.282 32.8 0.054 butterfli 4 22.1 0.002 23.6 0.499 23.5 0.194 25.1 2.693 25.5 9.465 24.4 0.196 24.7 0.253 24.7 0.051 head 4 31.6 0.002 32.2 0.614 32.3 0.226 32.2 2.925 32.4 10.875 32.5 0.235 32.6 0.262 32.6 0.055 woman 4 26.5 0.002 27.9 0.604 27.8 0.227 28.2 2.939 28.9 11.133 28.6 0.229 28.8 0.290 28.8 0.053 averag 4 28.42 0.002 29.69 0.916 29.69 0.348 30.09 4.031 30.48 10.284 30.29 0.346 30.46 0.382 30.45 0.075 speed-up respect a+ rang 4.6 9.3, increas methods, e.g. dl 9-1-5 9-5-5. competit term psnr compar dl 9- 5-5, substanti faster. consist best-perform ssim ifc dataset magnif factors, confirm good perform method. secondly, algorithm speed aisph (i.e. comparison s = 1 s = 7) rang 4.8 11 depend upscal factors. drop qualiti reduc rang 0.01 0.07db. s = 7 clearli outperform state-of-the-art method run time (with except bicubic) highli com- petit qualiti (psnr, ifc, ssim). figur 8 close-up visual inspection. subject evalu conson object results, imag present ring artifact sharper edges. vi. conclus paper extend previou work sr base dens train regressor spheric hash search [14]. initi work, achiev high qualiti perform vast speed-up compar origin work [13] contemporari methods. paper, analyz featur metric involv regress process. contribut paper threefold: (1) detect studi import antipod invari search space, propos us absolut valu cosin similar exhaust search time constrain (i.e. training), (2) propos novel trans- form boost antipod invari euclidean space, emb spheric hash algorithm heo et al. [24], obtain antipod invari spheric hashing, (3) present featur transform perform better thank better coars approxim upscal gradient obtain ibp. regressor obtain antipod invari metric neat gain psnr obtain euclidean distanc and, furthermore, aish optim adapt search loss qualiti compar exhaust search minimal. finally, experiment result compar algorithm recent state-of-the-art methods, propos main configur algorithm result (a) exhaust search (b) approxim search. approxim search configur wide rank term execut time, show highli competit qualiti results, e.g. improv 0.25db term psnr compar a+ 9 faster. acknowledg work partial support project tec2013-43935-r (financ spanish ministerio economa y competitividad european region develop fund) ; erc- start grant (dynam minvip) cluster excel rebirth. author gratefulli acknowledg support. refer [1] r. tsai t. huang, multipl frame imag restor regis- tration, proc. advanc vision imag processing, vol. 1, 1984. [2] m. irani s. peleg, improv resolut imag registration, cvgip: graphic model imag processing, vol. 53, no. 3, 1991. [3] s. baker t. kanade, limit super-resolut break them, ieee trans. pattern analysi machin intelligence, vol. 24, no. 9, pp. 11671183, 2002. [4] z. lin h.-y. shum, fundament limit reconstruction-bas superresolut algorithm local translation, ieee trans. pattern analysi machin intelligence, vol. 26, no. 1, 2004. [5] w. freeman, t. jones, e. pasztor, example-bas super-resolution, ieee trans. graphic applications, vol. 22, no. 2, 2002. [6] d. glasner, s. bagon, m. irani, super-resolut singl image, proc. ieee intern conf. vision, 2009. [7] g. freedman r. fattal, imag video upscal local self- examples, acm trans. graphics, vol. 30, no. 2, pp. 12:112:11, 2011. [8] j. yang, z. lin, s. cohen, fast imag super-resolut base in-plac exampl regression, proc. ieee conf. vision pattern recognition, 2013. [9] j. yang, j. wright, h. t.s., y. ma, imag super-resolut spars representation, ieee trans. imag processing, vol. 19, no. 11, pp. 28612873, 2010. [10] r. zeyde, m. elad, m. protter, singl imag scale-up sparse-representations, proc. intern conf. curv surfaces, 2012. perez-pellitero et al.: antipod invari sr 13 [11] x. gao, k. zhang, d. tao, x. li, joint learn single- imag super-resolut coupl constraint, ieee trans. imag processing, vol. 21, no. 2, pp. 469480, 2012. [12] h. chang, d.-y. yeung, y. xiong, super-resolut neigh- bor embedding, proc. ieee conf. vision pattern recognition, 2004. [13] r. timofte, v. d. smet, l. v. gool, anchor neighborhood regress fast example-bas super-resolution, proc. ieee intern conf. vision, 2013. [14] e. perez-pellitero, j. salvador, i. torres, j. ruiz-hidalgo, b. rosen- hahn, fast super-resolut dens local train invers regres- sor search, proc. asian conf. vision, lectur note science, 2014. [15] k. i. kim y. kwon, single-imag super-resolut spars regress natur imag prior, ieee trans. pattern analysi machin intelligence, vol. 32, no. 6, 2010. [16] m. tappen, b. russel, w. freeman, exploit spars deriv prior super-resolut imag demosaicing, 2003. [17] m. aharon, m. elad, a. bruckstein, k-svd: algorithm design overcomplet dictionari spars representation, ieee trans. signal processing, vol. 54, no. 11, 2006. [18] j. salvador e. perez-pellitero, naiv bay super-resolut forest, proc. ieee int. conf. vision, 2015, pp. 325 333. [19] s. schulter, c. leistner, h. bischof, fast accur imag upscal super-resolut forests, ieee conf, vision pattern recognition, june 2015. [20] k. zhang, d. tao, x. gao, x. li, z. xiong, learn multipl linear map effici singl imag super-resolution, ieee trans. imag processing, vol. 24, no. 3, pp. 846861, 2015. [21] c. dong, c. loy, k. he, x. tang, learn deep convolut network imag super-resolution, proc. european conf. com- puter vision, 2014. [22] g. peyre, manifold model signal images, vision imag understanding, vol. 113, no. 2, 2009. [23] r. timofte, v. d. smet, l. v. gool, a+: adjust anchor neighborhood regress fast super-resolution, proc. asian conf. vision, lectur note science, 2014. [24] j.-p. heo, y. lee, j. he, s.-f. chang, s.-e. yoon, spheric hash- ing, proc. ieee conf. vision pattern recognition, 2012. [25] p. indyk r. motwani, approxim nearest neighbors: remov curs dimensionality, proc. thirtieth annual acm symposium theori computing, ser. stoc 98, 1998. [26] y. weiss, a. torralba, r. fergus, spectral hashing, 2008. [27] j. wang, s. kumar, s.-f. chang, semi-supervis hash scalabl imag retrieval, 2010. [28] k. j. sun, comput nearest-neighbor field propagation- assist kd-trees, proc. ieee conf. vision pattern recognition, 2012. [29] j. matousek, borsuk-ulam theorem: lectur topolog- ical method combinator geometry. springer publish company, incorporated, 2007. [30] c. dong, c. loy, k. he, x. tang, imag super-resolut deep convolut networks, ieee trans. pattern analysi machin intelligence, vol. pp, no. 99, 2015. [31] z. wang, a. bovik, h. sheikh, e. simoncelli, imag qualiti assess- ment: error visibl structur similarity, imag processing, ieee trans. on, vol. 13, no. 4, pp. 600612, april 2004. [32] h. sheikh, a. bovik, g. veciana, inform fidel criterion imag qualiti assess natur scene statistics, ieee trans. imag processing, vol. 14, no. 12, 2005. [33] c.-y. yang, c. ma, m.-h. yang, single-imag super-resolution: benchmark, proc. european conf. vision, 2014. eduardo perez-pellitero receiv b.sc. imag sound engin m.sc. telecomun engin universitat politecnica catalunya (upc) 2010 2012, respectively. 2012, work to- ward ph.d. agreement leib- niz universitat hannov technicolor, hanover, germany. research concern super-resolut upscaling, machin learning, fast regress structures. jordi salvador project leader technicolor r&i hannov member technicolor fellowship network 2014. main research focu machin learn imag super resolu- tion restoration. formerly, obtain ph.d. degre 2011 universitat politecnica catalunya (upc), contribut project spanish scienc technolog (vision, provec) european fp6 project (chil) research assist multiview 3d reconstruction. serv review confer journals. research interest includ 3d re- construction, real-tim parallel algorithms, imag video restoration, invers problem machin learning. javier ruiz-hidalgo receiv degre telecommun engin universitat politecnica catalunya (upc), barcelona, spain 1997. 1998 1999, develop msc research field vision univers east anglia (uea) norwich, uk. 1999 join imag process group upc work imag video index context mpeg-7 standard. 2006, receiv phd. field imag processing. 1999 involv european project research imag process group upc. 2001 associ professor universitat politecnica catalunya. current lectur area digit signal system imag processing. current research interest includ 3d video coding, 3d analysi super-resolution. bodo rosenhahn studi scienc (minor subject medicine) univers kiel. receiv dipl.-inf. dr.-ing. uni- versiti kiel 1999 2003, respectively. 10/2003 till 10/2005, work postdoc univers auckland (new zealand), fund scholarship german research foundat (dfg). 11/2005-08/2008 work senior research max-planck institut scienc saarbruecken. 09/2008 professor leibniz-univers hannover, head group autom imag interpretation.