distribut train strategi vision deep learn algorithm distribut gpu cluster sciencedirect avail onlin www.sciencedirect.com procedia scienc 108c (2017) 315324 1877-0509 2017 authors. publish elsevi b.v. peer-review respons scientif committe intern confer comput scienc 10.1016/j.procs.2017.05.074 intern confer comput science, icc 2017, 12-14 june 2017, zurich, switzerland 10.1016/j.procs.2017.05.074 1877-0509 2017 authors. publish elsevi b.v. peer-review respons scientif committe intern confer comput scienc space reserv procedia header, us distribut train strategi vision deep learn algorithm distribut gpu cluster vctor campo , francesc sastr , maurici yagu , mriam bellver , xavier giro-i-nieto , jordi torr barcelona supercomput center (bsc) universitat politecnica catalunya abstract deep learn algorithm base success build high learn capac model million paramet tune data-driven fashion. model train process million examples, develop accur algorithm usual limit throughput comput devic trained. work, explor train state-of-the-art neural network vision parallel distribut gpu cluster. effect distribut train process address differ point view. first, scalabl task perform distribut set analyzed. second, impact distribut train method final accuraci model studied. keywords: distribut computing; parallel systems; deep learning; convolut neural network 1 introduct method base deep neural network establish state-of-the-art vision task [15, 27, 9], machin translat [29], speech gener [21] defeat world champion game [26]. develop algorithm span decad [16], potenti unlock increas comput power specif accelerators, e.g. graphic process unit (gpus), creation large-scal dataset [8, 13]. us specif hardwar devices, train algorithm computation intens days, weeks, converg singl machine. scale problem distribut set shorten train time crucial challeng research industri applications. 1 space reserv procedia header, us distribut train strategi vision deep learn algorithm distribut gpu cluster vctor campo , francesc sastr , maurici yagu , mriam bellver , xavier giro-i-nieto , jordi torr barcelona supercomput center (bsc) universitat politecnica catalunya abstract deep learn algorithm base success build high learn capac model million paramet tune data-driven fashion. model train process million examples, develop accur algorithm usual limit throughput comput devic trained. work, explor train state-of-the-art neural network vision parallel distribut gpu cluster. effect distribut train process address differ point view. first, scalabl task perform distribut set analyzed. second, impact distribut train method final accuraci model studied. keywords: distribut computing; parallel systems; deep learning; convolut neural network 1 introduct method base deep neural network establish state-of-the-art vision task [15, 27, 9], machin translat [29], speech gener [21] defeat world champion game [26]. develop algorithm span decad [16], potenti unlock increas comput power specif accelerators, e.g. graphic process unit (gpus), creation large-scal dataset [8, 13]. us specif hardwar devices, train algorithm computation intens days, weeks, converg singl machine. scale problem distribut set shorten train time crucial challeng research industri applications. 1 space reserv procedia header, us distribut train strategi vision deep learn algorithm distribut gpu cluster vctor campo , francesc sastr , maurici yagu , mriam bellver , xavier giro-i-nieto , jordi torr barcelona supercomput center (bsc) universitat politecnica catalunya abstract deep learn algorithm base success build high learn capac model million paramet tune data-driven fashion. model train process million examples, develop accur algorithm usual limit throughput comput devic trained. work, explor train state-of-the-art neural network vision parallel distribut gpu cluster. effect distribut train process address differ point view. first, scalabl task perform distribut set analyzed. second, impact distribut train method final accuraci model studied. keywords: distribut computing; parallel systems; deep learning; convolut neural network 1 introduct method base deep neural network establish state-of-the-art vision task [15, 27, 9], machin translat [29], speech gener [21] defeat world champion game [26]. develop algorithm span decad [16], potenti unlock increas comput power specif accelerators, e.g. graphic process unit (gpus), creation large-scal dataset [8, 13]. us specif hardwar devices, train algorithm computation intens days, weeks, converg singl machine. scale problem distribut set shorten train time crucial challeng research industri applications. 1 316 vctor campo et al. / procedia scienc 108c (2017) 315324distribut train strategi deep learn gpu cluster. vctor campo et al. affect comput [22] recent garner research attention. machin abl understand convei subject affect lead better human-comput interact kei field robot medicine. despit success constrain environ emot understand facial express [18], autom affect understand unconstrain domain remain open challeng far task machin approach surpass human performance. work, focu detect adject noun pair (anps) large-scal imag dataset collect flickr [12]. mid-level representations, rise approach overcom affect gap low level imag featur high level affect semantics, train accur model visual sentiment analysi visual emot prediction. work, explor train convolut neural network (cnns) anp classif acceler distribut gpu cluster. contribut three-fold: (1) studi trade-off final classif accuraci speedup analyz result learn hpc standpoints, (2) distribut train wai make cluster resources, leverag intra-nod inter-nod parallelism, (3) propos modif distribut configuration, order reduc resourc training. 2 relat work massiv number convolut matrix multipl neural network led gpu implement cuda [20] efficient, task specif primit cudnn [7]. earli deep learn framework caff [10] provid fast easi access primi- tives, initi design singl machin operation, support distribut environments. effort distribut framework tradit hpc tool spark mpi result project sparknet [19] theano-mpi [17]. nativ support distribut set includ recent framework tensorflow [2] mxnet [6]. however, scale train algorithm singl machin environ distribut set pose main challenges. comput perform stand- point, optim us resourc main goal, learn side, final accuraci suffer drop compar singl machin counterpart. consid task distribut convolut neural network (cnns), specif type feed-forward neural networks. cnn compos seri layer appli specif oper input, e.g. convolution, dot product pooling, train minim cost object mean gradient descent backpropag batch data. main approach propos literatur [14] train cnn multi-gpu environment, singl machin distribut setting: model parallel data parallelism. model parallel split layer cnn differ gpus, i.e. gpu oper batch input data, appli differ oper them, oper larg number paramet fit gpu memory. hand, data parallel consist place replica model gpu, oper differ batch data. model replica share parameters, method equival have larger batch size. modern cnn architectur aim reduc number paramet increas number layers, find bottleneck store intermedi activ memory. unlik model parallelism, data parallel introduc synchron point regardless number gpus, reduc commun overhead make suitabl current cnn architectures. besides, 2 distribut train strategi deep learn gpu cluster. vctor campo et al. balanc load gpu straightforward paradigm, requir care tune specif cnn architectur number gpu model paral- lelism approach. reasons, consid multi-gpu data parallel singl machin distribut settings. 3 dataset adject noun pair (anps) power mid-level represent [3] affect relat task visual sentiment analysi emot recognition. large-scal anp ontolog 12 differ languages, multilingu visual sentiment ontolog (mvso), collect jou et al. [12] follow model deriv psycholog studies, plutchick wheel emot [23]. noun compon anp understood ground visual appear entity, adject polar content posit neg sentiment, emot [12]. properti try bridg affect gap low level imag featur high level affect semantics, goe far recogn main object image. tradit object classif algorithm rec- ogniz babi image, finer-grain classif happi babi cry babi usual need fulli understand affect content convei image. captur sophist differ anp pose challeng task benefit leverag large-scal annot dataset mean high learn capac model [5]. experi consid subset english partit mvso, tag-restrict subset, contain 1.2m sampl cover 1,200 differ anps. imag mvso download flickr automat annot metadata, annot consid weak labels, i.e. label match real content images. tag-pool subset contain sampl annot obtain tag flickr instead metadata, annot like match real ground truth. 4 cnn architectur success applic cnn large-scal visual recognition, design improv architectur improv classif perform focus increas depth, i.e. number layers, keep reduc number trainabl param- eters. trend seen compar 8 layer alexnet [15], cnn-base method win imag larg scale visual recognit challeng (ilsvrc), dozens, hundreds, layer residu net (resnets) [9]. despit huge increas overal depth, resnet 50 layer roughli half paramet alexnet. however, impact increas depth notori memori footprint deeper architec- tures, store intermedi result come output singl layer, benefit multi-gpu setup allow us larger batch sizes. adopt resnet50 cnn [9] experiments, architectur 50 layer map 224 224 3 input imag 1,200-dimension vector repres probabl distribut anp class dataset. overall, model contain 25 106 single-precis floating-point paramet involv 4109 floating-point oper tune training. import notic computation demand cnn is, larger gain distribut train time spent parallel comput respect ad commun overhead. 3 vctor campo et al. / procedia scienc 108c (2017) 315324 317distribut train strategi deep learn gpu cluster. vctor campo et al. affect comput [22] recent garner research attention. machin abl understand convei subject affect lead better human-comput interact kei field robot medicine. despit success constrain environ emot understand facial express [18], autom affect understand unconstrain domain remain open challeng far task machin approach surpass human performance. work, focu detect adject noun pair (anps) large-scal imag dataset collect flickr [12]. mid-level representations, rise approach overcom affect gap low level imag featur high level affect semantics, train accur model visual sentiment analysi visual emot prediction. work, explor train convolut neural network (cnns) anp classif acceler distribut gpu cluster. contribut three-fold: (1) studi trade-off final classif accuraci speedup analyz result learn hpc standpoints, (2) distribut train wai make cluster resources, leverag intra-nod inter-nod parallelism, (3) propos modif distribut configuration, order reduc resourc training. 2 relat work massiv number convolut matrix multipl neural network led gpu implement cuda [20] efficient, task specif primit cudnn [7]. earli deep learn framework caff [10] provid fast easi access primi- tives, initi design singl machin operation, support distribut environments. effort distribut framework tradit hpc tool spark mpi result project sparknet [19] theano-mpi [17]. nativ support distribut set includ recent framework tensorflow [2] mxnet [6]. however, scale train algorithm singl machin environ distribut set pose main challenges. comput perform stand- point, optim us resourc main goal, learn side, final accuraci suffer drop compar singl machin counterpart. consid task distribut convolut neural network (cnns), specif type feed-forward neural networks. cnn compos seri layer appli specif oper input, e.g. convolution, dot product pooling, train minim cost object mean gradient descent backpropag batch data. main approach propos literatur [14] train cnn multi-gpu environment, singl machin distribut setting: model parallel data parallelism. model parallel split layer cnn differ gpus, i.e. gpu oper batch input data, appli differ oper them, oper larg number paramet fit gpu memory. hand, data parallel consist place replica model gpu, oper differ batch data. model replica share parameters, method equival have larger batch size. modern cnn architectur aim reduc number paramet increas number layers, find bottleneck store intermedi activ memory. unlik model parallelism, data parallel introduc synchron point regardless number gpus, reduc commun overhead make suitabl current cnn architectures. besides, 2 distribut train strategi deep learn gpu cluster. vctor campo et al. balanc load gpu straightforward paradigm, requir care tune specif cnn architectur number gpu model paral- lelism approach. reasons, consid multi-gpu data parallel singl machin distribut settings. 3 dataset adject noun pair (anps) power mid-level represent [3] affect relat task visual sentiment analysi emot recognition. large-scal anp ontolog 12 differ languages, multilingu visual sentiment ontolog (mvso), collect jou et al. [12] follow model deriv psycholog studies, plutchick wheel emot [23]. noun compon anp understood ground visual appear entity, adject polar content posit neg sentiment, emot [12]. properti try bridg affect gap low level imag featur high level affect semantics, goe far recogn main object image. tradit object classif algorithm rec- ogniz babi image, finer-grain classif happi babi cry babi usual need fulli understand affect content convei image. captur sophist differ anp pose challeng task benefit leverag large-scal annot dataset mean high learn capac model [5]. experi consid subset english partit mvso, tag-restrict subset, contain 1.2m sampl cover 1,200 differ anps. imag mvso download flickr automat annot metadata, annot consid weak labels, i.e. label match real content images. tag-pool subset contain sampl annot obtain tag flickr instead metadata, annot like match real ground truth. 4 cnn architectur success applic cnn large-scal visual recognition, design improv architectur improv classif perform focus increas depth, i.e. number layers, keep reduc number trainabl param- eters. trend seen compar 8 layer alexnet [15], cnn-base method win imag larg scale visual recognit challeng (ilsvrc), dozens, hundreds, layer residu net (resnets) [9]. despit huge increas overal depth, resnet 50 layer roughli half paramet alexnet. however, impact increas depth notori memori footprint deeper architec- tures, store intermedi result come output singl layer, benefit multi-gpu setup allow us larger batch sizes. adopt resnet50 cnn [9] experiments, architectur 50 layer map 224 224 3 input imag 1,200-dimension vector repres probabl distribut anp class dataset. overall, model contain 25 106 single-precis floating-point paramet involv 4109 floating-point oper tune training. import notic computation demand cnn is, larger gain distribut train time spent parallel comput respect ad commun overhead. 3 318 vctor campo et al. / procedia scienc 108c (2017) 315324distribut train strategi deep learn gpu cluster. vctor campo et al. cross-entropi output cnn ground truth, i.e. real class distribution, loss function l2 regular term weight decai rate 104. paramet model tune minim cost object batch gradient descent. 5 distribut cnn train process train cnn batch gradient descent decompos main steps: forward backward pass net. forward pass comput output batch data, error respect desir result calculated. error, cost, differenti respect paramet cnn call backward pass. finally, result gradient updat weight net. step iter repeat convergence, i.e. local minima error function reached. implement us data parallel paradigm, defin kind nodes. first, worker node replica model, oper separ batch data. second, paramet server (ps) node store updat model paramet [1]. essence, worker receiv model parameters, comput batch data, send gradient ps model updat order improv [4]. however, differ model updat polici chosen ps perform train synchron mode: case, ps wait worker node comput gradient respect data batches. gradient receiv ps, appli current weight updat model sent worker nodes. method fast slowest node, updat perform worker node finish computation, suffer unbalanc network speed cluster share users. however, faster converg achiev accur gradient estim obtained. author [4] present altern strategi allevi slowest worker updat problem backup workers. asynchron mode: time ps receiv gradient worker, model paramet updated. despit deliv enhanc throughput compar synchron counterpart, worker oper slightli differ version model, provid poorer gradient estimations. result, iter requir converg stale gradient updates. increas number worker result throughput bottleneck commun ps, case ps need added. mix mode: mix mode appear trade-off adequ batch size through- perform asynchron updat model parameters, synchron averag gradient come subgroup workers. larger learn rate thank increas batch size, lead faster convergence, reach throughput rate close asynchron mode. strategi reduc commun compar pure asynchron mode. others: improv tradit gradient descent algorithm appli distribut set propos literatur [31, 24, 25]. work, however, focu scale problem singl node distribut set minim modif train algorithm. 4 distribut train strategi deep learn gpu cluster. vctor campo et al. 6 experiment setup evalu experi gpu cluster, node equip 2 nvidia kepler k80 dual gpu cards, 2 intel xeon e5-2630 8-core processor 128gb ram. inter-nod commun perform trough 56gb/ infiniband network. cnn ar- chitectur train implement tensorflow1, run cuda 7.5 cudnn 5.1.3 primit improv performance. train process need submit slurm workload manager, task distribut commun node achiev greasy2. unlik work worker defin singl gpu [4, 11], us avail gpu node defin singl worker. given dual natur nvidia k80 cards, model replica place node. follow mix approach synchron averag gradient model replica node commun ps, perform asynchron model updates. setup offer main advantages: (1) commun overhead reduced, singl collect gradient need exchang network set model replicas, (2) worker larger effect batch size, provid better gradient estim allow us larger learn rate faster convergence. loss function minim rmsprop [28] per-paramet adapt learn rate optim method learn rate 0.1, decai 0.9 = 1.0. worker effect batch size 128 samples, i.e. 32 imag process time gpu. prevent overfitting, data augment consist random crop and/or horizont flip asynchron perform cpu previou batch process gpus. cnn weight initi model pre-train ilsvrc [8], practic proven benefici train large-scal dataset [30]. previou public distribut train tensorflow [1, 4] tend us differ server configur worker ps tasks. given ps store updat model need gpu computations, cpu-onli server task. hand, worker job involv matrix computations, server equip gpu used. node cluster experi gpu equip nodes, mean place ps worker differ node result under-util gpu resources. studi impact share resourc ps worker compar configur setup suitabl homogen cluster node equip gpu cards. 7 result discuss 7.1 intra-nod gpu parallel studi scalabl deploi multipl replica synchron model updat singl node gpu cluster. dual natur nvidia k80 cards, model replica deploi node. ensur proper weight share model replicas, variabl comput graph store ram, gpu perform oper cnn differ batch data. gradient comput replica averag perform weight update, step synchron point graph. 1 2 5 vctor campo et al. / procedia scienc 108c (2017) 315324 319distribut train strategi deep learn gpu cluster. vctor campo et al. cross-entropi output cnn ground truth, i.e. real class distribution, loss function l2 regular term weight decai rate 104. paramet model tune minim cost object batch gradient descent. 5 distribut cnn train process train cnn batch gradient descent decompos main steps: forward backward pass net. forward pass comput output batch data, error respect desir result calculated. error, cost, differenti respect paramet cnn call backward pass. finally, result gradient updat weight net. step iter repeat convergence, i.e. local minima error function reached. implement us data parallel paradigm, defin kind nodes. first, worker node replica model, oper separ batch data. second, paramet server (ps) node store updat model paramet [1]. essence, worker receiv model parameters, comput batch data, send gradient ps model updat order improv [4]. however, differ model updat polici chosen ps perform train synchron mode: case, ps wait worker node comput gradient respect data batches. gradient receiv ps, appli current weight updat model sent worker nodes. method fast slowest node, updat perform worker node finish computation, suffer unbalanc network speed cluster share users. however, faster converg achiev accur gradient estim obtained. author [4] present altern strategi allevi slowest worker updat problem backup workers. asynchron mode: time ps receiv gradient worker, model paramet updated. despit deliv enhanc throughput compar synchron counterpart, worker oper slightli differ version model, provid poorer gradient estimations. result, iter requir converg stale gradient updates. increas number worker result throughput bottleneck commun ps, case ps need added. mix mode: mix mode appear trade-off adequ batch size through- perform asynchron updat model parameters, synchron averag gradient come subgroup workers. larger learn rate thank increas batch size, lead faster convergence, reach throughput rate close asynchron mode. strategi reduc commun compar pure asynchron mode. others: improv tradit gradient descent algorithm appli distribut set propos literatur [31, 24, 25]. work, however, focu scale problem singl node distribut set minim modif train algorithm. 4 distribut train strategi deep learn gpu cluster. vctor campo et al. 6 experiment setup evalu experi gpu cluster, node equip 2 nvidia kepler k80 dual gpu cards, 2 intel xeon e5-2630 8-core processor 128gb ram. inter-nod commun perform trough 56gb/ infiniband network. cnn ar- chitectur train implement tensorflow1, run cuda 7.5 cudnn 5.1.3 primit improv performance. train process need submit slurm workload manager, task distribut commun node achiev greasy2. unlik work worker defin singl gpu [4, 11], us avail gpu node defin singl worker. given dual natur nvidia k80 cards, model replica place node. follow mix approach synchron averag gradient model replica node commun ps, perform asynchron model updates. setup offer main advantages: (1) commun overhead reduced, singl collect gradient need exchang network set model replicas, (2) worker larger effect batch size, provid better gradient estim allow us larger learn rate faster convergence. loss function minim rmsprop [28] per-paramet adapt learn rate optim method learn rate 0.1, decai 0.9 = 1.0. worker effect batch size 128 samples, i.e. 32 imag process time gpu. prevent overfitting, data augment consist random crop and/or horizont flip asynchron perform cpu previou batch process gpus. cnn weight initi model pre-train ilsvrc [8], practic proven benefici train large-scal dataset [30]. previou public distribut train tensorflow [1, 4] tend us differ server configur worker ps tasks. given ps store updat model need gpu computations, cpu-onli server task. hand, worker job involv matrix computations, server equip gpu used. node cluster experi gpu equip nodes, mean place ps worker differ node result under-util gpu resources. studi impact share resourc ps worker compar configur setup suitabl homogen cluster node equip gpu cards. 7 result discuss 7.1 intra-nod gpu parallel studi scalabl deploi multipl replica synchron model updat singl node gpu cluster. dual natur nvidia k80 cards, model replica deploi node. ensur proper weight share model replicas, variabl comput graph store ram, gpu perform oper cnn differ batch data. gradient comput replica averag perform weight update, step synchron point graph. 1 2 5 320 vctor campo et al. / procedia scienc 108c (2017) 315324distribut train strategi deep learn gpu cluster. vctor campo et al. figur 2a show throughput function number gpus. observ speed train linearli respect number gpu perform synchron updates, confirm optim polici intra-nod paralellism. 7.2 throughput increas distribut train paramet server perform task requir gpus. however, given ho- mogeneu configur cluster experi performed, node exclus ps impli under-util resources. section, studi impact share resourc ps worker tasks. tabl 1 show trade-off speedup resourc requir 4 worker node 4 gpu each, follow mix approach describ section 5. despit provid largest speedup, configur 3 dedic ps requir 7 node instead 4. addit gpu-equip node available, result figur 1 fix number nodes, solut worker provid largest speedups. given figures, share resourc worker ps task emerg effici solut homogeneu gpu cluster consid experiments. figur 1: throughput comparison differ distribut setups. set proper number paramet server (ps) kei maxim throughput. result tabl 2 figur 2b throughput speedup close linear configur resourc share ps worker tasks. figur demonstr scalabl propos approach constrain number node requir achiev them. 7.3 converg speedup distribut train studi impact distribut train process, main factor account. first, time requir model reach target loss value, target function optimized, determin speedup train process. 6 distribut train strategi deep learn gpu cluster. vctor campo et al. configur throughput speedup effici 1 node 124.18 img/sec - - 7 node (4 worker + 3 ps) 396.62 img/sec 3.19 0.46 4 node (4 worker + 3 ps) 374.73 img/sec 3.02 0.76 4 node (4 worker + 1 ps) 292.22 img/sec 2.35 0.58 tabl 1: comparison differ configur 4 workers. dedic node paramet server slightli improv throughput, involv larger resourc utilization. effici relat speedup number nodes, show clearli grade resourc exploitation. (a) (b) figur 2: throughput speedup differ number resources. (a) parallel speedup insid node differ number gpus. (b) distribut speedup node 4 gpu each, best configur figur 1. second, final accuraci determin asynchron updat affect optim cost function. despit throughput increas close linear respect number nodes, figur 3 show time requir setup reach target loss valu benefit linearli addit nodes. result expected, asynchron gradient descent method n worker model updat perform respect weight wich n1 configur throughput speedup effici 1 node 124.18 img/sec - - 2 node (2 worker + 1 ps) 195.60 img/sec 1.58 0.79 4 node (4 worker + 3 ps) 383.09 img/sec 3.09 0.77 8 node (8 worker + 7 ps) 809.10 img/sec 6.52 0.82 tabl 2: throughput achiev distribut configuration. speedup effici figur comput respect 1 node scenario. 7 vctor campo et al. / procedia scienc 108c (2017) 315324 321distribut train strategi deep learn gpu cluster. vctor campo et al. figur 2a show throughput function number gpus. observ speed train linearli respect number gpu perform synchron updates, confirm optim polici intra-nod paralellism. 7.2 throughput increas distribut train paramet server perform task requir gpus. however, given ho- mogeneu configur cluster experi performed, node exclus ps impli under-util resources. section, studi impact share resourc ps worker tasks. tabl 1 show trade-off speedup resourc requir 4 worker node 4 gpu each, follow mix approach describ section 5. despit provid largest speedup, configur 3 dedic ps requir 7 node instead 4. addit gpu-equip node available, result figur 1 fix number nodes, solut worker provid largest speedups. given figures, share resourc worker ps task emerg effici solut homogeneu gpu cluster consid experiments. figur 1: throughput comparison differ distribut setups. set proper number paramet server (ps) kei maxim throughput. result tabl 2 figur 2b throughput speedup close linear configur resourc share ps worker tasks. figur demonstr scalabl propos approach constrain number node requir achiev them. 7.3 converg speedup distribut train studi impact distribut train process, main factor account. first, time requir model reach target loss value, target function optimized, determin speedup train process. 6 distribut train strategi deep learn gpu cluster. vctor campo et al. configur throughput speedup effici 1 node 124.18 img/sec - - 7 node (4 worker + 3 ps) 396.62 img/sec 3.19 0.46 4 node (4 worker + 3 ps) 374.73 img/sec 3.02 0.76 4 node (4 worker + 1 ps) 292.22 img/sec 2.35 0.58 tabl 1: comparison differ configur 4 workers. dedic node paramet server slightli improv throughput, involv larger resourc utilization. effici relat speedup number nodes, show clearli grade resourc exploitation. (a) (b) figur 2: throughput speedup differ number resources. (a) parallel speedup insid node differ number gpus. (b) distribut speedup node 4 gpu each, best configur figur 1. second, final accuraci determin asynchron updat affect optim cost function. despit throughput increas close linear respect number nodes, figur 3 show time requir setup reach target loss valu benefit linearli addit nodes. result expected, asynchron gradient descent method n worker model updat perform respect weight wich n1 configur throughput speedup effici 1 node 124.18 img/sec - - 2 node (2 worker + 1 ps) 195.60 img/sec 1.58 0.79 4 node (4 worker + 3 ps) 383.09 img/sec 3.09 0.77 8 node (8 worker + 7 ps) 809.10 img/sec 6.52 0.82 tabl 2: throughput achiev distribut configuration. speedup effici figur comput respect 1 node scenario. 7 322 vctor campo et al. / procedia scienc 108c (2017) 315324distribut train strategi deep learn gpu cluster. vctor campo et al. step old average. final accuraci test set reach configur detail tabl 3. distribut setup abl reach final accuraci singl node model, confirm stale gradient neg impact final minima reach model converges. moreover, keep similar throughput worker node critic factor success learn process, worker constantli rest node aggrav stale gradient problem. worker (gpus) test accuraci time (h) speedup 1 node (4) 0.228 106.43 1.00 2 node (8) 0.217 62.78 1.69 4 node (16) 0.202 37.99 2.80 8 node (32) 0.217 22.50 4.73 tabl 3: result test set differ distribut configurations. despit benefit larger throughputs, setup node requir iter converge. figur 3: train loss evolut differ distribut configurations. nodes, faster target loss valu reached. 8 conclus futur work distribut train strategi deep learn architectur import size dataset increases. allow research receiv earlier feedback idea increas pace algorithm developed, understand best practic distribut train model kei research area. work, studi adapt train algorithm avail hardwar resourc order acceler train cnn homogeneu gpu cluster. first, show close linear speedup achiev intra-nod parallelism. base results, develop mix approach effici leverag inter-nod commun reduc compar pure asynchron policy. properli tune number paramet server configuration, method yield import speedup number sampl second process setup minimum hardwar overhead. spite good scalabl demonstr term throughput, configur node requir addit train step reach target loss value, increas throughput compens issu reduc train time considerably. 8 distribut train strategi deep learn gpu cluster. vctor campo et al. drawback import increas number nodes, result suggest differ strategi emploi highli distribut set dozen nodes. futur work compris main research lines. first, develop tool gain insight perform individu compon help detect bottleneck push scalabl system. hand, plan implement evalu pure synchron gradient descent strategy. despit solv stale gradient problem, overal throughput method determin slowest worker, effici mix approach propos work backup worker option achiev great accuraci maintain performance. besides, increas effect batch size neg impact gener capabl model, effect requir evalu experimentation. acknowledg work partial support spanish ministri economi competit contract tin2012-34557, bsc-cn severo ochoa program (sev-2011-00067), sgr programm (2014-sgr-1051 2014-sgr-1421) catalan govern framework project biggraph tec2013-43935-r, fund spanish ministerio economia y competitividad european region develop fund (erdf). like thank technic support team barcelona supercomput center (bsc) especi carlo tripiana. refer [1] m. abadi, p. barham, j. chen, z. chen, a. davis, j. dean, m. devin, s. ghemawat, g. irving, m. isard, m. kudlur, j. levenberg, r. monga, s. moore, d. g. murray, b. steiner, p. tucker, v. vasudevan, p. warden, m. wicke, y. yu, x. zheng. tensorflow: large-scal machin learning. arxiv e-prints, 2016. [2] martn abadi, ashish agarwal, paul barham, eugen brevdo, zhifeng chen, craig citro, greg s corrado, andi davis, jeffrei dean, matthieu devin, et al. tensorflow: large-scal machin learn heterogen systems. 2015. [3] damian borth, rongrong ji, tao chen, thoma breuel, shih-fu chang. large-scal visual sentiment ontolog detector adject noun pairs. acm mm, 2013. [4] jianmin chen, xinghao pan, rajat monga, sami bengio, rafal jozefowicz. revisit dis- tribut synchron sgd. iclr 2017 confer submission, 2016. [5] tao chen, damian borth, trevor darrell, shih-fu chang. deepsentibank: visual sentiment concept classif deep convolut neural networks. arxiv:1410.8586, 2014. [6] tianqi chen, mu li, yutian li, min lin, naiyan wang, minji wang, tianjun xiao, bing xu, chiyuan zhang, zheng zhang. mxnet: flexibl effici machin learn librari heterogen distribut systems. arxiv preprint arxiv:1512.01274, 2015. [7] sharan chetlur, cliff woolley, philipp vandermersch, jonathan cohen, john tran, bryan catanzaro, evan shelhamer. cudnn: effici primit deep learning. arxiv preprint arxiv:1410.0759, 2014. [8] jia deng, wei dong, richard socher, li-jia li, kai li, li fei-fei. imagenet: large-scal hierarch imag database. cvpr, 2009. [9] kaim he, xiangyu zhang, shaoq ren, jian sun. deep residu learn imag recognition. cvpr, 2016. 9 vctor campo et al. / procedia scienc 108c (2017) 315324 323distribut train strategi deep learn gpu cluster. vctor campo et al. step old average. final accuraci test set reach configur detail tabl 3. distribut setup abl reach final accuraci singl node model, confirm stale gradient neg impact final minima reach model converges. moreover, keep similar throughput worker node critic factor success learn process, worker constantli rest node aggrav stale gradient problem. worker (gpus) test accuraci time (h) speedup 1 node (4) 0.228 106.43 1.00 2 node (8) 0.217 62.78 1.69 4 node (16) 0.202 37.99 2.80 8 node (32) 0.217 22.50 4.73 tabl 3: result test set differ distribut configurations. despit benefit larger throughputs, setup node requir iter converge. figur 3: train loss evolut differ distribut configurations. nodes, faster target loss valu reached. 8 conclus futur work distribut train strategi deep learn architectur import size dataset increases. allow research receiv earlier feedback idea increas pace algorithm developed, understand best practic distribut train model kei research area. work, studi adapt train algorithm avail hardwar resourc order acceler train cnn homogeneu gpu cluster. first, show close linear speedup achiev intra-nod parallelism. base results, develop mix approach effici leverag inter-nod commun reduc compar pure asynchron policy. properli tune number paramet server configuration, method yield import speedup number sampl second process setup minimum hardwar overhead. spite good scalabl demonstr term throughput, configur node requir addit train step reach target loss value, increas throughput compens issu reduc train time considerably. 8 distribut train strategi deep learn gpu cluster. vctor campo et al. drawback import increas number nodes, result suggest differ strategi emploi highli distribut set dozen nodes. futur work compris main research lines. first, develop tool gain insight perform individu compon help detect bottleneck push scalabl system. hand, plan implement evalu pure synchron gradient descent strategy. despit solv stale gradient problem, overal throughput method determin slowest worker, effici mix approach propos work backup worker option achiev great accuraci maintain performance. besides, increas effect batch size neg impact gener capabl model, effect requir evalu experimentation. acknowledg work partial support spanish ministri economi competit contract tin2012-34557, bsc-cn severo ochoa program (sev-2011-00067), sgr programm (2014-sgr-1051 2014-sgr-1421) catalan govern framework project biggraph tec2013-43935-r, fund spanish ministerio economia y competitividad european region develop fund (erdf). like thank technic support team barcelona supercomput center (bsc) especi carlo tripiana. refer [1] m. abadi, p. barham, j. chen, z. chen, a. davis, j. dean, m. devin, s. ghemawat, g. irving, m. isard, m. kudlur, j. levenberg, r. monga, s. moore, d. g. murray, b. steiner, p. tucker, v. vasudevan, p. warden, m. wicke, y. yu, x. zheng. tensorflow: large-scal machin learning. arxiv e-prints, 2016. [2] martn abadi, ashish agarwal, paul barham, eugen brevdo, zhifeng chen, craig citro, greg s corrado, andi davis, jeffrei dean, matthieu devin, et al. tensorflow: large-scal machin learn heterogen systems. 2015. [3] damian borth, rongrong ji, tao chen, thoma breuel, shih-fu chang. large-scal visual sentiment ontolog detector adject noun pairs. acm mm, 2013. [4] jianmin chen, xinghao pan, rajat monga, sami bengio, rafal jozefowicz. revisit dis- tribut synchron sgd. iclr 2017 confer submission, 2016. [5] tao chen, damian borth, trevor darrell, shih-fu chang. deepsentibank: visual sentiment concept classif deep convolut neural networks. arxiv:1410.8586, 2014. [6] tianqi chen, mu li, yutian li, min lin, naiyan wang, minji wang, tianjun xiao, bing xu, chiyuan zhang, zheng zhang. mxnet: flexibl effici machin learn librari heterogen distribut systems. arxiv preprint arxiv:1512.01274, 2015. [7] sharan chetlur, cliff woolley, philipp vandermersch, jonathan cohen, john tran, bryan catanzaro, evan shelhamer. cudnn: effici primit deep learning. arxiv preprint arxiv:1410.0759, 2014. [8] jia deng, wei dong, richard socher, li-jia li, kai li, li fei-fei. imagenet: large-scal hierarch imag database. cvpr, 2009. [9] kaim he, xiangyu zhang, shaoq ren, jian sun. deep residu learn imag recognition. cvpr, 2016. 9 324 vctor campo et al. / procedia scienc 108c (2017) 315324distribut train strategi deep learn gpu cluster. vctor campo et al. [10] yangq jia, evan shelhamer, jeff donahue, sergei karayev, jonathan long, ross girshick, ser- gio guadarrama, trevor darrell. caffe: convolut architectur fast featur embedding. acm mm, 2014. [11] peter h jin, qiaochu yuan, forrest iandola, kurt keutzer. scale distribut deep learning? arxiv preprint arxiv:1611.04581, 2016. [12] brendan jou, tao chen, nikolao pappas, miriam redi, mercan topkara, shih-fu chang. visual affect world: large-scal multilingu visual sentiment ontology. acm mm. [13] andrej karpathy, georg toderici, sanketh shetty, thoma leung, rahul sukthankar, li fei- fei. large-scal video classif convolut neural networks. cvpr, 2014. [14] alex krizhevsky. weird trick parallel convolut neural networks. arxiv preprint arxiv:1404.5997, 2014. [15] alex krizhevsky, ilya sutskever, geoffrei e. hinton. imagenet classif deep con- volut neural networks. nips, 2012. [16] yann lecun, leon bottou, yoshua bengio, patrick haffner. gradient-bas learn appli document recognition. proceed ieee, 86(11), 1998. [17] ma, fei mao, graham w taylor. theano-mpi: theano-bas distribut train frame- work. arxiv preprint arxiv:1605.08325, 2016. [18] daniel mcduff, rana el kaliouby, jeffrei f cohn, rosalind w picard. predict ad like purchas intent: large-scal analysi facial respons ads. ieee transact affect computing, 6(3), 2015. [19] philipp moritz, robert nishihara, ion stoica, michael jordan. sparknet: train deep network spark. arxiv preprint arxiv:1511.06051, 2015. [20] cuda nvidia. comput unifi devic architectur program guide. 2007. [21] aaron van den oord, sander dieleman, heiga zen, karen simonyan, oriol vinyals, alex graves, nal kalchbrenner, andrew senior, korai kavukcuoglu. wavenet: gener model raw audio. arxiv preprint arxiv:1609.03499, 2016. [22] rosalind w. picard. affect computing, volum 252. mit press cambridge, 1997. [23] robert plutchik. emotion: psychoevolutionari synthesis. harper & row, 1980. [24] s sundhar ram, angelia nedic, venugop v veeravalli. asynchron gossip algorithm stochast optimization. gamenets, 2009. [25] frank seide, hao fu, jasha droppo, gang li, dong yu. 1-bit stochast gradient descent applic data-parallel distribut train speech dnns. interspeech 2014. [26] david silver, aja huang, chri j maddison, arthur guez, laurent sifre, georg van den driess- che, julian schrittwieser, ioanni antonoglou, veda panneershelvam, marc lanctot, et al. mas- tere game deep neural network tree search. nature, 529(7587), 2016. [27] christian szegedy, wei liu, yangq jia, pierr sermanet, scott reed, dragomir anguelov, dumitru erhan, vincent vanhoucke, andrew rabinovich. go deeper convolutions. cvpr, 2015. [28] tijmen tieleman geoffrei hinton. lectur 6.5-rmsprop: divid gradient run averag recent magnitude. coursera: neural network machin learning, 4, 2012. [29] yonghui wu, mike schuster, zhifeng chen, quoc v le, mohammad norouzi, wolfgang macherey, maxim krikun, yuan cao, qin gao, klau macherey, et al. googl neural machin trans- lation system: bridg gap human machin translation. arxiv preprint arxiv:1609.08144, 2016. [30] jason yosinski, jeff clune, yoshua bengio, hod lipson. transfer featur deep neural networks? nips, 2014. [31] sixin zhang, anna e choromanska, yann lecun. deep learn elast averag sgd. nips, 2015. 10