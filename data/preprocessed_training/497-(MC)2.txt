siam j. sci. comput. c 2015 societi industri appli mathemat vol. 00, no. 0, pp. 00000000 multilevel balanc domain decomposit extrem scale santiago badia , alberto f. martin , javier princip abstract. paper present fully-distributed, communicator-aware, recursive, interlevel-overlap message-pass implement multilevel balanc domain decomposit constraint (mlbddc) precon- ditioner. implement highli reli subcommun order achiev desir effect coarse-grain overlap comput communication, commun commun level hierarchi (name inter-level overlapping). essentially, main commun split non-overlap subset mpi task (i.e., mpi subcommunicators) level hierarchy. provid special resourc (core memory) devot level, care re-schedul map comput commun al- gorithm let high degre overlap exploit levels. subroutin associ data structur express recursively, mlbddc precondition arbitrari number level built re-us signific recurr part codes. approach lead excel weak scalabl result soon level-1 task mask coarser-level duties. provid model indic choos number level coarsen ratio consecut level determin qualit scalabl limit given choice. carri comprehens weak scalabl analysi propos implement 3d laplacian linear elast problems. excel weak scalabl result obtain 458,752 ibm bg/q core 1.8 million mpi tasks, time exact domain decomposit precondition (onli base spars direct solvers) reach scales. 1. introduction. simul scientif engin problem govern partial dif- ferenti equat (pdes) involv solut spars linear systems. time spent implicit simul linear solver rel overal execut time grow size problem number core [22]. order satisfi increas demand realiti complex- iti simulations, scientif comput advanc develop numer algorithm implement effici exploit largest amount comput resources, massiv parallel linear solver kei compon process. growth comput power pass increas number core chip, instead make core faster. gener supercomputers, abl reach 1 exaflop/s, expect reach billion cores. thus, futur scientif comput strongli relat abil effici exploit extrem core count [1]. numer algorithm compon scalabl effici run extrem scale supercomputers. extrem core counts, reduc commun synchron cores, overlap commun computation. largest scales, linear solver base precondit krylov subspac methods. algorithm scalabl precondition includ (algebraic) multigrid (mg) [30] domain decomposit (dd) algorithm [31]. however, theoret properti practic weak scalability, precondition allow massiv scalabl implementation. todai scalabl algorithms/implement present practic limit parallelism, e.g., small, coars problem solv hierarch process dd/amg, loss sparsiti denser commun pattern coarser level amg [7]. dd precondition explicit us partit global mesh, e.g., finit element (fe) integration, sub-mesh (subdomains), provid natur framework develop fast robust parallel solver tailor distributed-memori machines. one-level dd algorithm involv solut local problem nearest-neighbor communications. (second level) coars correct (coupl subdomains) requir algorithm scalability, harm practic (cpu time) weak scalability. two-level dd algorithm includ balanc neumann- centr internacion metod numer lenginyeria (cimne), parc mediterrani la tecnologia, upc, estev terrada 5, 08860 castelldefels, spain universitat politecnica catalunya, jordi girona 1-3, edifici c1, 08034 barcelona, spain. work fund european research council fp7 programm idea start grant no. 258443 - comfus: comput method fusion technolog fp7 numexa project grant agreement 611636. a. f. martn partial fund generalitat catalunya program ajut la incorporacio, amb caract temporal, person investigador junior le universitat publiqu del sistema universitari catala pdj 2013. acknowledg prace award access fermi base bologna (italy) cineca, gc juqueen base julich (germany) jsc. gratefulli acknowledg jsc staff general, dirk broemmel particular, support porting/debug fempar depend to/on juqueen. 1 neumann precondition (bnn) [23], balanc dd constraint precondition (bddc) [13], feti-dp precondition [15]. practic scalabl limit two-level dd implement determin coars solver computation, size increas (at best) linearli respect number subdomains. coars problem rapidli bottleneck algorithm increas number cores, irremedi produc sever parallel effici loss (all core involv coars solver comput idling). weak scalabl sustain incur notabl parallel effici loss, increas- ing number core deal coars solver, e.g., mean message-pass spars direct solver mump [2]. complex type solver quadrat dimension problems, quadrat increas number core need constant comput time. however, scalabl spars direct solver limit hundr cores, harm overal weak scalabl two-level bddc method point [3]. salient featur bddc method fact constrain neumann dirichlet local problems, coars problem, comput inexact way, e.g., amg cycl affect algorithm scalabl method [14]. us inexact coars solver possibl feti-dp methods, modif [20]. amg solver maintain weak scalabl message-pass spars direct methods, inexact version bddc feti-dp method exhibit improv weak scalability, implement incur parallel effici loss comment above. inexact feti-dp methods, local problem comput direct solver coars problem approxim amg, exploit [20]. bddc precondition salient properti permit overcom parallel over- head, make excel candid extrem scale solver design: (p1) allow mathemat support extrem aggress coarsen coars matrix similar sparsiti pattern origin matrix. memory-constrain supercom- puters, order 105 spars direct method [5] (see sect. 5). (p2) coars fine compon comput parallel, basi coars space construct wai orthogon fine compon space respect inner product endow matrix [5]. (p3) fact coars matrix similar structur origin matrix, multilevel extens algorithm possibl [25,32]. properti (p1) readili exploit bddc implementation. effici exploit (p2), i.e., orthogon coars fine spaces, trivial. however, properti make possibl parallel comput coars fine corrections, i.e., overlap time. [5], classifi duti exact (i.e., spars direct solvers) bddc-pcg algorithm fine coars duties. duti re-schedul achiev maximum degre overlap preserv data dependencies. actual implement idea requir signific code refactoring, involv switch spmd (singl program multipl data) mpmd (multipl program multipl data) parallel execut mode; core divid have fine grid duti have coars grid duties. bulk-asynchron approach reduc synchron cores, overlap communications/computations, follow exascal solver paradigm [1]. exploit [5], perform scalabl analys 3d poisson linear elast problem pair state-of-the-art multicore-bas distributed-memori machin (helio curie). excel weak scalabl attain 27k core reason high local problem sizes; local coars problem solv multi-thread spars direct solver pardiso [28]. further, clear reduct comput time memori requir inexact solver compar spars direct on possibl [6] excel weak scalabl result inexact/overlap implement two-level bddc preconditioner, 93,312 core 20 billion unknown juqueen. regard (p3), multilevel bddc (mlbddc) algorithm propos [25], coars problem bddc level approxim bddc approximation. implement mlbddc method exploit (p2) [29]. cpu cost coars problem reduced, coars problem serial respect fine component, implement [25] suffer parallel effici loss. further, condit number 2 bound slightli increas number level [25], detail numer experi [29] conclus prove benefit multilevel extens (see, e.g., tabl 4, 7, 8 [29]). work, extend approach [5] two-level bddc method mlbddc. exact two-level method serial coars solver effect till ten thousand cores. point, coars problem mask anymor fine duties. order larger core counts, extend approach [5] two-level bddc method mlbddc, i.e., exploit (p2) (p3). present fully-distributed, communicator-aware, recursive, interlevel- overlap message-pass implement mlbddc preconditioner. fully-distribut (versu centralized) mean data structur (and associ computations/communications) distribut cores, includ coarse-grid problem level mlbddc precondit hierarchy. communicator-awar refer fact code highli reli subcommun order achiev desir effect coarse-grain overlap comput communication, commun commun level hierarchi (name inter-level overlapping). essentially, main commun split non-overlap subset mpi task (i.e., mpi subcommunicators) level hierarchy. given intermedi k-th level, coarse-grid problem built level k-1 distribut subset mpi task devot level k. provid special hardwar resourc (core memory) devot level, care re-schedul map comput commun algorithm let high degre overlap exploit levels. finally, recurs implement mean subroutin associ data structur express recursively, mlbddc precondition arbitrari number level built re-us recurs recurr part code (e.g., commun creation, local solvers, inter-level data transfers, etc.). approach lead excel weak scalabl result soon level-1 task mask coarser- level duties. provid model indic choos number level coarsen ratio consecut level determin qualit scalabl limit given choice. finally, present comprehens weak scalabl analysi propos implement three- dimension (3d) laplacian linear elast problems. excel weak scalabl result obtain 458,752 core 1.8 million mpi tasks. unpreced result exact domain decomposit precondition (equip spars direct solvers) repres order magnitud (in term scalabl limits) improv respect best result up-to-now [5], show tremend potenti algorithm approach propos herein. thank results, softwar platform fempar [4] implement mlbddc algorithm high-q club (sinc 2014) scalabl code juqueen ibm blue gene/q supercomput [11]. let summar contribut work: novel recurs implement mlbddc method base multilevel overlap strategi time, order fulli mask coarse-grain task larger core count two-level methods. detail exposit effici exploit novel approach state-of-the-art supercomputers, includ exploit recurs multilevel set overlap deploy task construct sub-communicators. comprehens weak scalabl analysi propos strategi laplacian linear elast problem ibm blue gene/q supercomputer, 458,752 core 1.8 million mpi task (subdomains). work structur follows. mlbddc precondition present sect. 2. sect. 3, present overlap mlbddc implement algorithm, overlap multipl level computations, elabor model determin scalabl limit given choic number level coarsen ratio consecut levels. sect. 4 provid implement kei recurs creation mpi sub-communicators. sect. 5, report comprehens set numer experiments. finally, sect. 6, draw conclus defin futur line work. 2. multilevel balanc domain decomposition. section state mlbddc pre- conditioner. sect. 2.1, introduc basic notation. sect. 2.2 devot two-level bddc algorithm. finally, mlbddc algorithm defin recurs wai sect. 2.3, reli con- 3 cept introduc sect. 2.2 two-level algorithm. case, descript algorithm concise, refer reader [10,24] detail exposit two-level bddc methods, [25] multilevel extension. further, thorough present practic implement detail domain decomposit method [4]. 2.1. problem setting. let consid bound polyhedr domain rd d = 2, 3 quasi-uniform partit (mesh) t0 characterist size h0. usually, t0 partit tetrahedra/hexahedra d = 3 triangles/quadrilater d = 2. consid quasi-uniform partit t1 t0 nsbd1 sub-meshes, induc non-overlap domain decomposit subdomain i1, = 1, . . . , n sbd 1 (of characterist size h1). interfac 1 defin i1 := 1 \ interfac (skeleton) domain decomposit 1 := nsbd1 i=1 1. model problem, studi poisson problem linear elast , arbitrari forc term boundari condit (a soon problem well-posed). let consid conform fe space v1 h1(). denot vi1 restrict v1 i1 t1, i.e., local fe spaces. v1 := v11 . . .v nsbd1 1 global fe space function discontinu 1. let defin project e1 : v1 v1 weight averag interfac valu (see, e.g., [24]). note t0 fe type defin v1, t1 requir defin local discontinu space vi1 v1, respectively. galerkin approxim problem hand (with respect v1) lead global linear equat solved: a1x1 = f1. (2.1) subdomain fe matrix correspond vi1 denot k 1. k1 block-diagon global sub- assembl fe matrix v1. (along paper, denot letter k sub-assembl matrix correspond fulli assembl one.) analogously, defin local sub-assembl right-hand gi1 global counterpart g1. matrix a1 right-hand f1 obtain assembl k1 g1. non-overlap partit induc reorder dof interior interfac dofs, i.e., u1 = [u1i , u1 ] t. defin interior restrict oper r1iu1 := u1i . lead follow block structur global assembled, global sub-assembled, local matrices: a1 = [ a1ii a1i a1i a1 ] , k1 = [ a1ii k1i k1i k1 ] , ki1 = [ ai1ii 1i ai1i k 1 ] , respectively. matric a1ii , a1i , a1i k1 present block-diagon structur (veri amen parallelization). matric k1i k1i trivial extens (by zeroes) a1i a1i , respectively. 2.2. two-level bddc preconditioner. sequel, state two-level bddc algorithm, describ set-up precondition application. input requir set-up bddc precondition (t1, v1,k1). recal t1 subdomain partition, v1 global fe space, k1 global sub-assembl matrix. construct bddc precondition requir partit degre freedom (dofs) correspond v1 1 objects, corners, edges, faces. next, associ (or all) object coars dof. coars dof valu function corners, mean valu function edges/faces. common variant bddc method refer bddc(c), bddc(ce) bddc(cef), enforc continu corner coars dofs, corner edg coars dofs, corner, edge, face coars dofs, respectively. definit object coars dof implement automat process arbitrari partit physic problem (see [4]). involv us kernel detect mechan order preserv well-posed bddc precondition (see [33]). defin coars dofs, defin bddc fe space v1 subspac function v1 continu coars dofs; clearly, v1 v1 v1. bddc precondition schwarz-typ precondition combin interior correct correct bddc space v1 (see, e.g., [10, 31]). defin interior correct oper p1 := r t 1i a11iir1i , involv set-up (a 1ii )1, i.e., local dirichlet problems. bddc correct express e1k 1 1 e t 1, k1 galerkin project k1 v1. 4 set-up bddc correct requir elaboration. let consid decomposit bddc space v1 fine space v1f vector vanish coars dof k-orthogon complement v1c , denot coars space. result, bddc fe problem decompos fine coars components, i.e., x1 = k 1 1 e t 1r1 = x1f + x1c . fine coars space k1-orthogon definition, comput parallel. fine space function v1f vanish coars dof (which dof involv continu subdomains). k1-orthogonality, fine compon defin x1f := e1k 1 1f et1, k1f galerkin project k1 v1f . let note that, coars dof fix (to zero) fine correction, dof coupl subdomains, set-up k11f involv solut local neumann problem constrain valu correspond subdomain coars dofs, denot (ki1f ) 1. comprehens exposit implement issu solut constrain neumann problem refer [4]. coars space v1c v1 built v1c = span{11, 21, . . . , ncts1 1 }, 1 v1c coars shape function associ coars dof , i.e., take valu vanish rest coars dofs. result, comput involv k11f (see [4]). let note support 1 set subdomain contain . thus, subdomain comput coars space basi function relat own coars dofs. denot 1 matrix column coars shape functions. defin coars matrix k1c assembl subdomain local matric ki1c = 1 t ki1 1, = 1, . . . , n sbd 1 . further, two-level algorithm, set-up 1 1c , e.g., assembl subdomain contribut processor. fact p1a1p1 = p1, two-level bddc precondition state compact form as: m1 := p1 + (i1 p1a1)et1(1a 1 1c t1 +k 1 1f )e1(i1 p1a1)t, i1 ident matrix. 2.3. multilevel bddc preconditioner. two-level bddc algorithm involv input (t1, v1,k1) automat gener coars dof correspond coars bddc space v1c coars matrix k1c . easili observ bddc method suitabl multilevel gener (see [32] three-level [25] gener case). consid coars partit t2 t1, obtain aggreg (coarsening) subdomain t1. subdomain j 2 t2, portion coars matrix dispos assembl local sub-assembl matric ki1c subdomain 1 t1 i1 j 2, fashion fe matrices. further, coars space v1c fe-lik space associ t1, gener correspond space discontinu function respect t2. therefore, readili defin bddc precondition associ coars matrix k1c , coars space v1c , partit t2. sequel, express multilevel method formal way. creat hierarchi quasi-uniform partit (t1, t2, . . . , tn`1) aggregation, n` number level bddc method. t`+1 partit t`. subdomain i` t` j `+1 t`+1 ` j `+1, denot j fat(`, i). readili us notat sect. 2.2 replac 1 `. input mlbddc method, requir sub-assembl subdomain matrix k1 relat t1, global fe space v1, hierarch partit (t1, t2, . . . , tn`1). set-up mlbddc precondition follows: ` = 1, . . . , n` 1, set-up (p`,k 1 `f ,k`c ,`, e`) procedur describ sect. 2.2 replac (t1, v1,k1) (t`, v`,k`) ` == n` 1 set-up a1n`1c initi level v`+1 v`c , k`+1 k`c oper set-up, posit defin mlbddc precondition m recurs wai follows: m m1 m` = p` + (i` p`a`)et`(`m`+1 t ` +k 1 `f )e`(i` p`a`)t, ` = 1, . . . , n` 1, mn` = 1 n`1c . 5 refer [25] proof follow theorem, condit number bddc- precondit matrix. note result [25] origin prove laplacian problem. extend linear elast case combin two-level bound condit number bddc feti-dp method 3d linear elast (see, e.g., [21]) multilevel analysi [25]. theorem 2.1. condit number bddc precondit matrix laplacian linear elast problem bound (ma) c n`1 `=1 ( 1 + log ( h` h`1 ))2 , bddc(c) bddc(ce) 2d, bddc(ce) bddc(cef) 3d, c > 0 constant depend hierarch partition, i.e., number level n` characterist sizes. 3. extrem scale parallel distributed-memori implementation. section cover novel approach parallel distributed-memori implement mlbddc- precondit conjug gradient (pcg) algorithm. sect. 3.1, present rational underli novel approach pursu extrem scale implement algorithm. sect. 3.2 cover build block parallel algorithm subject study. sect. 3.3 discuss us case illustr techniqu propos appli three-level bddc-pgc solver order reach maximum perform benefit. finally, sect. 3.4 analyz choos valu coarsen ratio govern subdomain aggreg order let techniqu propos fulli effective, includ estim number subdomain global problem split reach goal. 3.1. rational underli novel implement approach. section present rational novel approach pursu extrem scale implement pcg- mlbddc solver. rational built fig. 3.1, illustr possibl implement approach algorithm. implement approach illustr fig. 3.1(a) typic follow exist code implement (ml)bddc relat dd algorithm [8, 26, 29]. come natur mind reflect multilevel structur preconditioner. rel easi code bulk-synchron structure. however, exploit parallel readili avail algorithm serial comput applic mlbddc hierarchy. example, discuss sect. 2, fine coarse-grid correct comput parallel k-orthogon constraint underli bddc space. opportun parallel exploit implement approach. (see sect. 3.3 coverag opportun case three-level mlbddc-pcg solver.) effect, signific loss parallel effici idl mpi tasks. aggress coarsen methods, number task level 1 larger higher levels. thus, implementations, notabl aggreg idl time roughli equal number core time time spent level higher 1. neg impact energi consumpt implementations. second effect, memori avail core respons coarsest-grid problem share data structur correspond levels. provid limit memori core current (and future) multicore-bas massiv parallel processor (e.g., 1gb ibm bg/q supercomputer), limit load core fit memori (and global problem size). fig. 3.1(b) illustr novel implement approach propos paper. pursu exploit parallel avail algorithm. order reach goal, global mpi commun split disjoint subset mpi tasks, i.e., subcommunicators, level mlbddc precondit hierarchy. mpi task charg singl subdomain particular level. besides, step algorithm re-organ (i.e., re-scheduled) wai comput commun global critic path performed/issu soon note distributed-memori solver coarsest-grid problem [16, 26, 29] mitig impact effects, parallel overhead idl mpi task remains. 6 ..... 1 2 3 4 p main mpi commun ..... parallel (distributed) global commun global commun ..... time idl idl ..... 1 2 3 4 p 1 1st level mpi comm ..... ..... 1 2 p 2 2nd level mpi comm ..... 3rd level mpi comm 1 parallel (distributed) global commun global commun ..... time (a) (b) fig. 3.1. pictori view possibl approach parallel distributed-memori implement mlb- ddc preconditioner. (a) recursive, fully-distributed. (b) recursive, fully-distributed, communicator-aware, inter-level overlapped. possible, let high degre coarse-grain overlap exploit levels. provid mpi task level map disjoint (specialized) comput nodes, memori avail core accommod data structur correspond level hierarchy. target tune number level number task level problem hand wai level duti complet absorb (i.e., mask) coarser-grid duti effect inter-level overlapping. note strategi pursu [5] two-level bddc precondition remark scalabl solut 3d laplacian linear elast problem medium-s cluster (up 27k cores). order boost scalabl current supercomput core count (in order million), work extend techniqu [5] two-level bddc method multilevel setting. 3.2. build blocks. alg. 1 present main phase parallel distributed-memori solut linear (2.1) mlbddc-pcg solver. distinguish initi phase encompass line 1-2 alg. 1, mlbddc precondition set-up, iter phase line 6, pcg solver acceler mean mlbddc preconditioner. pcg consist repeat sequenc follow basic operations: applic preconditioner, spars matrix-vector products, inner product vector updat [27]. algorithm 1: solv a1x1 = f1 1: set-up m (symbol stage) alg. 2 2: set-up m (numer stage) alg. 3 3: set initi solut x01 4: x01i := x 0 1i +a11iir1i (f1 a1x 0 1) 5: r01 := f1 a1x01 6: x1 := pcg(a1,m ,r 0 1,x 0 1) invok alg. 4 message-pass implement alg. 1, matrices, vectors, associ comput 7 distribut mpi task conform hierarchi non-overlap partit under- ly mlbddc preconditioner. let denot task(`, i) one-to-on map assign mpi task subdomain i` t`, level `. vector x` v` distribut conform t`, i.e., portion xi` x` store mpi task task(`, i). analogously, matrix k` distribut diagon block ki` store mpi task task(`, i). vector v` v` distribut v`, particular continu interface, i.e., valu interfac dof match mpi task share them. result, data structur describ distributed-memori layout vector provid addit glu information. refer reader [4] comprehens coverag data structur two-level bddc precondition context. discuss straightforwardli appli mlbddc preconditioner. algs. 2-3 collect step requir set mlbddc preconditioner, requir applic residu vector pcg iter shown alg. 4. work consid spars direct method [12] exact (up machin precision) solut subproblem mlbddc precondit hierarchy. indeed, reader observ algs. 2, 3 4 match stage involv direct solut spars linear systems. particular, precondition set-up split symbol stage alg. 2 (with gc denot graph describ sparsiti pattern matrix c), follow numer stage alg. 3. alg. 2 essenti build symbol factor graph associ local matric i` `ii , k i` `f , ` = 1, . . . , n` 1, i` = 1, . . . , nsbd` , global coarsest-grid matrix an`1c . symbol analysis, fill-in reorder appli graph. hand, alg. 3 charg numer factor matrices. spars choleski factor (each of) matric set-up, alg. 4 solv correspond linear system requir applic mlbddc precondition hierarchi mean spars forward/backward substitution. note dirichlet pre-correct alg. 4 (see line 2 3) omit level, interior residu zero initi interior pre-correct (see line 4 alg. 1). algorithm 2: mmlbddc set-up (symbol stage) 1: reord+symb fact(gai `ii ) ` 2: identifi local coars dof ` 3: reord+symb fact(gki `f ) ` 4: gather coarse-grid dof ` `+ 1 5: ` == n` 1 6: build gan`1c `+ 1 7: reord+symb fact(gan`1c ) `+ 1 8: 9: build g k j `c `+ 1 10: defin gk`+1 gk`c invok alg. 2 ` `+ 1 `+ 1, . . . , n` 11: end let final mean label end step algs. 2, 3 4. hand, mpi task task(`, ) charg step label `, mpi task n`k=`+1 task(k, ) perform label `+ 1, . . . , n`. note algs. 2, 3, 4 assuming, loss generality, an`1c central singl mpi task level (so serial spars direct solver solut coarsest-grid problem). however, problem distribut mpi task (and solv mean message-pass spars direct solver mump [2]). hand, label ` `+ 1 refer data transfer mpi task consecut levels. precisely, level ` mpi tasks, task(`, i), parent level `+ 1, i.e., task task(`+ 1, j) j = fat(`, i). 3.3. us case three-level bddc-pcg solver. accord sect. 3.1, step algs. 2-4 judici re-organ order fulli exploit parallel readili avail algorithms. tabl 3.1 depict result exercis three-level bddc- pcg solver. (we consid three-level algorithm sake simplic discuss 8 algorithm 3: mmlbddc set-up (numer stage) 1: num fact(ai`ii ) ` 2: num fact(ki`f ) ` 3: comput i` ` 4: comput ki`c := ( `) tki` ` ` 5: gather ki`c ` `+ 1 6: ` == n` 1 7: an`1c := assemble(k `c ) `+ 1 8: num fact(an`1c) `+ 1 9: 10: k j `c := assemble(ki`c), j = fat(`, i) `+ 1 11: defin k`+1 k`c invok alg. 3 ` `+ 1 `+ 1, . . . , n` 12: end algorithm 4: z := m1mlbddcr 1: ` > 1 2: comput i`i := (a `ii )1ri`i ` 3: comput ri` := r ` ai`i `i ` 4: end 5: comput ri` := (e` i)tr` ` 6: comput ri`c := ( `) tri` ` 7: gather ri`c ` `+ 1 8: comput si`f := (k `f )1ri` ` 9: ` == n` 1 10: rn`1 = assemble(r `c ) `+ 1 11: comput zn`1 := 1 n`1 rn`1 `+ 1 12: scatter zn`1 z `c `+ 1 ` 13: 14: r j `c := assemble(ri`c) j = fat(`, i) `+ 1 15: defin r`+1 r`c , z`+1 z`c , 16: invok alg. 4 ` `+ 1 `+ 1, . . . , n` 17: scatter z j `+1 z `c , j = fat(`, i) `+ 1 ` 18: end 19: comput si`c := `z `c ` 20: comput zi` := e `( `f + si`c) ` 21: comput zi`i := (a `ii )1ai`iz ` ` 22: ` > 1 23: comput zi`i := z `i + i`i ` 24: end overlap potenti approach, implement algorithm recurs appli arbitrari number levels; sect. 3.4 4.) table, step perform group colour region order clarifi exposition. particular, green region encompass local comput nearest neighbor commun resid level, blue region second level. green blue region separ gather scatter commun stage second level mpi task (uncolor table). blue region turn split commun stage second level mpi task (color gray). finally, red region includ comput level separ commun stages. let character balanc struck time spent color region tabl 3.1 order let techniqu propos fulli effective. characterization, 9 ` = 1 mpi task ` = 2 mpi task ` = 3 mpi task identifi local coars dof gather coarse-grid dof symb fact(g k i1 `f ) build g k i2 1c symb fact(g i1 `ii ) identifi local coars dof num fact(k i1 `f ) gather coarse-grid dof comput i1 1 symb fact(gki2 `f ) build ga2c k i1 1c := ( i1 1 ) tk i1 1 i1 1 symb fact(gai2 `ii ) symb fact(ga2c ) gather k i1 1c num fact(a i1 `ii ) k i2 1c := assemb(k i1 1c ) solv a1ii 0 1i = r1i r1 num fact(k i1 `f ) x01i := x 0 1i + 01i comput i2 2 r01 := f1 a1x01 k i2 2c := ( i2 2 ) tk i2 2 i2 2 algorithm 5 (k i1) gather ki22c num fact(a i1 `ii ) a2c := assemb(k i2 2c ) num fact(a2c) gather r i1 1c solv k i1 `f s i1 `f := r i1 ` r i2 1c := assemb(r i1 1c ) algorithm 5 (k i2) gather r i2 2c solv k i2 `f s i2 `f := r i2 ` r2c := assemb(r i2 2c ) solv a2cz2c = r2c scatter z2c z i2 2c algorithm 6 (k i2) scatter z i2 2 z i1 1c algorithm 6 (k i1) algorithm 5 ` > 1 k`i := (a k `ii )1rk`i rk` := r k ` ak`i k `i end rk` := (e k ` ) tr` r `c := (i`) tri` algorithm 6 sk`c := k ` z k `c zk` := e k ` (s k `f + sk`c) zk`i := (a k `ii )1ak`iz k ` ` > 1 zk`i := z k `i + k`i end tabl 3.1 map algs. 2-4 mpi task maxim inter-level overlap three-level bddc-pcg solver. basi kei observ deriv table: effect fulfil mpi task second level issu gather scatter commun oper second level mpi task time level. delai second level mpi task reach commun stage (with respect level mpi tasks) immedi impli parallel effici loss. situat (we later analyz factor depend it), step encompass green area fulli absorb (i.e., mask) latenc associ coarser-grid level duti (i.e., blue red region tabl 3.1). reader note point scenario lead remark perform scalability, provid step encompass green region local nature, fulli parallel. obviou necessari suffici condit achiev effect step encompass blue region second level time correspond green regions. let discuss factor depend green/blu region pair separately, restrict computationally-domin steps: green/blu region pair, symbol factor graph associ dirichlet constrain neumann problems, numer factor constrain neumann problem, comput i11 level, time symbol factor second level. provid numer factor higher order complex symbol factor (see sect. 3.4), goal rel 10 easi achieve. second green/blu region pair, numer factor dirichlet problem matrix, solut linear matrix, time numer factor constrain neumann dirichlet problem matric comput i22 second level. provid numer factor blue region green one, harder achiev goal case. feasibl though, e.g., enforc suffici smaller subdomain size second level compar size on level. analogously, green/blu region pair, solut constrain neumann problem level time solut dirichlet problem (see alg. 5 6), neumann constrain problem second level. point, note blue region turn split commun stage second level mpi task (color grai tabl 3.1). therefore, time spent red region larg enough, commun stage red region turn delai second level mpi task respect level on extent start threaten effect approach. fact, irremedi happen point number mpi task coarsest level kept fix weak scale analysis. order analyz effect, split discuss overlap regions, order infer coarsest level duti harm effect overlap second level mpi tasks. focu second region potenti level-2 idl overlap green/blu area: symbol factor level-3 coars matrix red region dominant, i.e., mask symbol factor graph associ dirichlet constrain neumann problem numer factor constrain neumann problem second blue region, level-2 mpi task wait correspond gather communication. result, red region (level-3) potenti degrad effect second green/blu overlap area. numer factor coarsest matrix (in charg level-3 mpi tasks) mask numer factor triangular solv second blue regions, level-2 task wait correspond gather communication. idl level-2 mpi task occur red region (one spars backward/forward solve) mask spars backward/forward solv blue region. result, second red region (level-3) potenti degrad effect green/blu overlap area. weak scale scenario, subdomain problem size second level fixed. hand, fix number mpi task level, level local problem size increas number subdomain second level. result, maxim scalabl method, interest subdomain coarsen second level aggress possible, keep second level (blue) region mask level (green) ones. strategi allow achiev effect rang core count interest, possibl solut introduc addit level hierarchi amelior growth time spent red regions. stress discuss section appli minor detail mlbddc precondit hierarchi arbitrari number levels. pair intermedi level (1-2, 2-3, 3-4, etc.), deal pair region close green/blu region pair tabl 3.1 (up fact input data structur level mpi task provid fe integr module, instead gather previou level). region intermedi level (includ on level) potenti threaten effect time spent suffici large, sort delay-rippl effect final caus level mpi task have wast time commun stage second level mpi tasks. case, effect coarsest-level duti green/blu region pair regardless number levels. 3.4. estim effect coarsen ratios. section provid comprehens discuss effect choic coarsen ratio govern subdomain aggreg partit differ levels, order overlap multilevel implement fulli effective. 11 depend number level subdomain size level, turn depend coarsen ratio definit hierarch partitions. enter subject, let recal complex main stage direct solut spars linear systems. spars direct methods, provid 3d problem solved, symbol factor cost csfn 4 3 , numer factor cost cnfn 2, triangular solv cost ctsn 4 3 , n size coeffici matrix, constant csf , cnf , ct depend particular algorithms, implementation, underli hardware. let enter subject, consid partit well-balanc levels, i.e., subdomain level (roughly) number elements. denot m` (resp. n`) local (resp. global) number element level `, p` number subdomain level `. note n`+1 = p`, m` = n` p` = p`1 p` = n` n`+1 = ( h`+1 h` )d . weak scale scenario one-level algorithms, global problem size n1 number subdomain p1 level-1 duti increas wai m1 = n1 p1 kept fixed. multilevel algorithm, aim effect precondition term iter counts, requir bound condit number given theorem 2.1 constant. thus, enforc local number element m` = p`1 p` fix level ` = 1, . . . , n`2, involv increas p` accordingly. level n` assum map (or fixed) number mpi tasks. given m` (number local mesh elements, denot element subdomain previou level), actual size local denot `m` (number local dofs), ` affect type dof bddc method ` > 1 order fe space ` = 1. let assum fix number level n` weak scalabl analysis. question aris are: choos coarsen ratio level attain optim scalability? fix hierarch partition, number mpi task overlap strategi stop effective? definit hierarch partit parametr desir number element level, m1, . . . ,mn`1. order answer (qualitatively) questions, let assumptions: 1. bulk cpu time overlap region spent differ phase spars direct solver (symbol factorization, numer factorization, triangular solves). nearest-neighbor commun level 1, commun level ` = 2, . . . , n`, step assum negligible. 2. cost phase spars direct method appli constrain neumann problem assum ident correspond on involv solut dirichlet problem. valid assumpt increas local problem size, mild reason load core (about 256 kb memori core), low cpu cost (and excel scalability) nearest-neighbor commun reduc number core level greater one. second assumpt reasonable, problem size. take account step green/blu region pair tabl 3.1, com- plexiti (see above), previou assumptions, easili infer step level `+ 1 fulli mask level-` step follow over-pessimist condit holds: cnf(`m`) 2 + 2cts(`m`) 4 3 2cnf(`+1m`+1)2 + ncts(`+1m`+1) 4 3 , (3.1) correspond second green/blu region tabl 3.1, n denot maximum number coars dof subdomain. suffici condit fulli mask coars duti level-1 (fine) duties, mean necessary. comment above, weak scale scenario fix number levels, coarsest-grid relat step cpu time increase. scalabl loss affect overal execut time level-1 region mask on level 2. consid effect coarsest level level-1 region (see above), follow addit suffici (over-pessimistic) condit fulli mask coars duti level-1 (fine) duties: 1m1 ( csf cnf ) 1 2 (n`mn`) 2 3 , (1m1) 4 3 (2m2) 4 3 + (n`mn`) 4 3 + ( cnf ct ) (n`mn`) 2, (3.2) 12 correspond second region, respectively. easili observ constraint challeng latter, mask step higher complexity. let consid worst-cas scenario level central singl mpi task. number element level increas weak scale scenario follows: mn` = pn`1 = pn`2 mn`1 = . . . = p1 mn`1 m2 . (3.3) replac (3.3) (3.1) (3.2), indic maximum number core loos parallel efficiency. however, order us formula, need provid valu ratio ct cnf csf cnf . estim constant regress spars direct solver code architectur sect. 5 3d laplacian problem structur cubic mesh (from 4, 096 512, 000 fes), get ct cnf 400 csf cnf 500. larg valu expected, cost flop numer factor low, intens us cach hierarchi mean level 3 blas, triangular solut symbol factor memory-bounded. let consid practic exampl sect. 5. 3d, linear fes, local problem size subdomain m1 = 20 3 fit 1 gb memori avail core. 1 linear fes, 4 bddc(ce) solv laplacian problem. case, take exampl three-level bddc(ce) method m2 = 7 3, condit (3.1) easili hold condit (3.2) combin (3.3) lead p1 . 422, 919 subdomains. level algorithm m1 = 20 3, m2 = 43, m3 = 33 lead p1 . 2, 252, 500. linear elasticity, valu multipli factor three, case lead p1 . 1, 714, 300. bound agreement numer result obtain sect. 5 combinations, indic potenti strategi present far. 4. code implement details. section, sketch, thorough (simplified) fortran95 code snippets, kei hint mpi-parallel implement approach present sect. 3. particular, list 1 sketch initi mlbddc precondition data structur (i.e., type(mlbddc) deriv data type), list 2, numer set-up stage (see alg. 2). stress that, simplic brevity, detail relat softwar engin practic softwar packag (see sect. 5) omitted, e.g., relat algorithm code parameters, error memori handling, data encapsulation. still, expect hint us practition will implement softwar novel techniqu propos herein. let start subroutin list 1. subroutin take input argument set mpi commun handlers. comm world commun handler includ mpi task contribut computation. (in case mpi comm world, conveniently, duplic it.) comm l1 comm lgt1 commun handler (disjoint) subcommun comm world includ mpi task level hierarchy, higher levels, respectively. finally, intcomm l1 lgt1 handler mpi intercommun subcommunicators. intercommun precondition set-up, actual iter solut phase. allow (one the) level mpi task signal (broadcast) higher level task iter process converg (or not). note commun creat outsid subroutine, initi stage simul software, encapsul deriv data type control mpi parallel environment. subroutin code need access object properli dispatch path follow mpi task comm world. finally, nlev, tsk lev dummi argument refer n`, p`, ` = 1, . . . , n`1, respectively, fat map arrai drive subdomain aggreg levels. particular, input root subroutine, fat map(`)=task(`+1, fat(`, i)) mpi task task(`, i). fat map gener preprocess phase, partit fe mesh performed, (recursively) partit graph subdomains. line 24-34 list 1, comm world split subcommun level mpi task provid fat map(1) color, second level ones, rank identifi comm l2. rest task contribut data transfer second level, provid color=mpi undefined. therefore, subcommun creat subset second level mpi task exchang data. (later section cover effici implement data transfers.) prepar recurs call, second level higher level mpi task enter 13 1 recurs subroutin mlbddc_init(comm_world ,comm_l1 ,comm_lgt1 ,intcomm_l1_lgt1 , & 2 nlev ,tsks_x_lev ,fat_map ,m) 3 integ , intent(in) :: comm_world ! world mpi commun 4 integ , intent(in) :: comm_l1 ! 1st lev task mpi comm 5 integ , intent(in) :: comm_lgt1 ! > 1st lev task mpi comm 6 integ , intent(in) :: intcomm_l1_lgt1 ! intercomm l1 <=> lgt1 7 integ , intent(in) :: nlev ! # level mlbddc 8 integ , intent(in) :: tsks_x_lev(nlev -1) ! # mpi task level 9 integ , intent(in) :: fat_map(nlev -1) ! 1st ->2nd map coars fe 10 type(mlbddc), intent(out) :: m ! mlbddc precondition 11 12 ! local variabl declar (ierr , lgt1_rank , etc.) 13 m%comm_world=comm_world 14 m%comm_l1=comm_l1 15 m%comm_lgt1=comm_lgt1 16 m%intcomm_l1_lgt1=intcomm_l1_lgt1 17 m%nlev=nlev 18 19 mpi_comm_get_rank(m%comm_world ,wrank ,ierr) 20 21 ! creat comm_world s subcommun level1 to/from level2 data transfer 22 ! * subcommun creat task comm_l2 (l2_rank =0,1,...) 23 ! * includ l2_rank task comm_l1 transfer data l2_rank 24 (belongs_to(m%comm_l1)) 25 mpi_comm_split(comm_world , color=fat_map (1), key=wrank , & 26 m%comm_l1_to_l2 , ierr) 27 (belongs_to(m%comm_l2)) 28 mpi_comm_get_rank(m%comm_l2 , l2_rank) 29 mpi_comm_split(comm_world , color=l2_rank , key=wrank , & 30 m%comm_l1_to_l2 , ierr) 31 32 mpi_comm_split(comm_world , color=mpi_undefin , key=wrank , & 33 m%comm_l1_to_l2 , ierr) 34 end 35 36 ! recurs init m%p_m_c (i.e., mlbddc precondition coars -grid problem) 37 (nlev >2 .and. belongs_to(comm_lgt1)) 38 ! split comm_lgt1 comm_l2 comm_lgt2 39 mpi_comm_get_rank(m%comm_lgt1 , lgt1_rank , ierr) 40 (lgt1_rank < tsks_x_lev (2)) 41 mpi_comm_split(m%comm_lgt1 , color=1, key=lgt1_rank , m%comm_l2 , ierr) 42 m%comm_lgt2 = mpi_comm_nul 43 44 mpi_comm_split(m%comm_lgt1 , color=2, key=lgt1_rank , m%comm_lgt2 , ierr) 45 m%comm_l2 = mpi_comm_nul 46 end 47 48 ! creat intercomm comm_l2 <=> comm_lgt2 49 (belongs_to(m%comm_l2)) 50 mpi_intercomm_create( m%comm_l2 , loc_lead=0, comm_world , & 51 & rem_lead=tsks_x_lev (2),intcomm_l2_lgt2 ,ierr) 52 53 mpi_intercomm_create( m%comm_lgt2 , loc_lead=0, comm_world , & 54 & rem_lead=0,intcomm_l2_lgt2 ,ierr) 55 end 56 57 ! comm_lgt1 => world_comm , comm_l2 => comm_l1 , comm_lgt2 => comm_lgt1 58 ! intcomm_l2_lgt2 => intcomm_l1_lgt1 59 mlbddc_init(m%comm_lgt1 , m%comm_l2 , m%comm_lgt2 , intcomm_l2_lgt2 , & 60 nlev -1, tsks_x_lev (2:), fat_map (2:), m%p_m_c) 61 end 62 end subroutin mlbddc_init 63 64 function belongs_to(mpi_comm) 65 integ , intent(in) :: mpi_comm 66 logic :: belongs_to 67 68 belongs_to = (mpi_comm /= mpi_comm_null) 69 end function belongs_to list 1 simplifi fortran95 mpi recurs subroutin charg precondition initialization. line 37-61, level exit subroutin search addit level duties. set task split comm lgt1 comm l2 comm lgt2 line 39-46, intercommun comm l2 comm lgt2 creat line 49-55. intercommun requir coarse-grid problem solved, e.g., (n` 1)-level bddc-pcg richardson 14 1 recurs subroutin mlbddc_setup_num(a,m) 2 type(par_matrix), intent(in) :: ! (distribut -memory) matrix 3 type(mlbddc) , intent(inout) :: m ! mlbddc precondition data structur 4 real (8), allocat :: subd_elmat (:,:) ! subdomain contrib coars matrix 5 6 (belongs_to(m%comm_l1)) 7 constrained_neumann_problem_setup(a,m) 8 compute_coarse_grid_basis_vectors(a,m) 9 allocate(subd_elmat(m%nl_coars ,m%nl_coarse)) 10 compute_subd_elmat(a,m,subd_elmat) 11 transfer_assemble_snd(m,subd_elmat) 12 deallocate(subd_elmat) 13 setup_dirichlet_problem(a,m) 14 (belongs_to(m%comm_l2)) 15 transfer_assemble_rcv(m) 16 end 17 18 (m%nlev >2 .and. belongs_to(comm_lgt1)) 19 ! recurs set -up m%p_m_c (i.e., mlbddc precondition coars problem) 20 mlbddc_setup_num(m%p_a_c ,m%p_m_c) 21 (m%nlev == 2 .and. belongs_to(comm_lgt1)) 22 ! set -up m%m_c (serial solver) m%a_c (serial coars -grid matrix) 23 serial_solver_setup_num(m%a_c ,m%m_c) 24 end 25 end subroutin mlbddc_setup_num 26 27 subroutin transfer_assemble_snd(m,subd_elmat) 28 type(mlbddc) , intent(inout) :: m ! mlbddc precondition data structur 29 real (8) , intent(in) :: subd_elmat(m%nl_coars , m%nl_coarse) 30 real (8), allocat :: buf_snd (:) ! messag buffer 31 ! rest local variabl declar (np, ierr , etc.) 32 33 mpi_comm_get_size(m%comm_l1_to_l2 ,np,ierr) 34 35 allocate(buf_snd(m%max_nl_coars **2)) 36 pack(m, subd_elmat , buf_snd) ! pack subd_elmat buf_snd 37 38 ! issu (parallel) global collect 39 mpi_gath ( buf_snd , m%max_nl_coars **2, mpi_double_precis , & 40 rcv_dum , int_rcv_dum , mpi_double_precis , & 41 root=np -1, m%comm_l1_to_l2 , ierr) 42 43 deallocate(buf_snd) 44 end subroutin transfer_assemble_snd 45 46 subroutin transfer_assemble_rcv(m) 47 type(mlbddc) , intent(inout) :: m ! mlbddc precondition data structur 48 real (8), allocat :: buf_snd (:), buf_rcv (:), all_subd_elmat (:) 49 ! rest local variabl declar (np, ierr , etc.) 50 51 mpi_comm_get_size(m%comm_l1_to_l2 ,np,ierr) 52 53 allocate(buf_snd(m%nl_coars **2),buf_rcv(np*m%max_nl_coars **2), & 54 all_subd_elmat(m%sz_all_subd_elmat)) 55 56 ! issu (parallel) global collect 57 mpi_gath ( buf_snd , m%max_nl_coars **2, mpi_double_precis , & 58 buf_rcv , m%max_nl_coars **2, mpi_double_precis , & 59 root=np -1, m%comm_l1_to_l2 , ierr) 60 61 unpack(m, buf_rcv , all_subd_elmat) ! unpack buf_rcv all_subd_elmat 62 63 ! assembl contrib serial (m%a_c) distribut (m%p_a_c) coars matrix 64 (m%nlev > 2) 65 par_mat_ass(np , m%p_coars , m%l_coars , all_subd_elmat , m%p_a_c) 66 ! m%nlev == 2 67 mat_ass(np , m%p_coars , m%l_coars , all_subd_elmat , m%a_c) 68 end 69 70 deallocate(buf_snd ,buf_rcv ,all_subd_elmat) 71 end subroutin transfer_assemble_rcv list 2 simplifi fortran 95 mpi recurs subroutin charg precondition set-up (numeric). iter solver (instead singl applic preconditioner). finally, data structur correspond coarse-grid problem n`1-level bddc precondition (i.e, m%p m c) initi 15 recurs mlbddc init line 60. note recurs set commun (comm lgt1, comm l2, comm lgt2, intcomm l2 lgt2) plai role (comm world, comm l1, comm lgt1, intcomm l1 lgt1), respectively, input recurs call. subroutin charg numer set-up stage mlbddc precondition shown list 2. take input instanc type(par matrix) deriv data type, set instanc m precondition data structure. deriv data type intern accommod distribut spars matrix, includ data describ memori layout conform non- overlap partit subdomains. stage precondition set-up encompass line 6- 16, involv second level mpi tasks. particular, line 7 13 level mpi task set local solver constrain neumann dirichlet problems, respectively, comput coarse-grid basi vector line 8. prepar coarse-grid problem matrix assembly, level mpi task comput subdomain contribut coarse-grid problem store subd elmat; line 10. then, second level mpi task enter transfer assembl snd transfer assembl snd line 11 15, respectively. mean subroutines, second level mpi task gather subdomain contribut (their corresponding) level mpi tasks. contribut assembl line 64-68 list 2. note contribut assembl type(par matrix) instanc m%p c, case intermedi level hierarchy, serial (centralized) spars matrix instanc m%a c, end hierarchi (see line 65 67, respectively). final stage list 2 encompass line 18-24, recurs set up, intermedi level, (n` 1)-bddc precondition second higher level mpi task (see line 20), set up, end hierarchy, serial solver instanc m%m c coarse-grid problem matrix instanc m%a c level mpi task (see line 23). implement inter-level data transfer list 2 deserv attention. sub- commun subset ` ` + 1 level mpi task exchang data creat precondition initialization. set defin have fat(`, i)) fat(`, i)) (their master) permit reus code implement two-level bddc coars solver solv serial singl separ mpi task [5]. data transfer implement line 39 57 wai multipl independ mpi gather oper issu simultan subcommunicators. mean perform analysi tools, confirm implement approach abl effici exploit underli net- work hardwar parallel ibm bg/q supercomputer. particular, commun stage asymptot constant time provid ` ` + 1 level mpi task scale proportion (i.e., weak scale scenario). technic code list 2 exploit fix messag size collectives, instead variabl messag size on (mpi gatherv), actual (variabl size) data sent, pack to/unpack (padded) messag buffer entry/exit mpi gather. actual code provid solutions, observ practic fix message-s collect lead better performance/scal (despit overhead associ padding). finally, like stress map tabl 3.1 three-level bddc-pcg solver (and correspond mlbddc precondition arbitrari number levels) static code software, instead result recurr applic techniques, recurs communicator-awareness, order strateg deploi differ path mpi task resid level. 5. numer experiments. section, studi weak scalabl mlbddc-pcg solver code base implement techniqu present sect. 3 4. model problems, consid laplacian (sect. 5.2) linear elast (sect. 5.3) 3d pde regular domains, discret structur (cartesian) fe meshes. stress, however, algorithm softwar design handl arbitrari geometri unstructur mesh well. perform metrics, focu number pcg iter requir converge, total comput time. experi report section, time includ precondition set-up precondit iter solut linear (2.1). 5.1. experiment framework. novel techniqu propos paper mlbddc- pcg solver implement fempar. fempar, develop member lssc team 16 cimne, parallel hybrid openmp/mpi, object-ori softwar packag massiv parallel finit element (fe) simul multiphys problem govern pdes. features, provid basic tool effici parallel distributed-memori implement substructur dd solver [4]. parallel code fempar heavili us standard comput kernel provid (highly-effici vendor implement of) bla lapack. besides, proper in- terfac parti libraries, local dirichlet constrain neumann problem intermedi level hierarchy, global coarsest-grid problem level, exactli solv spars direct solvers. work, particular explor hsl ma87 [18], provid highly-effici parallel multi-thread dag-bas code implement supernod spars direct choleski solver. nest dissect algorithm avail meti (v5.1.0) [19] fill-in reduc order hsl ma87. fempar releas gnu gpl v3 license, 200k line fortran95/2003/2008 code long. experi report section obtain fermi, locat bologna (italy) cineca, juqueen, locat julich (germany) julich supercomput center (jsc). belong gener ibm blue gene famili supercomputers, so-cal bg/q supercomputers. particular, fermi configur 10-rack system, featur total 10,240 comput nodes, juqueen 28-rack total 28,672 comput nodes. node interconnect extrem low-lat five-dimension (5d) toru interconnect network. comput node equip 16-core, 4-wai hardwar thread core, ibm power pc a2 processor, 16 gbyte sdram-ddr3 memori (i.e., 1gbyte/core), run lightweight proprietari cnk linux kernel. code compil ibm xlf fortran compil bg/q (v14.1) recommend optim flags. custom mpich2 librari avail system message-passing. code link blas/lapack avail single-thread ibm essl librari bg/q (v5.1), hsl ma87 (v2.1.1). 5.2. 3d laplacian results. section studi weak scalabl mlbddc-pcg solver code fempar solut 3d laplacian problem unit cube = [0, 1][0, 1] [0, 1], constant forc term f = 1, homogen dirichlet boundari condit boundari . consid global conform uniform mesh (partition) hexahedra trilinear fe discret (i.e., q1 fes). 3d mesh partit cubic grid p1 1 3 p1 1 3 p1 1 3 cubic subdomains, p1 total number level subdomains. level subdomain size m1 1 3 m1 1 3 m1 1 3 fes. size global fe mesh equal (m1p1) 1 3 (m1p1) 1 3 (m1p1) 1 3 . 5.2.1. sensit precondition robust subdomain aggregation. section study, three-level bddc preconditioner, impact coarsen ratio m2 govern aggreg (of level subdomain second level ones) precondition robustness. measur number iter precondit iter solver take meet converg criteria. figs. 5.1 5.2 plot number pcg iter (y-axis) function p1 (x-axis) three-level bddc(ce) bddc(cef) preconditioners, respectively. set initi solut vector guess x0 = 0 (see line 3 alg. 1), pcg iter stop residu rk1 given iter k satisfi rk12 106r012. set-up appli rest experi paper. kept fix p3 = 1, number subdomain second level scale proportion p1 = m2p2, p2 = k 3, respectively, k = 2, 3, 4, . . ., subject constraint p1+p2+p3 458,762, i.e., number core avail juqueen. consid valu m1 = 10 3, 203, 303 403 size level subdomains, valu m1, number level subdomain second level subdomain vari m2 = 4 3, 83, 123, 143 163 order determin sensit precondition robust m2. y-axi plot fig. 5.2 scale match on fig. 5.1 increas readability. observ figs. 5.1 5.2, expect condit number bound (see theorem 2.1), profil plot asymptot constant number pcg iter (or final be) reach valu p1. sensit robust three-level bddc(ce) precondition respect m2 observ fig. 5.1; lower impact observ fig. 5.2 three-level bddc(cef) preconditioner. gener trend small 17 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n s p1 weak scale 3-level bddc(ce) solver m1=10 3 (1k fes/core) m2=4 3 m2=8 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n s p1 weak scale 3-level bddc(ce) solver m1=20 3 (8k fes/core) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n s p1 weak scale 3-level bddc(ce) solver m1=30 3 (27k fes/core) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n s p1 weak scale 3-level bddc(ce) solver m1=40 3 (64k fes/core) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 fig. 5.1. sensit number three-level bddc(ce)-pcg solver iter m2 = 4 3, 83, 123, 143 163. bottom, left right: m1 = 10 3, 203, 303 403, respectively. valu p1 that, fix p1 m1, larger valu m2, smaller number pcg iterations. example, p1 =110k, m1 = 40 3, number pcg iter 44, 39, 33, 27, m2 = 4 3, 83, 123 163, respect (see fig. 5.1). counter-intuit observ (with respect condit number bounds) explain fact two-level bddc precondition robust (coarse-grid) linear get favour small second level subdomain grid small p1 larg m2. however, m2 impact fast asymptot regim reached, asymptot con- stant number pcg iter reached. particular, fix m1, larger valu m2, slower take reach asymptot constant number iterations, higher asymptot number iter is. justifi crossov observ plot correspond combin m2 (e.g., m2 = 4 3 m2 = 8 3 left corner fig. 5.1). case, balanc reach crossov achiev cases, achiev larg p1, rang core count interest. fact posit (unintended) effect perform strategi suggest sect. 3.3, i.e., maxim m2 keep blue region green on tabl 3.1. finally, compar plot figs. 5.1 on 5.2 observ three-level bddc(cef), apart sensit choic m2, robust three-level bddc(ce) preconditioner, take iter converge, special larg valu m1. enhanc robust price heavier second level coarse-grid problems. sect. 5.2.3, evalu extent increas absorb mean inter-level overlapping. 5.2.2. scalabl low load fermi. section studi weak scalabl mlbddc-pcg solver code fermi small level subdomain sizes, particular, m1 = 10 3 (1k) 153 (3.4k) fes. valu m1, memori consumpt core mpi 18 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n s p1 weak scale 3-level bddc(cef) solver m1=10 3 (1k fes/core) m2=4 3 m2=8 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n s p1 weak scale 3-level bddc(cef) solver m1=20 3 (8k fes/core) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n s p1 weak scale 3-level bddc(cef) solver m1=30 3 (27k fes/core) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n s p1 weak scale 3-level bddc(cef) solver m1=40 3 (64k fes/core) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 fig. 5.2. sensit number three-level bddc(cef)-pcg solver iter m2 = 4 3, 83, 123, 143 163. bottom, left right: m1 = 10 3, 203, 303 403, respectively. task level moder compar 1gb avail core fermi, particular 19.7 29.9 mb, respectively. hand, time spent green area tabl 3.1 moderate, pose challeng effect approach propos paper (see sect. 3.3). figs. 5.3(a) (b) show, 64k fermi cores, weak scalabl total comput time number pcg iterations, respectively, three-level (solid lines) four-level (dash lines) bddc(ce) (left-hand side) bddc(cef) (right-hand side) solvers, m1 = 10 3 (color black), m1 = 15 3 (color blue). consid differ set-up three-level bddc preconditioner, correspond m2 = 4 3, m2 = 6 3, respectively, singl four-level bddc, particular, m2 = 3 3, m3 = 3 3. three-level bddc, kept fix p3 = 1, number subdomain second level scale proportion p1 = m2p2, p2 = k 3, respectively, k = 2, 3, 4, . . .. example, point plot correspond m2 = 6 3 (see, e.g, curv unfil squar right-hand fig. 5.3(b)), obtain k = 2, 3, 4, 5 6, respectively. four-level bddc, number first, second, level subdomain scale proportion p1 = m2p2, p2 = m3p3, p3 = k 3, k = 2, 3, 4, point four-level bddc plots. result fig. 5.3 clearli reveal differ scenario balanc achiev time spent color region tabl 3.1. first, three-level bddc m2 = 4 3, heavi level reveal (i.e., time spent red area table). example, focu scalabl three-level bddc(ce) solver m1 = 10 3 left-hand fig. 5.3(a), scalabl start (significantly) degrad p1 = 4k cores; degrad sever three-level bddc(cef) solver (see right-hand figure), heavier coarsest-grid problem. order (try to) rid this, aggress aggreg level subdomain second level ones, is, consid larger valu m2 = 6 3, lead second interest scenario fig. 5.3. choic m2, reduc size coarsest- 19 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 3-lev m1=10 3 m2=4 3 3-lev m1=10 3 m2=6 3 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 weak scale mlbddc(ce) solver 3-lev m1=15 3 m2=4 3 3-lev m1=15 3 m2=6 3 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 4-lev m1=10 3 m2=3 3 m3=3 3 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 weak scale mlbddc(cef) solver 4-lev m1=20 3 m2=3 3 m3=3 3 (a) total comput time (secs.). 0 5 10 15 20 25 30 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 weak scale mlbddc(ce) solver 3-lev m1=10 3 m2=4 3 3-lev m1=10 3 m2=6 3 4-lev m1=10 3 m2=3 3 m3=3 3 3-lev m1=15 3 m2=4 3 3-lev m1=15 3 m2=6 3 4-lev m1=15 3 m2=3 3 m3=3 3 0 5 10 15 20 25 30 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 weak scale mlbddc(cef) solver 3-lev m1=10 3 m2=4 3 3-lev m1=10 3 m2=6 3 4-lev m1=10 3 m2=3 3 m3=3 3 3-lev m1=15 3 m2=4 3 3-lev m1=15 3 m2=6 3 4-lev m1=15 3 m2=3 3 m3=3 3 (b) number pcg iterations. fig. 5.3. weak scalabl total comput time (a) number pcg iter (b) mlbddc(ce) (left) mlbddc(cef) (right) solver solut 3d laplacian pde m1 = 10 3 153 fe fermi. grid problem (and time spent red area tabl 3.1), increas size second level subdomains. particular, observ fig. 5.3(a) m1 = 10 3, extent heavi second level revealed. confirm (initially) higher comput time three-level bddc solver m2 = 6 3 compar m2 = 4 3. stress that, scenario asymptot constant comput time reach (up 64k cores), lot comput resourc wast (and parallel energi efficiency), first-level core wait (for second-level ones) commun stage levels. final (desirable) scenario correspond four-level bddc method, reveal balanc effect achieved. small p1, comput time four-level bddc method compar three-level bddc method m2 = 4 3 (actual smaller smaller m2 = 3 3), case level duti mask coarser-grid duti (i.e., green area tabl 3.1 dominate). scale p1, four-level bddc maintain desir balanc (within rang core count studied), cut time spent red area introduct addit level hierarchy. 5.2.3. scalabl medium high load juqueen. section studi weak scalabl mlbddc-pcg solver code juqueen 28-rack ibm bg/q system, larger level subdomain size consid sect. 5.2.2, particular, m1 = 20 3 (8k), 253 (15.6k), 303 (27k), 403 (64k) fes. memori consumpt level mpi task case 80, 146, 233, 651mb, respectively. figs. 5.4(a) (b) show, juqueen (i.e., 458,762 cores), weak scalabl total comput time number pcg iterations, respectively, three-level (solid lines) four-level (dash lines) bddc(ce) (left-hand side) bddc(cef) (right-hand side) solvers, m1 = 20 3 (color black), 253 (color blue), 303 (color red), 403 (color green) 20 fes. result four-level bddc report refer valu m1. consid m2 = 7 3 three-level bddc preconditioner, m2 = 4 3, m3 = 3 3 four-level bddc one. number subdomain level proportion scale sect. 5.2.2, k 6 11, respectively. 0 5 10 15 20 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 30 40 50 60 70 80 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale mlbddc(ce) solver 3-lev m1=40 3 m2=7 3 0 5 10 15 20 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 30 40 50 60 70 80 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale mlbddc(cef) solver 3-lev m1=40 3 m2=7 3 (a) total comput time (secs.). 0 10 20 30 40 50 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale mlbddc(ce) solver 3-lev m1=20 3 m2=7 3 4-lev m1=20 3 m2=3 3 m3=3 3 3-lev m1=25 3 m2=7 3 4-lev m1=25 3 m2=3 3 m3=3 3 3-lev m1=30 3 m2=7 3 3-lev m1=40 3 m2=7 3 0 10 20 30 40 50 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale mlbddc(cef) solver 3-lev m1=20 3 m2=7 3 4-lev m1=20 3 m2=3 3 m3=3 3 3-lev m1=25 3 m2=7 3 4-lev m1=25 3 m2=3 3 m3=3 3 3-lev m1=30 3 m2=7 3 3-lev m1=40 3 m2=7 3 (b) number pcg iterations. fig. 5.4. weak scalabl total comput time (a) number pcg iter (b) mlbddc(ce) (left) mlbddc(cef) (right) solver solut 3d laplacian pde m1 = 20 3, 253, 303 403 fe juqueen. remark scalabl observ fig. 5.4(a) three-level bddc-pcg solver 28-rack ibm bg/q system, practic demonstr tremend potenti algorithm code subject study. valu m1 considered, time spent green region tabl 3.1 suffici larg extent effect obtain three-level method rang core count interest. words, balanc struck level duti complet absorb latenc associ coarser-grid levels. fulfil bddc(ce), bddc(cef) preconditioner, despit addit work incur coarser-grid level introduct face constraint bddc space. readili observ fig. 5.4 fact increas robust bddc(cef) (i.e., number pcg iter fig. 5.4(b)) immedi translat lower comput time (compar plot left right-hand fig. 5.4(a)). observ confirm evid comput time four-level bddc precondition close three- level bddc precondition (and three-level bddc method suffic control growth time spent coarsest-grid level). 5.2.4. rise challenge: extra concurr overdecomposition. section studi weak scalabl mlbddc-pcg solver code partit problem subdomain physic core involv parallel comput (i.e., overdecomposit problem hand). core ibm bg/q supercomput requir instruct 21 issu cycl differ hardwar thread order fulli instruct pipelin [17] (and achiev peak flop performance). therefore, mpi task physic core possibl improv aggreg effici parallel computation. particular explor section 4 mpi tasks/core. (we perform experi 2 mpi tasks/core, result 4 mpi tasks/cor confirm higher profit hardwar thread term aggreg efficiency.) however, usag techniqu significantli challeng scalabl algorithm/code/hardwar combination. particular, 4 mpi tasks/cor impli moder memori 256mb/mpi task, 4-fold increas coarse-grid problem size solv level hierarchy. challeng is, however, align current/and futur hpc trend have concurr memori core. besides, physic core respons comput commun differ subdomains. order cope smaller load core, larger coarse-grid problems, consid four-level bddc precondition cope growth time spent coarsest-grid level. figs. 5.5(a) (b) report weak scalabl total comput time number pcg iterations, respectively, four-level bddc(ce) bddc(cef) solvers, m1 = 10 3 (color black), 203 (color blue), 253 (color red) fes. consid m2 = 4 3, m3 = 3 3, scale number first, second, level mpi task proportion p1 = m2p2, p2 = m3p3, p3 = k 3, k = 2, 3, 4, . . . , 10. result number task map 4 time core (i.e., 4 mpi tasks/core). therefore, largest valu k, map 1.73m level subdomain 448.3k cores. 0 1 2 3 4 5 6 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k p1 m1=10 3 m1=20 3 m1=25 3 10 15 20 25 30 35 40 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k weak scale 4-level bddc(ce) solver m2=4 3, m3=3 3 #core m1=10 3 m1=20 3 m1=25 3 0 1 2 3 4 5 6 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k p1 m1=10 3 m1=20 3 m1=25 3 10 15 20 25 30 35 40 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k weak scale 4-level bddc(cef) solver m2=4 3, m3=3 3 #core m1=10 3 m1=20 3 m1=25 3 (a) total comput time (secs.). 0 10 20 30 40 50 60 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k p1 weak scale 4-level bddc(ce) solver m2=4 3, m3=3 3 #core m1=10 3 m1=20 3 m1=25 3 0 10 20 30 40 50 60 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k p1 weak scale 4-level bddc(cef) solver m2=4 3, m3=3 3 #core m1=10 3 m1=20 3 m1=25 3 (b) number pcg iterations. fig. 5.5. weak scalabl total comput time (a) number pcg iter (b) four-level bddc(ce) (left) bddc(cef) (right) solver solut 3d laplacian pde m1 = 10 3, 203 253 fe juqueen 4 mpi tasks/core. restrict inher ibm bg/q supercomput software/hardwar stack, 4 mpi tasks/phys core (i.e., 4 subdomain handl physic core). 22 observ fig. 5.5, remark scalabl achiev approach despit 4-fold increas number subdomains. particular, absorpt coarse-grid duti achiev mean interlevel-overlap combin precondition m1, four-level bddc(cef) m1 = 10 3, mild degrad scalabl achiev p1 = 592.7k subdomains. stress actual fine-tun valu m2 m3, number level experiment. larger valu m3 (e.g. 4 3), level, improv situat particular case. apart confirm remark scalability, compared, smaller test cases, comput time code 16 64 mpi tasks/nod (with number mpi tasks/level cases), confirm approxim 50% save aggreg effici exploit hardwar multi-thread (i.e., comput time 64 mpi tasks/nod approxim twice 16 mpi tasks/node). 5.3. 3d linear elast results. section evalu weak scalabl mlbddc-pcg code appli vector-valu problem, q1 fe approxim (compressible) 3d linear elast pde (with second lame paramet equal 1 1 10 , respectively) unit cube = [0, 1][0, 1][0, 1], constant forc term f = 1, homogen dirichlet boundari condit boundary. experi set-up select three-level bddc precondition sect. 5.2.3 consid here, size level subdomains, set-up m1 = 15 3 (3.4k), 203 (8k), 253 (15.6k) fes. largest subdomain size set smaller largest consid 3d laplacian problem sect. 5.2.3 (i.e., m1 = 40 3). linear elast problem vector-valu problem 3 unknown fe mesh node. impli that, given fe mesh, size discret oper 3 time larger laplacian problem, 9 time nonzero entries. indeed, m1 = 25 3 memori consumpt level mpi task 713mb, compar 146mb valu m1 case 3d laplacian problem. fig. 5.6(a) (b) report, 28-rack ibm bg/q system, weak scalabl total comput time number pcg iterations, respectively, three-level bddc(ce) bddc(cef) solvers, m1 = 10 3 (color black), 203 (color blue), 253 (color red) fes. evidenc fig. 5.6, approach pursu paper abl obtain remark scalabl computation intens problem linear elast problem. provid suffici larg m1, i.e., 20 3 253 fe bddc(ce) bddc(cef), respectively, mlbddc hierarchi equip 3 level suffici fulli overlap coarser-grid duti rang core avail juqueen supercomputer. smaller valu m1, (mild) degrad scalabl observ point, e.g., 153, p1 = 175.6k level subdomains, requir addit level perfect scalability. 6. conclus futur work. article, present highli scalabl parallel implement exact mlbddc methods, i.e., base spars direct solver local (and coarsest) problems. propos implement base recurs communicator-awareness, order strateg deploi mpi task duti level overlap interlevel tasks, base fact fine coars correct level mlbbdc method comput parallel. result mpmd bulk-asynchron implement reduc synchron cores, overlap communications/comput differ levels. recurs implementation, arbitrari number levels. implement lead close perfect weak scalabl result far (embarrassingli parallel) level-1 duti mask task higher levels. provid model help choos effect coarsen ratio core indic overlap strategi start loos effectiveness. case, main motiv work attain excel weak scalabl reduc drastic aggreg idling, interlevel-overlap implementation. turn improv parallel efficiency, energi awareness, reduc time-to-solution. detail scalabl analysi carri propos implement algo- rithms, reach juqueen blue gene/q, 458,752 core 1.8 million mpi task (subdomains), laplacian linear elast problems. largest scale problem 23 0 10 20 30 40 50 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 3-lev m1=15 3 m2=7 3 3-lev m1=20 3 m2=7 3 60 80 100 120 140 160 180 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale 3-lev bddc(ce) solver 3-lev m1=25 3 m2=7 3 0 10 20 30 40 50 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 3-lev m1=15 3 m2=7 3 3-lev m1=20 3 m2=7 3 3-lev m1=25 3 m2=7 3 60 80 100 120 140 160 180 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale 3-lev bddc(cef) solver (a) 0 10 20 30 40 50 60 70 80 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale 3-lev bddc(ce) solver 3-lev m1=15 3 m2=7 3 3-lev m1=20 3 m2=7 3 3-lev m1=25 3 m2=7 3 0 10 20 30 40 50 60 70 80 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale 3-lev bddc(cef) solver 3-lev m1=15 3 m2=7 3 3-lev m1=20 3 m2=7 3 3-lev m1=25 3 m2=7 3 (b) fig. 5.6. weak scalabl total comput time (a) number pcg iter (b) three-level bddc(ce) (left) bddc(cef) (right) solver solut 3d linear elast pde m1 = 10 3, 203 253 fe juqueen. report far exact dd preconditioners. three-level four-level algorithm ex- plored. order stress propos implementation, (1) coarsest-level problem solv processor, i.e., parallel exploit level, (2) consid overdecomposition, i.e., us partit problem subdomain physic core in- volv parallel computation, reach 1.8m subdomains. result close perfect weak scalabl low moder local problem sizes, i.e., infra-util memori resourc core. us level distribut comput coarsest problem thousand-fold increas number core expect near future. futur work includ extens framework krylov-cycl mlbddc methods, order increas precondition robust constant number iter levels, develop inexact mlbddc (hybrid amg-bddc) algorithm implementa- tions. final note implement approach applic preconditioners, like bpx (addit mg) algorithm [9], order improv parallel efficiency, time-to-solution, weak scalability. references. [1] report workshop extreme-scal solvers: transit futur architectures, u.s. depart energy, 2012. [2] p.r. amestoy, i.s. duff, j.-y. lexcellent, multifront parallel distribut symmetr unsymmetr solvers, method appli mechan engin 184 (2000), no. 24, 501520. [3] s. badia, a. f. martn, j. principe, enhanc balanc neumann-neumann precondit comput fluid solid mechanics, intern journal numer method engin 96 (2013), no. 4, 203230. [4] , implement scalabl analysi balanc domain decomposit methods, archiv computa- tional method engin 20 (2013), no. 3, 239262. 24 [5] , highli scalabl parallel implement balanc domain decomposit constraints, siam journal scientif comput (2014), c190c218. [6] , overlap coarse/fin implement balanc domain decomposit inexact solvers, sub- mit (2014). [7] a. h. baker, t. gamblin, m. schulz, u. m. yang, challeng scale algebra multigrid modern multicor architectures, parallel distribut process symposium (ipdps), 2011 ieee international, 2011, pp. 275 286. [8] s. balay, j. brown, k. buschelman, w. d. gropp, d. kaushik, m. g. knepley, l. c. mcinnes, b. f. smith, h. zhang, petsc web page, 2012. [9] j. h. bramble, j. e. pasciak, j. xu, parallel multilevel preconditioners, mathemat comput 55 (1990), no. 191, 122. [10] s. c. brenner r. scott, mathemat theori finit element methods, 3rd edition, springer, 2010. [11] d. brommel p. gibbon, high-q club highest scale code juqueen, innov supercomput deutschland 11 (2013), no. 2, 106107. [12] t. a. davis, direct method spars linear systems, vol. 2, siam, 2006. [13] c. r. dohrmann, precondition substructur base constrain energi minimization, siam journal scientif comput 25 (2003), no. 1, 246258. [14] , approxim bddc preconditioner, numer linear algebra applic 14 (2007), no. 2, 149168 (en). [15] c. farhat, k. pierson, m. lesoinne, second gener feti method applic parallel solut large-scal linear geometr non-linear structur analysi problems, method appli mechan engin 184 (2000), no. 24, 333374. [16] v. hapla, d. horak, m. merta, us direct solver tfeti massiv parallel implementation, appli parallel scientif computing, 2013, pp. 192205. [17] r.a. haring, m. ohmacht, t.w. fox, m.k. gschwind, d.l. satterfield, k. sugavanam, p.w. coteus, p. heidelberger, m.a. blumrich, r.w. wisniewski, a. gara, g.l.-t. chiu, p.a. boyle, n.h. chist, c. kim, ibm blue gene/q comput chip, ieee micro 32 (2012), no. 2, 4860. [18] j. hogg, j. reid, j. scott, design multicor spars choleski factor dags, siam journal scientif comput 32 (2010), no. 6, 36273649. [19] g. karypis, softwar packag partit unstructur graphs, partit meshes, comput fill-reduc order spars matrices. version 5.1.0, univers minnesota, depart scienc engi- neering, minneapolis, mn, 2013. avail [20] a. klawonn o. rheinbach, highli scalabl parallel domain decomposit method applic biome- chanics, zamm - journal appli mathemat mechan 90 (2010), no. 1, 532 (en). [21] a. klawonn o. b. widlund, domain decomposit method lagrang multipli inexact solver linear elasticity, siam journal scientif comput 22 (2000), no. 4, 11991219. [22] p. t. lin, j. n. shadid, m. sala, r. s. tuminaro, g. l. hennigan, r. j. hoekstra, perform parallel alge- braic multilevel precondition stabil finit element semiconductor devic modeling, journal comput physic 228 (2009), no. 17, 62506267. [23] j. mandel, balanc domain decomposition, commun numer method engin 9 (1993), no. 3, 233241. [24] j. mandel c. r. dohrmann, converg balanc domain decomposit constraint energi mini- mization, numer linear algebra applic 10 (2003), no. 7, 639659. [25] j. mandel, b. sousedk, c. dohrmann, multispac multilevel bddc, comput 83 (2008), no. 2, 5585. [26] o. rheinbach, parallel iter substructur structur mechanics, archiv comput method en- gineer 16 (2009), no. 4, 425463 (en). [27] y. saad, iter method spars linear systems, 2nd ed., siam, 2003. [28] o. schenk k. gartner, fast factor pivot method spars symmetr indefinit systems, electron transact numer analysi 23 (2006), 158179. [29] b. sousedk, j. sstek, j. mandel, adaptive-multilevel bddc parallel implementation, comput 95 (2013), no. 12, 10871119. [30] k. stuben, review algebra multigrid, journal comput appli mathemat 128 (2001), no. 12, 281 309. [31] a. toselli o. widlund, domain decomposit method - algorithm theori (r. bank, r. l. graham, j. stoer, r. varga, h. yserentant, eds.), springer-verlag, 2005. [32] x. tu, three-level bddc dimensions, siam journal scientif comput 29 (2007), no. 4, 17591780. [33] j. sstek, m. certkova, p. burda, j. novotny, face-bas select corner 3d substructuring, mathemat comput simul 82 (2012), no. 10, 17991811. 25