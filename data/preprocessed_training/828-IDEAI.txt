real-tim human-robot interact base gestur assist scenario real-tim human-robot interact base gestur assist scenario gerard canala,b,d,, sergio escalerac,a, cecilio angulob acomput vision center, campu uab, edifici o, 08193 bellaterra (cerdanyola), barcelona, spain bdept. automat control, upc - barcelonatech, fme building, pau gargallo 5, 08028 barcelona, spain cdept. matematica aplicada analisi, ub, gran le cort catalan 585, 08007 barcelona, spain dinstitut robotica informatica industrial, csic-upc, lloren artiga 4-6, 08028 barcelona, spain abstract natur intuit human interact robot system kei point develop robot assist peopl easi effect way. paper, human robot interact (hri) abl recogn gestur usual emploi human non-verb commun introduced, in-depth studi usabl performed. deal dynam gestur wave nod recogn dynam time warp approach base gestur specif featur comput depth maps. static gestur consist point object recognized. point locat estim order detect candid object user refer to. point object unclear robot, disambigu procedur mean verbal gestur dialogu performed. skill lead robot pick object behalf user, present difficulti itself. overal compos nao wifibot robots, kinecttm v2 sensor laptop firstli evalu structur lab setup. then, broad set user test completed, allow assess correct perform term recognit rates, easi us respons times. keywords: gestur recognition, human robot interaction, dynam time warping, point locat estim correspond author. email addresses: (gerard canal), (sergio escalera), (cecilio angulo) preprint submit vision imag understand april 1, 2016 1. introduct autonom robot make wai human inhabit environ- ment home workplaces: entertainment, help user domest activ daili living, help disabl peopl person care basic activities, improv autonomi qualiti life. order deploi robot system inhabit unstructur social spaces, robot endow commun skill user in- teract intuit do, eventu consid minim training. besides, given great human commun carri mean non-verb channel [1, 2], skill like gestur recogni- tion human behavior analysi reveal us kind robot systems, includ view understand surround human inhabit them. gestur recognit activ field research vision benefit machin learn algorithms, tempor warp [3, 4, 5], hidden markov model (hmms), support vector machin (svms) [6], random forest classifi [7] deep learn [8], mention them. moreover, gestur recognit person techniqu propos [9] adapt given user. studi human interact (hci) specif human robot interact (hri) advantag field. hence, recent contribut [10, 11, 12, 13, 14] consid kinecttm-lik sensor recogn gestur given discrimin inform provid multi-mod rgb-depth data. kinecttm base ap- plicat introduc [15] take order servic elderli care robot. static bodi postur analyz assist robot [16] detect user open robot interact not. commun gestur contrast daili live activ [17] intuit human robot inter- action. novic user gener his/her gestur librari semi-supervis wai [18], recogn non-parametr stochast seg- mentat algorithm. [19], user defin specif gestur mean messag human-robot dialogue, [20] framework defin user gestur control robot presented. deep neural network [21] recogn gestur real time consid rgb information. point gestures, similar propos paper, stud- i focus hand gestur [22], hand orient face pose [23]. point direct estim [24, 25] gaze finger orientation, deictic gestur interact peopl us refer object environ studi [26]. relat point interact robot guidanc [27]. work introduc real time human robot interact (hri) object allow user commun robot easy, natur intuit gesture-bas fashion. experiment setup compos humanoid robot (aldebaran nao) wheel platform (wifibot) carri nao humanoid kinecttm sensor. set-up, multi- robot abl recogn static dynam gestur human 2 base geometr featur extract biometr inform dynam program techniques. gestur understand deictic visual indic user, robot assist him/her task pick object floor bring user. order valid extract robust conclus interact behavior, propos test offlin experiments, report high recognit rates, extens set user test 67 peopl assess performance. remaind paper organ follows: section 2 introduc method gestur recognit human robot interaction. section 3 present experiment result includ offlin user test and, finally, section 4 conclud paper. 2. gestur base human robot interact aim studi gestur commun hri, robot develop abl understand differ gestur human user interact it: wave (hand rais move left right), point (with outstretch arm), head shake (for express disagreement) nod (head gestur agreement). overal robot involv elements: aldebaran nao robot, small size humanoid robot suitabl interact hu- man users; microsoft kinecttm v2 sensor rgb-depth visual data environ track user; and, given vision sensor ex- ceed nao robot capabl (in size comput performance), nexter robot wifibot wheel platform carri sensor nao, eas navig precis long ranges. fact, propos robot take inspir darpa robot challeng 20151 humanoid robot drive car place exit car order finish work foot. similar way, wheel robot ad order carri sensor littl humanoid, exit complet task walking. multi-robot setup allow nao us inform kinectstm v2 sensor eas navigation. side, nao charg directli interact user, abl act environment, instance, grasp objects. overal setup shown figur 1, nao seat wifibot. setup includ laptop intel i5 processor deal kinecttm data intel core 2 duo laptop, send command robot robot oper (ros)2 [28]. depth map process point cloud librari (pcl)3 [29], bodi track inform obtain 1theroboticschallenge.org 2ros.org 3pointclouds.org 3 kinecttm v2 sdk. figur 1: robot design work. program interact application, test user differ ag relat robot world (see section 3.2). 2.1. real time gestur recognition: interact robot section explain method perform gestur recognit imag understanding. given applic enhanc interact human user robot, defin gestur natur user possible, avoid user train learn specif set gestures. instead, robot understand gestur human understand human gestures, repli visual stimulu real time. consid set human gestur divid categories, depend movement involv execution: static gestur user place his/her limb specif posit stand while, dynam movement in- volved. case, transmit inform obtain static pose configuration. point object exampl static gesture. dynam gestur are, contrast, movement main gestur feature. transmit inform come type 4 movement execut velocity. contain partic- ular pose limb movement. exampl dynam gestur wave salut gestur hand ask approach user location. differ gestur includ design interact robot, dynam remain static. dynam gestur wave, nod facial negat gesture. static point object. categori tackl differ approaches. extract features, gestur recognit method gestur semant inform extracted. 2.1.1. definit gestur specif featur gestur recognit perform base featur extract user bodi inform obtain depth maps. includ arm gestur possibl new gestur involv bodi parts, skelet data obtain depth imag kinecttm sensor kinecttm sdk v2.0. given limb gestur wave depend posit part bodi legs, rest bodi taken consider recognit performed. so, directli joint coordin body, [4, 30], propos method take account involv limb distinct featur extracted. approach allow recogn gestur time skelet data properli track sensor, includ situat sit (for instanc person wheelchair), stand crouching. applic abl recogn gestures: point at, wave, nod head negation. point gestur featur skeleton displai figur 2a. describ as: p, euclidean distanc hand hip joint bodi part. featur discrimin point posit rest arm outstretch side bodi point place. p, elbow joint angle, defin angl vector elbow joint shoulder vector elbow hand joint. defin arm outstretched. p, posit hand joint. given present setup overal structur robot system, featur account larg point gestur (with arm ex- tended), on us point lai ground. featur dynam wave gestur shown figur 2b. defin as: 5 w, euclidean distanc neck hand joints. necessari order perform test current set gestures, measur normal divid longitud arm standard valu rang [0, 1] handl bodi variations. w, elbow joint angle, defin point gesture. elbow angl featur requir normal- izat affect differ bodi heights. (a) point gestur fea- tures. (b) wave gestur featur dynamics. figur 2: skelet gestur features. orient face provid sensor nod gestur (vertic movement head) negat (horizont movement head). usual angular ax pitch, roll yaw instead take absolut values, deriv emploi frame features, oi,a = oi,aoi1,a, oi,a orient degre face frame accord axis. moreover, f frame comput featur filter noisi orient estimations, valu threshold given valu d order end sequenc direct changes. formally, featur frame axi a, fi,a, comput as: fi,a = (|oi,a| d) sign(oi,a). (1) figur 3 depict facial gestures. 2.1.2. dynam gestur recognit dynam time warp (dtw) [31] approach detect dy- namic gestures. dtw algorithm match tempor sequenc find 6 figur 3: facial gestur featur dynamics. vertic arrow repres nod gestur horizont on negation. minimum align cost them. sequenc refer gestur model gestur g, rg = {r1, . . . , rm}, in- stream s = {s1, . . . , s}, ri si featur vectors. featur depend gestur recognized: wave, ri = {wi , w } ri = {fi,pitch, fi,roll, fi,yaw} facial gestures. sequenc align mean comput m n dynam program matrix m, n length tempor window discret infi- nite time, data keep enter gestur identified. provid gestur spot needed, minimum valu n two. element mi,j m repres distanc subsequ {r1, . . . , ri} {s1, . . . , sj}, comput as: mi,j = d(ri, sj) +min(mi,j1,mi1,j ,mi1,j1), (2) d(, ) distanc metric choice. differ distanc metric implementation. instance, ham distance: dh(ri, sj) = o k=0 {rki 6= s k j }, (3) o number featur gesture, facial gestur case. weight l1 distanc emploi case wave gesture, comput as: dl1(ri, sj) = o k=0 k|rki s k j |, (4) k posit weight constant. 7 gestur g consid recogn subsequ input data stream s similar refer sequenc rg: mm,k g, k, (5) g obtain train method gestur g, detail section 3.1.1. order assur fulfil real time constraint, dtw execut multi-thread wai differ gestur spread be- tween differ thread run gestur recognit method simultaneously, stop case method find gestur input sequence. case need properli segment gestur begin-end manner, valid purposes, warp path locat begin gestur sequence. warp path: w = {w1, . . . ,wt }, (6) max(m,n) t < m+ n+ 1, matrix pair index contigu element matrix m defin map refer ges- ture rg subsequ input sequenc s, subject follow constraints: w1 = (1, j) wt = (m, j). wt1 = (a, b) wt = (a, b) 1 b b 1. warp path w minim warp cost: cw(m) = min ww 1t t t=1 mwt , (7) matrix m backtrack minimum path mm,j , m1,k, k start point segment gestur j end it. 2.1.3. static gestur recognit static approach select static gestur recognition, sens gestur consid recogn featur certain valu given number contigu frame small movement involved. number frame featur threshold obtain similar train method dynam case. case, point gestur recogn when, certain number frame f , elbow angl greater threshold tea indic arm outstretch distanc hand hip greater certain distanc td mean arm rest position. moreover, hand coordin order check constraint 8 posit hold moving. is, gestur recogn follow constraint held fp frames: p > td, p > tea, de( p , p i1) 0, (8) repres euclidean distance. run static gestur recognit parallel dynam one, multi-thread way. 2.1.4. point locat estim point gestur recognized, inform need extract order perform associ task help user. main inform deictic gestur give point location, region surround space element user. estim it, floor plane description, point direct coordin belong ground needed. all, arm posit obtain order know point direction. so, arm joint frame gestur averag obtain mean direct avoid track errors. then, coordin hand joint h elbow joint e point direct eh = h e vector. kinecttm v2 sensor provid inform hand tip joint, direct provid elbow hand vector prove precis hand hand tip preliminari tests. ground plane extract plane estim method pcl librari [32]. depth imag kinecttm obtain convert point cloud, plane segment random sampl consensu (ransac) method [33]. plane similar orthogon vector refer calibr plane floor planes. refer plane automat obtain start segment plane depth imag keep paramet plane orthogon vector vertic axi (y axis) sensor. case camera parallel posit ground plane fulfil condition, refer plane obtain user click point ground graphic interface, plane estimated. then, ground point coordin obtain pick element floor cloud. therefore, let pf ground point ~nf = (a,b,c) orthogon vector floor plane f = ax+by+cz+d = 0, point point pp obtain by: pp = h + (pf h) ~nf eh ~nf eh. (9) exampl point locat estim shown figur 4a. test users, observ bone correctli track kinecttm sensor precis accur 9 (a) point locat esti- mation. (b) exampl user point deviation. figur 4: exampl point gesture. point direction. clear point gestur per- form hand body. also, user tend actual point farther object location, real point line inter- sect objects, observ figur 4b. order deal imprecision, correct point posit slightli translat point locat backwards. 2.1.5. near point object segment disambigu similar human respons point gesture, want robot look surround estim point locat detect possibl object user refer to. notic case care recogn actual object detect presence. perform extract set point x scene point cloud xi x select euclidean distanc point point smaller certain valu r, de(xi, pp) r, x spheric point cloud radiu r center point point pp. extract floor plane, z = x \ {xi | xi f}, object isol cluster algorithm appli sub point cloud z order join point object smaller point cloud object. cluster algorithm euclidean cluster extract method [32], start cluster pick point zi z join neighbor zj z de(zi, zj) < dth, dth user defin threshold. process repeat neighbor 10 point found, case cluster ci obtained. remain point cloud z process wai object clusters. object found, centroid point comput mean coordin point cluster, 1|ci| zci z, cluster convex hull reconstruct order comput area. allow notion posit space size (see figur 4a). however, case point locat clearli near singl object, doubt refer one. situat arises, spoken disambigu process start robot ask user object. so, robot ask person point biggest object object clearli differ sizes, ask rel position, instanc ask question likei object right?. user respond question ye utterance, recogn nao built speech recognit software, perform equival facial gestures, robot know refer object them, ask question case dubiou object sight. flowchart disambigu process includ supplementari material. 2.2. robot interact human gestur recognit make robot abl understand human gestures. but, human user abl recogn robot interact success pleasant. case, mean robot work order fulfil task respond user appropri way. instance, wifibot abl perform precis navigation, nao ideal interact speak user act environment. mean answer visual stimuli person expect them, natur respons gesture. figur 5 show flow applic normal us case. applic program state machin paradigm control workflow. detail implement state machin shown supplementari material. wave gesture, expect respons wave user, perform similar gestur him/her mayb perform utterance. case point gesture, robot approach point locat analyz object present, try deduc object user refer to. notic need user point place field view sensor, possibl point object farther awai robot point locat check objects. object known disambigu case doubt, nao goe wifibot (figur 6) approach object, shown user perform gestur hand head expos understood object correctly, seen figur 7. note extend grasp object bring user. 11 figur 5: exampl applic us case. 3. experiment result order evalu design system, experi carri out, includ offlin evalu method onlin evalu extens set user tests. 3.1. offlin evalu gestur recognit method evalu offlin set order valid perform method tune set paramet values. hence, small data set hupba sequenc gener labeled. includ 30 sequenc 6 differ user (5 sequenc user) perform gestur abl recognize, arbitrari gestur choice; perform random order. gestur model dynam gestur recognit modul specif record purpos user perform gestur ideal way. ideal wai taken observ record sequences, take account observ gestur base system quotidian interact people. model subject subject data set. order evalu system, metric usual domain adopted: jaccard index (also known overlap) defin j(a,b) = |ab| |ab| , f1 score, comput f1score = 2tp 2tp+fp+fn . 12 figur 6: nao go wifibot approach object. figur 7: nao show point object. 3.1.1. paramet evalu result order comput perform measure, leave-one-subject-out cross valid (losocv) techniqu used. it, subject data set left grid search perform order tune best paramet differ method gestur system. then, paramet sequenc left user perform metric obtained. procedur repeat subject result averag subject sequenc order obtain final score. carri paramet tuning, interv valu test set recordings, keep perform better. interv paramet test includ dtw threshold wave [6.75, 9.5], consid equal distribut valu step 0.25, nod = negat [4.5, 20] step 0.5. distanc weight 13 wave gestur [0.1, 0.55] step 0.05. facial gestur paramet test orient deriv threshold d [5, 30] step 5 number frame sampl f [1, 20] increment 1 unit. static gestures, threshold number frame td [0.1, 0.45] step 0.5 tea [2.0, 2.55] step 0.05. rang chosen empir perform initi test sequenc includ variat gestures, record purpose. figur 8 show obtain result standard deviat differ- ent users. figur 8a plot result f1 measur differ overlap threshold decid overlap consid tp . meanwhile, figur 8b show result jaccard index measur differ number care frames. observed, wave point gestur on better recognit rates, point slightli better accord jaccard index. facial gestures, nod present better perform negat measures. facial gestur present wors perform fact user perform gestur subtli differ length vari consider wai term orientation. get hamper distanc user camera orient valu subtl farther user is. though, losocv f1 score 0.6 0.61 (mean standard deviat loso subjects) nod gestur 0.610.15 negat overlap threshold 0.4, result accept natur interact real time system. focus jaccard index plot figur 8b, observ best mean perform obtain 7 care frame used, reach 0.65 0.07 overlap. us care frame comput jaccard index make sens natur interact applic goal segment gestur frame level detect gestur itself, despit frame detect start ended. us 7 frame (the previou beginning, begin frame it) solv tempor differ detect label data. 3.2. user test evalu order evalu system performance, test differ user real scenario. opinion collect us easi con- sider accord need extern intervent communication. test user select differ ag group educ back- grounds, seen humanoid robot before, analyz behavior check task fulfillment. test took place differ envi- ronments, try user known comfort scenarios, includ high schools, commun center elderli social association. total 67 user particip experiments. 14 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 overlap threshold f 1 m e s u f1 measur result wave nod negat point mean (a) f1 measur results. 0 5 10 15 20 25 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 number "do care" frame j c c rd n d e x m e s u jaccard index measur result wave nod negat point mean (b) overlap (jaccard index) measur results. figur 8: offlin perform evalu results. 15 screenplai test follows: user stand robot object place ground, meter far. user wave robot, point object election, answer facial gestur robot ask question disambiguate. otherwise, user ask perform facial gestur end test. procedur usual repeat twice user, questionnair experi end. video show execut exampl includ supplementari material. object milk bottl cooki box, gestur recog- nition paramet obtain train mechan previous explained, time hupba sequenc tune parameters. object cluster extraction, radiu 55 centimet point locat used, suitabl valu objects. figur 9 show user perform test differ environments. 3.2.1. user survei analysi section highlight interest result obtain user questionnair test. result analyz ag groups. figur 10 show bar plot relev questions, aggreg ag group. tabl 1 includ answer numer question questionnaires. mean sd question min max 9-34 year 35-60 year 61-86 year wave respons speed 1 5 3.79 0.74 3.89 0.90 4.00 1.05 point at respons speed 1 5 3.66 0.91 3.88 1.02 4.00 1.41 figur point object 1 5 4.00 1.16 3.76 1.09 3.55 1.75 nao clearli show guess 1 5 4.32 0.97 4.12 0.99 3.82 1.72 natur interact 2 5 3.57 0.63 3.53 1.00 4.14 0.90 tabl 1: numer user answer survei (to answer number 1 5). summary, user ag 9 86 years, averag 34.823.98. divid groups: 9 34, 35 60 61 86 years, youngest user group ag 71. gender balanced, 55% user males, seen figur 10a. moreover, zero small previou contact kind robots. wave gestur agre natur users, ag groups, user problem reproduc need explan wave way. respons obtain robot expect consid quick enough, mean robot act natur wai need help understand respons gave, seen figur 10b, 10c tabl 1. result point gestur similar, 16 (a) user high school. (b) user high school. (c) user commun center. (d) user elderli social association. figur 9: exampl user perform tests. 17 natur fast equival result differ ag groups, user expect robot object grasp open bottl (figur 10d, 10e). moreover, user thought point time 35% user felt time (although kept point object robot said gestur recognized), shown figur 10f. nao response, robot miss right object cases, thought clearli show object robot understood ambiguities, seen tabl 1. facial gestur perform users, felt comfort them, nod exagger them. fact, 46% peopl youngest group nod gestur felt unnatur exaggerated, shown figur 10g. negat gestur similar respons (see figur 10h). general, facial gestur present disadvantag long hair peopl hair cover face perform (special negat case), impli face tracker lost face gestur recognized. 88% user thought easi answer yes/no question system. finally, overal interact felt natural, seen tabl 1, user felt frustrat misunderstand gestures, seen figur 10i. user know robot moment test shown figur 10j, case languag difficulty, robot spoke english4. hence, 36% user speak english need extern support translation. 92% user state enjoi test (100% elderli group did), vast major user thought applic kind us assist peopl household environments, special elder on reduc mobility, depict figur 10l. moreover, thought easi commun task gestur manner, figur 10k shows. question ask possibl gestur addit system. interest respons includ gestur come back, start, stop indic nao sit wifibot. 3.3. time recognit rate order obtain object evalu metrics, 30 addit test perform user (five gestur user) conducted. respons time differ gestur recognit rates, execut time object detect modul extract them. tabl 2 tabl 3 obtain results. seen, respons time tabl 2, span end gestur start robot response, suitabl natur interaction, gestur answer second average. 4most user mother tongu spanish catalan. 18 9 34 35 60 61 86 0 3 6 9 12 15 18 21 24 27 30 33 36 39 ag (years) n u m b e r o f u s e rs user ag distribut male femal (a) natur unnatur hard perform 0 4 8 12 16 20 24 28 32 36 40 44 48 52 answer n u m b e r o f u s e rs wave gestur natur 9 34 year 35 60 year 61 86 year (b) ye 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 answer n u m b e r o f u s e rs expect wave respons 9 34 year 35 60 year 61 86 year (c) natur unnatur hard perform 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 answer n u m b e r o f u s e rs point gestur natur 9 34 year 35 60 year 61 86 year (d) ye 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 answer n u m b e r o f u s e rs expect point at respons 9 34 year 35 60 year 61 86 year (e) ye 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 answer n u m b e r o f u s e rs thought point long 9 34 year 35 60 year 61 86 year (f) figur 10: user answer questionnaire. 19 natur exagger unnatur hard perform 0 2 4 6 8 10 12 14 16 18 20 22 24 answer n u m b e r o f u s e rs nod gestur opinion 9 34 year 35 60 year 61 86 year (g) natur exagger unnatur hard perform 0 1 2 3 4 5 6 7 8 9 answer n u m b e r o f u s e rs negat gestur opinion 9 34 year 35 60 year 61 86 year (h) ye 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 answer n u m b e r o f u s e rs felt frustrat test 9 34 year 35 60 year 61 86 year (i) ye 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 answer n u m b e r o f u s e rs felt confus point 9 34 year 35 60 year 61 86 year (j) ye 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 answer n u m b e r o f u s e rs easi tell task 9 34 year 35 60 year 61 86 year (k) ye 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 answer n u m b e r o f u s e rs us household environ 9 34 year 35 60 year 61 86 year (l) figur 10: user answer questionnaire. 20 item time (seconds) mean sd recognit rate wave gestur 1.72 0.62 83.33% point gestur 1.91 0.67 96.67% nod gestur 1.99 0.49 73.33% negat gestur 1.47 0.47 33.33% object detect 0.53 0.29 63.33% tabl 2: respons execut time recognit rate differ gestur object detect 30 tests. dynam gestur recognit time span end gestur response, static on start gestur object response. gestur time measur standard chronomet oper test controller. object detection, compris time order robot segment object respons wifibot laptop, comput second. look recognit rates, best recogn gestur point one. negat gestur lowest recognit rates, case offlin results, mainli face track face sideward camera. show high recognit rate object detect errors, detail tabl 3. caus rate wrong point locat estim 3.33% object detect wrong object detect 16.67% disambigu failur 3.33% navig error (did reach place) 13.33% tabl 3: error rate caus object detect step 30 tests. 4. conclus work, present multi-robot design interact human user real time gestur base manner. proof concept show import interact phase order abl assist user special needs, elderli handicap people. con- sequently, interact robot wai human beings, robot us inform provid user help them. instance, robot pick floor need actual recogn object know person refer deictic gesture. 21 includ gestur recognit method base kinecttm v2 sensor take account dynam gestures, recogn dtw specif featur face body, static gestur deictic on refer present environment. multi-robot shown effect wai combin effort special robots, carri weight sensor comput power precis navigation, abl speak interact natur wai user. collabor perform task lead success interaction. furthermore, extens set user test carri 67 user littl contact robot abl perform test minim extern indications, result natur interact cases. offlin test show high recognit rate perform real time gestur detect spot specif record data set. nevertheless, differ element detect point direct improv futur work. instance, us accur hand pose estim like on propos [34, 35, 36] allow direct finger obtain point direction, probabl result precis locat estimation. facial gestur element highli improved, try us better facial tracker properli handl view (which clearli affect detect negat gesture), explor ad kind features. acknowledg like thank la garriga town council, youth center radio silenc help user test commun organization, follow entities, associ peopl locat la garriga: associacio gent gran lesplai lespaicaixa, la torr del fanal com- muniti center, institut villa romana, escola sant llu goncaga, pujol- buckl famili allow perform test facilities. special thank dr. marta daz guidelin user test analyses, joan guasch josep maria canal help wifibot adapta- tions, vctor vlchez proofreading. work partial support spanish ministri economi competitiveness, project tin 2012-38416-c03-01 tin2013-43478-p. research fellow gerard canal thank fund grant issu catalunya - la pedrera foundation. refer [1] j. devito, m. hecht, nonverb commun reader, waveland press, 1990. 22 [2] c. breazeal, c. kidd, a. thomaz, g. hoffman, m. berlin, effect non- verbal commun effici robust human-robot team- work, in: proceed ieee/rsj intern confer in- tellig robot systems, 2005. (iro 2005), 2005, pp. 708713. doi:10.1109/iros.2005.1545011. [3] a. hernandez-vela, m. a. bautista, x. perez-sala, v. ponce-lopez, s. es- calera, x. baro, o. pujol, c. angulo, probability-bas dynam time warp bag-of-visual-and-depth-word human gestur recog- nition rgb-d, pattern recognit letter 50 (0) (2014) 112121, depth imag analysis. doi:10.1016/j.patrec.2013.09.009. [4] m. reyes, g. domnguez, s. escalera, featur weight dynam time warp gestur recognit depth data, in: proceed 2011 ieee intern confer vision workshop (iccv workshops), 2011, pp. 11821188. doi:10.1109/iccvw.2011.6130384. [5] k. kulkarni, g. evangelidis, j. cech, r. horaud, continu action recog- nition base sequenc alignment, intern journal vision 112 (1) (2015) 90114. doi:10.1007/s11263-014-0758-9. [6] b. liang, l. zheng, multi-mod gestur recognit skelet joint motion trail model, in: l. agapito, m. m. bronstein, c. rother (eds.), vision - eccv 2014 workshops, vol. 8925 lectur note science, springer intern publishing, 2015, pp. 623638. doi:10.1007/978-3-319-16178-5_44. [7] n. camgz, a. kindiroglu, l. akarun, gestur recognit templat base random forest classifiers, in: l. agapito, m. m. bronstein, c. rother (eds.), vision - eccv 2014 workshops, vol. 8925 lectur note science, springer intern publishing, 2015, pp. 579594. doi:10.1007/978-3-319-16178-5_41. [8] d. wu, l. shao, deep dynam neural network gestur segmenta- tion recognition, in: l. agapito, m. m. bronstein, c. rother (eds.), vision - eccv 2014 workshops, vol. 8925 lectur note science, springer intern publishing, 2015, pp. 552571. doi:10.1007/978-3-319-16178-5_39. [9] a. yao, l. van gool, p. kohli, gestur recognit portfolio personal- ization, in: 2014 ieee confer vision pattern recog- nition (cvpr), 2014, pp. 19231930. doi:10.1109/cvpr.2014.247. [10] o. lopes, m. reyes, s. escalera, j. gonzalez, spheric blur shape model 3-d object pose recognition: quantit analysi hci applic smart environments, ieee transact cyber- netic 44 (12) (2014) 23792390. doi:10.1109/tcyb.2014.2307121. 23 [11] s. iengo, s. rossi, m. staffa, a. finzi, continu gestur recognit flexibl human-robot interaction, in: proceed 2014 ieee inter- nation confer robot autom (icra), ieee, 2014, pp. 48634868. doi:10.1109/icra.2014.6907571. [12] h. kim, s. hong, h. myung, gestur recognit algorithm move kinect sensor, in: proceed 2013 ieee ro-man, 2013, pp. 320 321. doi:10.1109/roman.2013.6628475. [13] a. ramey, v. gonzalez-pacheco, m. a. salichs, integr low-cost rgb-d sensor social robot gestur recognition, in: proceed 6th intern confer human-robot interaction, hri 11, acm, new york, ny, usa, 2011, pp. 229230. doi:10.1145/1957656. 1957745. [14] t. fujii, j. hoon lee, s. okamoto, gestur recognit human- robot interact applic robot servic task, in: proceed intern multiconfer engin scientist (imec 2014), vol. i, intern associ engineers, newswood limited, 2014, pp. 6368. [15] x. zhao, a. m. naguib, s. lee, kinect base call gestur recog- nition take order servic elderli care robot, in: proceed- ing 23rd ieee intern symposium robot human interact commun (ro-man 2014), 2014, pp. 525530. doi: 10.1109/roman.2014.6926306. [16] d. mccoll, z. zhang, g. nejat, human bodi pose interpret classi- ficat social human-robot interaction, intern journal social robot 3 (3) (2011) 313332. doi:10.1007/s12369-011-0099-6. [17] a. chrungoo, s. manimaran, b. ravindran, activ recognit nat- ural human robot interaction, in: m. beetz, b. johnston, m.-a. william (eds.), social robotics, vol. 8755 lectur note sci- ence, springer intern publishing, 2014, pp. 8494. doi:10.1007/ 978-3-319-11973-1_9. [18] e. bernier, r. chellali, i. m. thouvenin, human gestur segment base chang point model effici gestur interface, in: proceed 2013 ieee ro-man, 2013, pp. 258263. doi:10.1109/roman. 2013.6628456. [19] d. michel, k. papoutsakis, a. a. argyros, gestur recognit support- ing interact human social assist robots, in: g. be- bis, r. boyle, b. parvin, d. koracin, r. mcmahan, j. jerald, h. zhang, s. drucker, c. kambhamettu, m. el choubassi, z. deng, m. carlson (eds.), advanc visual computing, vol. 8887 lectur note science, springer intern publishing, 2014, pp. 793804. doi:10.1007/978-3-319-14249-4_76. 24 [20] m. obaid, f. kistler, m. hring, r. bhling, e. andr, framework user- defin bodi gestur control humanoid robot, intern journal social robot 6 (3) (2014) 383396. doi:10.1007/s12369-014-0233-3. [21] p. barros, g. parisi, d. jirak, s. wermter, real-tim gestur recognit humanoid robot deep neural architecture, in: 2014 14th ieee-ra intern confer humanoid robot (humanoids), 2014, pp. 646651. doi:10.1109/humanoids.2014.7041431. [22] j. l. raheja, a. chaudhary, s. maheshwari, hand gestur point loca- tion detection, optik - intern journal light electron optic 125 (3) (2014) 993 996. doi:10.1016/j.ijleo.2013.07.167. [23] m. pateraki, h. baltzakis, p. trahanias, visual estim point target robot guidanc fusion face pose hand orienta- tion, vision imag understand 120 (0) (2014) 1 13. doi:10.1016/j.cviu.2013.12.006. [24] c. park, s. lee, real-tim 3d point gestur recognit mobil robot cascad hmm particl filter, imag vision comput 29 (1) (2011) 51 63. doi:10.1016/j.imavis.2010.08.006. [25] d. droeschel, j. stuckler, s. behnke, learn interpret point gestur time-of-flight camera, in: proceed 2011 6th acm/iee intern confer human-robot interact (hri), 2011, pp. 481488. [26] c. matuszek, l. bo, l. zettlemoyer, d. fox, learn unscript deictic gestur languag human-robot interactions, in: proceed 28th nation confer artifici intellig (aaai), quebec city, quebec, canada, 2014. [27] a. jevtic, g. doisy, y. parmet, y. edan, comparison interact modal- iti mobil indoor robot guidance: direct physic interaction, person following, point control, ieee transact human-machin system 45 (6) (2015) 653663. doi:10.1109/thms.2015.2461683. [28] m. quigley, k. conley, b. p. gerkey, j. faust, t. foote, j. leibs, r. wheeler, a. y. ng, ros: open-sourc robot oper system, in: icra workshop open sourc software, 2009. [29] r. b. rusu, s. cousins, 3d here: point cloud librari (pcl), in: pro- ceed ieee intern confer robot autom (icra), shanghai, china, 2011. [30] t. arici, s. celebi, a. s. aydin, t. t. temiz, robust gestur recog- nition featur pre-process weight dynam time warp- ing, multimedia tool applic 72 (3) (2014) 30453062. doi: 10.1007/s11042-013-1591-9. 25 [31] h. sakoe, s. chiba, dynam program algorithm optim spo- ken word recognition, ieee transact acoustics, speech signal process 26 (1) (1978) 4349. doi:10.1109/tassp.1978.1163055. [32] r. b. rusu, cluster segmentation, in: semant 3d object map everydai robot manipulation, vol. 85 springer tract advanc robotics, springer berlin heidelberg, 2013, ch. 6, pp. 7585. doi:10. 1007/978-3-642-35479-3_6. [33] m. a. fischler, r. c. bolles, random sampl consensus: paradigm model fit applic imag analysi autom car- tography, communun acm 24 (6) (1981) 381395. doi: 10.1145/358669.358692. [34] j. tompson, m. stein, y. lecun, k. perlin, real-tim continu pose recoveri human hand convolut networks, acm transact graphic (tog) 33 (5) (2014) 169:1169:10. doi:10.1145/2629500. [35] f. kirac, y. e. kara, l. akarun, hierarch constrain 3d hand pose estim regress forest singl frame depth data, pattern recognit letter 50 (2014) 91 100, depth imag analysis. doi: //dx.doi.org/10.1016/j.patrec.2013.09.003. [36] t. sharp, c. keskin, d. robertson, j. taylor, j. shotton, d. kim, c. rhe- mann, i. leichter, a. vinnikov, y. wei, d. freedman, p. kohli, e. krupka, a. fitzgibbon, s. izadi, accurate, robust, flexibl real-tim hand track- ing, in: proceed 33rd annual acm confer human fac- tor comput systems, chi 15, acm, new york, 2015, pp. 3633 3642. doi:10.1145/2702123.2702179. gerard canal receiv bachelor degre sci- enc universitat politecnica catalunya (upc) 2013. obtain master degre artifici intellig universitat politecnica catalunya (upc), universitat barcelona (ub) universitat rovira virgili (urv), 2015. main research interest includ develop novel assist technolog base social robot involv vision. current pursu ph.d. assist human-robot interact vision techniques. sergio escalera obtain ph.d. degre multi-class vi- sual categor system vision center, uab. obtain 2008 best thesi award scienc universitat autonoma barcelona. lead human pose recoveri behavior analysi group. as- sociat professor depart appli mathemat analysis, universitat barcelona. member vision center campu uab. director 26 chalearn challeng machin learning. vice-chair iapr tc-12: multimedia visual inform systems. research interest include, be- tween others, statist pattern recognition, visual object recognition, hci systems, special human pose recoveri behavior analysi multi-mod data. cecilio angulo receiv m.s. degre mathemat univers barcelona, spain, ph.d. degre scienc universitat politecnica catalunya - barcelonatech, spain, 1993 2001, respectively. 1999 2007, universitat politecnica catalunya, assist professor. nowadai as- sociat professor depart automat control, university. 2011 he serv director master degre automat control robotics. he current director knowledg engin research group respons research project area social cognit robotics. cecilio angulo author 250 technic publications. research interest includ cognit robotics, machin learn algorithm social robot applications. 27 introduct gestur base human robot interact real time gestur recognition: interact robot definit gestur specif featur dynam gestur recognit static gestur recognit point locat estim near point object segment disambigu robot interact human experiment result offlin evalu paramet evalu result user test evalu user' survei analysi time recognit rate conclus