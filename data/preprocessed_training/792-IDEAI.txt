1 neighborhood-bas stop criterion contrast diverg enriqu romero merino ferran mazzanti castrillejo jordi delgado pin abstractrestrict boltzmann machin (rbms) gener unsupervis learn devic ascertain gener model data distributions. rbm train contrast diverg learn algorithm (cd), approxim gradient data log-likelihood. simpl reconstruct error stop criterion cd, author rais doubt concern feasibl procedure. case evolut curv recon- struction error monoton log-likelihood not, indic good estim optim stop point learning. however, altern reconstruct error discuss literature. estim log-likelihood train data base anneal import sampl feasibl computation expensive. manuscript present simpl cheap alternative, base inclus inform contain neighbor state train set, stop criterion cd learning. i. introduct learn algorithm deep multilay neural network known long time [1], usual outperform simpler, shallow networks. way, deep multilay network wide solv larg scale real-world problem decad [2], [3]. 2006, deep belief network (dbns) [4] came real breakthrough field, learn algorithm propos end feasibl practic method train deep networks, interest result [5][9]. dbn restrict boltzmann machin (rbms) [10] build blocks. rbm topolog constrain boltzmann machin (bms) layers, hidden visibl neurons, intralay connections. properti make work rbm simpler regular bms, particular stochast comput log-likelihood gradient perform effici mean gibb sampl [2], [11]. 2002, contrast diverg (cd) learn algo- rithm propos effici train method product- of-expert models, rbm special case [12]. observ cd train rbm work practice. fact import deep learn enriqu romero merino departa cienci la computacio, universitat politecnica catalunya, spain (e-mail: ferran mazzanti castrillejo departa fsica en- ginyeria nuclear, universitat politecnica catalunya, spain (e-mail: fer- jordi delgado pin departa cienci la computacio, universitat politecnica catalunya, spain (e-mail: author suggest multilay deep neural network better train layer pre-train separ singl rbm [5], [6], [13]. thus, train rbm cd stack good wai design deep learn architectures. however, pictur nice looks, cd flawless train algorithm. despit cd approxim true log-likelihood gradient [14], bias converg case [15][17]. moreover, observ cd, variant persist cd [18] fast persist cd [19] lead steadi decreas log-likelihood learn [20][22]. therefore, risk learn diverg impos requir stop criterion. main method decid stop learn process. base monitor reconstruct error [23]. base estim log-likelihood anneal import sampl (ais) [24], [25]. reconstruct error easi comput practice, adequaci remain unclear monoton [21]. ai work better reconstruct error cases, consider expens compute, fail [20]. work approach problem differ perspective. gener cdn tend concentr proba- biliti small subset train data, leav littl probabl rest states. undesir featur prevent build good model. work propos stop criterion tri detect likelihood start degenerate. boltzmann distribut probabl given state proport exponenti energy, state similar energi similar probability. base fact energi continu smooth function variables, close neighborhood high-prob state expect acquir signific probability. sense, argu inform contain neighborhood train data valuable, incorpor learn process rbms. us ham distanc measur close differ state are. propos stop criterion depend inform contain train set neighbor detect chang curvatur log-likelihood. sens criterion local need explor space states. furthermore, order computation tractable, build stop criterion wai independ partit function model, computation intract 2017 ieee. person us materi permitted. permiss ieee obtain uses, current futur media, includ reprinting/republish materi advertis promot purposes,cr new collect works, resal redistribut server lists, reus copyright compon work works. doi: 10.1109/tnnls.2017.2697455 2 real-world larg spaces. moreover, propos quantiti monitor learn cheaper evalu estim log-likelihood ais. section defin neighborhood-bas stop criterion cdn perform data sets. ii. learn restrict boltzmann machin a. energy-bas probabilist model energy-bas probabilist model defin probabl dis- tribut energi function, follows: p (x,h) = eenergy(x,h) z , (1) x h stand (typic binary) visibl hidden variables, respectively. normal factor z call partit function read z = x,h eenergy(x,h) . (2) x observed, interest margin distribut p (x) = h e energy(x,h) z , (3) evalu partit function z computation- alli prohibit involv exponenti larg number terms. way, measur directli p (x). energi function depend paramet , adjust learn stage. maximiz- ing likelihood data. energy-bas models, deriv log-likelihood express logp (x; ) = e p (h|x) [ energy(x,h) ] e p ( x) [ e p (h| x) [ energy( x,h) ]] , (4) term call posit phase second term neg phase. expression, e p ( x) stand expect valu probabl visibl states, involv evalu partit function accord definit e p ( x) [f( x)] = x p ( x)f( x) p ( x) defin eq. (3). seen, exact comput deriv log-likelihood usual unfeas neg phase (4), come deriv partit function. b. restrict boltzmann machin restrict boltzmann machin energy-bas proba- bilist model energi function is: energy(x,h) = btx cth htwx , (5) w two-bodi weight connect pair hidden visibl units, b c correspond bia terms. rbm core dbn [4] deep architectur us rbm unsupervis pre-train previou supervis step [5], [6], [13]. consequ particular form energi function rbm p (h|x) p (x|h) factorize. wai possibl comput p (h|x) p (x|h) step, make possibl perform gibb sampl efficiently, contrast gener model like boltzmann machin [26]. c. contrast diverg common learn algorithm rbm us algorithm estim deriv log-likelihood product expert model. algorithm call contrast diverg [12]. contrast diverg cdn estim deriv log-likelihood given point x logp (x; ) ' e p (h|x) [ energy(x,h) ] e p (h|xn) [ energy(xn,h) ] . (6) xn sampl gibb chain start x obtain n steps: h1 p (h|x) x1 p (x|h1) ... hn p (h|xn1) xn p (x|hn) . usually, e p (h|x) [ energy(x,h) ] easili computed. altern cdn persist cd [18], fast per- sistent cd [19], parallel temper [22], dissimilar cd [27], averag cd [28] mean field correct [29]. d. monitor learn process rbm learn rbm delic procedur involv lot data process seek perform reason speed order abl handl larg space huge states. so, drastic approxim understood statist averag sens perform [30]. relev point consid learn stage good wai determin good solut not, decid learn process stop. wide criteria stop base monitor reconstruct error, measur capabl network produc output consist data input. rbm probabilist models, reconstruct error data point x(i) comput probabl x(i) given expect valu h x(i): r(x(i)) = logp ( x(i)| p (h|x(i)) [h] ) , (7) probabilist extens sum-of-squar re- construct error determinist network (x(i)) = ||x(i) x(i)n || 2 . (8) express x(i)n stand n-th reconstruction, gibb chain mention above, i-th member train set. practice, eq. (7) comput analytically. 3 evalu expect valu hidden unit given input, condit probabl visibl unit given that. author shown that, cases, learn induc undesir decreas likelihood goe un- detect reconstruct error [20], [21] (both r(x(i)) (x(i)) usual decreas monotonically). increas reconstruct error take place train appar wai detect chang behavior log- likelihood cdn. alternatively, evalu estim likeli- hood train data mean ai algorithm. theoret possible, expens comput point view size large, case clear perform [20]. iii. propos stop criterion propos stop criterion base monitoriza- tion ratio quantities: geometr averag probabl train set, sum probabl point given neighborhood train set. formally, monitor d = [n i=1 p (x (i)) ]1/n yd p (y) , (9) d subset point ham distanc equal d train set. usual, distanc given point data set taken minimum distanc given point element set. notic point train set improv learn present works, [27]. idea definit evolut d learn stage expect captur main trend log-likelihood certain valu d d. notic interest limit cases. hand, d span space (thu d equal maxim possibl ham distance), d exactli likelihood data denomin eq. (9) add 1. hand, data train set involv calcul d = 0. choic d d problem- dependent, case sensibl choices, take d small local estimator, d reason size order d computation feasible. work, propos stop cdn learn maximum d, expect close shown log-likelihood data. hold suitabl valu d d, shown experi sections. motiv analyt form d eq. (9) twofold. hand numer denomin monitor differ things. numerator, essenti likelihood data, sensit accumul probabl mass reduc subset train data, typic featur cdn. continu reasons, denomin strongli correl sum probabl train data. problem learnt, probabl close neighborhood train set high. valu d result delic fig. 1. averag logarithm probabl (left panel) fraction sign chang small weight chang (right panel) probabl state function ham distanc thousand run rbm 12 visibl units. equilibrium quantiti (see section iv). hand, notic partit function expens quantiti evaluate, explicitli build d z- independ quantity. necessari condit impos design quantiti monitorized. way, structur d, partit function z involv numer denomin cancel out. words, comput d equival defin d = [n i=1 h e energy(x(i),h) ]1/n yd h e energy(y,h) . (10) particular topolog rbm allow comput h e energy(z,h) effici [2]. fact dramat de- creas comput cost involv calculation, unfeas real-world problem rbm successfulli applied. defin probabilist neighborhood train sampl gener problemat clearli depend valu weight bia network, computation expensive. choic ham distanc measur probabl proxim justifi statist sense: energi function sum term involv singl bit visibl units, expect chang total energi smaller fewer bit changed, small rang ham distances. moreover, expect chang probabl nearbi state follow direction. order illustr point conduct seri synthet experi randomli chosen gaussian weights, small fraction space acquir signific probabl mass. wai goal reproduc usual learn problems, train set small compar space. figur 1 show result averag thousand run rbm 12 18 visibl hidden units, respectively. paramet adjust approxim 5% total number state exhaust approxim 0.8 total probability. left panel show averag probabl neighbor state probabl ones, function ham distance. seen plot, averag probabl smooth function ham distanc 4 show monoton behavior certain point. right panel show fraction sign chang averag probabl small variat 1% weight perform (which account small updat learn epoch). seen, neighbor state follow sign chang origin state, reinforc idea continu probabl space. way, idea ham distanc measur probabilist similar support statist sense. furthermore, simplest cheapest metric evaluate. clear ham distanc fail cases, criterion base hypothesi domin case. way, non-trivi metric on propos [31], [32] used. numer d directli evalu data train set, problem find suitabl valu y d remains. indeed, set point given ham distanc d train set independ weight bia network. way, built begin process requir learning. therefore, issu sort criterion applied. decid suitabl valu d. experi differ problem problem dependent, illustr experiment section below. second choic subset d, strongli depend size space explored. small space safe us complet set point distanc equal d, forbiddingli larg real world problems. reason explor possibilities: includ point includ random subset size train set, expens deal train set. arbitrari decis chang will, keep mind need larg set point increas comput cost significantly. iv. experi perform experi explor afore- mention criterion defin section iii studi be- havior d comparison log-likelihood reconstruct error data problems. exact analysis, explor problem size log-likelihood exactli evalu compar propos d parameter. moreover, includ result larg benchmark datasets, calcul exact log-likelihood unfeas approxim ai algorithm [25]. a. small problem small problem, denot bar stripe (bs), tri identifi vertic horizont line 44 pixel images. train set consist set imag contain possibl horizont vertic line (but both), rang line (blank image) complet fill imag (black image), produc 2 24 2 = 30 differ imag (avoid repetit fulli fulli white images) space 216 possibl imag black white pixels. second small problem, name label shifter ensembl (lse), consist learn 19-bit state form follows: given initi 8-bit pattern, gener new state concaten bit sequenc 001, 010 100. final 8-bit pattern state origin shift bit left intermedi code 001, copi unchang code 010, shift bit right code 100. gener train set possibl 28 3 = 768 state creat form, space consist possibl 219 differ state build 19 bits. problem explor [21] adequ current context since, large, dimension space allow direct monitor partit function log- likelihood learning. sake completeness, test propos criterion randomli gener problem differ space dimensions, number state learnt significantli smaller size space. particular, gener differ data set (ran10, ran12, ran14 ran16) consist nv = 10, 12, 14, 16 binari input unit 2nv/2 exampl learnt, suggest [33]. follow discuss learn process problem binari rbms, emploi contrast diverg algorithm cdn n = 1 n = 10 describ section ii-c. bs case rbm 16 visibl 8 hidden units, lse problem number 19 10, respectively. random data set 10 hidden unit case. simul carri total 50000 epochs, measur taken 50 epochs. moreover, point subsequ plot averag differ simul start differ random valu weight bias. weight decai used, momentum set 0.8. learn rate chosen order sure log-likelihood degenerates, wai present clear maximum detect d. subsect present result seri experiments. (section iv-a1) analyz case state d includ given d. second (section iv-a2) relax comput cost evalu d select small subset state d. 1) complet neighborhoods: present result problem hand, show analyz instanc differ plot correspond actual log-likelihood problem d differ valu d, things. order identifi contribut d differ neighborhood train set, defin differ sets: da contain state distanc equal d, ds account state distanc exactli equal d. comput d d = da d = ds experi comment following. figur 2 show result ran10 data set. upper left panel show log-likelihood data training. seen, clear maximum 5 0 200 400 600 800 1000 -230 -220 -210 -200 -190 -180 0 200 400 600 800 1000 0,005 0,01 0,015 0,02 0 200 400 600 800 1000 0.003 0.0035 0.004 0.0045 0.005 0.0055 0.006 0 200 400 600 800 1000 0.001 0.0015 0.002 0.0025 0.003 0.0035 0.004 0 200 400 600 800 1000 0.001 0.0015 0.002 0.0025 0.003 0.0035 0.004 0 200 400 600 800 1000 -1 0 1 2 3 4 5 6 7 0 200 400 600 800 1000 0 0,1 0,2 0,3 0,4 0 200 400 600 800 1000 0,3 0,4 0,5 0,6 0,7 0,8 0 200 400 600 800 1000 0,86 0,88 0,9 0,92 0,94 0,96 0,98 1 0 200 400 600 800 1000 0 0,2 0,4 0,6 0,8 1 1,2 0 200 400 600 800 1000 0 0.05 0.1 0.15 0.2 0.25 0 200 400 600 800 1000 0,005 0,01 0,015 0,02 0 200 400 600 800 1000 0,003 0,004 0,005 0,006 0,007 0,008 0 200 400 600 800 1000 0 0,002 0,004 0,006 0,008 0,01 0,012 0 200 400 600 800 1000 0 0,1 0,2 0,3 0,4 fig. 2. result ran10 problem. column show log-likelihood (top) reconstruct error (7) (8) (center bottom). column first, second row depict d d = da (black curves), sum probabl denomin d d = da (brown curves), d d = ds (green curves) d = 0, 1, 2, 3, respectively. x-axi number epoch simul divid 50 plots. data y-axi arbitrari units. 0 200 400 600 800 1000 -1250 -1225 -1200 0 200 400 600 800 1000 0,0025 0,003 0,0035 0 200 400 600 800 1000 0,00025 0,0003 0,00035 0,0004 0 200 400 600 800 1000 0,00008 0,00009 0,00010 0,00011 0 200 400 600 800 1000 6e-05 7e-05 8e-05 0 200 400 600 800 1000 2 4 6 8 10 0 200 400 600 800 1000 0,0027 0,003 0,0033 0,0036 0 200 400 600 800 1000 0,0003 0,00035 0,0004 0,00045 0 200 400 600 800 1000 0,00012 0,00014 0,00016 0 200 400 600 800 1000 0,00015 0,0002 0,00025 0,0003 0,00035 fig. 3. result ran14 problem. column show log-likelihood reconstruct error (7) (top panels). column upper lower row d d = da d d = ds d = 0, 1, 2, 3, respectively. identifi stop point. panel reconstruct error (7) (8) clearli fail identifi desir extremum. rest column result distanc d = 0, 1, 2 3. row depict d da, state requir distanc taken account. seen, start d = 1 criterion robust consist detect maximum log- likelihood right place, reinforc idea neighborhood data contain valuabl information. second row show denomin d correspond row, is, sum probabl state includ case. notic d = 3 sum equal d exactli equal likelihood data. interestingly, sum far awai one, happen d = 1, d consist find desir point. behavior observ rest data set analyzed. final row show d ds , show behavior criterion appli differ shells. d = 1 2 criterion detect reason maximum log-likelihood identifi desir stop point. notice, though, data alone, entir contain d = 0, capabl reproduc behavior. moreover, d larger 2 criterion fails, expect start certain distanc inform model lost. notic initi transitori behavior plot meaningless omit cut. equival result ran14 case shown figur 3. 6 0 200 400 600 800 1000 -350 -300 -250 -200 0 200 400 600 800 1000 0 0,01 0,02 0,03 0 200 400 600 800 1000 0 0,0005 0,001 0,0015 0,002 0,0025 0 200 400 600 800 1000 0 0,00025 0,0005 0,00075 0,001 0,00125 0 200 400 600 800 1000 0 0,0002 0,0004 0,0006 0,0008 0,001 0 200 400 600 800 1000 0 2,5 5 7,5 10 12,5 0 200 400 600 800 1000 0 0,005 0,01 0,015 0,02 0,025 0,03 0,035 0 200 400 600 800 1000 0,001 0,0015 0,002 0,0025 0,003 0,0035 0 200 400 600 800 1000 0 0,001 0,002 0,003 0,004 0,005 0,006 0,007 0 200 400 600 800 1000 0 0,005 0,01 0,015 fig. 4. figur 3 bs data set. 0 200 400 600 800 1000 -10250 -10000 -9750 -9500 -9250 -9000 -8750 0 200 400 600 800 1000 0 0,0001 0,0002 0,0003 0,0004 0,0005 0,0006 0 200 400 600 800 1000 2,5e-05 5e-05 7,5e-05 0 200 400 600 800 1000 1e-05 1,5e-05 2e-05 2,5e-05 0 200 400 600 800 1000 5e-06 1e-05 1,5e-05 0 200 400 600 800 1000 5 7,5 10 12,5 0 200 400 600 800 1000 0 0,0001 0,0002 0,0003 0,0004 0,0005 0 200 400 600 800 1000 2,5e-05 5e-05 7,5e-05 0,0001 0 200 400 600 800 1000 1e-05 2e-05 3e-05 4e-05 0 200 400 600 800 1000 1e-05 1,5e-05 2e-05 2,5e-05 3e-05 3,5e-05 4e-05 fig. 5. figur 3 lse data set. log-likelihood probabilist reconstruct error (7) depict upper lower panel column, respectively. panel d da ds , d = 0, 1, 2, 3 (top rows, second fifth columns). previou case, reconstruct error fail detect maximum likelihood, us present context. contrary, stop point obtain d select near-optim model. notic criterion robust distanc explored, desired. similar result ran12 ran16 cases. plot bs lse problem figur 4 5. again, reconstruct error decreas monoton useless present context. lse problem, d d larger 1 successfulli task d = da d = ds . however, bs case work d = da d = ds d > 1. infer results, optim valu d fix problem-dependent. 2) incomplet neighborhoods: despit success criterion built d = da, clear larg space unpract number state neighborhood train set large. reason, test criterion randomli select subset da da size train set, computation tractable. sense, denot d evalu d da. figur 6 show d compar d previou figur bs (first row) lse (second row) problems. precisely, column show log- likelihood data train process, rest column plot d d d = 0, 1, 2 3. notic absolut scale d d vari mainli valu sum probabl denominators. however, precis valu quantiti irrelevant, decid scale properli sake comparison. d built smaller set d, case captur signific featur d instead it. sense, d provid good stop criterion cd1, robust d strong reduct state contribut d compar enter d. reduct illustr tabl i, number neighbor state data set differ distanc bs lse problems. increas number state includ d, converg d expect expens increas comput cost. however, present result indic that, problem hand, number exampl similar train set evalu 7 data set ham distanc 1 2 3 4 5 6 7 8 9 10 bar stripe 480 3216 11360 20744 19296 8688 1632 90 - - label shifter ensembl 8434 41160 110326 165088 132976 54160 10368 966 40 2 tabl number neighbor differ ham distanc bs lse data sets. 0 200 400 600 800 1000 -350 -300 -250 -200 0 200 400 600 800 1000 0 0,01 0,02 0,03 0,04 d ~ d 0 200 400 600 800 1000 0 0,01 0,02 0,03 0,04 0,05 d (x 15) ~ d 0 200 400 600 800 1000 0 0,1 0,2 0,3 0,4 0,5 d (x 400) ~ d 0 200 400 600 800 1000 0 5 10 15 20 25 30 d (x 30000) ~ d 0 200 400 600 800 1000 -10000 -9500 -9000 -8500 0 200 400 600 800 1000 0 0,0001 0,0002 0,0003 0,0004 0,0005 d ~ d 0 200 400 600 800 1000 0 0,0002 0,0004 0,0006 0,0008 0,001 d (x 10) ~ d 0 200 400 600 800 1000 0 0,0005 0,001 0,0015 0,002 d (x 50) ~ d 0 200 400 600 800 1000 0,001 0,002 0,003 d (x 175) ~ d fig. 6. comparison d (black curves) d (red curves) bs lse data set (upper lower rows). notic magnitud paramet irrelevant, curv scale sake clarity. column plot log-likelihood data simulation. 0 200 400 600 800 1000 -9500 -9000 -8500 -8000 0 200 400 600 800 1000 0 0,0002 0,0004 0,0006 d d ~ 0 200 400 600 800 1000 0,0005 0,001 0,0015 0,002 d (x 15) d ~ 0 200 400 600 800 1000 0,001 0,002 0,003 0,004 da (x 75) d ~ 0 200 400 600 800 1000 0,002 0,004 0,006 0,008 0,01 d (x 250) d ~ fig. 7. figur 6 lse problem cd10. d detect maximum log-likelihood data. result present point good propos stop criterion learn cd1. how- ever, underli idea appli differ learn algorithm try maxim log-likelihood data. wai repeat previou experi cd10 similar result on above. example, figur 7 show log-likelihood, d d d = 0, 1, 2, 3 cd10 lse data set. clearli seen, qualiti result similar cd1 case, stress robust criterion. final remark, note bs problem train rbm stop propos criterion abl qualit gener sampl similar train set. figur 8 complet train set (two upper rows) number gener sampl (two lower rows) obtain rbm train cd1 stop 5000 epochs, maximum shown d=1, approxim coincid optim valu log-likelihood. import realiz that, ultimately, qualiti model direct measur qualiti cd1 learning, model gener plot largest d, close largest likelihood. b. persist cd persist cd (pcd) known cheap altern plain cd help improv learn [18], [19]. test stop criteria set previou section pcd, lead similar results. justifi fact known certain condit pcd degener [20], [21] cd does, probabl concentr hand states. therefore, measur qualit captur log- likelihood behavior cd expect work pcd. illustr figur 9, d d shown lse problem learnt pcd. previou cases, evolut propos estim simul qualit resembl ground truth, stop criteria detect reason good stop point. c. mnist data set mnist data set known benchmark problem correspond 28 28 binar imag hand-written digit huge space 2764 possibl state 1. 1 8 fig. 8. train data (two upper rows) gener sampl (two lower rows) bs problem weight bia obtain stop point detect d d = 1. 0 200 400 600 800 1000 -10000 -9500 -9000 -8500 0 200 400 600 800 1000 0 0,00025 0,0005 d ~ d 0 200 400 600 800 1000 0 0,0005 0,001 d (x 10) ~ d 0 200 400 600 800 1000 0 0,0005 0,001 0,0015 0,002 d (x 50) ~ d 0 200 400 600 800 1000 0 0,001 0,002 0,003 0,004 d (x 175) ~ d fig. 9. figur 6 lse problem pcd. 0 200 400 600 800 1000 -200 -175 -150 -125 0 200 400 600 800 1000 -100 -90 -80 ~ d s ~ d 0 200 400 600 800 1000 -95 -90 -85 -80 -75 -70 -65 ~ d s ~ d 0 200 400 600 800 1000 -100 -90 -80 -70 -60 -50 -40 ~ d s ~ d 0 200 400 600 800 1000 -80 -60 -40 -20 0 ~ d s ~ d fig. 10. comparison d d = da (red curves) d = ds (blue curves) mnist problem. column show ais-estim log-likelihood data, rest column d d = 0, 5, 10 20, respectively. case rbm 764 visibl 500 hidden unit employed. calcul refer log-likelihood train set approxim ai technique, total 100 run chain 1000 k [25]. valu chosen effici reason check provid reason estim likelihood compar result obtain larger values. rbm run total 1000 epochs, learn rate momentum chosen follow figur 0.0001 0.8, respectively. weight decai used, explor non-zero valu show littl influenc final results. left panel figur 10 show ais-estim log- likelihood train set. plot depict d d = 0, 5, 10, 20 correspond d = da d = ds . notic incomplet neighborhood estim evalu total neighbor train set given distanc exceedingli larg practic use. remarkably, measur work equal cases, show propos estim principl abl captur lead featur likelihood larg problems. notic case d = 0 provid good estim stop point. think ai estim likelihood provid equal good stop point. true, worth notic that, standard paramet real calcul base ais, comput cost increas order magnitude. example, paramet [34] total 5000 run chain 105 ks, comput cost approxim 104 time larger. additionally, order compar exact result [25], test stop criterion mnist problem 25 hidden units. notic case exact partit function evaluated, estim ai approximation. best result achiev learn rate = 103, stop point accord exact likelihood locat epoch 100. contrast, criteria d = ds d = da similar result suggest stop epoch 120. d. larg problem extend analysi large-s problem rel high dimensionality: adult-a5a, connect-4, dna, 9 dataset lr optim ai ds d = 0 ds d = 5 ds d = 10 ds d = 20 epoch logl epoch logl epoch logl epoch logl epoch logl adult-a5a 0.01 976 -13.67 1000 -13.88 880 -13.90 976 -13.67 998 -14.07 caltech101 0.0005 158 -157.91 260 -188.36 354 -209.09 995 -333.74 1000 -328.05 connect-4 0.001 968 -13.77 356 -14.61 992 -13.93 992 -13.93 1000 -13.86 dna 0.01 998 -62.32 056 -80.70 987 -62.34 992 -62.48 991 -62.48 mushroom 0.001 997 -13.41 999 -13.71 1000 -13.58 1000 -13.58 1000 -13.58 nips-0-12 0.05 568 -83.95 094 -128.68 184 -98.96 325 -88.17 993 -85.87 ocr-lett 0.01 086 -41.80 051 -42.01 185 -42.73 492 -44.25 913 -46.06 rcv1 0.01 059 -52.21 050 -52.82 053 -52.77 051 -52.85 097 -54.64 web-w6a 0.001 945 -28.07 967 -28.52 971 -28.47 997 -28.32 998 -28.60 mnist 0.0001 357 -125.73 201 -127.48 266 -126.26 319 -125.85 424 -126.44 dataset lr optim ai da d = 0 da d = 5 da d = 10 da d = 20 epoch logl epoch logl epoch logl epoch logl epoch logl adult-a5a 0.01 976 -13.67 1000 -13.88 1000 -13.88 1000 -13.88 1000 -13.88 caltech101 0.0005 158 -157.91 260 -188.36 262 -189.31 256 -187.92 316 -200.47 connect-4 0.001 968 -13.77 356 -14.61 388 -14.50 356 -14.61 335 -14.70 dna 0.01 998 -62.32 056 -80.70 062 -79.83 056 -80.70 051 -81.50 mushroom 0.001 997 -13.41 999 -13.71 999 -13.71 999 -13.71 999 -13.71 nips-0-12 0.05 568 -83.95 094 -128.68 130 -112.30 111 -119.38 104 -123.05 ocr-lett 0.01 086 -41.80 051 -42.01 051 -42.01 064 -41.87 061 -42.11 rcv1 0.01 059 -52.21 050 -52.82 055 -52.60 053 -52.77 054 -52.69 web-w6a 0.001 945 -28.07 967 -28.52 970 -28.44 969 -28.48 972 -28.42 mnist 0.0001 357 -125.73 201 -127.48 201 -127.48 242 -126.60 242 -126.60 tabl ii optim ais-estim stop point, d = ds d = da predict function distanc d, large-s problem [35][37]. lr logl stand learn rate log-likelihood, respectively. epoch log-likelihood optim stop point reported. row includ result mnist problem. mushrooms, nips-0-12, ocr-letter, rcv1, web-w6a (use [35], [36], example) caltech101 silhouett dataset (use [37], example). dataset down- load tgz topolog references. case perform run averag result curves, mnist problem. tabl ii show result (stop epoch ais-estim log-likelihood epoch) obtain d = ds d = da distanc d. notic result report tabl repres gener behavior, obtain run differ learn rates. seen, criteria work cases. likelihood achiev maximum, usual detect criteria, yield good estim optim likelihood. still, case criterion fail detect good stop point, happen caltech101 nips-0-12. however, case valuabl inform recovered, criteria detect likelihood achiev maximum point after- ward degenerates, suggest start learn process lower learn rate. best likelihood achiev epoch training, criteria usual indic stop near end, case d = ds perform better (dna, connect-4). overall, criteria successfulli detect good stop point taken end learn process. v. conclus work introduc contribut neigh- bore point train set build stop criterion learn cd. shown train set neighbor state contain valuabl inform follow evolut network training. base fact learn tri increas contri- bution relev state decreas contribut rest, continu smooth energi function assign probabl state close train data. kei idea propos stop criterion. fact, differ relat estim (depend number state comput them) propos test experimentally. includ state close train set, second take fraction state small size train set. estim robust requir us forbiddingli larg states, second tractabl captur featur one, provid suitabl stop learn criterion. second estim shown work equal mnist larg datasets, exact comput log-likelihood possible. additionally, main idea proxim train set explor aspect relat learn futur work. furthermore, try differ metric measur proxim neighbour states. acknowledg er: research partial fund spanish research project tin2016-79576-r. fm: work support grant no. fis2014- 56257-c2-1-p dgi (spain). 10 jd: work partial support sgr2014-890 (macda) generalitat catalunya mineco project apcom (tin2014-57226-p) refer [1] d. e. rumelhart, g. e. hinton, r. j. williams, learn intern represent error propagation, parallel distribut process- ing: explor microstructur cognit (vol. 1), d. e. rumelhart j. l. mcclelland, eds. mit press, 1986, pp. 318362. [2] y. bengio, learn deep architectur ai, foundat trend machin learning, vol. 2, no. 1, pp. 1127, 2009. [3] j. schmidhuber, deep learn neural networks: overview, neural networks, vol. 61, pp. 85 117, 2015. [4] g. e. hinton, s. osindero, y. teh, fast learn algorithm deep belief nets, neural computation, vol. 18, no. 7, pp. 15271554, 2006. [5] g. e. hinton r. r. salakhutdinov, reduc dimension data neural networks, science, vol. 313, no. 5786, pp. 504507, 2006. [6] h. larochelle, y. bengio, j. lourador, p. lamblin, explor strategi train deep neural networks, journal machin learn research, vol. 10, pp. 140, 2009. [7] h. lee, r. grosse, r. ranganath, a. y. ng, convolut deep belief network scalabl unsupervis learn hierarch representations, intern confer machin learning, 2009, pp. 609616. [8] q. v. le, m. a. ranzato, r. monga, m. devin, k. chen, g. s. corrado, a. y. ng, build high-level featur larg scale unsupervis learning, 29th intern confer machin learning, 2012. [9] r. sarikaya, g. e. hinton, a. deoras, applic deep belief network natur languag understanding, ieee/acm transact audio, speech, languag processing, vol. 22, no. 4, pp. 778 784, 2014. [10] p. smolensky, inform process dynam systems: founda- tion harmoni theory, parallel distribut processing: explo- ration microstructur cognit (vol. 1), d. e. rumelhart j. l. mcclelland, eds. mit press, 1986, pp. 194281. [11] s. geman d. geman, stochast relaxation, gibb distributions, bayesian restor images, ieee transact pattern analysi machin intelligence, vol. 6, no. 6, pp. 721741, 1984. [12] g. e. hinton, train product expert minim contrast divergence, neural computation, vol. 14, pp. 17711800, 2002. [13] y. bengio, p. lamblin, d. popovici, h. larochelle, greedi layer- wise train deep networks, advanc neural inform process (nips06), vol. 19. mit press, 2007, pp. 153160. [14] y. bengio o. delalleau, justifi gener contrast divergence, neural computation, vol. 21, no. 6, pp. 16011621, 2009. [15] m. a. carreira-perpinan g. e. hinton, contrast diverg learning, intern workshop artifici intellig statistics, 2005, pp. 3340. [16] a. yuille, converg contrast divergence, advanc neural inform process system (nips04), vol. 17. mit press, 2005, pp. 15931600. [17] d. j. c. mackay, failur one-step learn algorithm, 2001, unpublish technic report. [18] t. tieleman, train restrict boltzmann machin approx- imat likelihood gradient, 25th intern confer machin learning, 2008, pp. 10641071. [19] t. tieleman g. e. hinton, fast weight improv persist contrast divergence, 26th intern confer machin learning, 2009, pp. 10331040. [20] h. schulz, a. muller, s. behnke, investig converg restrict boltzmann machin learning, nip 2010 workshop deep learn unsupervis featur learning, 2010. [21] a. fischer c. igel, empir analysi diverg gibb sampl base learn algorithm restrict boltzmann machines, intern confer artifici neural network (icann), vol. 3, 2010, pp. 208217. [22] g. desjardins, a. courville, y. bengio, p. vincent, o. delalleau, parallel temper train restrict boltzmann machines, 13th intern confer artifici intellig statist (aistats), 2010, pp. 145152. [23] g. e. hinton, practic guid train restrict boltzmann machines, neural networks: trick trade. springer, 2012, pp. 599619. [24] r. m. neal, anneal import sampling, 1998, technic report 9805, dept. statistics, univers toronto. [25] r. salakhutdinov i. murray, quantit analysi deep belief networks, intern confer machin learning, 2008, pp. 872879. [26] e. aart j. korst, simul anneal boltzmann machines. stochast approach combinatori optim neural computing. john wiley, 1990. [27] a. r. sankar v. n. balasubramanian, similarity-bas contrast diverg method energy-bas deep learn models, jmlr: workshop confer proceedings, vol. 45, pp. 391406, 2015. [28] x. ma x. wang, averag contrast diverg train restrict boltzmann machines, entropy, vol. 18, no. 1, p. 35, 2016. [online]. available: [29] m. gabrie, e. w. tramel, f. krzakala, train restrict boltz- mann machin thouless-anderson-palm free energy, ad- vanc neural inform process system 28, 2015, pp. 640 648. [30] a. fischer c. igel, train restrict boltzmann machines: introduction, pattern recognition, vol. 47, pp. 2539, 2014. [31] l. li, j. lv, z. yi, non-neg represent learn algorithm select neighbors, machin learning, vol. 102, no. 2, pp. 133153, 2016. [32] x. peng, z. yu, z. yi, h. tang, construct l2-graph robust subspac learn subspac clustering, ieee transact cybernetics, vol. pp, no. 99, pp. 114, 2016. [33] p. buhlmann s. van geer, statist high-dimension data: methods, theori applications. springer scienc & busi media, 2011. [34] m.-a. cote h. larochelle, infinit restrict boltzmann machine, neural computation, vol. 28, pp. 12651288, 2016. [35] h. larochelle, y. bengio, j. turian, tractabl multivari binari densiti estim restrict boltzmann forest, neural com- putation, vol. 22, pp. 22852307, 2010. [36] h. larochel i. murray, neural autoregress distribut estimator, intern workshop artifici intellig statistics, 2011, pp. 2937. [37] b. m. marlin, k. swersky, b. chen, n. freitas, induct principl restrict boltzmann machin learning, intern workshop artifici intellig statistics, 2010, pp. 509516.