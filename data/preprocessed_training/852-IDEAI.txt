april 2018 feature-tre label case base mainten nariman nakhjiri a,1, maria salam miquel snchez-marr b facultat matemtiqu informtica, universitat barcelona institut complex system (ubics), universitat barcelona (ub) b knowledg engin machin learn group (kemlg-upc) intellig data scienc artifici intellig research centr (ideai-upc) dept. scienc universitat politcnica catalunya (upc) abstract. case base mainten (cbm) algorithm updat content case base aim improv case-bas reason performance. paper, introduc novel cbm method call feature-tre label (ftl) focu increas gener accuraci case-bas reason (cbr) system. propos ftl algorithm design detect remov noisi case case base, base valu distribut individu featur avail data. compet ftl method compar well-known state-of- the-art cbm algorithms. test 25 dataset select uci repository. result ftl obtain higher accuraci state-of-the-art method cbr, statist signific degree. keywords. case-bas reasoning, case base maintenance, decis tree, 1. introduct compet case-bas reason (cbr) [12] measur ac- cord aspects. gener accuraci second size case base. field case base mainten (cbm) [9], method aim increas accu- raci predict refer compet enhanc [3] algorithms, method focus reduc size case base keep accuraci level call compet preserv [15] algorithms. classif new instanc cbr heavili depend individ- ual instanc case base, order increas averag classif accuraci data, import defin model distinguish remov noisi case case base. model built base inter-rel instanc case base. paper, propos feature-tre label (ftl) algorithm. compet enhanc case base mainten algorithm look valu distribut featur train data. ftl compet model contain decis tree 1correspond author: univers barcelona, spain; e-mail: april 2018 classifi feature, built cart [2] technique. eval- uation ftl algorithm demonstr abl achiev higher averag accuraci comparison well-known state-of-the-art methods. basic model cbr state-of-the-art cbm method test ftl perform perspective. analyz performance, chosen 25 dataset uci [7] repository. result statist signific improv accuraci ftl compar baselin cbr state-of-the-art methods. structur rest paper follows. first, section 2 present relat work case base mainten us decis tree cbr system. section 3 detail propos method section 4 discuss performance. finally, section 5 devot conclus futur work. 2. relat work literature, case base mainten refer algorithm appli case base case-bas reason system, remov harm content improv perfor- mance. earli attempt cbm compet enhanc categori wilson edit nearest neighbor (enn) [14]. enn decrement algorithm consid case case base misclassifi k-nearest neighbor noise. remov them. enn inspir later research. tomek method repeat edit nearest neighbor (renn) [13], take enn strategi appli multipl time case base nois detected. branch cbm algorithm build compet model properti set case case base. blame-bas nois reduct (bbnr) [6] techniqu introduc delany, us liabil set determin compet instances. delani later expand work properti set reachability, dissimilarity, coverage, liabil rdcl [5] method. rdcl categor case case base profil base perform leave-one-out test case base. tree classifi common tool classification. tend highest accuraci methods, reason popular easi interpret structure, low time consumpt train/test abil handl numer nomin attributes. numer studi tri synthes decis tree (dt) cbr differ aspects, cbm system. mention richardson warren work [1] decis tree add weight featur chang fan im- plement [4] hybrid method cbr, decis tree genet algorithm medic domain, approach success results. experi combin dt cbr data mine (dm) structure. increas dm, research turn focu emb tech- niqu cbr order retriev better inform improv perform case base reasoner. studies, experi dt cbr system, interesting, work huang, chen lee [8] cbr data-min diseas diagnosis. finally, mention perner work [11], focus data-min cbr spars data decis tree. mentioned, area decis tree mainten cbr fulli cover propos ftl method open new path case base maintenance. april 2018 3. method propos feature-tre label (ftl) cbm algorithm us decis tree classifi creat reform version case base, detect noisi cases. 3.1. introduct feature-tre label noisi harm case detect requir compet model. gener data expert advic specif rule point out, sourc data distribut interrelations. goal ftl discov addit inform distribut feature. extra knowledg help propos method distinguish regular irregular case case base. ftl label featur case case base common class locality. label represent data help detect suspici case decid on removed. dataset n attributes, process ftl summar steps: 1. constructor: train decis tree featur total n trees. valu tree come case case base. 2. labeler: build reform version case base n train deci- sion trees. 3. ham nearest neighbors: run modifi version nearest neigh- bor classifi reform case base. 4. removal: remov case class neigh- borhood majority. step ftl algorithm categor phase procedure. step belong data prepar phase, creat reform case base, later step defin mainten cycle, ftl detect remov noisi cases. algorithm function present paper us cb notat case base n attributes, consist case like c cb : c = {c1,c2, ...,cn,lc}, ci valu ith attribut lc class label case c. algorithm 1 ftl input: case base cb n attributes. output: maintain version case base, cbedit . // phase 1: data prepar t =constructor(cb) . step 1. cblbd = labeler(cb,t ) . step 2. // phase 2: mainten cycl cbedit = clbd cblbd cbtest =cblbd{clbd} neighbor = hamming_nearest_neighbours(cbtest ,clbd) . step 3. predicted_class = ma jority_vote(neighbors) predicted_class == actual class clbd cbedit .append(c) . step 4. return cbedit april 2018 algorithm 1 pseudo-cod ftl method. phase algo- rithm consist constructor label functions. function detail section 3.2. second phase ftl includ step 3 step 4, take place loop leave-one-out test case reform case base (cblbd). case c step 4, ad cbedit , correspond case raw data clbd cblbd . explan second phase ftl describ section 3.3. 3.2. data prepar phase ftl method data preparation. phase reform version case base produc (cblbd). phase consist step 1 constructor, explain section 3.2.1, step 2 labeler, detail section 3.2.2. data prepar ftl us decis tree classifi built breiman classifica- tion regress tree (cart) [2] technique. cart allow creat tree clas- sifier featur data independ numer nomin attribute. gini impur measur split node trees. worth mention that, tree classifi ftl built featur data, need depend attribut select splits. improv construct time tree algorithm compar complex tree built features. 3.2.1. constructor constructor function present algorithm 2 devot train n tree classifi featur valu case case base. output constructor function set t = {t1, t2, t3, ..., tn}, ti decis tree classifi train valu ith attribut case case base. accord algorithm 2, train set decis tree classifi ti contain partial case instanc case base. partial case instanc c, consist singl featur valu ci class label c. algorithm 2 constructor input: cb: case base n attributes. output: set t = {t1, t2, t3, ..., tn}. t = {1,2, ...,n} train_set = c cb partial_cas = {ci,lc} train_set.append(partial_case) ti = decisiontree.train(train_set) t.append(ti) return t decisiontree.train algorithm 2, call train routin decis tree cart train_set data return decis tree classifier. april 2018 3.2.2. label step train decis tree constructor, us classifi build reform case base. label function design replac featur valu case case base classif result produc attribut tree. accord algorithm 3, featur valu ci case case base, call predict routin tree classifi (ti.predict). result label domain data class labels, l. label valu replac ci label case base (cblbd). algorithm 3 label input: cb: case base n attributes, t: set t produc constructor func- tion. set attribut cb. output: cblbd : label version case base. cblbd = . label case base c cb clbd = . label case label = ti.predict(ci) clbd .append(label) cblbd .append(clbd) return cblbd better understand exampl ftl data transform iri dataset. instanc small dataset variat iri flower. iri dataset, = {sepallength, sepalwidth, petallength, petalwidth} l = {setosa ,versicolour, virginica}. case c = {4.7,3.2,1.3,0.2} class label lc = {setosa}, ftl process transform clbd = {versicolour,setosa,virginica,setosa}. 3.3. mainten cycl second phase ftl method us leave-one-out test label case base produc data preparation. leave-one-out test allow retriev neighbor case (step 3) test compet (step 4). section 3.3.1 section 3.3.2, respectively. 3.3.1. ham nearest neighbor algorithm 4 implement modifi version k-nn classifier, suitabl cblbd , retriev nearest neighbor given case. differ k-nn classifi ham nearest neighbor (hnn) ftl. differ li metric hnn uses. suggest ham distanc chosen metric ftl.duo fact cblbd contain nomin features, ham distanc reason option distanc comparison. second differ hnn k-nn number neighbor method retrieve. k-nn classifi return constant number case (k) cycle, hnn retriev case cblbd minimum distanc target instance. nomin featur cblbd appli ham metric, april 2018 possibl outcom distanc case rang {1,2,3, ...,n}, n repres number featur data. limit outcom distanc situat case posit nearest neighbor classif procedure. case differ origin featur values, similar repre- sentat cblbd . have case distanc choos constant number k undepend choos randomli case similar distanc exce number case want retrieve. algorithm 4 detail procedur ham nearest neighbors. algorithm 4 ham nearest neighbor input: cblbd : reform case base, ctest : label case test, ctest = {c1,c 2,c 3, ...,c n,lc} output: neighbors: set closest instanc cblbd ctest distanc = c cb distancec = 0 ci == ci distancec+= 0 distancec+= 1 distances.append(distancec) distancemin = minimum(distances) neighbor = c cb distancec == distancemin neighbors.append(c) return neighbor 3.3.2. remov rational remov polici ftl irregular case common valu featur accord class membership repres class futur classifications. final step ftl method describ algorithm 1, comput major class retriev neighbor hnn case. predict class label version case c differ actual class, case c irregular featur valu categor noise. case predict actual class wai maintain case base return final output ftl algorithm. 4. evalu evalu ftl method, 25 dataset uci [7] repositori select testing. purpos comparison, well-known state art algorithm implement tests. algorithm are, enn [14], renn [13], bbnr [6], rdcl [5]. rdcl method evalu set remov april 2018 case case base dl dcl profiles. method evalu us 10-fold cross validation, 1nn classifi heterogen metric (euclidean numer ham nomin values) cbr system. 4.1. dataset tabl 1 show detail dataset column repres acronym dataset, second column reflect name. column tag #num #nom repres number numer nomin attribut dataset. presenc miss valu dataset shown fifth column tabl 1, sixth dedic number classes. column show popul percent- ag crowd class rarest one. dataset # # miss # class ratio num nom valu class (most/least) bl bal 4 0 3 46.1% /7.8% bi biopsi 24 0 2 51.6% / 48.4% br breast-w 9 0 ye 2 65.5% / 34.5% bp bupa 6 0 2 58.0% / 42.0% cm cmc 2 7 3 42.7% / 22.6% colic 7 15 ye 2 63.0% / 37.0% cr credit-a 6 9 ye 2 55.5% / 44.5% fi fi 21 0 2 56.0% / 44.0% gl glass 9 0 6 35.5% / 4.2% gr grid 2 0 2 50.0% / 50.0% hc heart-c 6 7 ye 2 54.5% / 45.5% hh heart-h 6 7 ye 2 63.9% / 36.1% hs heart-statlog 13 0 2 55.6% / 44.4% hp hepat 6 13 ye 2 79.4% / 20.6% io ionospher 34 0 2 64.1% / 35.9% ir iri 4 0 3 33.3% / 33.3% lb labor 8 8 ye 2 64.9% / 35.1% mx mx 0 11 2 50.0% / 50.0% pi pima-indian 8 0 2 65.1% / 34.9% sg segment 19 0 7 14.3% / 14.3% sn sonar 60 0 2 53.4% / 46.6% sb soybean 0 35 ye 19 13.5% / 1.2% ve vehicl 18 0 4 25.8% / 23.5% wi wine 13 0 3 39.9% / 27.0% zo zoo 1 16 7 40.6% / 4.0% tabl 1. dataset detail 4.2. result tabl 2 present averag accuraci 10-fold cross valid obtain ftl well-known state-of-the-art method dataset. row tabl 2 overal averag accuraci case base reduct rate method. accord tabl 2 averag accuraci obtain ftl method 25 dataset highest valu peer wai cbr. furthermore, greatest im- provement happen colic (co), bal (bl), heart-h (hh), heart-statlog (hs) soy- bean (sb) dataset 5 percent improv accuracy. compari- son baselin cbr, largest increas accuraci belong colic (co) dataset 11.16%. april 2018 data cbr enn renn bbnr rdcl ftl bl 76.16 85.13 85.28 84.01 82.9 84.64 bi 83.18 81.43 81.61 82.8 82.4 83.08 br 95.86 96.11 96.11 96.41 96.12 96.84 bp 62.93 60.27 61.21 63.52 63.2 61.21 cm 44.4 45.61 44.52 46.97 46.9 45.95 73.36 81.77 82.61 83.98 84.22 84.52 cr 81.76 85.66 85.36 85.06 85.36 85.37 fi 63.93 64.37 62.54 63.98 65.37 63.93 gl 66.31 69.22 68.7 67.6 67.26 66.05 gr 96.13 95.76 95.92 96.5 96.34 92.8 hc 74.2 80.44 80.43 76.5 76.13 79.13 hh 72.83 79 80.72 76.61 76.28 80.28 hs 74.07 77.78 77.04 78.15 76.3 79.63 hp 78 82.48 82.55 81.26 80.05 81.33 io 86.93 84.93 84.93 88.6 87.46 86.93 ir 95.33 96 96 94 95.33 96 lb 83.38 79.71 79.71 85.38 85.38 86.48 mx 78.61 76.21 75.87 72.26 76.85 78.51 pi 70.73 75.56 75.8 72.94 74.11 71.13 sg 97.36 95.76 95.54 97.14 97.23 95.32 sn 86.84 81.57 80.71 81.68 84.56 86.84 sb 82.15 88.2 86.88 89.21 89.5 87.79 ve 69.44 67.55 67.3 67.17 67.16 68.36 wi 95.64 95.64 94.46 95.64 95.64 95.64 zo 94.63 92.52 89.85 92.79 94.63 92.52 acc 79.37 80.75 80.47 80.81 81.07 81.21 cb red. 0.00 20.40 22.51 15.67 9.29 15.45 tabl 2. averag result method averag case base reduct method shown tabl 2 (cb red.). ftl method reduc size case base 15.45% average. valu rang bbnr method overal stand accept case base reduct consid ftl compet enhanc method. aspect cbr compet time case base mainten process take. tabl 3 reflect comput cost ftl compar state-of-the-art methods. compar time aspect, period measured: averag time preprocess (tpreprocessing) time classif new case (tclassi f ication) 25 datasets, shown tabl 3. method tpreprocess tclassi f icat cbr 0.0000 0.0051 enn 1.4849 0.0040 renn 1.8855 0.0037 bbnr 1.5832 0.0040 rdcl(dl,dcl) 1.8785 0.0043 ftl 3.5635 0.0040 tabl 3. averag record time tabl 3 shows, renn, bbnr rdcl preprocess time same, ftl take twice long. lead ftl disadvantage. however, time classification, case base reduct aspect ftl stand second best time bbnr enn algorithms. note that, method reduc classif time reduc size case base comput requir retriev neighbor case case base. method largest case base reduct score lower better time. additionally, analyz signific result ftl performance, fried- man [?]-nemenyi [10] test applied. friedman test statist test multipl com- parison give rank method evaluation. lower mean rank mean april 2018 better perform algorithm. friedman test rank ftl best method. figur 1 illustr friedman test result ftl state-of-the-art algorithm signific threshold method provid nemenyi test. figur 1. friedman-nemenyi test result method analysi signific threshold nemenyi test shown figur 1 set 95% con- fidence. method overlap boundaries, differ statisti- calli signific degree. dash line figur 1, draw boundari signif- icanc threshold ftl method. blue dash line show, averag accuraci ob- tain ftl, significantli better cbr, enn, renn, rdcl algorithms. improv ftl compar bbnr pretti close signific threshold. 5. conclus futur work feature-tre label case base mainten algorithm build decis tree data attributes. tree classifi transform raw case base new format. new format attribut case case base replac class label classif rel decis tree. label instances, ftl us label case base detect remov instanc correctli classifi neighbors. label version case misclassifi label neighbor tend featur valu repres class accord member case base. analysi ftl perform show competence. averag accuraci obtain ftl 25 dataset better baselin cbr well-known state-of-the-art method compar to. friedman-nemenyi test show statist signific better perform ftl comparison baselin cbr, enn, renn, rdcl algorithms. feature-tre label succe improv gener accuraci cbr sys- tem, room improv compet model. point possibl futur works. first, experi robust prune polici decis tree ftl. second path futur work data prepa- ration phase ftl mainten cycl strategies. transform data april 2018 ftl model extract knowledg data space qualiti individu case futur classifications. acknowledg work partial support spanish ministri scienc innova- tion (grant number tin2015-71147-c2-2), catalan agenc univers re- search grant manag (agaur) (grant number 2017 sgr 341 2017 sgr 574), spanish network "learn machin singular problem applica- tion (mapas)" (tin2017-90567-redt, mineco/fed eu). refer [1] j. g. bazan. hierarch classifi complex spatio-tempor concepts. transact rough set ix, page 474750. springer, 2008. [2] l. breiman. classif regress trees. routledge, 1984. [3] h. brighton c. mellish. advanc instanc select instance-bas learn algorithms. data mine knowledg discovery, 6(2):153172, 2002. [4] p.-c. chang, c.-y. fan, w.-y. dzan. cbr-base fuzzi decis tree approach databas classi- fication. expert system applications, 37(1):214225, 2010. [5] s. j. delany. good, bad incorrectli classified: profil case case-bas editing. intern confer case-bas reasoning, page 135149. springer, 2009. [6] s. j. delani p. cunningham. analysi case-bas edit spam filter system. european confer case-bas reasoning, page 128141. springer, 2004. [7] d. dheeru e. karra taniskidou. uci machin learn repository, 2017. [8] m.-j. huang, m.-y. chen, s.-c. lee. integr data mine case-bas reason chronic diseas prognosi diagnosis. expert system applications, 32(3):856867, 2007. [9] d. b. leak d. c. wilson. categor case-bas maintenance: dimens directions. european workshop advanc case-bas reasoning, page 196207. springer, 1998. [10] p. nemenyi. distribution-fre multipl comparisons. biometrics, volum 18, page 263. interna- tional biometr soc 1441 st, nw, suit 700, washington, dc 20005-2210, 1962. [11] p. perner. mine spars big data case-bas reasoning. procedia science, 35:1933, 2014. [12] m. m. richter r. o. weber. case-bas reasoning: textbook. springer, 2013. [13] i. tomek. experi edit nearest-neighbor rule. ieee transact systems, man, cybernetics, (6):448452, 1976. [14] d. l. wilson. asymptot properti nearest neighbor rule edit data. ieee transact systems, man, cybernetics, (3):408421, 1972. [15] d. r. wilson t. r. martinez. reduct techniqu instance-bas learn algorithms. machin learning, 38(3):257286, 2000.