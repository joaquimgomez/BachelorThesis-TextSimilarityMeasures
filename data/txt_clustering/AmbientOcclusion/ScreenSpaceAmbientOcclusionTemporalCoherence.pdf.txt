




























































Volume xx (200y), Number z, pp. 1–12

High-Quality Screen-Space Ambient Occlusion using
Temporal Coherence

paper1092

Abstract
Ambient occlusion is a cheap but effective approximation of global illumination. Recently, screen-space ambient
occlusion (SSAO) methods, which sample the frame buffer as a discretization of the scene geometry, have be-
come very popular for real-time rendering. We present temporal SSAO (TSSAO), a new algorithm which exploits
temporal coherence to produce high-quality ambient occlusion in real time. Compared to conventional SSAO,
our method reduces both noise as well as blurring artifacts due to strong spatial filtering, faithfully represent-
ing fine-grained geometric structures. Our algorithm caches and reuses previously computed SSAO samples, and
adaptively applies more samples and spatial filtering only in regions that do not yet have enough information
available from previous frames. The method works well for both static and dynamic scenes.

Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling

1. Introduction

Ambient occlusion (AO) describes the percentage of incident
ambient light on a surface point that is occluded by surround-
ing geometry. AO is used for shading by modulating the am-
bient color of the surface point with it. From a physical point
of view, ambient occlusion could be seen as the diffuse illu-
mination due to the sky [Lan02]. Ambient occlusion of a
surface point p with normal np is computed as [CT81]:

ao(p,np) =
1
π

∫
Ω

V (p,ω)max(np ·ω,0)dω, (1)

where ω denotes all directions on the hemisphere and V
is the (inverse) binary visibility function, with V (p,ω) = 1
if visibility was blocked by an obstacle, 0 otherwise. The
visibility term usually considers obstacles within a certain
sampling radius only.

Ambient occlusion is heavily used in production render-
ing and recently also in real-time rendering and many high-
profile games [Mit07,FM08], because it is a local effect (due
to the limited sampling radius) that is very cheap compared
to a full global illumination solution, but still greatly en-
hances the realism of the scene. Screen Space Ambient Oc-
clusion (SSAO) techniques decouple the shading from the
scene complexity by using the frame buffer as a discrete

approximation of the scene geometry. The performance of
SSAO depends mainly on the number of samples per frame,
hence a relatively small number of samples is typically used
to reach the desired performance, and the resulting noise is
blurred with a spatial depth-aware filter. However, a small
number of samples can be insufficient for shading complex
geometry with fine details, especially if a large sampling ra-
dius is used. The final image will look either noisy or blurry,
depending on the size of the filter kernel. Generally a care-
ful tuning of the filter parameters is required in order to pro-
vide stable results, otherwise artifacts may appear, e.g., halos
around small depth discontinuities.

In this paper, we present an algorithm that achieves high-
quality ambient occlusion which is neither blurry nor prone
to noise artifacts, with a minimum amount of samples per
frame. We reuse the available AO information from previous
frames by exploiting temporal coherence between consecu-
tive image frames. We identify pixels describing identical
world positions by means of temporal reprojection. The cur-
rent state of the solution is cached in a so-called ambient oc-
clusion buffer. Each frame we compute a few new AO sam-
ples, then blend these with the accumulated samples from
the previous frames. The ambient occlusion solution is then
combined with the image resulting from direct diffuse illu-
mination in a separate step. See Figure 1 for a comparison of
SSAO with and without employing temporal coherence.

submitted to COMPUTER GRAPHICS Forum (6/2010).



2 paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence

Figure 1: From left to right: SSAO without temporal coherence (23 FPS) with 32 samples per pixel, with (a) a weak blur, (b)
a strong blur. (c) TSSAO (45 FPS), using 8–32 samples per pixel (initially 32, 8 in a converged state). (d) Reference solution
using 480 samples per frame (2.5 FPS). All images at 1024x768 resolution and using 32 bit precision render targets. The scene
has 7M vertices and runs at 62 FPS without SSAO shading.

Recently, reprojection techniques have been used for a
number of useful applications, like antialiasing or shadow
mapping. SSAO is an especially interesting case because
it takes a pixel neighborhood into account, which leads
to unique challenges, especially for dynamic scenes. First,
since SSAO is a postprocessing operation, information about
reprojection needs to be stored during the main rendering
pass. Second, the validity of reprojected pixels does not de-
pend only on the pixel itself, but also on its neighborhood,
requiring a new validity test for reprojected pixels. Third,
the convergence of the solution allows us to reduce or com-
pletely omit spatial filtering that is typically necessary in
SSAO methods, and allows us to rely on a minimal num-
ber of new samples per frame. For pixels that have not con-
verged, e.g., when cached samples have been invalidated,
we can use information about convergence in order to re-
construct the value from nearby samples using an adaptive
convergence-aware filter that gives more weight to already
converged samples.

2. Related Work

2.1. Ambient Occlusion

Ambient occlusion is a shading technique that measures the
reverse surface exposure and was introduced by Zhukov et
al. [ZIK98]. It is extensively used in areas like production
rendering [Lan02,PG04] because it is a relatively simple and
cheap technique which greatly enhances the appearance of
an image.

In recent years several conceptually different approaches
were proposed that make online calculation of ambient oc-
clusion in dynamic scenes feasible for real-time applica-

tions. Some object-based approaches distribute the occlu-
sion power of each object on all the other objects in the
range of interest [KL05, MMAH07]. These methods have
problems handling overlapping occluders. A feasible way
of testing blocker visibility using spherical harmonics expo-
nentiation for soft shadows and low-frequency illumination
was proposed by Ren et al. [RWS∗06]. Bunnell [Bun05] pro-
posed a per-vertex ambient occlusion method for dynamic
scenes, which uses a hierarchical approach to break down the
complexity, and can be extended to compute diffuse color
bleeding.

Image-space methods, on the other hand, interpret the
frame buffer as a discrete scene approximation and are of-
ten referred to as screen-space ambient occlusion [Mit07].
They are usually used in combination with deferred shad-
ing [ST90, Shi05]. The original implementation compared
the depth of ambient occlusion samples against the depth
buffer in order to estimate the number of samples that are oc-
cluded [Mit07]. Shanmugam and Arikan [SA07] proposed a
two-stage image space method, one stage for the low fre-
quency far field illumination, and one for high frequency
near field ambient occlusion. The high frequency part treats
each pixel of the depth buffer as a spherical occluder. Fox
and Compton [FC08] proposed a similar SSAO shading
technique that samples the depth buffer and weights the con-
tribution by the cosine between surface normal and the vec-
tor to the sample as well as the sample distance. Recently,
methods that use horizon mapping for computing the oc-
cluded part of the surface were proposed [BSD08]. Ritschel
et al. [RGS09] extended an SSAO generation method to in-
clude directional information and first-bounce global illumi-
nation. Their SSAO method is similar to the original method
of Mittring, but also takes the angle to the sample point

submitted to COMPUTER GRAPHICS Forum (6/2010).



paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence 3

Figure 2: This figure compares rendering without (left) and
with (right) ambient occlusion, and shows that AO allows
much better depth perception and feature recognition, with-
out requiring any additional lighting.

into account to accurately weight the samples that passed
the depth test. Bavoil et al. [BS09] proposed some general
techniques that aim at reducing the problems of SSAO algo-
rithms due to information missing in the depth buffer repre-
sentation. They also proposed a multi-resolution approach
that computes half resolution SSAO in regions with low
difference in the SSAO value range, and switches to high
resolution SSAO only in regions with high difference. Our
method also makes use of adaptive sampling, in our case
with the different goal to reduce the computational effort put
into sampling in regions that are already sufficiently con-
verged due to temporal coherence.

Ambient occlusion for character animation can be
achieved by precomputing ambient occlusion for a couple
of key poses and then interpolating between them [KA06,
KA07], at considerable storage cost.

Figure 2 demonstrates the visual impact of SSAO for the
depth perception of a scene.

2.2. Reprojection Techniques

In order to use temporal coherence for GPU rendering, we
employ a technique called reverse reprojection, which was
independently proposed by Scherzer et al. [SJW07] (referred
to as temporal reprojection) and Nehab et al. [NSL∗07]. This
allows the reuse of pixel content from previous frames for
the current frame by associating image pixels from the cur-
rent frame with the pixels from the previous frame that rep-
resent the same world space position. The technique was
shown to be useful for a variety of applications, like shadow
mapping, anti-aliasing, or even motion blur [Ros07].

Smedberg et al. [SW09] independently proposed using re-
projection to enhance the quality of SSAO in a GDC talk.
However, exact details about the method are not available,
and they do not specifically address the proper invalidation
of incorrect cache values.

3. Our algorithm

3.1. SSAO generation

SSAO methods aim to approximate the original AO inte-
gral in screen space. Several versions of SSAO with differ-
ent assumptions and trade-offs have been described [BSD08,
Mit07, RGS09]. While we demonstrate our technique with
only two different ambient occlusion methods, it works with
many more, and could be used for several other shading
method that depend on a screen-space sampling kernel. We
assume that such a shading method can be written as an av-
erage over contributions C which depend on a series of sam-
ples si:

AOn(p) =
1
n

n

∑
i=1

C(p,si) (2)

A typical example contribution function for SSAO would
be

C(p,si) =V (p,si)cos(si− p,np)D(|si− p|).

V (p,si) is a binary visibility function that gives 0 if si is
visible from p and 1 otherwise. Visibility is for example de-
termined by checking whether si is visible in the z-buffer.
We assume that the samples si have been precomputed and
stored in a texture, for example a set of 3D points uniformly
distributed in the hemisphere, which are transformed into the
tangent space of p for the evaluation of C [Mit07]. D is a
monotonic decreasing function between 1 and 0 of the dis-
tance from p to si . In the simplest case D is a step function,
although a smooth falloff provides better results, e.g., given
by an exp() function. In order to achieve faster convergence
we use a Halton sequence for sample generation, which is
known for its low discrepancy [WH00].

3.2. Reprojection

Reprojection techniques use two render targets in ping-
pong fashion, one for the current frame and one represent-
ing the cached information from the previous frames (de-
noted as real-time reprojection cache [NSL∗07] or history
buffer [SJW07]). In our context we cache ambient occlusion
values and therefore denote this buffer as ambient occlusion
buffer.

For static geometry, reprojection is constant for the whole
frame, and can be carried out in the pixel shader or in a
separate shading pass (in the case of deferred shading) us-
ing current and previous view (V ) and projection (P) ma-
trices, where t denotes the post-perspective position of a
pixel [SJW07]:

toldx′ ,y′ ,z′ = PoldVoldV
−1
newP

−1
newtnewx,y,z (3)

submitted to COMPUTER GRAPHICS Forum (6/2010).



4 paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence

Figure 3: Frame-to-frame 3D optical flow of a Happy Bud-
dha model rotating around its base. The fixpoint of the rota-
tion is visible as a dark spot.

In our deferred shading pipeline, we store eye linear depth
values for the current frame and the previous frame, and use
them to reconstruct the world space positions p. Note that
pold can be obtained from the above formula by applying
the inverse view-projection matrix to t. For dynamic scenes,
this simple formula does not work because reprojection de-
pends on the transformations of moving objects. Nehab et
al. [NSL∗07] therefore propose to do the reprojection in the
vertex shader by applying the complete vertex transforma-
tion twice, once using the current transformation parameters
(modeling matrix, skinning etc.) and once using the param-
eters of the previous frame.

However, in a deferred shading pipeline, pold needs to
be accessed in a separate shading pass, where information
about transformation parameters is already lost. Therefore,
we store the 3D optical flow pold − pnew in the frame buffer
as another shading parameter (alongside normal, material
etc.), using a lower precision than for the absolute depth val-
ues (16 bit instead of 32 bit). See Figure 3 for a depiction of
optical flow induced by a rotating Happy Buddha model.

During reprojection, we have to check for pixels that be-
came invalid (e.g. due to a disocclusion). This will be de-
scribed in Section 3.4, where we also show that SSAO im-
poses some additional constraints for a pixel to stay valid.

3.3. Temporal refinement

The main idea of our algorithm is to spread the computation
of ambient occlusion (Equation 2) over several frames by
using reprojection. Whenever possible we take the solution
from the previous frame that corresponds to an image pixel
and refine it with the contribution of new samples computed
in the current frame. In frame t +1, we calculate a new con-

tribution Ct+1 from k new samples:

Ct+1(p) =
1
k

nt (p)+k

∑
i=nt (p)+1

C(p,si) (4)

and combine them with the previously computed solution
from frame t:

AOt+1(p) =
nt(p)AOt(pold)+ kCt+1(p)

nt(p)+ k
(5)

nt+1(p) = min(nt(p)+ k,nmax), (6)

where nt keeps track of the number of samples that have
already been accumulated in the solution. If the cached AO
value is invalid, then the previous solution can be discarded
by setting nt to 0. Otherwise, the solution will reach a stable
state after a few frames.

Theoretically, this approach can use arbitrarily many sam-
ples. In practice, however, this is not advisable: since repro-
jection is not exact and requires bilinear filtering for recon-
struction, each reprojection step introduces an error which
accumulates over time. This error is noticeable as an increas-
ing amount of blur [YNS∗09]. Furthermore, the influence of
newly computed samples becomes close to zero, and previ-
ously computed samples never get replaced. Therefore we
clamp nt to a user-defined threshold nmax, which causes the
influence of older contributions to decay over time. Thus,
conv(p) = min(nt(p),nmax)/nmax is an indicator of the state
of convergence. Note that for nmax→∞, Equation 6 would
converge to the correct solution – unlike the exponential
smoothing used in previous approaches [SJW07, NSL∗07],
which acts as a temporal filter kernel.

In our experience, a threshold nmax in the range
[500..1500] provides a sufficiently fast update frequency to
avoid major blurring artifacts while avoiding undersampling
artifacts like temporal flickering. Figure 4 (left) depicts how
the influence of a new sample changes from the moment it
is introduced, and (right) the exponential decay of the influ-
ence of a previously computed AO solution after the thresh-
old nmax was reached.

3.3.1. Implementation notes

The value nt is stored in a separate channel in the ambient
occlusion buffer. We also store the starting index into the set
of precomputed samples, which is used to take the new sam-
pling positions for the current frame from a precomputed
low-discrepancy Halton sequence. Hence the current index
position is propagated to the next frame by means of reverse
reprojection like the SSAO values. In order to prevent the
index position to be interpolated by the hardware and intro-
duce a bias into the sequence, it is important to always fetch
the index value from the nearest pixel center.

submitted to COMPUTER GRAPHICS Forum (6/2010).



paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence 5

0 500 1000 1500 2000
ðSamples

0.002

0.004

0.006

0.008

0.010
Weight

ntmax=1500

nmax=1000

nmax=500

0 1000 2000 3000 4000 5000
ðSamples

0.2

0.4

0.6

0.8

1.0
Weight

nmax=500

nmax=1000

nmax=1500

Figure 4: The weight (1/nt ) of a new sample (top) and of a
previously computed AO solution (bottom) after given num-
ber of samples using thresholds nmax = 500,1000,1500.

3.4. Detecting and dealing with invalid pixels

When reprojecting a fragment, we need to check whether
the cached AO value is valid. If it is not, nt is reset to 0
and a new AO solution is computed. In order to detect such
invalid pixels, we check if either one of the following two
conditions has occured: 1) a disocclusion of the current frag-
ment [SJW07,NSL∗07], and 2) changes in the sample neigh-
borhood of the fragment. In the following we discuss both
conditions.

3.4.1. Detecting disocclusions

Previous approaches [SJW07, NSL∗07] check for disocclu-
sions by comparing the depth of the reprojected fragment
position dnew to the depth of the cached value dold :

|dnew−dold |< ε

However, we found that neither screen-space depth val-
ues nor eye-linear depth values gave reliable results. Screen-
space depth values are only accurate in a small region
around the near plane, while eye-linear depth comparisons
are overly sensitive in regions near the far plane and not sen-
sitive enough near the near plane. The solution is to store

s
2 old

s
2s

1

s
3

s
4

p

s
2 old

s
1 old

s
3 old

s
4 oldp

old

Frame t-1

Frame t

|s
2 old -p

old |

|s
2 -p|

Reproject into
previous frame

Figure 5: The distance of p to sample point s2 in the current
frame differs significantly from the distance of pold to s2old
in the previous frame, hence we assume that a local change
of geometry occurred, which affects the shading of P.

eye-linear depth values, but to consider relative depth differ-
ences instead of absolute ones, by checking for

|1−
dnew
dold
|< ε, (7)

which gives good results even for large scenes with a wide
depth range. Note that pixels which were outside the frame
buffer in the previous frame are also marked as invalid.

3.4.2. Detecting changes in the neighborhood

Testing for disocclusion is a sufficient condition for validity
in a purely static scene, or for pure antialiasing. However, for
shading kernels that access neighboring pixels in dynamic
scenes, like SSAO, we have to take into account that the
shading of the current pixel can be affected by nearby mov-
ing objects, even if there is no disocclusion. Consider for
example a scene where a box is lifted from the floor. The
SSAO values of pixels in the contact shadow area surround-
ing the box change even if there is no disocclusion of the
pixel itself.

The size of the neighborhood to check is equivalent to
the size of the sampling kernel used for SSAO. Checking the
complete neighborhood of a pixel would be prohibitively ex-
pensive, therefore we use sampling. Actually, it turns out that
we already have a set of samples, namely the ones used for
AO generation. That means that we effectively use our AO
sampling kernel for two purposes: for computing the current
contribution Ct(p), and to test for validity. A pixel p is con-
sidered to be valid for a sample si if the relative positions
of sample and pixel have not changed by more than ε (see
Figure 5):

submitted to COMPUTER GRAPHICS Forum (6/2010).



6 paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence

Figure 6: This figure shows pixels recently invalidated by
our algorithm. The left image depicts a rotation, the middle
image a translation, and the right image an animated (walk-
ing) character. Red pixels were invalidated more recently
than black ones while white pixels are sufficiently converged.

∣∣|si− p|−|siold− pold |∣∣< ε, (8)
where the reprojected position siold is computed from the

offset vector stored for si (recall that the first rendering pass
stores the offset vectors for all pixels in the frame buffer for
later access by the SSAO shading pass). Note that we there-
fore use only those samples for the neighborhood test that lie
in front of the tangent plane of p, since only those samples
actually modify the shadow term. If this test fails for one of
the samples evaluated for pixel p, then p is marked invalid
and nt is set to 0.

Theoretically we could also check if the angle between
surface normal and vector to the sample point has changed
by a significant amount from one frame to the next, and
there could be practical cases where the vector length is not
enough. However, this would require more information to be
stored (surface normal of pixel in the previous frame), and
in all our tests we found it sufficient to test for condition 8.

Note that in order to avoid one costly texture look up when
fetching pold , the required values for this test and for the
ambient occlusion computation should be stored in a single
render target.

In the case of dynamic objects, a large portion of the pixel
information from the previous frame is often reusable. See
Figure 6 for a visualization of pixels recently invalidated
by our algorithm. E.g., for rotational movements, disocclu-
sions occur on the silhouette, or at pixels that were hidden
by creases or folds on the surface in previous frames. The
amount of disocclusion depends on the speed of rotation, and
is usually reasonably low for up to some degrees per frame.

3.5. Dealing with undersampled regions

Our algorithm ensures high-quality AO for sufficiently con-
verged pixels. However, in screen regions that have been in-
validated recently, the undersampling may result in temporal

flickering. Disoccluded regions are often coherent and lead
to distracting correlated noise patterns over several frames.
We solve this by a new convergence-aware spatial filter.

3.5.1. Adaptive Convergence-Aware Spatial Filter

SSAO methods usually apply a spatial filtering pass af-
ter shading computations in order to prevent noise arti-
facts caused by insufficient sampling rates. We also ap-
ply spatial filtering, but only as long as the temporal co-
herence is not sufficient. Variants of the cross bilateral fil-
ter [ED04, BSD08] are typically used, where filtering over
edges is avoided by taking the depth differences into ac-
count. Although this filter is not formally separable, in a
real-time setting it is usually applied separately in x and y
directions to make evaluation feasible. We follow this ap-
proach, too.

In contrast to previous approaches, we have additional
information for the filter which can greatly reduce noise:
the convergence conv(p) of our AO values. Recently disoc-
cluded pixels (e.g., in a thin silhouette region) can gather
more information from nearby converged pixels instead of
from other unreliable pixels. Furthermore, we apply the fil-
ter kernel directly to world-space distances like Laine et
al. [LSK∗07], which automatically takes depth differences
into account, and can detect discontinuities in cases of high
depth differences.

AO f ilt(p) =
1

k(x, p) ∑x∈F
g(|p− x|)conv(x)AO(x), (9)

where x are the individual filter samples in the screen-
space support F of the filter (e.g., a 9x9 pixel region), k(x, p)
is the normalization ∑x∈F g(|p− x|)conv(x), and g is a spa-
tial filter kernel (e.g., a Gaussian). As a pixel gets more con-
verged, we shrink the screen-space filter support smoothly
using the shrinking factor s:

s(p) =
max(cadaptive− conv(p),0)

cadaptive
, (10)

so that when convergence has reached cadaptive, we turn
off spatial filtering completely. We found the setting of
cadaptive to be perceptually uncritical, e.g., 0.2 leads to un-
noticeable transitions.

3.6. Optimizations

In this section we describe some optimizations of the core
algorithm that allow for faster frame rates or better image
quality.

submitted to COMPUTER GRAPHICS Forum (6/2010).



paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence 7

3.6.1. Smooth invalidation

For moderately fast moving objects, the ambient occlusion
is perceptually valid for more than one frame. If only the
neighborhood changes (i.e., the second condition for inval-
idation from Section 3.4), a full invalidation of the current
pixel is a waste of useful information, and noise and flicker-
ing artifacts may occur. Hence we rather clamp nt to a low
value instead of fully discarding the current solution by re-
setting nt to 0. Note that in practice, we achieve this by re-
ducing nmax for a single frame. We found an nmax value in
the range of 32 and 64 to be a good tradeoff between full
invalidation and no invalidation at all in many cases. Using
this small optimization, the AO will appear smoother over
time. Hence we denote it as smooth invalidation. From our
experiments we can state that the resulting slight motion blur
effect is less distracting than temporal flickering.

3.6.2. Adaptive sampling

While spatial filtering can reduce noise, it is even better to
provide more input samples in undersampled regions. Or, to
put it differently, once the AO has reached sufficient con-
vergence, we can just reuse this solution and do not have to
use as many samples as before. We adapt the number k of
new AO samples per frame depending on convergence (note
that these samples are completely unrelated to the screen-
space samples used for spatial filtering in the previous sec-
tion, where the kernel size is adapted instead of changing
the number of samples). Since disoccluded regions are often
spatially coherent (as can be seen in Figure 6), the required
dynamic branching operations in the shader are quite effi-
cient on today’s graphics hardware.

Note that it is necessary to generate at least a minimum
amount of samples for the same reasons that we clamp nt(p)
in Equation 5, i.e., to avoid blurring artifacts introduced
by bilinear filtering. Furthermore, a certain number of sam-
ples is required for detecting invalid pixels due to changing
neighborhoods (Section 3.4.2). In order to introduce a min-
imum amount of branching, we chose a simple two-stage
scheme, with k1 samples if conv(p)< cspatial and k2 samples
otherwise (refer to Table 1 for a list of parameters actually
used in our implementation).

parameter name value
Initial samples k1 32
Converged samples k2 8–16
Threshold cadaptive 0.2
Threshold cspatial 0.3
Threshold crot 0.5
nmax 500–1500
Filter width F 5x5

Table 1: Recommended parameters for the TSSAO algo-
rithm.

3.6.3. Frame buffer borders

A problem inherent in SSAO is the handling of samples that
extend beyond the frame buffer borders. As there is no best
method, we settle for reusing the values that appeared on
the border by using clamp-to-edge. To avoid artifacts on the
edges of the screen due to the missing depth information,
we can optionally compute a slightly larger image than we
finally display on the screen – it is sufficient to extend about
5–10% on each side of the screen depending on the size of
the SSAO kernel and the near plane [BS09]. Note that this is
a general problem of SSAO and not a restriction caused by
our algorithm. Because these border samples carry incorrect
information that should not be propagated, we detect those
samples that were outside the frame buffer in the previous
frame using our invalidation scheme (see Section 3.4).

3.6.4. Noise patterns

As in most AO approaches, we rotate the sampling pattern by
a different random vector for each input pixel. However, this
leads to a surprisingly large performance hit, supposedly due
to texture cache thrashing [SW09]. Therefore we turn off the
rotation once convergence has reached a certain threshold
crot .

Figure 7: Used test scenes: Sibenik cathedral (7,013,932
vertices) and Vienna (21,934,980 vertices).

4. Results and implementation

We implemented the proposed algorithm in OpenGL using
the Cg shading language, and tested it on two scenes of dif-
ferent characteristics (shown in Figure 7), the Sibenik cathe-
dral and the Vienna city model. Both scenes were populated
with several dynamic objects. The walkthrough sequences
taken for the performance experiments are shown in the ac-
companying video. For all our tests we used an Intel Core
2 processor at 2.66GHZ (using 1 core) and an NVIDIA
GeForce 280 GTX graphics board. The resolution of our
render targets is either 1024x768 or 800x600. To achieve
sufficient accuracy in such large-scale scenes like Vienna,
we use 32 bit depth precision. Both the ambient occlusion
buffer and the SSAO texture are 32 bit RGBA render targets.
In practical applications [FM08], SSAO is often computed
on a half- resolution render target, and then upsampled for
the final image using a spatial filter. Hence we also tested the

submitted to COMPUTER GRAPHICS Forum (6/2010).



8 paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence

performance of the algorithm when using this common ac-
celeration method. Note that the result images were all com-
puted using full resolution render targets.

To prove that our algorithm is mostly independent of the
SSAO generation method, we implemented both the method
of Ritschel et al. [RGS09], and the algorithm introduced by
Fox and Compton [FC08]. The latter algorithm uses screen-
space sampling like Shanmugam and Arikan [SA07], and is
more of an artistic approach and less physically motivated.
In our opinion it preserves more small details when using
large kernels for capturing low-frequency AO. In order to
ensure that the sampling kernel will always cover the same
size in world space, we implemented the following exten-
sion to the method: we multiply the initial kernel radius with
the w coordinate from the perspective projection of the cur-
rent pixel, i.e., the perspective foreshortening factor. As a
result we obtain AO values that are invariant to zooming.
Also, this method is prone to reveal the underlying tessela-
tion, therefore we do not count samples at gracing angles of
the hemisphere (cosine smaller ε)) as a remedy. Also, we do
not count samples where the lenght from the pixel center to
the intersection point with the depth buffer is more than 2
times the length of the maximum sample radius. This is a
tradeoff between preventing wrong occlusion caused by dis-
tant, disconnected objects and accounting for correct occlu-
sion in the vicinity of the current pixel.

The TSSAO algorithm has a couple of parameters used
to control temporal refinement, which we list in Table 1
together with some recommended values. In all results we
applied the optimizations discussed in Section 3.6. TSSAO
uses 8 samples for Vienna respectively 16 samples for
Sibenik when converged, and 32 samples otherwise. SSAO
always uses 32 samples.

Figure 8: Translational motion of the Happy Buddha model.
(left) If we check for disocclusions of the current fragment
only, we get strong artifacts due to motion. (middle) A full
invalidation removes these artifacts, but there is some noise
in the shadow regions. (right) Smooth invalidation: Assign-
ing a small weight to the invalidated pixels allows to make
better use of the temporal coherence.

In the two walkthrough sequences for Vienna and Sibenik,
we aimed to consider the major cases of motion that occur in

Figure 9: Rotational movement using TSSAO. (Left) Without
filter. (Right) Using our filter. Note how the filter is only ap-
plied in the noisy regions, while the rest stays crisp. Closeups
of the marked regions are shown in the bottom row.

real-world applications, as can be seen in the accompanying
videos. We included rigid dynamic objects as well as ani-
mated characters with deforming parts. All videos were shot
with vertical refresh synchronization on, and encoded in 30
FPS. Note that the convergence shown in the video does not
fully correspond to the convergence rate in real time: In se-
quences where more than 30 FPS are reached (the absolute
minimum for games) the TSSAO method would converge
faster. The video compression posed major problems. Quan-
tization artifacts appear in the videos which cannot be seen
in the live application or the uncompressed video, resulting
in dark shades from contact shadows which are dragged be-
hind, and slightly frayed shadow boundaries. This happened
even using the high-quality H.264 codec.

Figure 8 shows the importance of our novel invalidation
scheme on a scene with a translational movement. Only
checking disocclusions but not the pixel neighborhood for
invalidation causes artifacts visible as wrong contact shad-
ows (left image). Pixels that are not longer in shadow due to
the movement can be detected by checking the pixel neigh-
borhood (middle image). A smooth invalidation further im-
proves the quality of animated sequences, effectively reduc-
ing the noise at the transitions between the silhouettes of
moving objects and the background (right image).

Figure 9 depicts the Stanford Dragon rotating above a
floor plane in the Vienna scene. It shows the influence of
the adaptive convergence-aware filter on the quality of the
TSSAO solution. Note that the filter blurs only the contact
shadow region where temporal coherence is low, while the
rest stays crisp.

In terms of visual image quality, TSSAO performed bet-
ter than SSAO in all our tests. It corresponds to at least a
32 sample SSAO solution (since 32 samples are used for

submitted to COMPUTER GRAPHICS Forum (6/2010).



paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence 9

Figure 10: From left to right: SSAO without temporal coherence using 32 samples, with a (a) weak, (b) strong blur. (c) TSSAO
using 8 - 32 samples per frame. (d) Reference solution using 480 samples. The results are shown for the methods of Fox and
Compton (top row) and Ritschel et al. (bottom row). All at 1024x768 resolution and using 32 bit precision render targets.

disocclusions), while the converged state takes up to several
hundred (nmax) samples into account. However, we have to
keep in mind that using the smooth invalidation optimization
causes the algorithm to deviate from a correct solution for
the benefit of a better visual impression. Note that a similar
quality SSAO solution would be prohibitively slow. As can
be seen in Figure 1 (using the method of Fox and Compton)
and Figure 10 (using either the method of Fox and Compton
or the method of Ritschel et al.), and also in the accompa-
nying videos, TSSAO provides finer details and less noise
artifacts, while at the same time being faster. We compare
TSSAO to SSAO with a weak and a strong blur filter, which
gives a high respectively low weight to discontinuities. Fur-
thermore, we compare TSSAO to a reference solution using
480 samples per frame – which was the highest number of
samples our shader could compute in a single frame. Notice
that the TSSAO method is very close to the reference solu-
tion in the still images, to which it converges after a short
time.

Figure 11 shows the Sibenik cathedral populated with the
Bunny model at high resolution 1600 x 1200. While SSAO
without temporal coherence works quite well, the TSSAO
algorithm provides good quality even for fine details in the
background. Also, the TSSAO algorithm maintains a better
performance at such high resolutions.

The videos show that objects with deforming parts like
the Paladin character in Figure 6 can be handled quite well
by our algorithm. Also, the algorithm works sufficiently well
even for fast moving objects, as can be seen in the video that
shows all sorts of movement with a static camera. Although
there is increased amount of blur for dynamic motions com-
pared to the reference solution, using temporal coherence
improves the image quality compared to plain SSAO. It can

Figure 11: Sibnik cathedral populated with the Bunny model
at 1600x1200 resolution, and extreme closeup at the distant
Bunny (using the method of Fox and Compton). (left) SSAO
using 32 samples without temporal coherence at 12 FPS,
(right) TSSAO using 8 - 32 samples per frame at 26 FPS.

be seen that TSSAO effectively reduces flickering artifacts
in animated sequences. The visual refinement of the SSAO
solution over time is almost unnoticeable due to the adaptive
filter.

Figure 12 shows a comparison to a ray-traced reference
solution. The reference solution (middle) uses the Mental
Ray ambient occlusion shader with 256 hemispherical sam-
ples and a linear falloff function. TSSAO is shown using the
method of Ritschel et al. (left) and the method of Fox and
Compton (right), computing only 8 new samples per frame

submitted to COMPUTER GRAPHICS Forum (6/2010).



10 paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence

Figure 12: Comparison of a ray traced solution using 256 samples (middle) with TSSAO using 8 samples per frame (initially
32) to the method of Ritschel (left), and the method of Fox and Compton (right), in the Sibenik cathedral.

(32 initially). Note that for Ritschel et al., we weight sam-
ples equally, which corresponds to a linear falloff equivalent
to the Mental ray solution, and used the same maximum ra-
dius as for the ray-traced solution. The SSAO solutions ex-
hibit some minor differences to the reference solution (e.g.,
at the complex geometry of the balustrade), and the method
of Fox and Compton overemphasizes the underlying geo-
metric structure. Otherwise the visual quality of both solu-
tions is mostly comparable to the reference solution.

Table 2 shows average timings of our walkthroughs, com-
paring our method (TSSAO) with SSAO without temporal
coherence and the performance-baseline method: deferred
shading without SSAO. In all results we applied the opti-
mizations discussed in Section 3.6. TSSAO uses 8 respec-
tively 16 samples when converged and 32 otherwise for Vi-
enna and for Sibenik, whereas SSAO always uses 32 sam-
ples. The cost of the SSAO/TSSAO algorithms (difference
in frame times to baseline) is relatively independent of the
scene complexity, and scales with the number of pixels in
the frame buffer. TSSAO is always faster than SSAO when
using the same number of samples, for full and for half res-
olution ambient occlusion buffers. TSSAO does not apply
spatial filtering or rotate the sampling filter kernel with the
noise patterns after convergence has been reached.

Figure 13 shows the frame time variations for both walk-
throughs. Note that online occlusion culling [MBW08] is en-
abled for the large-scale Vienna model, and thus the frame
rate for the baseline deferred shading is quite high for such
a complex model. The framerate variations for TSSAO stem
from the fact that the method generates adaptively more sam-
ples for recently disoccluded regions, which can be quite
a lot if we have dynamic objects that are large in screen
space. For closeups of dynamic objects, the frame times of
TSSAO are almost the same as the frame times of SSAO.
For more static parts of the walkthroughss, TSSAO is signif-
icantly faster.

Scene Vienna (21.934.980 vertices)
resolution SSAO TSSAO Deferred
1600 x 1200 14 FPS 29 FPS 73 FPS
1024 x 768 30 FPS 51 FPS 97 FPS
800 x 600 42 FPS 63 FPS 102 FPS
800 x 600 halfres 65 FPS 77 FPS
Scene Sibenik (7.013.932 vertices)
resolution SSAO TSSAO Deferred
1600 x 1200 10 FPS 12 FPS 38 FPS
1024 x 768 21 FPS 25 FPS 65 FPS
800 x 600 29 FPS 34 FPS 67 FPS
800 x 600 halfres 47 FPS 49 FPS

Table 2: Average timings for the two walkthrough sequences
shown in the videos. We compare standard SSAO, our
method (TSSAO), and deferred shading without SSAO as a
baseline. For SSAO we used 32 samples in all scenes. For
TSSAO we used 8–32 samples in the Vienna scene and 16–
32 samples in the Sibenik cathedral. We used 32 bit depth
precision for SSAO and TSSAO. Note that occlusion culling
is used in the Vienna scene.

4.1. Discussion and limitations

In the case of deforming objects, using a full invalidation
on the deforming parts in each frame would cause temporal
coherence to be constantly broken. Hence most of the visual
improvements compared to conventional SSAO come from
the smooth invalidation optimization (refer to the closeup of
the cloak in Figure 14). Setting the smooth invalidation to a
good value is quite important here – using this optimization
too loosely can result in artifacts like a noticeable dark trail
following moving objects.

There is definitely a limit to exploiting temporal coher-
ence once the objects are moving or deforming too quickly.
Also, invalidation may fail in cases of thin (with respect to
the SSAO kernel size), quickly moving structures, which are
missed by the invalidation algorithm. The legs of the Skele-
ton character in the Sibenik walkthrough are an example for

submitted to COMPUTER GRAPHICS Forum (6/2010).



paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence 11

 0  500  1000  1500  2000  2500

 10

 20

 70

 40

 50

 60

 3000

DEFERRED
TSSAO

SSAO

FRAMES

 30

TIME (ms)

 3000  3500

TIME (ms)
 70

 1000

 60

 50

 40

 30

 20

 10

 500  1500 0

DEFERRED
TSSAO

SSAO

 2000  2500
FRAMES

Figure 13: Frame times of the Vienna walkthrough (with oc-
clusion culling, top) and the Sibenik walkthrough (bottom) at
resolution 1024x768 (using 32 bit precision render targets).

a difficult case. Likewise, a very large kernel size can also
cause problems with the invalidation, because the sampling
could miss disocclusion events.

The main targets of this algorithm are real-time visual-
ization and games. The adaptive sample generation cannot
guarantee constantly better frame times than conventional
SSAO, and there are definitily fluctuations in the frame time
which could be disturbing in games. However, we have to
keep in mind that games usually undergo extensive play-
testing in order to avoid annoying frame rate drops. Faster
SSAO in most frames is useful, because more time for other
effects is available.

Also, note that the adaptive sample generation is just an
optimization feature of the algorithm – without it we would
fall back to the speed of conventional SSAO, but still have
the improvements in image quality. Furthermore, the major
purpose of this algorithm is to speed up the standard case, i.e.
a moderate amount of dynamic movement. In these cases it
has been shown to work well although, it may break down at
extreme cases, which is probably true for most methods that
depend on temporal coherence.

5. Conclusion and future work

We presented a screen space ambient occlusion algorithm
that utilizes reprojection and temporal coherence to produce
high-quality ambient occlusion for dynamic scenes. Our al-
gorithm reuses available sample information from previous
frames if available, while adaptively generating more sam-
ples and applying spatial filtering only in the regions where

not enough samples have been accumulated yet. We have
shown an efficient new pixel validity test for shading algo-
rithms that access only the affected pixel neighborhood. Us-
ing our method, such shading methods can benefit from tem-
poral reprojection also in dynamic scenes with animated ob-
jects. While we restricted our results to show ambient occlu-
sion, our algorithm can be seen as a cookbook recipe that is
applicable to many other screen-space sampling algorithms
with a finite kernel size.

In the future, we want to improve the smooth invalidation
and put it on a more theoretical basis. In essence, smooth in-
validation avoids the binary decision of fully discarding the
previous AO stored in the ambient occlusion buffer. How-
ever, instead of the binary definition of invalidation, we
should rather use a continous one. In particular, we estimate
a confidence value between 0 and 1 to weight the confidence
into the validity of the previous SSAO solution. This confi-
dence can be steered by the length differences between the
validation sample distances from Equation 8, which gives an
indication of the magnitude of the change.

In the future, we want to explore other useful sampling-
based techniques and how they can be enhanced with tem-
poral coherence, and aim to derive general rules regarding
invalidation that are applicable to different classes of prob-
lems. Also we plan to further investigate the problems with
the blurriness that occurs because of bilinear filtering in
combination with fast zooms. As these artifacts mainly oc-
cur on surfaces that are nearly perpendicular to the view di-
rection, we want to investigate how they are related to the
projection error known from shadow mapping.

References

[BS09] BAVOIL L., SAINZ M.: Multi-layer dual-resolution
screen-space ambient occlusion. In SIGGRAPH ’09: SIGGRAPH
2009: Talks (New York, NY, USA, 2009), ACM, pp. 1–1. 3, 7

[BSD08] BAVOIL L., SAINZ M., DIMITROV R.: Image-space
horizon-based ambient occlusion. In SIGGRAPH ’08: ACM SIG-
GRAPH 2008 talks (New York, NY, USA, 2008), ACM, pp. 1–1.
2, 3, 6

[Bun05] BUNNELL M.: Dynamic Ambient Occlusion and Indirect
Lighting. Addison-Wesley Professional, 2005, ch. 14, pp. 223–
233. 2

[CT81] COOK R. L., TORRANCE K. E.: A reflectance model for
computer graphics. In SIGGRAPH ’81: Proceedings of the 8th
annual conference on Computer graphics and interactive tech-
niques (New York, NY, USA, 1981), ACM, pp. 307–316. 1

[ED04] EISEMANN E., DURAND F.: Flash photography enhance-
ment via intrinsic relighting. In ACM Transactions on Graph-
ics (Proceedings of Siggraph Conference) (2004), vol. 23, ACM
Press. 6

[FC08] FOX M., COMPTON S.: Ambient occlusive crease shad-
ing. Game Developer Magazine (March 2008). 2, 8

[FM08] FILION D., MCNAUGHTON R.: Starcraft ii: Effects &
techniques. In Proceedings of the conference on SIGGRAPH

submitted to COMPUTER GRAPHICS Forum (6/2010).



12 paper1092 / High-Quality Screen-Space Ambient Occlusion using Temporal Coherence

Figure 14: Closeup of the deforming cloak of the Paladin
character. (left) SSAO using 32 samples and (right) TSSAO
using 8 – 32 samples. Although deforming objects are a dif-
ficult task for a temporal coherence algorithm, there is still
some benefit from TSSAO in form of reduced surface noise
(and reduced flickering artifacts when animated).

2008 course notes, Advanced Real-Time Rendering in 3D Graph-
ics and Games, Chapter 5 (2008), ACM Press, pp. 133–164. 1,
7

[KA06] KONTKANEN J., AILA T.: Ambient occlusion for ani-
mated characters. In Rendering Techniques 2006 (Eurographics
Symposium on Rendering) (jun 2006), Wolfgang Heidrich T. A.-
M., (Ed.), Eurographics. 3

[KA07] KIRK A. G., ARIKAN O.: Real-time ambient occlusion
for dynamic character skins. In I3D ’07: Proceedings of the 2007
symposium on Interactive 3D graphics and games (New York,
NY, USA, 2007), ACM, pp. 47–52. 3

[KL05] KONTKANEN J., LAINE S.: Ambient occlusion fields. In
I3D ’05: Proceedings of the 2005 symposium on Interactive 3D
graphics and games (New York, NY, USA, 2005), ACM, pp. 41–
48. 2

[Lan02] LANDIS H.: Production-ready global illumination. In
Proceedings of the conference on SIGGRAPH 2002 course notes
16 (2002). 1, 2

[LSK∗07] LAINE S., SARANSAARI H., KONTKANEN J.,
LEHTINEN J., AILA T.: Incremental instant radiosity for real-
time indirect illumination. In Proceedings of Eurographics Sym-
posium on Rendering 2007 (2007), Eurographics Association,
pp. 277–286. 6

[MBW08] MATTAUSCH O., BITTNER J., WIMMER M.: Chc++:
Coherent hierarchical culling revisited. Computer Graphics Fo-
rum (Proceedings Eurographics 2008) 27, 2 (Apr. 2008), 221–
230. 10

[Mit07] MITTRING M.: Finding next gen - cryengine 2. In Pro-

ceedings of the conference on SIGGRAPH 2007 course notes,
course 28, Advanced Real-Time Rendering in 3D Graphics and
Games (2007), ACM Press, pp. 97–121. 1, 2, 3

[MMAH07] MALMER M., MALMER F., ASSARSSON U.,
HOLZSCHUCH N.: Fast precomputed ambient occlusion for
proximity shadows. journal of graphics tools 12, 2 (2007), 59–
71. 2

[NSL∗07] NEHAB D., SANDER P. V., LAWRENCE J.,
TATARCHUK N., ISIDORO J. R.: Accelerating real-time
shading with reverse reprojection caching. In Proceedings
of the 22nd ACM SIGGRAPH/EUROGRAPHICS symposium
on Graphics Hardware 2007 (Aire-la-Ville, Switzerland,
Switzerland, 2007), Eurographics Association, pp. 25–35. 3, 4, 5

[PG04] PHARR M., GREEN S.: Ambient Occlusion. Addison-
Wesley Professional, 2004, ch. 14, pp. 279–292. 2

[RGS09] RITSCHEL T., GROSCH T., SEIDEL H.-P.: Approxi-
mating dynamic global illumination in image space. In I3D ’09:
Proceedings of the 2009 symposium on Interactive 3D graphics
and games (New York, NY, USA, 2009), ACM, pp. 75–82. 2, 3,
8

[Ros07] ROSADO G.: Motion Blur as a Post-Processing Effect.
Addison-Wesley Professional, 2007, ch. 27, pp. 575–576. 3

[RWS∗06] REN Z., WANG R., SNYDER J., ZHOU K., LIU X.,
SUN B., SLOAN P.-P., BAO H., PENG Q., GUO B.: Real-time
soft shadows in dynamic scenes using spherical harmonic expo-
nentiation. In SIGGRAPH ’06: ACM SIGGRAPH 2006 Papers
(New York, NY, USA, 2006), ACM, pp. 977–986. 2

[SA07] SHANMUGAM P., ARIKAN O.: Hardware accelerated
ambient occlusion techniques on gpus. In Symbosium of Inter-
active 3D graphics and games (2007), pp. 73–80. 2, 8

[Shi05] SHISHKOVTSOV O.: Deferred Shading in S.T.A.L.K.E.R.
Addison-Wesley Professional, 2005, ch. 2, pp. 143–166. 2

[SJW07] SCHERZER D., JESCHKE S., WIMMER M.: Pixel-
correct shadow maps with temporal reprojection and shadow
test confidence. In Rendering Techniques 2007 (Proceedings
Eurographics Symposium on Rendering) (June 2007), Kautz J.,
Pattanaik S., (Eds.), Eurographics, Eurographics Association,
pp. 45–50. 3, 4, 5

[ST90] SAITO T., TAKAHASHI T.: Comprehensible rendering of
3-d shapes. SIGGRAPH Computer Graphics 24, 4 (1990), 197–
206. 2

[SW09] SMEDBERG N., WRIGHT D.: Rendering techniques in
gears of war 2, 2009. 3, 7

[WH00] WANG X., HICKERNELL F. J.: Randomized halton se-
quences. Mathematical and Computer Modelling 32 (2000),
2000. 3

[YNS∗09] YANG L., NEHAB D., SANDER P. V., SITTHI-
AMORN P., LAWRENCE J., HOPPE H.: Amortized supersam-
pling. ACM Transactions on Graphics (Proc. of SIGGRAPH Asia
2009) 28, 5 (2009), 135. 4

[ZIK98] ZHUKOV S., IONES A., KRONIN G.: An ambient light
illumination model. In Rendering Techniques (1998), pp. 45–56.
2

submitted to COMPUTER GRAPHICS Forum (6/2010).


