











































Real-time Ambient Occlusion and Halos with Summed Area Tables

Abstract

Volume models often show high depth complexity. This poses difficulties to the observer in judging the spatial relationships
accurately. Illustrators usually use certain techniques such as improving the shading through shadows, halos, or edge darkening in
order to enhance depth perception of certain structures. Both effects are difficult to generate in real-time for volumetric models.
Either they may have an important impact in rendering time, or they require precomputation that avoids changing the transfer
function interactively. In this paper we present two methods for the fast generation of ambient occlusion on volumetric models. The
first one is a screen-space approach that does not require any precomputed data structure. The second one is a view independent one
that stores volumetric information in the form of a Summed Area Table of the density values, and thus, allows for the interactive
change of transfer functions on demand, although with a cost of memory space. Despite the fact that similar quality results are
obtained with both approaches, the 3D version is more suitable for objects with discontinuous structures such as a vessel tree or the
intestines, and it obtains better framerates. The first case, the 2D version is more suitable in constrained GPU memory environments
because it does not need extra 3D texture storage. As an extra result, our screen-space technique also allows for the computation of
view dependent, interactively configurable halos using the same data structure.

Key words:
GPU-based volume ray casting, Depth perception enhancement, Ambient occlusion, Halo rendering

1. Introduction

One of the most important goals of visualization is to provide
the adequate insights into the data being analyzed. With the in-
crease in computational power and the improvements of cap-
turing processes, volume visualization algorithms are being fed
with more and more complex datasets. Understanding space
arrangement of such complex volumetric models is a difficult
task, especially with renderings of volume data acquired from
CT or MRI. Rendered images of such models are difficult to in-
terpret because they present many fine, overlapping structures.
In specialized media, such as medicine books, illustrators often
use a set of special techniques in order to obtain the depictions
that reduce perception ambiguity, such as halos, contours, cut-
away views, and so on. Computer Graphics researchers have
borrowed some of those techniques and applied them success-
fully in visualization. Improving perception of volumetric mod-
els has usually been faced under two different kinds of methods:
a more realistic shading model or using non-photorealistic tech-
niques.

Volumetric data is typically illuminated by one or several
point light sources and the shading of each sample point in the
volume is calculated by using a Phong model. This illumina-
tion model provides good perceptual cues but mainly on the
orientation of isosurfaces. However, this usually results in im-
ages with poor depth cues. Shadows can be used to solve this
problem, but shadow generation is costly and may introduce
illumination discontinuities on the surface.

In this paper we propose two simple and fast approaches for
enhancing depth perception of medical models. In both cases
we propose a data structure and an algorithm for fast render-

ing of such effects. Both are based on Summed Area Tables.
The first one: Vicinity Occlusion Maps (VOM), is a data struc-
ture that is computed on the fly for each frame and allows to
generate view dependent ambient occlusion and halos at rela-
tively low cost. The second one, Density Summed Area Ta-
ble (DSAT3D) uses a precomputed 3D Summed Area Table of
the density values of a model, and allows the computation of
view independent ambient occlusion with an almost negligible
impact in the rendering process. The main difference between
both is that the first one is view dependent, and built on a depth
map, similar to other recent ambient occlusion methods, while
the second takes into account 3D information making thus this
approach view independent. This paper is an extension of [1],
where we presented the VOM data structure for halo rendering
and ambient occlusion simulation. In this paper, we present the
new method that uses DSAT3D data structure, and the CUDA
implementations for the construction of both data structures.

Our goal is to enhance depth perception for volumetric mod-
els, and to do so, we add ambient occlusion (vicinity shading)
and halos in real-time. Our main contributions can be summa-
rized as:

• Vicinity Occlusion Map (VOM): A data structure that is
computed on the fly and whose main advantages are: i)
No need of precomputation. ii) Limited (i. e. constant per
frame) impact on rendering. iii) It may be used for com-
puting view-dependent real-time halos (with interactively
changeable size or color).

• Denstity Summed Area Table (DSAT3D): A 3D Summed
Area Table of the density values of the model that allows
for the fast computation of ambient occlusion. Its main

Preprint submitted to Computers and Graphics October 15, 2009



(a) Ray Casting (b) Vicinity Shading (c) Halos and Vicinity Shading

Figure 1: Perceptually enhanced rendering of a volumetric model. Left image shows the original ray cast volume. Center image has been enhanced by adding
vicinity shading (see how the back ribs are darkened), and the right one shows how a halo can emphasize the shape of a model. All images are computed at real
time frame rates.

features are: i) View independent ambient occlusion com-
putation. ii) Almost no impact in rendering performance.
iii) Requires precomputing a 3D texture, with the corre-
sponding space cost.

VOMs do not require precomputation and generate view de-
pendent ambient occlusion shadows, as they are computed on
screen space. Moreover, they allow for the computation of flex-
ible halos. On the other hand, DSAT3Ds require the precom-
putation of a 3D texture (whose cost goes from 40 milliseconds
to several seconds). This second structure is also stored on the
GPU for the rendering stage, so we need extra memory space.
However, rendering is faster, with a penalty of at most 8% of
the rendering time in the models we tested. For Vicinity Oc-
clusion Maps have a larger impact in framerates (though we
still get real-time if the original model performed fast enough),
that mainly depends on the size of the image downloaded to the
CPU.

The rest of the paper is organized as follows: First, Section 2
overviews Previous Work. In Section 3 we present some back-
ground useful for the remainder of the paper. Section 4 presents
our first data structure: Vicinity Occlusion Maps, and the algo-
rithms that use it for simulating ambient occlusion and render-
ing halos. In Section 5 we introduce the Density 3D Summed
Area Tables and show how to use them for fast ambient occlu-
sion computation. The results are analyzed in Section 6, where
we also discuss the advantages and disadvantages of both meth-
ods and how they compare to previous approaches. Finally,
Section 7 concludes the paper and points to some lines for fu-
ture research.

2. Previous Work

In this section we will analyze previous methods for enhanc-
ing the perception of depth in volumetric models. The two main

approaches are realistic shading and non-photorrealistic tech-
niques.

Realistic shading models. These methods simulate classical
rendering effects, such as shadows, by applying different gener-
ation processes. There are several proposals for applying shad-
ows in volume rendering ([2–4]) although the generation pro-
cess may differ importantly. For instance, Yagel et al. use re-
cursive ray tracing while Behrens and Ratering add shadows
to texture-based volume renderings. Kniss et al. [4] also add
other effects such as phase functions and forward scattering.
Following the same idea of increasing the realism of the shad-
ing model, Stewart developed a technique named vicinity shad-
ing that enhances depth perception through the use of obscu-
rances [5]. Concretely, he proposed a model that performs shad-
ing according to the presence of occlusors on the neighborhood
of a voxel. Obscurances (also coined as ambient occlusion by
Landis [6] and others) is a model of ambient illumination that
produces realistic shading at relative low cost. Ambient Oc-
clusion-based techniques measure or approximate, with respect
to a pixel, the total solid angle that is occluded by nearby ele-
ments. Once this measurement is done, the pixel is darkened in
proportion to it. Our technique allows for a fast computation of
an approximation to vicinity shading without the need of trac-
ing extra rays into the volume. A similar lighting technique has
also been applied by Tarini et al. [7] to enhance molecular vi-
sualization, by exploiting the fact that the rendered geometry
is built upon atoms, which have a particular spherical shape.
Ambient occlusion techniques have become popular thanks to
their simplicity and efficiency. Although of the papers com-
pute ambient occlusion information in a preprocess ([7–10]),
a new group of papers estimate the occlusion based on depth
maps [11, 12].

Ruiz et al. [13] implement the obscurances method for vol-

2



ume models. In order to determine the ambient occlusion in a
point, several rays are traced sampling the surrounding of that
point. This leads to large precomputation times of several min-
utes even for relatively small models (up to 256 × 256 × 415
slices).

Ropinski et al. [14] have also developed an approach for fast
ambient occlusion-based rendering of volume models. Their
method uses a precomputed set of local histograms (similar
to [15]). This set may become too big for relatively large
datasets. On the other hand, like our approach, they generate
ambient occlusion images on the fly, and perform fast updates
when Transfer Function changes. This system also computes
color bleeding and volume glows with the same data structure.

Non-photorealistic techniques. Expressive visualization is a
set of techniques that provide visual cues on certain features
of the model in order to achieve a wide range of effects such
as artistic renderings, or focus of attention. There are differ-
ent strategies that put a different accent on different parts of the
model, that may be used to enhance depth perception. Con-
tours are generated based on the magnitude of local gradients
and the angle between the viewing direction and the gradient by
Csébfalvi et al. [16]. Hauser et al. [17] propose two-level vol-
ume rendering that is able to use different rendering paradigms
(direct volume rendering, Maximum Intensity Projection, and
so on) for the different structures within a dataset, and show
how to use it as a focus+context technique. Nagy et al. [18]
combine line drawings and direct volume rendering techniques.
Yuan and Chen [19] enhance survey sees in volume render-
ing images with silhouettes, ridge, and valley lines. Bruckner
and Gröller [20] use volumetric halos for enhancing and high-
lighting structures of interest using a GPU-based direct volume
rendering. Halos have also been used by Wenger et al. [21],
Ebert and Rheingans [22], Svakhine and Ebert [23], or Ritter et
al. [24], and Tarini et al. [7]. These methods usually calcu-
late halos on a preprocess and therefore they may not easily be
modified on the fly. The halos built by Bruckner and Gröller are
similar to ours in that they may flexibly change size and color.
However, their building process involves a set of steps while
our halos are built in a single step posterior to the rendering,
using the Vicinity Occlusion Map. This yields a roughly con-
stant cost per frame (that depends on the window size), often
smaller than in the case of their algorithm.

In all cases, we render the volume models GPU-based ray
casting [25], which yields real time response. Next Section will
introduce Vicinity Occlusion Maps, and then we will present
Density Summed Area Tables.

3. Preliminaries

In this section we introduce two concepts that will be neces-
sary for the remaining of the paper: Summed Area Tables and
Ambient Occlusion.

3.1. Summed Area Tables
A Summed Area Table [26] (from now on SAT) is a structure

that stores, for each cell (x, y), the sum of the values from the

initial position (0, 0) until (x, y) included. Given a table t, a
certain cell (x, y) from its corresponding SAT can be computed
as:

S AT [x, y] =
x,y∑

i=0, j=0

ti, j (1)

In Computer Vision community, this data structure is also
called integral image ([27]). Once the SAT is built, they pro-
vide a way to filter arbitrarily large rectangular regions of an
image in a constant amount of time. In order to compute the
sum of a rectangular region, only 4 accesses are required (see
Figure 2), independently of the area size. This allows us to com-
pute the approximation of occlusion due to neighbor regions in
a constant time (see the following Section).

SAT[Xl, Yb]
SAT[Xr, Yb] - 
SAT[Xl, Yb]

SAT[Xr, Yt] - 
SAT[Xl, Yt] -
SAT[Xr, Yb] +
SAT[Xl, Yb]

SAT[Xl, Yt] - 
SAT[Xl, Yb]

SAT

Xl Xr

Yb

Yt

Figure 2: Area calculation with a Summed Area Table.

Summed Area Tables may be built incrementally on a CPU
by a simple algorithm as depicted in Figure 3. Therefore, the
construction cost is linear with the number of cells.

3.2. Ambient Occlusion in Volumetric Models

Ambient Occlusion ([6]) for volumetric models was first in-
troduced by Stewart with the name of Vicinity shading [28].
This technique was tailored to increase depth perception in vol-
umetric models. The original idea is due to Zhukov et al. [5],
who developed a fast estimation of global illumination dubbed
obscurances. For a surface point, the algorithm attenuates the
illumination coming from all directions in space by measuring
the occlusions in the local vicinity of the point. In order to
do this, the occlusion is computed by sampling the environ-
ment of the voxel to be shaded with more than 1000 sample
directions. Although the developed algorithm traces rays in an
efficient way, sampling such a high number of directions re-
sults in an important impact in rendering time. Recent papers
deal with the generation of ambient occlusion based shadows
in real-time. Unfortunately, most of them require either a set
of precomputed structures, or need knowledge of the 3D ge-
ometry. Our approach needs no precomputation and does not

3



First step Second step                      

Third step                      

2DSAT[0, 0] = Input [0, 0] 2DSAT[x, 0] = 2DSAT[x-1, 0] + 
               + Input [x, 0]

2DSAT[0, y] = 2DSAT[0, y-1] +    
                + Input [0, y]

2DSAT[x, y] = 2DSAT[x-1, y] + 2DSAT [x, y-1] - 2DSAT[x-1, y-1] + Input [x, y]

      2DSAT[x-1, y]                              2DSAT [x, y-1]                                   2DSAT[x-1, y-1]    

Figure 3: Summed Area Table construction process in CPU.

require knowledge of the geometry, which is often not avail-
able in volume rendering, because the shading will depend on
the transfer function, that can be changed interactively.

Given a point p on the surface of a model with surface normal
np, the irradiance reaching p can be defined as:

E(p) =
∫

Ω

np · ωL(ω)dω (2)

where L(ω) is a scalar with magnitude equal to the radiance
coming from direction ω, and Ω is the set of directions above
the surface, i.e. the direction for which np · ω > 0. This can
be approximately evaluated by discretizing the domain Ω into
k sectors ωi with a possible uniform solid angle measure |ωi|,
and, for each sector, evaluating the radiance L only for a sample
direction ωi:

E(p) =
k∑

i=1

np · ωiL(ωi) |ωi| (3)

If we consider only an environment where light comes uni-
formly from each direction, we may even get a simpler formu-
lation, as the radiance reaching a point may be substituted by
a binary function that determines the reachability of that point
(O(ω)), that is, O(ω) evaluates to 1 if the point light is not oc-
cluded in a certain direction. This results in an approximation
of the rendering equation:

E(p) =
1

4Π

k∑
i=1

np · ωiO(ωi) (4)

4. Ambient Occlusion and halo rendering using Vicinity
Occlusion Maps

We work on volumetric models rendered using ray casting on
GPUs [25], which yield real time response. Our method con-

sists on generating depictions that increase the insights on the
models being visualized. These reduce ambiguity through the
addition of depth cues by simulating Stewart’s vicinity shading
and rendering halos. For the fast generation of ambient occlu-
sion and halos, we have developed a new data structure dubbed
Vicinity Occlusion Map ([1]).

4.1. VOM Data Structure

We will first concentrate on simulating ambient occlusion.
This will result in a new data structure that can also be used to
render halos in real time.

The main idea of ambient occlusion is to measure, for a cer-
tain point, the amount of ambient light occluded by its neigh-
boring geometry. Actually, what we do is to reduce the light
contribution by an amount proportional to the occlusions pro-
duced by the geometry close to the point to be shaded. Given a
rendered view and its associated OpenGL depth map, we may
approximate this value by analyzing, for each pixel, the depth
values of its closer surrounding pixels, and counting the number
of pixels whose depth is smaller (they are closer to the viewer)
than the one to shade. Naively counting the number of elements
in the depth map would result in a too large number of texture
queries, thus reducing framerate. The solution developed by
Crytek ([11]) consists on subsampling this region using a cer-
tain pattern. This may yield noise artifacts that must be further
filtered in order to reduce the effects of sampling. We adopt a
completely different approach: we compute the average depth
using a new data structure.

A Vicinity Occlusion Map (VOM) is a data structure that en-
codes the information contained in a depth map in two tables,
one containing the accumulated depth of the neighborhood of a
pixel, and another one that contains the number of values that
contributed to the sum. We build VOMs from the depth map

4



CPU 

GPU

Color MapRay casting

Final Image

Medical Images

3D Texture

2D Texture +

Volume Model

Depth Map

Processing 2D Texture

Vicinity Occlusion Map

GPU 

2D Shading

2DSAT

Figure 4: Application architecture: The CPU reads a volumetric model from a DICOM file, builds a 3D texture from it, and uploads it to the GPU. Then, we perform
a GPU ray casting on the model which generates a color map and a depth map. The depth map is passed to the CPU. This is used to generate the Vicinity Occlusion
Map (VOM). With the VOM and the color map, the GPU composites a new image as a result.

resulting from ray casting a volumetric model from a view-
point. The depth map contains, for each fragment, the distance
to the observer at the moment the opacity of the ray gets to
one. Our data structure is built by processing the depth map
and generating its Summed Area Table together with informa-
tion concerning the number of fragments that effectively con-
tributed to the sum. Therefore, VOMs consist of two tables,
dubbed S ATdepth (Summed Area Table of the depth map) and
S AT Ndepth (Summed Area Table of a bitmap containing 1 for
pixels not belonging to background and 0 otherwise), respec-
tively. This information is generated on the fly for each frame
by passing the depth map (encoded in a texture) to the CPU.
Then, the CPU computes the Summed Area Table and uploads
the resulting Vicinity Occlusion Map to GPU, which is used for
the final shading.

4.2. System Overview

In this Section we present the architecture of our application.
Initially, we load the model on CPU and build a 3D texture

from it, that is loaded to GPU texture memory. Then, given a
Transfer Function and a viewpoint, our system performs two
rendering passes. The first one consists in a GPU-based ray
casting of the model that generates a color map and a depth
map. This depth map is passed to the CPU in order to build the
Vicinity Occlusion Map (VOM). Then, the VOM is uploaded
to GPU and the second rendering pass performs the shading by
simply rasterizing a screen-aligned quad, and using the VOM to
produce the final result. Hence, only a Ray Casting is required
for the whole process. This application architecture is depicted
in Figure 4.

Although the VOM could also be computed on the GPU, this
requires multiple render passes ([29]). Taking into account that

the ray casting is a fragment shader-intense algorithm, we pre-
ferred to download the work to the CPU. Despite that, we have
also implemented a CUDA version that yields similar results.

4.3. Ambient Occlusion using VOMs

We want to measure, for a certain point, the amount of am-
bient light occluded by its neighboring geometry. We approx-
imate this computation by using VOM to approximate the av-
erage depth surrounding the point of interest. From this value,
we infer the amount of occlusion produced by opaque voxels
around a point in an efficient way.

The average depth can be computed using S ATdepth. How-
ever, average depth is not enough, thus, we use S ATN in order
to know how many pixels contributed to the average depth. Fi-
nally, we use the difference between the average depth and the
current depth of the point to shade in order to obtain a better
approximation of the percentage of hemisphere covered by the
close voxels.

We consider regions that go from 10 × 10 to 25 × 25 pixels
around the point to be shaded (which means taking into account
100 to 625 pixels) to create plausible shading and halos, as we
will show with examples. From this simple idea we may build
a darkening function that, added to the Phong shading function,
will generate depth cues that provide a better understanding of
the rendered model. The function is implemented in a shader
and executes for all fragments whose depth is different from
1.0, that is, for the fragments belonging to the final color of the
object. It is modified by subtracting from the color generated
by the ray tracing process (RCcolor) a certain amount dark of the
vicinity color, so the fragment color will be:

Fcolor = RCcolor − darkv · vicinitycolor (5)

5



(a) Original (b) Depth-enhanced

Figure 5: Depth enhancement of direct volume rendering generated images.
Left column shows two models without our technique and right column shows
two images enhanced for better depth understanding using vicinity shading.

where darkv is the darkening factor due to vicinity shading that
is computed taking into account the average depth of the sur-
rounding pixels. If we use white as vicinitycolor, we may darken
the fragment, while if we use a color out of the grey range, we
may color the selected structure in order to emphasize it. The
darkening factor is computed by taking into account the num-
ber of pixels that contributed to the depth calculation, and the
resolution of the used region. So for a fragment (x, y) we will
use:

darkening = (avgdepth − depth(x, y)) ∗ vicinityFactor, (6)

where vicinityFactor is a term that modulates the influence of
the vicinity shading onto the final image, and the average depth
is computed as:

avgdepth =

∑x+sizex,y+sizey
i=x−sizex, j=y−sizey

depth(i, j)

N f rag|depth f rag > 0
, (7)

where sizex and sizey is the resolution of the 2D region of the
depth map we use to compute ambient occlusion. Once the
darkening factor is computed, we add it to the final color by
filtering its weight in a smooth way:

darkv = smoothstep(0., 0.99, darkening), (8)

In Figure 5 we may see a comparison of renderings without
(left) and with (right) vicinity shading. Increasing the vicini-
tyFactor reduces overall lighting and increases depth cues. If
we want to increase the contrast in areas of the image enhanced
by vicinity factor, we replace this factor with a power of the
difference in depths.

We approximate this computation by using a data structure
built using the information of depth maps, from which we

may infer the amount of occlusion produced by opaque voxels
around a point in an efficient way.

Summed Area Tables may become very large in terms of bit
depth. The number of bits required to store each component in
a SAT of resolution w × h is:

N = log2 w + log2 h + Pi (9)

where Pi is the precision required by the input. This makes the
management of SAT quite difficult. As noted by Hensley [29],
this may overflow the texture resolution if not handled accu-
rately. In our case, we perform the SAT computation in long
integer format and then transfer the result into a pair of 32 bits
textures to the GPU (one for the depth sum and another for the
accumulated number of elements different from 0).

In order to compute the avgdepth term we need to compute the
sum of depths and the number of fragments that contributed to
the sum. Each value can be calculated by accessing the SAT
4 times as shown in Figure 2. Furthermore, we might even
code both tables into a single 64 bit texture and therefore save
4 texture accesses.

However, most of these optimizations have little impact, be-
cause the major cost incurred by our algorithm is data transfer,
as it will be analyzed later in Section 4.6.

4.4. Halo rendering
In order to render halos on selected parts of the model, we

perform a similar algorithm to the one used to simulate vicinity
shading. In this case, we have to take into account that the
halo has to be rendered close to the object of interest and has to
decay as we go further from the object.

The user may decide the halo extension (in pixels) by manip-
ulating a slider. Moreover, he or she can also change the halo
color interactively. In order to render a halo, for each pixel, we
query the VOM to calculate the average depth of the neighbor-
hood, and, if the current fragment is outside the object (this can
be determined by querying the depth map), we render a halo
with a color that decays with the distance to the object by using
the following formula:

Fcolor = RCcolor + darkh · halocolor, (10)

where RCcolor is the color generated by the Ray Casting algo-
rithm, and the amount of darkening (darkh) due to the halo is
determined by:

darkh =
(avgdepth − depth(x, y)) · halow · NumElems

resolution
, (11)

where halow is a weight that can be changed by the user,
NumElems is the number of elements that contributed to the av-
erage depth calculation, and resolution is the size of the region
of interest. By dividing the number of fragments whose depth
is not zero by the resolution of the search region we are creating
a function that decays with the distance to the object.

The result is a halo painted with the color selected by the user
and whose intensity and area of influence may be changed in-
teractively. All these changes are possible because the number
of texture accesses required to perform the effect is constant.
We may see a couple of examples of halos around the opaque
regions as selected by the transfer function in Figure 6.

6



(a) (b)

Figure 6: Improvement of visual impression by using halos around two different
objects.

(a) Ray (b) Vicinity (c) Vicinity
Casting Shading Shading

Figure 7: Effect of the factor in vicinity Shading. (a) shows the original ray cast
image and (b) and (c) show two different values of vicinityFactor. A higher
factor darkens the affected region more strongly.

4.5. Features
Our algorithm performs a real-time computation of both

vicinity shading and halos. Since all the calculations are car-
ried out from scratch each frame, the parameters involved can
be changed interactively without cost. Although the default pa-
rameters may be useful for a wide variety of models, we also
provide the user with a wide range of flexibility. Thus, we let
the user manipulate several parameters in order to improve the
depth perception or structure accentuation:

• Vicinity weight and size: Depth complexity among mod-
els or regions of a model may vary, and therefore, applying
a fixed function may enlighten a certain part and hide an-
other. In our application, the user may manipulate a slider
to change the default value given to the darkening func-
tion. We may see the effect of such changes in Figure 7.
The number of pixels taken into consideration can also be
interactively changed in order to fine tune the results if
necessary. With no extra cost, we let the user change the
emphasizing color, in order to change from darkening to
enlightening if the depth variations need to be accentuated
in a different way.

• Halo size and color: Thanks to the use of Summed Area
Tables, different sizes of halos require the same number of

(a) Black halo (b) Red Halo

Figure 8: Influence of halo color change in shape perception. Black halo does
not provide enough shape cues because it is difficult to judge if some pixels
rendered with a dark color belong to the structure of the object (and are placed
at a high depth), or to the halo.

Figure 9: Improving object identification through the creation of a halo around
the object of interest, in this case the liver.

texture accesses. Therefore, we let the user change halo
size, again with the user of a slider. Nonetheless, the color
can also be defined interactively because different struc-
tures or different contexts might require a different color
to emphasize the object of interest. In Figure 8 we com-
pare two different colors: black and red, for halo-based
shape emphasis.

• Structure selection: Our application allows for interac-
tively selection of structures from a set of previously seg-
mented structures, and the shading effects are restricted to
them. This allows us to create halos or colored shading for
restricted parts of the model, in order to provide better vi-
sual insights on the structures of interest. Figure 9 shows
this effect in order to properly identify a vascular tree in
the liver from the surrounding structures.

From the effects presented above, the only one requiring ex-
tra information is structure selection, where we need some sort
of data structure for object segmentation. The implementation
requires some changes: When rendering the color image and
depth map, the ray caster identifies, for each rendered fragment
whose opacity is 1.0, the structure it belongs to with an extra
fetch to an 3D texture of identifiers, built on a preprocess. This

7



then produces as a result a modified depth map, that only stores
depth values for the structure of interest. The final halo is com-
puted using this depth map (see Figure 9).

4.6. Results
We have implemented the proposed method and have tested

the results in a PC equipped with an Intel Core 2 Duo CPU
running at 3.0 GHz. It also has 4GB of RAM memory and a
NVidia GeForce GTX 280 GPU with 1GB of RAM. We have
analyzed models with different sizes. The dimensions of the
models are shown in the following list:

• Brain: 128 x 128 x 58 slices.

• Ear(Baby): 256 x 256 x 98 slices.

• Engine: 256 x 256 x 256 slices.

• Intestines: 512 x 512 x 127 slices.

• Bonsai: 512 x 512 x 182 slices.

• Body: 512 x 512 x 512 slices.

With this machine, we may obtain real-time framerates for
relatively complex models (of up to 512 × 512 × 600 slices).
The penalty suffered by our shading approach is relatively low
compared to the cost of ray tracing the scene. All the examples
of the paper were rendered in a Viewport of 512 × 512 resolu-
tion and the ray casting samples three times per voxel, as lower
density sampling usually results in poor renderings. The quality
of the images is really high and the influence of vicinity shad-
ing and halo sizes may be changed quite easily by the user. The
ray casting process dominates the overall cost, and this mainly
depends on the transfer function used. That is the reason why
we obtain similar framerates for models of different number of
slices. As we can see in Figures 5 and 6, we have used mod-
els that go from high opacity such as the bonsai, or with large
semi-transparent regions, such as the body.

Concerning efficiency, the main advantage of our approach is
that its cost is constant, because the number of texture accesses
is constant independent of the size of the model. However, it is
linear with the number of pixels of the viewport.

Table 1 shows the results obtained with our method. We may
see that the exploration can be done in real time. If a better
framerate is required, the size of VOM may be reduced either
by quantizing the results or by building a mip map, although
this last requires fine tuning in the shader in order to achieve
comparable results.

We have also implemented a CUDA-based version of the
SAT construction. Unfortunately, the connection between the
regular graphics pipeline and the CUDA pipeline is still not
properly accelerated. Thus, we still require downloading the
depth map to CPU before it may be uploaded to CUDA mem-
ory. We compare the data management of the two versions of
our algorithm in Figure 10. Due to the download, that is the
bottleneck of the algorithm, the framerates obtained are only
similar to the framerates obtained with the CPU version, as it
is shown in Table 2. Future versions of CUDA are expected

Model Viewport Ray casting VOM
Brain 256 x 256 100.19 100.11

512 x 512 100.18 58.04
Ear(Baby) 256 x 256 100.22 83.30

512 x 512 87.08 43.19
Engine 256 x 256 99.00 63.98

512 x 512 59.16 35.41
Intestines 256 x 256 36.12 30.89

512 x 512 22.97 18.46
Bonsai 256 x 256 39.70 33.35

512 x 512 24.18 19.17
Body 256 x 256 10.70 10.15

512 x 512 6.46 5.67

Table 1: Performance comparison with different models. Note how the impact
of adding vicinity shading and halos is low compared to the complete rendering
process.

to deliver the Frame Buffer Object feature that, theoretically,
should allow to pass textures from GPU memory to CUDA
memory, thus saving bandwidth and time. Therefore, we be-
lieve the CUDA version of our algorithm may improve fram-
erates over the CPU version. A more GPU-friendly approach,
though requiring multiple rendering passes was developed by
Hensley [29]. However, we believe that a CUDA-based ap-
proach will be more flexible.

Model Viewport VOM CUDA-VOM
Brain 256 x 256 100.14 100.11

512 x 512 60.11 58.04
Ear(Baby) 256 x 256 98.81 83.30

512 x 512 46.45 43.19
Engine 256 x 256 85.72 63.98

512 x 512 37.48 35.41
Intestines 256 x 256 34.90 35.82

512 x 512 18.80 18.46
Bonsai 256 x 256 38.01 33.35

512 x 512 19.68 19.17
Body 256 x 256 21.19 19.88

512 x 512 11.00 10.79

Table 2: Comparison of framerates between our original algorithm (VOM)
and the CUDA-accelerated version of the Summed Area Table computation
(CUDA-VOM).

As shown, the pipeline necessary to interleave OpenGL and
CUDA computations is not currently the most efficient one.
However, we may measure the amount of time required to com-
pute a Summed Area Table using CUDA and using the CPU.
This demonstrates that the CUDA approach is certainly more
efficient, and with future versions of the driver, it may improve
the framerates. The time needed to build the Vicinity Occlusion
Map is only dependent on the resolution of the image, and it is
shown in Table 3.

Although VOMs are suitable for increasing depth informa-
tion of isosurfaces detected by volume ray casting, they may

8



CPU 

Depth Map
Vicinity Occlusion 
           Map

GPU 

2DSAT

GPU - CUDA 

GPU 

2DSAT

CPU 

Parallel processing

Incremental processing

Depth Map
Vicinity Occlusion 
           Map

Allocate device memory

Copy input from host to device Free device memory

Copy result from device to host

Figure 10: CPU Summed Area Table computation compared to the CUDA-based algorithm.

Viewport CPU time (ms) CUDA time (ms)
256 x 256 0.608 0.327
512 x 512 2.427 1.450
768 x 768 17.951 1.982

Table 3: Comparison of the computation time of the VOM in CPU and with
CUDA.

Figure 11: Close up of a Vicinity Occlusion rendering. Increasing the vicinity
factor shows that some regions (far from the user) are slightly over darkened.

also be used for semi-transparent objects in a similar way. The
only key point is the depth map creation, that must be gener-
ated accordingly to the information of the structure we want to
analyze.

On the other hand, objects with a lot of disconnected struc-
tures, or structures that are partially isolated in 3D space may
result in over darkened regions with this algorithm. This is due
to the fact that the occlusion function is built on a 2D basis, and
not taking into account 3D geometry. In most cases, this will
probably not be a visual problem (such as in Figure 1 where
frontmost ribs darken the ones in the bottom). However, in
some cases this might be noticeable and even yield a some-
what unexpected result, as in the intestines picture in Figure 11.
In this image, the vicinity factor is raised a little bit in order

to make the darkened regions more obvious. Note that some
regions are too dark because the geometry acting as occluder
is close to the viewer, and hence there is a large depth jump
in the depth map, while the actual occlusion in 3D might be
lower than the one estimated, as the occluding geometry does
not span for a large 3D region in space. This is not a complete
disadvantage in the sense that it still helps judging the 3D ar-
rangement of structures, but a better approximation might yield
a more qualitative result.

In order to improve the shading quality, the information of
the 3D occlusion should be taken into account. This is the ob-
jective of the DSAT3D data structure and rendering algorithm,
presented in the following section.

5. Ambient occlusion using Density Summed Area Tables

So far, we have presented a data structure tailored to simu-
late ambient occlusion effects in real-time for volumetric mod-
els. As stated, VOMs take into account 2D information cap-
tured from the depth map. This information is view-dependent.
Thus, it does not take into account the volume around the point
to shade, but only the 2D projection of the dataset on the view
direction. The practical consequence is that non opaque re-
gions do not contribute to the shading because the depth map
is built at the distance the intersection ray reaches total opacity.
A more realistic approach should take into account this infor-
mation. Ruiz et al. [13] explore the surrounding of a point by
tracing rays in all directions. Nevertheless, we want to avoid
tracing so many rays, because it implies an important perfor-
mance penalty.

5.1. Data Structure
As already stated, we will build a 3D version of a Summed

Area Table. 3D Summed Area Tables have been used previ-
ously for importance information storage ([30]). In our case,

9



First step Second step                      

Third step                      

Fourth step                      

3DSAT[0, 0, 0] = Input [0, 0, 0] 3DSAT[x, 0, 0] = 3DSAT[x-1, 0, 0] + 
                + Input [x, 0, 0]

3DSAT[0, y, 0] = 3DSAT[0, y-1, 0] + 
                + Input [0, y, 0]

3DSAT[x, 0, 0] = 3DSAT[x-1, 0, 0] + 
                 + Input [x, 0, 0]

     3DSAT[x, 0, z] = 3DSAT[x-1, 0, z] + 
+ 3DSAT [x, 0, z-1] - 3DSAT[x-1, 0, z-1] + 
                   + Input [x, 0, z]

     3DSAT[x, y, 0] = 3DSAT[x-1, y, 0] + 
+ 3DSAT [x, y-1, 0] - 3DSAT[x-1, y-1, 0] + 
                     + Input [0, 0, 0]

    3DSAT[0, y, z] = 3DSAT[0, y-1, z] + 
+ 3DSAT [0, y, z-1] - 3DSAT[0, y-1, z-1] + 
                     + Input [0, y, z]

3DSAT[x, y, z] = 3DSAT[x-1, y, z] + 3DSAT [x, y, z-1] - 3DSAT[x-1, y, z-1] + 3DSAT[x, y-1, z] - 3DSAT[x-1, y-1, z] - 
                                          - 3DSAT[x, y-1, z-1] + 3DSAT[x-1, y-1, z-1] + Input [x, 0, z]

  3DSAT[x-1, y, z]       3DSAT [x, y, z-1]       3DSAT[x-1, y, z-1]     3DSAT[x, y-1, z]     3DSAT[x-1, y-1, z]    3DSAT[x, y-1, z-1]     3DSAT[x-1, y-1, z-1]

Figure 12: Construction of the 3D Summed Area Table.

we store the density values of the dataset. The construction of a
3D Summed Area Table is similar to the 2D version. Again, the
construction may be done incrementally, as shown in Figure 12.

v2

v4

v8 v7

v5v6
v3

v1

Figure 13: Average density computation in a 3D Summed Area Table.

Now, in order to compute the average density of a region
with the DSAT3D we require eight texture accesses (as com-
pared with the 4 queries required by the 2D SAT), as shown in
Figure 13. The average density on a cubic region is therefore
computed as D = (V1 − V2 − V3 + V4 − V5 + V6 + V7 − V8)/8.

5.2. Algorithm Overview
Our method consists in building a 3D Summed Area Table of

the density values of the volume. As a result, we will be able to
approximate the occlusion of the 3D environment by analyzing
the opacity of the neighborhood. The pipeline of this algorithm
is depicted in Figure 14.

D1D2

D3D4

D5D6

D7D8

Figure 15: Cube subdivision in 8 regions in order to improve density evaluation.

In contrast to the previous system, the DSAT3D may be built
as a preprocess and used during the rendering. As a conse-
quence, we will obtain higher frame rates because there is no
need to further transfer information from the GPU to the CPU
and backwards. Moreover, our approach is valid even for Trans-
fer Function changes, because the occlusion is computed by
evaluating the Transfer Function on the density values, that re-
main constant.

5.3. Ambient Occlusion using DSAT3D

In order to simulate ambient occlusion in a point, we evalu-
ate the opacity of the voxels that surround the point. In order
to do this, we could sample the density of each neighboring

10



CPU GPU

Color map

Opacity map

Ray casting

Densityavg computation

Final imageMedical Images

3d Texture

3d Texture

+

DSAT3D

DSAT3D

Volume Model

Figure 14: Application architecture: The CPU reads a volumetric model from a DICOM file, builds a 3D texture from it. It also builds a Summed Area Table of
the 3D texture (DSAT3D), which is stored in another 3D texture. Both textures are uploaded to the GPU. For the rendering, the GPU performs a ray casting on the
model and once the ray has reached a certain opacity, the DSAT3D is queried in order to evaluate ambient occlusion.

voxel, determine its opacity by applying the transfer function,
and reduce illumination according to it. However, like in the
2D version, this would yield too many texture reads. Instead,
what we do is evaluating the average density values by using
the DSAT3D. The naive approach would consist in evaluating
the average density, D, of a cubic region around the point of in-
terest. However, taking D as the average density and applying
the Transfer Function in order to estimate occlusion will intro-
duce an error that might become excessive. This is especially
true if the average takes into account a large number of voxels.
With little more effort, we may evaluate occlusion as a sum of
the occlusion caused by the 8 cubic regions around the point of
interest, as depicted in Figure 15. This yields a much better ap-
proximation for close geometry. Thus, the occlusion evaluation
function will be:

Occlusion = sqrt(
8∑

i=1

Opacity(Di)/8), (12)

where Di are the average densities depicted in Figure 13, and
Opacity value is the opacity given by evaluating the Transfer
Function on the given density value. Using the square root
makes a low opacity value to count slightly over the linear func-
tion. Being Opacity a function defined in [0, 1], the square root
increases the contribution of low average opacity, but keeping
the maximum reachable opacity value to be 1. The darkening
value is computed applying the following function:

darkv = smoothstep(minimumDark, 0.99,Occlusion) (13)

This helps the user to fine tune the amount of darkening de-
sired. Finally, the resulting color is computed by evaluating the
same function as with the previous approach:

Fcolor = RCcolor − darkv · vicinitycolor (14)

The new data structure, DSAT3D, stores a sum of the den-
sity values. This means that the storage required to hold all the
values is higher than the original 3D model. This is a typical
tradeoff of space versus speed, but might become a problem if
the memory is limited.

5.4. Results

The presented approach can be used to compute ambient oc-
clusion in a fast way, at the cost of a large data structure. The
results are similar to the ones obtained with the 2D version for
models that do not present isolated disconnected features. We
may see an example in Figure 16, where the ear of the Baby
model is shaded using VOM (left) and DSAT3D (right).

(a) VOM (b) DSAT3D

Figure 16: 2D Vicinity Occlusion shading versus 3D ambient occlusion using
Summed Area Tables. For objects with no large depth jumps or disconnected
structures, the results are fairly similar.

We can see a couple more examples in Figure 17. Left col-
umn shows the original ray casting result and the right one
shows the ambient occlusion enhanced rendering.

Like the previous results, the framerates correspond to a PC
equipped with an Intel Core 2 Duo CPU running at 3.0 GHz. It
also has 4GB of RAM memory and a NVidia GeForce GTX 280
GPU with 1GB of RAM. The raycasting analyzes 3 samples
per voxel in order to guarantee a high quality result. All images
were rendered using a 512 × 512 viewport. The results appear
in Table 4.

We may see that the time impact is lower for the DSAT3D
than for the Vicinity Occlusion Maps. An analysis of the impact
is presented in Table 5 where we show the values for the 512 ×
512 viewport. Note how we may deduce that the cost of the 2D
version is roughly constant per frame if we compare models that
render at similar speeds and have similar resolutions, such as
the Intestines and the Bonsai models. The cost of implementing
a DSAT3D occlusion is lower, although, as already stated, it
consumes more memory.

11



Ray casting DSAT3D ambient occlusion

Figure 17: Ambient occlusion as computed with the DSAT3D structure.

Model Viewport Ray casting DSAT3D
Brain 256 x 256 100.19 100.18

512 x 512 100.18 100.15
Ear(Baby) 256 x 256 100.22 100.20

512 x 512 87.08 80.09
Engine 256 x 256 99.00 94.63

512 x 512 59.16 58.17
Intestines 256 x 256 36.12 35.82

512 x 512 22.97 22.04
Bonsai 256 x 256 39.70 38.82

512 x 512 24.18 22.84
Body 256 x 256 10.70 10.40

512 x 512 6.46 6.25

Table 4: Framerates obtained with the different models of our test.

For the models we tested, the amount of extra storage re-
quired for having a DSAT3D structure is rather high, although
it may compete with other approaches that store more elabo-
rated information such as the method by Ropinski et al. [14]. In
their case, the time and storage requirements for relatively large
models (such as the ones presented here) may be prohibitive. In
our case, we usually need a 32 bit texture with the same dimen-
sions as the input dataset (see Table 6). This more than doubles
memory requirements, but we did not face the problem of com-
pressing this structure yet. Fortunately, the information con-
tained is used to calculate average densities and therefore we
expect we may reduce the size significantly without important
quality loss.

We have also implemented a CUDA-based algorithm in order
to improve construction timings. As we may see in Table 7, the
timings are better for the CUDA version as the model sizes in-
crease. CUDA timings are very good for large models, as they

Model Ray VOM DSAT3D
casting (impact) (impact)

Brain 100.2 58.04 (-42.06%) 100.2 (-0.03%)
Ear(Baby) 87.08 43.19 (-50.39%) 80.09 (-8.02%)
Engine 59.16 35.41 (-40.14%) 58.17 (-1.67%)
Intestines 22.97 18.46 (-19.65%) 22.04 (-4.05%)
Bonsai 24.18 19.17 (-20.71%) 22.84 (-5.55%)
Body 11.98 10.79 (-9.88%) 11.82 (-1.35%)

Table 5: Comparison of the impact incurred by using the both methods pre-
sented in this paper for the 512 × 512 viewport. Note how the impact of the
DSAT3D system is really low, up to an 8% at most.

Model Sat3D Maximum # bits
generation value required

Brain 0.04 sec 31238068 25
Ear(Baby) 0.74 sec 198720120 28

Engine 2.07 sec 187475047 28
Intestines 5.75 sec 4294967192 32

Bonsai 6.08 sec 136596356 28
Body 17.1 sec 4294967162 32

Table 6: DSAT3D building features: Second column shows the time required
for the construction of the DSAT3D and the right column the two rightmost
columns show the maximum reached value and the minimum number of bits
required to build it.

are substantially smaller than the time required to load big vol-
ume models. Like in the previous case, the bottleneck is placed
at the data communication bus, because the resulting informa-
tion cannot stay as texture memory and has to be downloaded
again to CPU, as depicted in Figure 18. This is something ex-
pected to be solved as soon as the Frame Buffer Objects are
incorporated to CUDA.

For the GPU implementation, the SAT3D computation algo-
rithm had to be changed. Indeed, porting naively the CPU algo-
rithm to GPU lead to a memory pattern that did not fit the GPU.
Even though the input was read a few times, and locally, it did
not meet the memory pattern requirements to use maximal per-
formance of the GPU. We thus implemented a new algorithm
based on the following observation: the SAT3D computation
can be separated.

S X(i, j, k) =
∑

x≤i in(x, j, k)
S Y(i, j, k) =

∑
y≤ j S X(i, y, k)

S AT (I, j, k) = S Z(i, j, k)
=
∑

z≤k S Y(I, j, z)
=
∑

z≤k
∑

y≤ j S X(i, y, z)
=
∑

z≤k
∑

y≤ j
∑

x≤i in(x, y, z)

The algorithm is now changed to the following:

1. Compute SX in a first pass, Z-slice after Z-slice, by load-
ing a line in shared memory, perform a parallel computa-
tion (see [31]) for the line, and write back the line.

2. Compute SY in a single pass reading memory with a nice
GPU pattern-coalesced reads.

12



Model load time CPU time CUDA time
Brain 0.12s 0.04s 0.11s
Ear (baby) 0.20s 0.74s 0.18s
Engine 0.46s 2.07s 0.31s
Intestines 1.42s 5.73s 0.74s
Bonsai 1.87s 6.08s 0.75s
Body 3.58s 17.10s 1.64s

Table 7: Timings of DSAT3D construction by using a CPU or CUDA. Note
that the DSAT3D construction using CUDA takes lower than the time required
to load the model, which is an acceptable result. The CPU algorithm for large
models is relatively time consuming.

3. Compute SZ in a single pass reading memory with a nice
GPU pattern-coalesced reads.

This new algorithm leads to a significant performance im-
provement:

1. CPU algorithm on GPU (nave port) - 3DSAT computation
w/o memory transfers - 512x512x512 : 3.997118 s on a
Tesla M1060.

2. New GPU algorithm - 3DSAT computation w/o memory
transfers - 512x512x512 : 0.191357 s on a Tesla M1060.

This performance gap is explained by the memory pattern of
the algorithm. The first naive algorithm uses a scattered mem-
ory pattern in reads (reading memory that has just been written),
when the second algorithm reads data in a coalesced manner
which is the GPU friendly way that yields very high bandwidth.

6. Discussion

We begin our discussion by comparing VOMs with previous
approaches for ambient occlusion and halo rendering.

As in Stewart’s approach [28], our shading algorithm reduces
ambiguity by adding depth cues on the surface of rendered
models. However, instead of tracing rays through the neigh-
bor voxels, we analyze the depth information by querying the
Vicinity Occlusion Map. The efficiency of this approach makes
changing lighting parameters interactive. Thus, the user may
change how vicinity shading affects the overall rendering and,
even the color of the shading. Changing shading color helps
accentuating structures in a stylish way.

Ropinski et al. [14] also compute ambient occlusion on real
time, but require a long preprocessing time of several hours and
a huge amount of information storage that prevents them to deal
with relatively large models. Compared to this approach, ours
does not require preprocessing. Moreover, the amount of gen-
erated information is not so high, and therefore the models we
can deal with are larger.

Compared to Ruiz et al. [13], our approach runs in real-time,
because it takes advantage of the use of Summed Area Tables,
that limit the number of texture access to a constant amount,
even for arbitrary sizes. Moreover, its cost is limited, as the
work to be done per frame is constant. This has the advantage of
guaranteeing interactive framerates but implies a larger impact

CPU 

GPU - CUDA 

GPU 

DSAT3D

Parallel processing

Allocate device memory

Copy input from host to device

Free device memory

Copy result from device to host

3d Texture3d Texture

CPU 

CPU 

Incremental processing

DSAT3D

Figure 18: CPU Density Summed Area Table 3D computation compared to the
CUDA-based algorithm.

(in fps) for smaller models that yield very high framerates for
the GPU Ray Casting algorithm.

Compared to other surface depth-based ambient occlusion
methods ([11, 12]), we do not need to sample so exhaustively
the depth map, and the numbers obtained from the Summed
Area Tables already yield a filtered value of the depth map,
thus not requiring further filtering for avoiding sampling arti-
facts when camera moves.

Our halo rendering method also takes advantage of VOMs
in order to reduce the computation cost. As in Bruckner and
Gröller [20], our halos are flexible because they are generated
on the fly, and we may change their size and color interactively.
In contrast to their approach, we generate the halos in a ray
casting method by analyzing depth maps in a single (extra) pass.
Thus, our approach has a low impact on the rendering time. It
is only bounded by the size of the viewport, but its cost is linear
with the number of pixels, as it performs the computations on
2D views.

Concerning our second data structure, it is an example of
trading of space for speed. The required space is relatively high,
but has the advantage of the impact in rendering, which is al-
most null. Compared to Ropinski et al. [14], we improve the
preprocess times, as ours is only a question of seconds (in the
case of a CPU processing) while theirs may take up to several
hours. Furthermore, the CUDA implementation takes less than

13



(a) VOM (b) DSAT3D

(c) Depth VOM (e) Opacity DSAT3D

(f) Original RC (g) VOM-DSAT3D

Figure 19: 2D Vicinity Occlusion shading versus 3D ambient occlusion us-
ing Summed Area Tables. Top row compares the 2D ambient occlusion and
the 3D ambient occlusion algorithms implemented using Summed Area Tables.
Middle row show the depth map used to build the VOM data structure and the
Opacity Map computed from the DSAT3D. Finally, we may see the original ray
casting without depth enhancement and the use of both VOM and DSAT3D to
improve ambient occlusion computation.

two seconds, which is lower than the time required to load a
relatively large model. In any case, we may deal with larger
models. Notwithstanding, the memory cost may pose a prob-
lem for large models or low memory GPUs.

Finally, the two algorithms presented here yield quite similar
results for a large number of models. In most cases, deciding
which to apply will consist on a tradeoff between memory and
speed. For disconnected structures, the results may be slightly
different, although the result will probably be acceptable in both
cases. We may see a comparison in Figure 19. We show the
initial ray casting (f ) and the results applying VOM and DSAT
(a and b, respectively). In order to interpret where the shadows
come from, we also provide the depth map generated for the
VOM in c and the opacity map created through the DSAT3D
ambient occlusion execution in d. In such cases, we also have
tested how a combination of both effects would look like, and
this is shown in Figure g.

Although combining both algorithms requires an extra ren-
dering step, the bottleneck is at the VOM computation, and
therefore the framerates we obtain are the same than the ones
of the VOM.

7. Conclusions and Future Work

In this paper we have presented a fast method for ambigu-
ity reduction in volume rendering. We achieve this through the
use of two different techniques: vicinity shading and halo ren-
dering. For ambient occlusion simulation, we have presented

two different data structures: VOM and DSAT3D. Both of them
take advantage of the use of Summed Area Tables to accelerate
computation.

VOMs are generated at each frame, and provide a fast
method for screen-space ambient occlusion computation. The
impact in rendering is constant per frame, and depends on the
size of the viewport. With this data structure, we are also able
to generate halos on the fly. Our approach is fast, and highly
configurable, and, therefore, the user gains flexibility in the ex-
ploration process through an intuitive interface.

DSAT3D consists in a 3D Summed Area Table of the density
values of the original model. With this structure, we may esti-
mate ambient occlusion by analyzing 3D geometry around the
point of interest. This makes the simulation to be view indepen-
dent. This method is slightly superior than VOMs for objects
consisting in discontinuous or separated structures. For other
objects, the results are similar to VOMs but with a lower impact
in rendering. The cost is an increase in memory consumption,
which may represent a problem in environments with limited
GPU memory. As a consequence, one of the issues we want to
point to in the future is the reduction of DSAT3D size.

Acknowledgment

This work has been supported by project TIN2007-67982-
C02-01 of the Spanish Government.

References

[1] J. Dı́az, H. Yela, P.-P. Váazquez, Vicinity occlusion maps: Enhanced
depth perception of volumetric models, in: Computer Graphics Interna-
tional, 2008, pp. 56–63.

[2] R. Yagel, A. Kaufman, Q. Zhang, Realistic volume imaging, in: VIS ’91:
Proceedings of the 2nd conference on Visualization ’91, IEEE Computer
Society Press, Los Alamitos, CA, USA, 1991, pp. 226–231.

[3] U. Behrens, R. Ratering, Adding shadows to a texture-based volume ren-
derer, in: VVS ’98: Proceedings of the 1998 IEEE symposium on Volume
visualization, ACM, New York, NY, USA, 1998, pp. 39–46.

[4] J. Kniss, G. Kindlmann, C. Hansen, Multidimensional transfer functions
for interactive volume rendering, IEEE Transactions on Visualization and
Computer Graphics 8 (3) (2002) 270–285.

[5] S. Zhukov, A. Iones, G. Kronin, An ambient light illumination model, in:
Rendering Techniques, 1998, pp. 45–56.

[6] H. Landis, Production-ready global illumination, in: Siggraph ’02 Course
Notes, Washington, DC, USA, 2002.

[7] M. Tarini, P. Cignoni, , C. Montani, Ambient Occlusion and Edge Cueing
to Enhance Real Time Molecular Visualization, IEEE Transactions on
Visualization and Computer Graphics 12 (5) (2006) 1237–884.

[8] J. Kontkanen, S. Laine, Ambient occlusion fields, in: I3D ’05: Proceed-
ings of the 2005 symposium on Interactive 3D graphics and games, ACM,
New York, NY, USA, 2005, pp. 41–48.

[9] M. Malmer, F. Malmer, U. Assarsson, N. Holzschuch, Fast precomputed
ambient occlusion for proximity shadows, Journal of Graphics Tools
12 (2) (2007) 59–71.

[10] K. Zhou, Y. Hu, S. Lin, B. Guo, H.-Y. Shum, Precomputed shadow fields
for dynamic scenes, in: ACM SIGGRAPH 2005, ACM, New York, NY,
USA, 2005, pp. 1196–1201.

[11] M. Mittring, Finding next gen: Cryengine 2, in: SIGGRAPH ’07: ACM
SIGGRAPH 2007 courses, ACM, New York, NY, USA, 2007, pp. 97–
121.

[12] L. Bavoil, M. Sainz, Multi-layer dual-resolution screen-space ambient oc-
clusion, in: SIGGRAPH ’09: SIGGRAPH 2009: Talks, ACM, New York,
NY, USA, 2009, pp. 1–1.

14



[13] M. Ruiz, I. Boada, I. Viola, S. Bruckner, M. Feixas, M. Sbert,
Obscurance-based volume rendering framework, in: Proceedings of
IEEE/EG International Symposium on Volume and Point-Based Graph-
ics, 2008, pp. 113–120.

[14] T. Ropinski, J. Meyer-Spradow, S. Diepenbrock, J. Mensmann, K. H. Hin-
richs, Interactive volume rendering with dynamic ambient occlusion and
color bleeding, Computer Graphics Forum (Eurographics 2008) 27 (2)
(2008) 567–576.

[15] E. Penner, R. Mitchell, Isosurface ambient occlusion and soft shadows
with filterable occlusion maps, in: Volume and Point-Based Graphics
2008, 2008, pp. 57–64.

[16] B. Csébfalvi, L. Mroz, H. Hauser, A. König, E. Gröller, Fast Visualization
of Object Contours by Non-Photorealistic Volume Rendering, Computer
Graphics Forum 20 (3) (2001) 452–460.

[17] H. Hauser, L. Mroz, G. I. Bischi, M. E. Gröller, Two-level volume ren-
dering, IEEE Transactions on Visualization and Computer Graphics 7 (3)
(2001) 242–252.

[18] Z. Nagy, J. Schneider, R. Westermann, Interactive Volume Illustration, in:
B. Girod, H. Niemann, H.-P. Seidel, G. Greiner, T. Ertl (Eds.), Proceed-
ings of Vision, Modeling and Visualization 2002, Akademische Verlags-
gesellschaft Aka GmbH, Berlin, 2002, pp. 497–504.

[19] X. Yuan, B. Chen, Illustrating Surfaces in Volume, in: O. Deussen, C. D.
Hansen, D. A. Keim, D. Saupe (Eds.), Proceedings of VisSym’04, Joint
IEEE/EG Symposium on Visualization (Konstanz, Germany, May 19–21,
2004), Eurographics Association, 2004, pp. 9–16, 337.

[20] E. G. S. Bruckner, Enhancing depth-perception with flexible volumetric
halos, IEEE Transactions Visualization and Computer Graphics 13 (6)
(2007) 1344–1351.

[21] A. Wenger, D. F. Keefe, S. Zhang, Interactive volume rendering of thin
thread structures within multivalued scientific data sets, IEEE Transac-
tions on Visualization and Computer Graphics 10 (6) (2004) 664–672,
member-David H. Laidlaw.

[22] D. Ebert, P. Rheingans, Volume illustration: Non-photorealistic rendering
of volume models, in: T. Ertl, B. Hamann, A. Varshney (Eds.), Proceed-
ings Visualization 2000, 2000, pp. 195–202.

[23] N. A. Svakhine, D. S. Ebert, Interactive volume illustration and feature
halos, in: PG ’03: Proceedings of the 11th Pacific Conference on Com-
puter Graphics and Applications, IEEE Computer Society, Washington,
DC, USA, 2003, p. 347.

[24] F. Ritter, C. Hansen, V. Dicken, O. Konrad, B. Preim, H.-O. Peitgen, Real-
Time Illustration of Vascular Structures, IEEE Transactions on Visualiza-
tion and Computer Graphics 12 (5) (2006) 877–884.

[25] J. Kruger, R. Westermann, Acceleration techniques for gpu-based volume
rendering, in: Proceedings of the 14th IEEE Visualization 2003, IEEE
Computer Society, Washington, DC, USA, 2003, p. 38.

[26] F. C. Crow, Summed-area tables for texture mapping, in: SIGGRAPH
’84: Proceedings of the 11th annual conference on Computer graphics
and interactive techniques, ACM, New York, NY, USA, 1984, pp. 207–
212.

[27] P. Viola, M. Jones, Rapid object detection using a boosted cascade of sim-
ple features, Computer Vision and Pattern Recognition, IEEE Computer
Society Conference on 1 (2001) 511.

[28] A. J. Stewart, Vicinity shading for enhanced perception of volumetric
data, in: Proceedings of the 14th IEEE Visualization 2003, IEEE Com-
puter Society, Washington, DC, USA, 2003, p. 47.

[29] J. Hensley, T. Scheuermann, G. Coombe, M. Singh, A. Lastra, Fast
summed-area table generation and its applications, in: Eurographics ’05,
ACM, ACM Press, 2005.

[30] A. W. Klein, P.-P. J. Sloan, A. Finkelstein, M. F. Cohen, Stylized video
cubes, in: ACM SIGGRAPH Symposium on Computer Animation, 2002,
pp. 15–22.

[31] M. Harris, S. Sengupta, J. D. Owens, Parallel prefix sum (scan) with cuda,
in: H. Nguyen (Ed.), GPU Gems 3, Addison Wesley, 2007.

15


