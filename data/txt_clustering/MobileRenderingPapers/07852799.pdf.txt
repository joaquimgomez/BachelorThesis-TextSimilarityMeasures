






































                                 978-1-5090-3710-0/16/$31.00 ¬©2016 IEEE                                 696

2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics(CISP-BMEI 2016)                       

Intuitive Volume Rendering on Mobile Devices
Yuchen Xin*, Hon-Cheng Wong**

Faculty of Information Technology
Macau University of Science and Technology, Macao, China

*Email: mo.xin.yuchen@ieee.org
**Corresponding author: hcwong@ieee.org

Abstract‚ÄîNowadays, mobile devices, virtual reality and aug-
ment reality technologies are developing faster and faster. With
a variety of equipments, people are no longer only using PC to
handle tasks. Some traditional system frameworks are migrating
to these new technology areas, and direct volume rendering is
one of them.

In this paper, we propose a real-time and intuitive volume
data exploration framework on mobile devices. Our framework
introduces a direct-touch transfer function design method that
is able for the user to pick voxels directly on a volume rendered
image. With one-pass shader, user interaction and volume
rendering can be handled efficiently in real-time. The user only
needs to learn a few related knowledge to explore a volume data
and get its rendered image. As a result, the time cost of transfer
function design for direct volume rendering is significantly
reduced. Our framework is implemented with OpenGL ES 3.0
and GLSL shader. Experimental results show the advantages of
our framework. Researchers, and even general users, can easily
obtain volume rendered images of volume data.

Keywords: Direct volume rendering, mobile devices, trans-
fer function design, volume data, direct touch, user interface
design

I. INTRODUCTION

The computational power of mobile devices is much
stronger, and some traditional tasks on computer can be
handled by mobile devices. One of the most obvious areas is
graphics on mobile devices. Usually people use vertex data to
represent polygons and combine multiple polygons to describe
the surface of an object. The surface of an object can be
reconstructed easily with polygons, but the inner structures
of the object is invisible. Most of the time, such as in
3D computer games, people do not concern the structures
inside the object. However, some fields including science,
engineering, and medicine, visualizing inner structures of a
volume data is important. Therefore, direct volume rendering
(DVR) [1], [2] was developed to visualize volume data. As
mobile devices are getting ubiquitous, people need portable
equipment solutions for DVR. Although the computation time
of DVR is more expensive than that of polygon data, there is
still a need to implement DVR on latest mobile devices with
new graphics technologies.

In order to reveal the inner structures of a volume data, the
user needs to design transfer functions (TFs). But this task is
quite difficult for the general user. Transfer functions are the
key component that gives the relationship between volume
data values and optical properties. After designing transfer

functions, the volume data can be represented with color
and rendered with a volume rendering algorithm to show the
inner structures of the data. Without the knowledge of the
transfer function design, the general user cannot provide a
well mapping for volume data. Thus we also need to solve
the user interaction problems for exploring volume data on
mobile devices.

In our previous work, we propose a transfer function design
method in a What You See Is What You Design fashion [3].
Our framework provides better ways to explore volume data
than that of traditional exploration framework. In brief, it
provides intuitive user interaction for the general user and
reduce the design cost for transfer functions. The general
user can exploit our method to design transfer functions in
few minutes with less knowledge. When the user clicks or
touches on the rendering screen under iso-surface mode, they
can find the borders of the object being rendered; when the
user views the full rendering result, they can confirm the
perspective structures.

In this paper, we implement our framework [3] with
OpenGL ES and GLSL Shaders on mobile devices. Our
mobile framework has three components, namely, rendering,
histogram mapping, and transfer function design. We
implement a one-pass shader in order to allow the system
work in real-time. High dimension transfer functions can
be easily used in our framework without losing the user
interaction intuition. The user interface and transfer function
design steps are user friendly to the non-professional user.

The remainder of this paper is organized as follows: Re-
lated work is introduced in Section II. Our framework is
described in Section III. The system implementation including
user interface, one-pass shader, and other details is given in
Section IV. Experimental results and discussions are provided
in Section V. Finally, we conclude our work and state our
future plan in Section VI.

II. RELATED WORK

In recent years, there are some volume rendering
implementations for mobile devices proposed by researchers
[4]. Most of them [5], [6] are based on WebGL, which
is a popular cross-platform graphics API and it is based
on HTML5 and OpenGL ES 2.0. However, OpenGL ES



                                                                                                                                          697

2.0 only supports 2D texture, thus the volume data texture
has to be input as 2D textures. In [7], they compared the
performance of OpenGL ES and Metal API in medical
volume visualization and found that the performance of Metal
API is better than OpenGL ES. They also pointed out that
specific graphics APIs may provide higher performance for
specific platforms, but for cross-platform schemes, it may cost
more time for writing programs. A remote volume rendering
visualization framework was developed [8]. In their work, the
rendering was done in high performance machine and then
stream the rendering results to the mobile clients. This kind
of technologies are widely using on game content streaming
and game remote control. A perfect network and good render
machine may bring real-time experience for volume data
exploration.

For volume data exploration, most of them are more
related to transfer function design. One may consider the
transfer function being combined [9] or morphed [10].
Or just use transfer function manipulation user interface
[11] for designing transfer functions. Also some image
processing algorithms are available for transfer function
texture, for example, one may split transfer function images
with normalized cut and then color the different parts of the
transfer function images [12]. For the user interaction, the
best way is to design the transfer function in what you see
is what you get fashion [13]. For the general user, they may
not be familiar with the knowledge about volume data, thus
a simple and better user interaction method can help them to
explore volume data in more intuitive way.

III. FRAMEWORK

There are three components in our framework as shown
in Figure 1. They are rendering, histogram mapping, and
transfer function design. Commonly, we use intensity and
gradient to generate the histogram of the volume data being
rendered. The histogram is a texture which maps color on
transfer function texture to the corresponding intensity and
gradient. By inputting a volume data with transfer function
texture, the volume data can be colored and explored. In
addition, one-pass shader for multiple texture outputs is now
supported on mobile devices. Therefore the rendering result,
intensity, and gradient textures are generated in the same time
and stored in the texture memories. When the user clicks
or touches on the rendering result texture, we can get the
corresponding intensity and gradient more efficiently.

A. Rendering

The major performance problem on mobile devices is
caused by the limitation of texture copy process. The less
transfers between graphics card memory and CPU memory,
the higher performance can be achieved. Thus an one-pass
shader is important and necessary. In this component, we
store the main textures only in the memory of the graphics

Fig. 2. Architecture of our framework.

rendering device. And only the transfer function texture will
be changed with the user interaction. The one-pass shader
provides the ability of outputting coordinates directly after
one pass. Therefore the times of texture transfer can be
reduced significantly. Moveover, general-purpose computing
on graphics processing units (GPUs) is still not valid in all
devices, for example, current smart mobile phones. GPU
shader should be the common selection for handling the main
processes in our framework. The performance advantages
of sampling textures in shader provides the possibility of
implementing our direct-touch one-pass scheme.

B. Histogram mapping

Histogram mapping is the important component connecting
rendering and transfer function design. The histogram with
(ùêº , ùê∫) coordinate property of a volume data can be generated
directly. How the transfer function texture mapping on
histogram is the main problem in our framework. The click or
touch from the user interaction on the position of iso-surface
rendering result brings the voxel position clicked by the user.
With clicked or touched voxel position we can calculate the
intensity and gradient with target voxel. Then the mapping to
transfer function texture is available.

C. Transfer function design

Transfer function design includes two tasks, one is to
provide transfer functions, the other one is to validate the
transfer functions. Iso-surface compositing mode lets the user
click on the surface of object and feedbacks target voxel.
After the histogram mapping, the transfer function texture
can be obtained. Then the transfer functions can be used in
the rendering component. Finally, the rendering result can be
validated in full rendering mode. If the user is not satisfied
with the rendering result, he or she can repeatedly operate
on this components based on the previous generated transfer
functions until a desired result is reached.

IV. IMPLEMENTATION

A. Architecture

Our framework is developed on Android with OpenGL
ES. Figure 2 shows the structure of our framework. Xamarin



                                                                                                                                          698

Fig. 1. Our framework.

with mono C# provides the feature to share cross-platform
code. This saves time to port our previous work on mobile
devices. OpenGL ES and GLSL shaders are used to render
the volume data. Moreover, OpenTK is the cross-platform
toolkit for OpenGL and based on Xamarin C#. Therefore,
on mobile devices, DVR can build on these cross-platform
components.

Compared to other platform, our architecture has more
advantages as Xamarin now is available on Android, iOS,
Mac and so on. The performance of Xamarin is also well
in our experience. Java, on the other hand, is the default
development program language in Android, but it is not a
good choice for iOS or Mac.

In addition, OpenGL ES 3.0 is a pretty mature API for
cross-platform graphics applications and it is also supported
by mainstream platforms. Therefore, it is much easier if we
implement our framework on the iOS platform. WebGL is
limited by browser and Javascript performance, so we suppose
the performance of our system is better. Of cause, the specific
graphics APIs can be implemented in our framework and
may provide better performance. For general cross-platform
cases, we suppose OpenGL ES API serials are more stable
for further development.

B. Graphics pipeline

There are at least four textures used in our framework.
They are back face, render result, intensity, and gradient
textures. First we need to render the back face texture to
store the back position of the proxy polygon. Then we
draw the render result texture, with inputting the back face
texture for calculating the ray casting direction. At the same
time we write intensity and gradient to the corresponding
textures. Finally swap the render result buffer to screen. Each

texture should be clear with transparent color, otherwise, the
direct-touch design may failed as reading outdated textures.
The user interface (UI) components are not placed above the
render view, because it may effect the rendering performance.

C. User interaction

1) User interface: The user interface of our system is
designed somewhat similar to a general volume exploration
software. There are two areas as shown in Figure 3: (A)
Control area; (B) Rendering area. The user can touch the
rendering area directly to design the two-dimensional (2D)
transfer functions. The system will calculate the touched
voxels and find their intensity and gradient. The color selected
will be applied onto the transfer functions. Therefore, the
user do not need to concern about the actual 2D transfer
function image.

‚àô Draw touch mode check box: The default drag action
on screen is to rotate the volume object. When this
check box is checked, the system will switch to draw
mode. The touch or drag action will apply color onto the
corresponding position on the transfer function texture.

‚àô 2D transfer function check box: If this check box is
checked, the system will enable 2D transfer functions.

‚àô Compositing mode radio group: There are four
compositing modes: X-ray, MIP, iso-surface, and full
rendering.

‚àô Showing texture radio group: There are four textures
which can be shown: back face, final, intensity, and
gradient textures.



                                                                                                                                          699

Fig. 3. User interface: (A) Control area; (B) Rendering area.

‚àô Pen color radio group: There are four colors that can
be selected in this control panel. One may design a
better color selector for specific the brush color.

‚àô Other controls: More controls that have not yet been
implemented in this control panel. For example, the ray
casting steps can be controlled, more steps bring higher
render quality, fewer steps bring better performance. The
pen size and sharp can also be controlled, even the 2D
transfer function image can be paint directly. Due to the
complex of the system, we have not yet implemented all
the controls.

2) Direct-touch design: The goal of the inner structures
exploration is to separate the components of a volume data.
For example, we want to find the neoplastic tissues in human
body. A volume data is a data set of voxels. Usually the
voxels of the same type components will have the similar
intensity and gradient values. When we use 2D transfer
functions, from the meaning of histogram, the area blocks

of the histogram map to the similar intensity and gradient
voxels. So if we need to separate the specific components of
the volume data, we only need to apply the opacity properties
on the corresponding area blocks in the histogram. Transfer
function texture is just for saving the opacity properties
corresponding to the histogram.

Therefore the goal of direct-touch scheme is to pick
the specific voxel by touching the volume rendering result
directly. But traditional method for picking polygon data is
not suitable for volume data. We need to combine different
compositing modes to finish the picking voxel task.

Listing 1. Direct-touch design
void TouchOnRenderingResult(int X, int Y, ColorBrush Brush)
{

int Intensity = GetIntensity(X, Y);
int Gradient = GetGradient(X, Y);
// Assignment Color on Transfer Function Texture
TransferFunctionTexture[Intensity][Gradient] = Brush.Color;

}

D. Shader in OpenGL ES 3.0

1) 3D Texture: We choose OpenGL ES as the graphics
component, especially the 3.0 version. Because the support
of three-dimensional (3D) texture is only available above 3.0
version. Although implement a DVR algorithm based on 2D
textures is common, it usually lowers the performance.

2) One-pass shader: In GLSL, we can use glFragColor
to output single texture and glFragData to output multiple
ones. But this feature is different in OpenGL ES 3.0, and we
need to use frame buffer object and bind the output texture by
ourselves. We implement the similar rendering pipeline as our
previous work which was implemented on PC. This is very
important to help mobile GPUs to reduce the computation
tasks and improve the performance.

Listing 2. One-pass shader
layout(location = 0) out vec4 FragmentColor0;
layout(location = 1) out vec4 FragmentColor1;
layout(location = 2) out vec4 FragmentColor2;
void FragmentShader main(void)
{

FragmentColor0 = vec4(ColorAccumulation); // Output Rendering Result Texture
FragmentColor1 = vec4(Intensity); // Output Intensity Texture
FragmentColor2 = vec4(Gradient); // Output Gradient Texture

}

V. EXPERIMENTAL RESULTS

A. Environment

Our framework is developed using Xamarin C# and
OpenGL ES 3.0 on Huawei Honor 6 with a Kirin 920 CPU
(A7 1.3GHz x 4, A15 1.7GHz x 4), 3GB memory, and an
ARM Mali-T628MP4 GPU. The optimization is not enough
for our system, and we are planning to do more work in the
future. The volume rendered texture size is set to 720 √ó 720
pixels without generating mipmaps. All the step counts of ray



                                                                                                                                          700

Fig. 4. CT Head (Data size: 1283).

inside volume data proxy polygon are set to 128 which is the
same as their slice count.

B. Evaluation

There are six cases: CT Head (Figure 4), Foot (Figure
5), Engine (Figure 6), Lobster (Figure 7), Skull (Figure
8), and Vessels (Figure 9). We show the four compositing
modes for CT Head in the figure, and paint in iso-surface
mode then validate in full rendering mode. Each volume
data exploration case costs around 10 minutes. We can see
the borders of the inner structures of volume data in the
rendering results. The result is what we expected, but the
performance should be improved with more accelerating
considerations. The data exploration level is based on the
time cost and trial-and-error procedures. Our approach shows
the advantages for classifying the volume data in a short
period of time.

The rendering performance of our framework in the X-
ray, MIP, and iso-surface modes are around 11 FPS, almost
the same. For the full rendering mode, due to its heavy
computation, the rendering performance is much slower as 2
FPS. Basically we can achieve real-time DVR on this mobile
device by reducing unnecessary frames. For example, if the
volume data is static, we do not need to update the rendering

Fig. 5. Foot (Data size: 1283).

Fig. 6. Engine (Data size: 1283).

Fig. 7. Lobster (Data size: 1283).



                                                                                                                                          701

Fig. 8. Skull (Data size: 1283).

Fig. 9. Vessels (Data size: 1283).

result every frame. Only when the user moves, rotates, scales
or other actions on the volume data, the rendering result should
be updated.

VI. CONCLUSION AND FUTURE WORK

In conclusion, we has developed an intuitive volume
rendering framework on mobile devices. Our implementation
exploits the 3D texture feature provided in OpenGL ES
3.0 and one-pass shader, thus user interaction and volume
rendering can be handled efficiently in real-time. Our
framework also introduces a direct-touch transfer function
design method that is able for the user to pick voxels directly
on a volume rendered image. The user only needs to learn a
few related knowledge to explore a volume data and get its
rendered image. As a result, the time cost of transfer function
design for direct volume rendering is significantly reduced.
Experimental results show the advantages of our framework.
As the fast GPU development on mobile devices, we can
expect the volume data exploration can be more efficiently in
the future.

However, there are also some disadvantages in our im-
plementation. For example, click or touch transfer function

designs still have some few deviations with our vision. As
our future work, we may have more algorithm optimizations,
update the control panel with more controllers, and include
more schemes for transfer function design.

ACKNOWLEDGMENT

This work was supported by the Science and Technology
Development Fund of Macao SAR (019/2014/A1). We
also thank for the volume data providers at Voreen
(http://www.voreen.org/).

REFERENCES

[1] K. Engel, M. Hadwiger, J. Kniss, C. Rezk-Salama, and D. Weiskopf.
Real-Time Volume Graphics. A K Peters, 2006.

[2] C. L. Bajaj, V. Pascucci, and D. R. Schikore. The contour spectrum. In
Proceedings of the IEEE Visualization ‚Äô97, pages 167‚Äì173, 1997.

[3] Y. Xin, H.-C. Wong, and U.-H. Wong. Transfer function for direct
volume rendering in the fashion of WYSIWYD (What You See Is What
You Design). International Journal of Advancements in Computing
Technology, 5(15):84‚Äì95, Nov. 2013.

[4] J. M. Noguera and J. R. Jimenez. Mobile volume rendering: Past, present
and future. IEEE Transactions on Visualization and Computer Graphics,
22(2):1164‚Äì1178, Feb. 2016.

[5] M. M. Movania and F. Lin. Ubiquitous medical volume rendering on
mobile devices. In Proceedings of the 2012 International Conference
on Information Society (i-Society), pages 93‚Äì98, 2012.

[6] M. M. Movania, W. M. Chiew, and F. Lin. On-site volume rendering with
gpu-enabled devices. Wireless Personal Communications, 76(4):795‚Äì
812, Jun. 2014.

[7] Y. Abdallah, A. Abdelhamid, T. Elarif, and A.-B. M. Salem. Comparison
between opengl es and metal api in medical volume visualisation. In
Proceedings of the 2015 IEEE Seventh International Conference on
Intelligent Computing and Information Systems (ICICIS 2015), pages
156‚Äì160, 2015.

[8] R. Unterhinninghofen, F. Giesel, and R. Dillmann. Mediframe ‚Äî remote
volume rendering visualization framework. In Proceedings of the 2011
Annual International Conference of the IEEE Engineering in Medicine
and Biology Society, pages 2368‚Äì2371, 2011.

[9] L. Zhou, M. Schott, and C. Hansen. Transfer function combinations.
Computers & Graphics, 36(6):596‚Äì606, Jun. 2012.

[10] H.-C. Wong, U.-H. Wong, and Z. Tang. Direct volume rendering by
transfer function morphing. In Proceedings of the 7th International
Conference on Information, Communications and Signal Processing,
pages 1‚Äì4, 2009.

[11] J. Kniss, G. Kindlmann, and C. Hansen. Interactive volume rendering
using multi-dimensional transfer functions and direct manipulation wid-
gets. In Proceedings of the IEEE Visualization ‚Äô01, pages 255‚Äì262,
2001.

[12] C. Y. Ip, A. Varshney, and J. JaJa. Hierarchical exploration of volumes
using multilevel segmentation of the intensity-gradient histograms. IEEE
Transactions on Visualization and Computer Graphics, 18(12):2355‚Äì
2363, Dec. 2012.

[13] H. Guo, N. Mao, and X. Yuan. WYSIWYG (what you see is what
you get) volume visualization. IEEE Transactions on Visualization and
Computer Graphics, 17(12):2106‚Äì2114, Dec. 2011.

6


