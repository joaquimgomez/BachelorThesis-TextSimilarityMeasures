








































 Published by the IEEE Computer Society 0272-1716/16/$33.00 © 2016 IEEE IEEE Computer Graphics and Applications 19

Editor: 
Theresa-Marie RhyneVisualization Viewpoints

Interacting with Large 3D Datasets 
on a Mobile Device
Chris Schultz
Nvidia

Mike Bailey
Oregon State University

Visualizing 3D datasets requires a high level of GPU performance due to the com-putational demands of volume render-
ing algorithms. In the past, the GPU hardware 
found on mobile devices was too underpowered 
for high-resolution datasets (512 × 512 × 512 
and higher). Rendering algorithms that produce 
higher-quality images, such as ray-cast volume 
rendering, put even more pressure on those de-
vices. To accommodate these performance short-
comings, much of the previous work has been 
done with client-server-based implementations.1

Although powerful, these applications require a 
reliable network of suf� cient bandwidth to per-
form interactively. 

To avoid the additional complexities that a net-
work introduces, we focus on local implementa-
tions—rendering done on the device itself. Such 
applications typically use 2D texture-based al-
gorithms, lower sample rates, and lower resolu-
tion volumes.1 These methods have resulted in 
frame rates of up to 7 frames/second.1 However, 
they produce lower-quality images compared with 
desktop implementations. Additionally, resolution 
downsizing in a professional setting is frowned 
upon due to the data underrepresentation. 

Today, graphics hardware in mobile devices has 
become more powerful, allowing rendering al-
gorithms to produce higher-quality images more 
quickly. An example of such a device is the Nvidia 
Shield Tablet, which has a Tegra K1, a mobile chip 
with 192 GPU cores. This tablet can perform 
ray-cast volume rendering at interactive frame 
rates, with high sample rates, when the volume 

resolution is approximately 256 × 256 × 256. We 
experimented with datasets ranging from 128 × 
128 × 128 to 512 × 512 × 512 and found 256 × 
256 × 256 to be the ideal resolution. Although 
the 512 × 512 × 512 test sample did run, the user 
interface was unresponsive and, at times, showed 
corruption. 

One way to interact with a 512 × 512 × 512 
(or larger) volume in such a situation is to lower 
the resolution. This works, but it is not optimum 
because much of the detail is lost. Instead, we 
propose an implementation that avoids downsiz-
ing by implementing a detail-on-demand scheme 
using subvolumes to keep the data’s resolution 
intact, while constraining the texture size to per-
form interactively.

Finding the Appropriate 3D Texture Size
One of the main reasons that ray-casting has been 
a rarity in mobile devices was not only due to the 
lack of GPU power, but that most devices, up until 
recently, didn’t have 3D texture support. Fortu-
nately, the Shield Tablet does have this support. 

One in� uence on the ray-cast algorithm’s per-
formance is the size of the 3D texture. For this 
project, we use a 512 × 512 × 894 scan of a dog’s 
head (see Figure 1). This dataset was provided by 
Sarah Nemanic of the Oregon State University 
College of Veterinary Medicine. It provides a good 
example because it is a representative use case 
within the � eld of veterinary medicine.

We � rst render the entire dog’s head dataset, 
non-downsized. Our application achieves a frame 
rate of 4 to 6 frames per second (fps) while rotating. 

g5vis.indd   19 8/22/16   4:03 PM



20 September/October 2016

Visualization Viewpoints

For testing reasons, we keep the sample rate at 500 
steps per ray. In comparison, an iPad 2 ray-cast 
implementation achieves a frame rate of 0.8 fps 
with 80 steps per ray and uses a smaller dataset 
(512 × 512 × 384).2 Even though there is a sig-
nificant increase in performance, 4 to 6 fps is not 
enough. While interacting with the application, 
whether it is adjusting render variable parameters 
or rotating the scene, there are noticeable delays 
between performing a gesture and the applica-
tion’s reaction to it.

It is clear that the dataset’s resolution is too high 
for the GPU to handle directly. The next step is 
to scale the volume down by a factor of two in 
each dimension. Rendering the new volume with 
the size of 256 × 256 × 447 increases the rotating 
frame rate to 17 to 18 fps. 

To compare our performance, we used a 2D 
textured-based implementation discussed in ear-
lier work3 that uses a dataset of 256 × 256 × 256, 
sampled at a rate of 128 slices, that achieves 
7.3 fps. When zoomed in (more pixels being 
rendered), our implementation reaches approxi-
mately 8 fps. In other words, our worst case, us-
ing a higher-quality algorithm, performs just as 
well as a 2D texture-based application. Also, this 
resolution gives a much more responsive user 
experience and the ability to view the dataset 
as a whole.

Nevertheless, we needed to resort to downsiz-
ing, which we know is not an acceptable long-term 
solution. Additionally, lowering the resolution in-
troduces visual artifacts and provides less accurate 
data, which Figure 2 shows. If the dog’s head da-
taset was at a higher resolution (such as 1,024 × 
1,024 × 1,024), more severe visual artifacts and 
data loss would occur. 

Now that we have a texture size that we know 
suits the Shield Tablet, we need to address the 
challenge of not resorting to volume downsizing 
by experimenting with subvolumes. 

Figure 1. Example slices from the dog’s head dataset. This 512 × 512 × 894 scan is a representative use case 
within the field of veterinary medicine.

(a)

(b)

Figure 2. Visual artifacts produced by downsizing:  
(a) 512 × 512 × 894 rendering and (b) 256 × 256 × 447 
rendering. When zoomed in, more profound wood grain 
artifacts are visible in the lower resolution. The trade-
off here is the increased frame rate, from 4–6 frames 
per second (fps) to 17–18 fps, for less accurate data.

g5vis.indd   20 8/22/16   4:03 PM



 IEEE Computer Graphics and Applications 21

Subvolumes
Instead of using the 256 × 256 × 447 downscaled 
volume, we can extract same-dimensioned subvol-
umes from the original dataset. The effect is as if 
there is a magnifying glass inside the original da-
taset, allowing the user to view the unaltered, full-
resolution data. Figure 3 shows how much more 
detail exists for the number of pixels being ren-
dered. The main drawback of subvolumes is that 
we aren’t able to explore the entire dataset without 
manually specifying what subvolume to load.

The Grid View
To address the issue of manually loading sub-
volumes to maneuver within the dataset, we in-
troduce a method that lets the user explore the 
various subvolumes. We first split the original da-

taset into chunks that are half the size in each di-
mension. So, in the case of the dog’s head dataset, 
each chunk has a resolution of 128 × 128 × 223. 
We then render all eight of these chunks at once 
as if they are just one 3D texture. Figure 4 depicts 
how this is done within our application.

To keep track of which subvolumes to load, we use 
a data structure called grid points. Each grid point 
resides at the intersection of each slice and holds 
which subvolumes to load and how the subvolumes 
spatially relate to each other within the rendering 
algorithm (similar to the color coding in Figure 4). 
Additionally, each neighboring grid point shares at 
least one subvolume. This creates continuity when 
traversing the data. Figure 5 provides an example 
of such data continuity. Without the middle image 
shown in Figure 5b, the edges of the left and right 
frames will not be shown together, and the user 
must flip between the two outer frames to under-
stand the relationship between the two edges. 

(a)

(b)

Figure 3. Same-dimensioned subvolumes: (a) the 256 
× 256 × 447 downsized volume and (b) the 256 × 256 
× 447 native resolution subvolume. Both images are 
rendered with the same number of pixels, but the 
latter is rendered with native resolution data, which 
allows for data preservation while keeping interactive 
frame rates. The only drawback with the lower image 
is the inability to view the entire dataset. 

S: [0., .5)
T: [0., .5)
P: [0., .5)

S: [0., .5)
T: [.5, 1.]
P: [0., .5)

S: [0.5, 1.]
T: [0., .5)
P: [.5, 1.]

S: [.5, 1.]
T: [.5, 1.]
P: [.5, 1.]

S: [0., .5)
T: [0., .5)
P: [.5, 1]

S: [0., .5)
T: [.5, 1.]
P: [.5, 1.]

S: [.5, 1.]
T: [0., .5)
P: [0., .5)

P: [0., .5)
T: [.5, 1.]
S: [.5, 1.]

S: [0., 1.]

T: [0., 1.]

P: [0., 1.]

Figure 4. 
Rendering 
eight small 
3D textures as 
one. Given the 
position of the 
ray within the 
geometry, we 
page into the 
correct texture 
and offset 
accordingly. 
(S, T, P) 
are texture 
coordinates, 
similar to 
Cartesian (X, 
Y, Z).

g5vis.indd   21 8/22/16   4:03 PM



22 September/October 2016

Visualization Viewpoints

The grid points not only hold information about 
what to load, but they act as a way to keep track of 
where the user is within the volume. The number-
ing scheme for the grid points is chosen in such a 
way that simple arithmetic can be applied to the 
current grid point to arrive at a neighboring grid 
point. For example, in Figure 5a if we start at grid 
point 4 and want to move left, we need to sub-
tract one from the current grid point and end up 
at grid point 3. If we want to go up to grid point 7, 
we add the width of the grid view (in this case, it 
has a width of three). We also limit the user from 
skipping over neighboring grid points in order to 
maintain the aforementioned continuity. For ex-
ample, we don’t allow the user to go from grid 
point 0 to grid point 8.

Now that we have a method to keep track of 
what to load and how to traverse the dataset, the 
final piece is to understand which direction the 
user wants to move. We use the user’s view direc-
tion and, from that, determine which neighboring 
grid point to visit next. For example, looking at 
Figure 5a, if we are at grid point 4 and are viewing 
straight up, we will end up at grid point 7. 

The Bread Slice View
We found the dog’s head dataset to be intuitive to 
explore. We all know what a dog looks like, so it 
is easy to understand which subregion we are cur-
rently viewing (such as a snout or lower jaw). But 
what if we are using the scan of the torso and it 
isn’t easy to tell what we are looking at or toward? 

To address this concern, we introduce what we 
call the bread slice view. Essentially, we allow the 
user to flip through the volume as if it were a loaf 
of bread. Once an interesting slice has been cho-
sen, tapping the image selects a subregion of the 
slice, as Figure 6 shows. The application then uses 
this selection to extract a subvolume containing 
the subregion selection and renders the image, re-
sulting in something similar to Figure 3b.

To help increase the user’s spatial awareness, we 
use a highlighted version of the downsized volume 
as a frame of reference (see Figure 6c). This not 
only helps with understanding where the subvol-
ume is located within the entire dataset, but it also 
helps show the current selection’s context and the 
user’s spatial orientation.

(a)

(c)

(b)

Figure 6. Bread slice view. (a) In the initial view, the 
red square represents the subvolume to be loaded. 
We also show (b) the resulting subvolume from the 
bread slice view and (c) a highlighted version of the 
downscaled view showing the selected subvolume as 
a frame of reference. 

Take the 
CS Library 
wherever 
you go!

IEEE Computer Society magazines and Transactions are now 
available to subscribers in the portable ePub format.

Just download the articles from the IEEE Computer Society Digital 
Library, and you can read them on any device that supports ePub. 
For more information, including a list of compatible devices, visit

www.computer.org/epub

(a)

(b)

0 1 2

3 4 5

6 7 8

Figure 5. Using grid points for data continuity. (a) For the 2D example, 
grid point 1 shares two subvolumes with grid points 0, 4, and 2, but 
only one subvolume with 3 and 5. (b) The example from our application 
shows transitioning through the different grid points. (Imagine going 
from grid point 3 to 4 to 5.)

g5vis.indd   22 8/22/16   4:03 PM



 IEEE Computer Graphics and Applications 23

Not only does constraining the texture size benefit the GPU’s performance, but it also 
bounds the memory footprint of the volume being 
rendered. Although we show that the Shield Tablet 
was able to successfully load a 512 × 512 × 894 
dataset, the voxel representation was only an 8-bit 
single channel. If the volume used a much higher 
voxel representation, as in 16-bit RGBA, the mem-
ory footprint would have been much higher and 
the data wouldn’t have fit on the device, because 
Android severely limits the amount of memory 
for a given application. Our method is designed 
to account for devices with both performance and 
memory limitations. 

References
 1. J.M. Noguera and J.R. Jimenez, “Mobile Volume 

Rendering: Past, Present, and Future,” IEEE Trans. 
Visualization and Computer Graphics, vol. 22, no. 2, 
2016, pp. 1164–1178.

 2. J. Noguera and J. Jimenez, “Visualization of Very 
Large 3D Volumes on Mobile Devices and WebGL,” 
Proc. WSCG Comm., 2012, pp. 105–112.

 3. M.B. Rodriguez and P.V. Alcocer, “Practical Volume 
Rendering in Mobile Devices,” Proc. Int’l Symp. 
Visual Computing, 2012, pp. 708–718.

Chris Schultz is a software engineer at Nvidia and a former 
graduate student of Oregon State University. Contact him at 
cschultz18@gmail.com.

Mike Bailey is a professor of computer science at Oregon 
State University. Contact him at mjb@eecs.oregonstate.edu.

Contact department editor Theresa-Marie Rhyne at 
theresamarierhyne@gmail.com.

Selected CS articles and columns are also available 

for free at http://ComputingNow.computer.org.

Take the 
CS Library 
wherever 
you go!

IEEE Computer Society magazines and Transactions are now 
available to subscribers in the portable ePub format.

Just download the articles from the IEEE Computer Society Digital 
Library, and you can read them on any device that supports ePub. 
For more information, including a list of compatible devices, visit

www.computer.org/epub

g5vis.indd   23 8/22/16   4:03 PM


