

























































Immersive Collaborative Analysis of Network Connectivity:
CAVE-style or Head-Mounted Display?

Maxime Cordeil, Tim Dwyer, Karsten Klein, Bireswar Laha, Kim Marriott, Bruce H. Thomas

Abstract— High-quality immersive display technologies are becoming mainstream with the release of head-mounted displays (HMDs)
such as the Oculus Rift. These devices potentially represent an affordable alternative to the more traditional, centralised CAVE-style
immersive environments. One driver for the development of CAVE-style immersive environments has been collaborative sense-
making. Despite this, there has been little research on the effectiveness of collaborative visualisation in CAVE-style facilities, especially
with respect to abstract data visualisation tasks. Indeed, very few studies have focused on the use of these displays to explore and
analyse abstract data such as networks and there have been no formal user studies investigating collaborative visualisation of abstract
data in immersive environments. In this paper we present the results of the first such study. It explores the relative merits of HMD
and CAVE-style immersive environments for collaborative analysis of network connectivity, a common and important task involving
abstract data. We find significant differences between the two conditions in task completion time and the physical movements of
the participants within the space: participants using the HMD were faster while the CAVE2 condition introduced an asymmetry in
movement between collaborators. Otherwise, affordances for collaborative data analysis offered by the low-cost HMD condition were
not found to be different for accuracy and communication with the CAVE2. These results are notable, given that the latest HMDs will
soon be accessible (in terms of cost and potentially ubiquity) to a massive audience.

Index Terms—Oculus Rift, CAVE, Immersive Analytics, Collaboration, 3D Network

1 INTRODUCTION
In 2016 personal head-mounted displays (HMDs) for immersive vir-
tual reality (VR) are reaching a mass market with the release of con-
sumer products such as the Rift by Oculus and the Vive by HTC. While
there are still technical hurdles around miniaturisation, focus depth and
resolution, the current massive investment [7, 26, 40] into these tech-
nologies can be reasonably expected to resolve these issues in the near
future. It is therefore timely to ask:
Does collaborative immersive visualisation no longer require the use
of expensive equipment at universities or corporate data centres?

Over the last two decades expensive room-filling immersive visu-
alisation facilities have been built at many universities and data cen-
tres (pioneered by the CAVE Automatic Virtual Environment [5]) to
enable visualisation (particularly in scientific and engineering appli-
cations). Collaborative sense-making has long been regarded as one
of their most important potential benefits. In particular, spatially im-
mersive platforms such as the Allosphere (UCSB), the YURT (Brown
University), CAVE2 (UIC), all consider support for multiple simulta-
neous users as a key requirement.

Collaborative visual data analysis is undoubtedly important. It al-
lows multiple people to work together directly in a shared environ-
ment, improves shared understanding of the data, and is crucial for
teams of users with different domain expertise [14]. While there have
been some studies into collaborative scientific visualisation in immer-
sive environments, to the best of our knowledge (§2) the study de-
scribed here is the first to investigate collaborative visualisation of ab-
stract data in any immersive environment.

There are obvious differences between these CAVE-style environ-
ments and the new breed of consumer HMD devices, for example:

• Maxime Cordeil, Tim Dwyer, Karsten Klein, and Kim Marriott are with
Monash University. E-mail: maxime.cordeil@monash.edu.

• Bruce H. Thomas is with the University of South Australia Email:
Bruce.Thomas@unisa.edu.au.

• Bireswar Laha is with the Stanford University, USA. E-mail:
laha@stanford.edu.

Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of
Publication xx xxx. 201x; date of current version xx xxx. 201x.
For information on obtaining reprints of this article, please send
e-mail to: reprints@ieee.org.
Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx/

resolution, presence and freedom of movement (§1). Thus, to answer
the question above, we need first to consider:
Will these differences be significant impediments to adoption of low-
cost HMD devices for collaborative visualisation of abstract data?

This paper sets out to experimentally evaluate the relative merits of
the HMD and CAVE-style environments in order to answer these ques-
tions for one representative collaborative visualisation task. Specifi-
cally, we focus on a type of data and task that is rarely considered in
VR type environments: analysis of connectivity in network data.

We asked pairs of users to collaboratively complete two tasks:
counting the number of triangles (3-cliques) and finding the shortest
path between two nodes. Our research questions consider the effect
of the VR platform on user task performance, collaboration and expe-
rience as detailed in §3, with more detailed hypotheses in §4.1. We
found (§5) that:

• There were significant differences in task completion time and
physical interaction between the two VR platforms;

• On the other hand, no significant differences were found in the de-
gree and type of collaboration used by participants between the two
VR platforms.

This is the first formal user study into collaborative analysis of ab-
stract data using immersive virtual reality. Such a study is inherently
complex with many linked aspects. It required:

• Developing an experimental platform for collaborative network vi-
sualisation tasks that tries to make the best use of immersion with-
out unfairly disadvantaging either of the particular display technolo-
gies tested. This was a difficult and carefully considered part of our
experimental design (§4.2).

• Constructing a multifaceted evaluation model to evaluate and anal-
yse collaborative abstract visualisations in the VR platforms. This
combines techniques and measures from HCI, VR, and InfoVis
communities, and includes a mixture of objective and subjective
measures. We consider self-perception of collaboration as well as
shared focus (i.e. the proportion of time spent viewing the same
parts of the 3D network) and communication (i.e. the verbal and
non-verbal oral exchanges) between collaborators (§4.7).

In our study we chose to use a 3D network visualisation. While
the use of 3D representations for abstract data like networks is not
common in the Information Visualisation community there is some



Fig. 1: The HT (purple, highlighting a node, left) and non-HT (green, right) participants viewing the 3D network in the CAVE2. The scene is
rendered from the HT position. The virtual green wand is aligned from the HT participant view position.

evidence that stereoscopic 3D representations have advantages over
2D [11] and VR platforms naturally support such representations.
However, our focus was not on comparing 2D and 3D network rep-
resentations, rather it was on exploring the impact of VR platform on
task performance, collaboration and user experience for some repre-
sentative abstract visualisation.

2 RELATED WORK

Immersive environments for visualisation: During the last three
decades, visualisation research employing Immersive Environments
has largely focused on large spatial immersion displays and Virtual
Environments (VEs), such as CAVEs and Head Mounted Displays
(HMDs). These technologies are employed to immerse users into data
graphics. High-resolution tiled displays have been shown to improve
perception and navigation for visual tasks by Ball and North [3]. Their
study showed that larger display area and resolution positively affects
peoples’ ability to find and compare visual targets. Shupp et al. [34]
studied viewport size and curvature of large high resolution displays.
They found that curved tiled displays increase user performance for
route and target search tasks in the context of map visualisation.

VEs have proven effective in many scientific applications such as
brain tumour analysis [41], archaeology [22, 35], geographic informa-
tion systems [4], geosciences [17, 15] or physics [20]. Features such
as head tracking and stereoscopy clearly improve user performance.
These studies were mainly focused on the general field of scientific vi-
sualisation in a single-user virtual environment. Collaborative visual
sense-making remains largely untested in immersive environments.

Abstract data visualisation in virtual reality: Raja et al. [33] eval-
uated 3D scatterplot visualisations in a CAVE environment for indi-
vidual users. The results of the study suggested that higher degrees of
physical immersion (e.g. number of projected walls and head tracking)
allow less errors and completion time to solve the tasks. The authors
also found that when enabled, head tracking reduces disorientation.

Ware and Mitchell [38] studied the perception of 3D node-link dia-
grams in different immersive conditions, such as stereoscopy and mo-
tion. They showed that stereoscopy reduces errors and response time
in the perception of paths in 3D node-link diagrams. Alper et al. stud-
ied 3D stereo highlighting of 2D graphs [2]. They found that stereo
enhances visual task performance such as finding elements in a graph
and counting connections between nodes. Kwon et al. [23] investi-
gated the effectiveness of graph visualisation and the impact of differ-
ent layout techniques on readability in an HMD. The authors report
that their 3D stereoscopic graph visualisation with an Oculus Rift out-

performed traditional 2D graph visualisations.
Collaborative visualisation: Collaboration can play an important role
in information visualisation by allowing groups of people to make
sense of data [13, 19], and is a significant method for successfully
understanding big and complex data [6, 9]. Paul et al. [32] have high-
lighted how important sense-making is when seeking information to-
gether, in addition to combining different roles and expertise of group
members to resolve ambiguity in data interpretation. To date, little re-
search has focused on collaborative and immersive environments for
abstract visualisation.

VR platforms have shown benefits to support collaborative tasks
such as puzzle solving [16], navigation with individualized views [39],
and complex manipulations such as moving a ring on a U-shaped
hoop [30]. Distributed virtual reality systems such as MASSIVE have
been developed for supporting communications between people [10].
Szalavri et al. developed a collaborative augmented reality system with
see-through HMDs in order to support collaborative scientific visuali-
sation [36]. They observed that their system seems to be superior to a
classic desktop environment, but did not provide formal results.

Mahyar and Tory explored how communication and coordination
can be supported to facilitate synchronous collaborative sensemaking
activities in Visual Analytics [28]. Recently, Donalek et al. published a
progress report of the exploration of VR as a collaborative platform for
information visualisation [6]. They describe iVIZ, a web-distributed
collaborative VR visualisation system that supports the Oculus Rift.
Their studies are still at an exploratory level and thus the authors did
not provide evidence of how effective this system is for collaborative
visualisation of big and complex data. Telearch [22], a virtual reality
system for collaborative archaeology, and Shvil [27], an augmented re-
ality system for collaborative land navigation, both support distributed
collaboration. However, both systems are designed for specific use
cases. A further prominent use of augmented reality is in industrial
applications, e.g. to support asynchronous collaboration [18].

VR platforms featuring 3D stereoscopic vision, high resolution and
head tracking, provide considerable benefits for collaborative visual
analytics. But as we have seen there have been no formal user-studies
evaluating their use for collaborative analysis of abstract data. While
the current technologies each have some limitations (§3), they are now
adequate to allow us to explore a design space for collaborative ab-
stract data exploration in immersive environments.

3 CHARACTERISTICS OF THE VIRTUAL PLATFORMS
The focus of this paper is the study of collaborative abstract visualisa-
tion of 3D network diagrams in CAVE-style and HMD platforms. We
chose these because CAVE-style platforms are currently considered



the state-of-the-art for immersive collaborative visualisation, while
HMD displays like the Oculus Rift will soon be commodity devices
that may represent a low-cost CAVE alternative. These systems are
located at the same place on the Mixed Reality continuum [29] as Vir-
tual Reality devices. However, CAVE-style devices are designed to
be collaborative and present physical affordances, while HMDs are
designed for a personal and more immersive experience with limited
physical movements (Table 1 shows platform differences).

CAVE2 Network of Oculus Rift
+ Leap Motion

Display Higher angular reso-
lution
330 degrees horizon-
tal field of regard, 360
degrees field of view

Lower angular resolu-
tion
360 field of regard,
100 degrees horizontal
field of view

Physical
movements
for changing
viewpoints

Unrestricted full body
movements for navi-
gation (single head-
tracking)

Head movements for
navigation (user teth-
ered to a computer)

Tracking in-
put

6DOF physical ob-
jects (wands)

Hand and finger-based
tracking

Collaboration
space

Physically collocated
Can see other users

Collocated and remote
Cannot see other users

Table 1: Differences between our two platforms

CAVE2 The CAVE2 is a room with a 3.7m radius, which contains
eighty 46-inch HD screens with a resolution of 1366×768 pixels per
screen. The screens in the room are arranged in a horseshoe shape,
which provides a display field of 330 degrees (Fig. 1). Users are phys-
ically immersed in the room in which they can walk within the virtual
models and naturally see one another. The size and the shape of the
display allow users to visualise graphics at the maximum field of view
of the human eye (180 degrees).

The CAVE2 is equipped with a 6 degrees-of-freedom (6DOF) track-
ing system with a latency of 5ms. The screens render 3D scenes in pas-
sive stereo (users wear polarised glasses). The tracking system obtains
one user’s 6DOF head pose for 3D scene rendering. A set of 6DOF
controllers (Fig.1) can also be tracked in the room allowing direct in-
teraction with the 3D scene. The other non-head-tracked (non-HT)
users do not benefit from the same physical immersion as the scene is
not rendered from their perspective, but they are able to use the 6DOF
controllers. As a consequence, when the HT user moves, non-HT users
see the scene moving beyond their control. A second consequence is
that when non-HT users point at 3D objects with a 6DOF wand, the
origin of the virtual wand in the scene is offset to its physical position
in the room, according to the HT user (Fig. 1). This is a physical lim-
itation of CAVE-style displays common to all but a few experimental
installations used exotic view multiplexing techniques, e.g. [1].
Oculus Rift Unlike a CAVE-style set-up, an HMD is designed for a
single user. The Oculus Rift DK2 (OR DK2) is equipped with a posi-
tional head tracker with a latency of 20ms. This allows a 360-degree
field of regard. However, the OR DK2 limits the human field of view
to 100 degrees horizontally, and also limits body movements as users
are tethered to the computer with a cable. In addition, the OR DK2
is not see-through, which makes it a pure VR device according to the
Milgram classification. HMDs, like CAVE-style VR displays, employ
trackers for sensing user interaction such as hand gestures. A com-
mon practice is to mount a Leap Motion controller 1 on the face of the
Oculus Rift to enable finger tracking and gesture recognition in front
of the user’s viewpoint, Fig 2 (a).

With the noticeable differences summarised in Table 1, one can ex-
pect a strong impact of the VR environment on collaborative visuali-
sation of abstract data. Factors that differ across environment are: the
users’ physical movements for viewpoint changes; the input tracker;

1www.leapmotion.com

the field of view; and the ability to physically see the collaborator.
These disparities need to be examined for collaborative abstract visu-
alisation in order to understand how groups of users can use these de-
vices to efficiently work together. Hence, we introduce the following
research questions which ask how the different immersive VR plat-
forms will affect:

RQ1 [Functionality] the ease with which groups of users can com-
plete analysis tasks?

RQ2 [Collaboration] the degree and kind of collaboration used in
connectivity analysis?

RQ3 [User Experience] the qualitative usability aspects?

In the next section, we present a user study which aims to answer these
questions.

4 USER STUDY
We designed an experimental set-up that allowed us to compare col-
laborative task performance between two VR platforms, 1) a set of two
HMDs (OR DK2s, Fig. 2) and 2) a CAVE-style environment (CAVE2,
Fig. 1) with respect to our research questions. In this section we for-
mulate hypotheses to test the research questions, introduce a method-
ology to evaluate and compare the two platforms, and explain our ex-
perimental design. In summary, the independent variable is the VR
platform, the dependent variables will be highlighted in the following.
For our study we chose analysis of connectivity in 3D networks as
representative abstract data visualisation task.

4.1 Hypotheses

RQ1 [Functionality] We expected that the resolution and the size
of the display will affect the accuracy but we had no initial assump-
tion on the results. We expected that the ability to physically walk to
change viewpoints would influence the completion time. Because of
the head-tracking, we expected to observe the collaboration strategies
to be less tightly-coupled in the HMD than in the CAVE2, i.e. the non-
HT participants were expected to follow the HT participants more in
the CAVE2. Our first hypotheses relate to the affect of platform on
basic independent variables:

H1 The VR platform will affect task completion time.
H2 The VR platform will affect accuracy.

RQ2 [Collaboration] In the CAVE2, users can directly see each oth-
ers’ body position, facial expressions and gaze direction. In contrast,
in the HMD condition participants see only their partner’s finger po-
sition and an indication of their field of view (§4.3).We expected this
disparity in non-verbal cues would make it easier to communicate and
share points of interest in the CAVE2, while workload would be more
evenly distributed in the HMD because of their independent points
of view. We also expect that HT participants in the CAVE2 will have
more head movements and be more physically engaged than their non-
HT collaborators.

H3 The VR platform will affect strategies reported by our partici-
pants in the post-hoc survey.

H4 The VR platform will affect the measured degree of collabora-
tion between participants regarding:

H4.1 the amount of shared focus,
H4.2 the amount of verbal communication during collaboration,
H4.3 the perceived determination of sub-tasks and their assignment,
H4.4 the perceived emergence of leadership,
H4.5 the balance of physical interaction.

RQ3 [User Experience] We suspected that people would be more sat-
isfied in the CAVE2 because of its seamless merging of the virtual and
real-world, where participants can see their own and their collabora-
tor’s physical body while performing the task in VR. Further, both
HMDs and CAVEs suffer from the vergence-accommodation conflict



which is widely known to affect performance and cause visual fatigue
[21], which could potentially vary between the platforms. The only
downside in the CAVE2 that we expected was that the non-HT partic-
ipants would have lowered satisfaction as are forced to play a subordi-
nate role in the collaboration. That is, they lack the independent head-
tracked viewpoint and due to visuo-proprioceptive mismatch [31] may
experience confusion and motion sickness.

H5 The VR platform will affect the self-reported usability.

H6 The CAVE2 will differently affect the HT and the non-HT par-
ticipants’ self-reported usability.

4.2 Experimental design

We used a between-participants 1× 2 (one factor VR platform with
two levels: HMD and CAVE) experimental design, in which we tested
two collaborative visualisation tasks. Participants were allocated ran-
domly to the HMD groups and the CAVE2 groups. The order of ap-
pearance of the stimuli was initially randomized so that the partici-
pants did not view them in a predictable order. The task order was
counterbalanced (§4.4). Participants were asked to evaluate their lead-
ership level initially by responding on a Likert 1-5 scale to “In team
work, I often take the leadership role”. This response was used to
counterbalance participants wearing HT by leadership (i.e. equal num-
bers of leader and non-leader participants had the correct viewpoint).
The HT participant was then fixed for the experiment.

Design rationale We decided to make the experimental set-up as
fair as possible and make the best use of each display platform. We ad-
dressed the shortcomings in each platform and tried to provide equiv-
alent features to the two groups of participants (§4.3). This choice
influenced a series of design considerations which are highlighted in
the following and summarised in Table 2.

Display The CAVE2 has an inherent horseshoe layout of display
screens, as previously mentioned. We made use of the maximum dis-
play surface of the 80 screens and used the horseshoe geometry to
make the 3D network wrap around the participants, Fig. 1. To avoid
issues related to incorrect parallax and to minimise the participants’
obstruction, the visualisation was placed beyond the screens with a
positive stereo (i.e. no graphics were rendered at the centre of the
room). The inner and outer nodes were located at a range of 0.4m
to 2m beyond the CAVE2 screens. The horizontal layouts of network
covered most of the CAVE2 display surface (∼300◦). They varied
vertically but were up to 80% of the vertical field.

In the HMD condition, because of the limited physical interactions,
the visualisation was scaled down to 10 percent (approximately 47cm
radius) of the CAVE2 size, so that the participants were immersed
at the centre of the horseshoe and the visualisation was within arms
reach, Fig. 2 (a). The visualisation proportions were preserved, i.e. the
network was closer to their point of view but not distorted.

Physical movements and highlighting interaction In the CAVE2,
participants were allowed to walk around the room. They could high-
light parts of the visualisation with a wand: a Playstation Move con-
troller with a 6DOF tracker mapped to a virtual wand rendered in the
3D scene, Fig. 1. When the virtual wand intersected a node, the node
doubled in size and turned yellow.

For safety reasons, participants wearing the HMD were sitting on
a chair next to each other, as the HMD was tethered to a desktop PC.
Participants were allowed to lean and rotate fully on their chair. A mini
virtual selection wand was attached to the index finger (Fig. 2) of each
participant in the HMD condition, using the Leap Motion sensor. The
same node highlighting function as with the CAVE2 was enabled. We
decided against using sophisticated navigation mechanisms involving
“magical” interactions (e.g. flying around the model) due to the degree
of training they may require.

Display Interactions

CAVE

1. 3D network enlarged,
viewable through the
CAVE2 screens in negative
parallax
2. horseshoe network layout

1. free walking
2. wand-based interaction

HMD

1. 3D network displayed
within arm’s reach of co-
located participants
2. horseshoe network layout

1. users seated in chairs;
head-rotations and leaning
2. bare-hand (finger-
tracked) interactions with
Leap Motion

Table 2: Design choices for the experimental set-up

We were concerned about the time participants spent in the HMD.
In order to avoid eye fatigue and dizziness that stereoscopic displays
can induce, we designed the experiment to not last longer than 40 min-
utes. We forced participants to take a 5-minute break between blocks
of tasks, but also allowed them (optionally) to take up to 3 minutes
after each trial.

Finally, we restricted the size of the collaborative groups to two in-
dividuals in order to reduce the agreement and decision-making time.
We also specified how groups reported their answers as a team. In or-
der to limit interaction in both environments, participants both had to
say we agree before reporting their answer to the experimenter.

4.3 A minimal set-up for collaboration with two networked
Oculus Rift DK2

We designed a minimal collaborative environment which combines
two HMDs (Oculus Rifts DK2) connected to a local network (Fig. 2).
We mounted a Leap Motion sensor on each HMD in order to enable
hand and finger tracking. The vertical field of view of the Leap Motion
sensor is 150 degrees wide. When mounted on an HMD, the physical
position and the rotation of the Leap Motion sensor are mapped to
those of the participant’s head. Hence the direction and position of the
sensor’s field of view is updated when users rotate their head. This
allows continuous finger tracking in the visual frame of reference.

We developed collaborative virtual environment software clients to
support collaborative visualisation with Unity. Each HMD position
and rotation data, along with finger tracking data from the Leap Mo-
tion, are sent over the network using a UDP socket connection. In
order to augment the sense of presence [25], the field of view of each
user was rendered in the virtual space (a wire-frame pyramid drawn
from each user’s perspective), along with their index finger position
(rendered as a virtual mini wand, Fig. 2).

This minimal set-up allowed the participants to highlight the nodes
of the visualisation using natural gestures as in the CAVE2, with in-
dependent perspective for each user, but with restricted movements
(users could not walk freely because they were tethered to the com-
puter by the Oculus Rift DK2 HDMI cable). User interactions (such
as pointing and highlighting nodes) from each participant were sent
over the network and rendered in real-time in each client.

4.4 Stimuli and tasks
We tested two 3D network visualisation tasks, which address the topo-
logical level of graph analysis [24]:

• Path: Finding the shortest path between two nodes
• Triangles: Counting the number of triangles (number of 3-vertex

cliques).

These tasks were selected because of their potential for collabora-
tion, such as divide and conquer. The shortest path task consisted of
finding the least number of edges between two green nodes of the 3D-
network (all the other nodes were coloured blue). Participants were
asked to report orally the list of nodes of the shortest path. Three dif-
ferent path lengths were used with six 3D-networks: eight, nine and



Fig. 2: (a) Minimal set-up for a collaborative environment using two OR DK2 + Leap motion, connected on LAN. Each user has an independent
view of the visualisation in a Unity client, and sees each other’s view frustum and wands. (b) Determination of the participants’ focus points
(FP1, FP2) using the intersection of a near peripheral vision cone with the network visualisation nodes (dashed nodes).

ten nodes. The triangles task consisted of searching and counting the
“triangles” in the 3D network. A triangle (i.e. a 3-vertex clique) was
presented as three mutually connected nodes. Participants were asked
to orally report the number of triangles they found in the 3D network.
We used three different numbers of triangles with six 3D-networks:
three, four and five triangles. The size of the 3D-networks (number
of nodes and edges) varied slightly between tasks (75 nodes and 110
to 140 links for the path task; 80 nodes and 100 to 110 links for the
triangle task).

The networks were randomly generated and Cola [8] was used to
generate the 3D layout. The nodes were rendered as blue shaded
spheres and the edges as white-grey shaded cylinders. The node
size/edge thickness ratio was preserved across the two VR platforms.
Labels (numbers) were placed above each node and rendered to a bill-
boarded quad. In the HMD condition, the labels were always facing
the camera (i.e., the participants’ eye). In the CAVE2 condition, the
labels were pointing towards the centre of the room, in order to be
legible for both participants. A multi-directional light was placed at
the centre of the visualisation to ensure visual clarity of the rendered
graphics.

4.5 Procedure

The participants were first given a pre-experiment survey concern-
ing demographic data and questions about previous knowledge of the
VR platforms and the visualisation tasks. Participants were then ac-
quainted with the VR equipment in their particular experimental con-
dition. In the CAVE2 platform, one participant was given the head-
tracker (mounted on polarized eyeglasses), while the other participant
was given a cap with a 6DOF tracker layout (Fig. 1, used to measure
their head position and rotations only). Team members then received
a 6DOF wand tracker. In the CAVE2 condition, the different head-
tracking capabilities were explained to the team.

Next, a 3D network visualisation was displayed in the virtual envi-
ronment in order to familiarise the participants with the visualisation.
In the HMD condition, the participants were asked to use the Leap Mo-
tion sensor to select specific target nodes to practise with the sensor. In
the CAVE2, participants practised with the wands and were asked to
highlight specific target nodes. Once comfortable in the VR platform,
the participants were instructed to report their answers: both had to
say we agree to stop the trial and provide the answer. The first task
was then explained to the team, which was either finding the shortest
path or counting the number of triangles in the network, according to
the counter-balanced order. Participants were told that the main goal
was to find the correct answer and that they should try to do it as fast
as possible. They were also encouraged to collaborate and find their
own strategy. The participants were also encouraged to discuss those

strategies between the trials. The team was then trained with the task
with one network.

Once ready, the participants completed six trials for the first task in
the following order:

• Both team members started at the centre of the visualisation.
• The experimenter displayed a new 3D network on the VR platform,

using a control interface on a tablet PC wirelessly connected to the
virtual environment (the start time of the trial was automatically
recorded), and participants started the task.

• When the participants found the answer and both said “we agree”,
the experimenter used the control interface to stop the timer (the
trial end time was then recorded) and to collect their answer (the
path chain or the number of triangles).2

• In the CAVE2, participants were then told to go back to their start-
ing positions. In the HMDs, participants’ view and chairs were
re-centred.

• The display was blank for five seconds and the next 3D network
was sent from the control interface.

At the end of the first six trials, the participants were given a five
minute break. They then proceeded to do the other six trials for the
second task, following the same steps. Last, the participants com-
pleted a post-experiment survey on a laptop, where information was
collected on their experience. Participants were not allowed to com-
municate while filling out the survey.

4.6 Participants

There were 34 participants recruited. They were allocated to nine
teams of two participants in the HMDs, and eight teams of two par-
ticipants in the CAVE2. The mean age of the participants was 36
(SD = 9.25) for the HMD group (15 males and 3 females), and 31
(SD = 9.23) for the CAVE2 group (6 males and 10 females). Partic-
ipants came from diverse backgrounds but had mainly computer sci-
ence and engineering backgrounds. In the CAVE2, participants were
50% familiar with network diagrams and 66% familiar with the en-
vironment. In HMD, 73% of the participants were familiar with net-
work diagrams and 62% were familiar with the HMD. Participants
were asked if they knew each other (50% knew each other in each
condition). This information was used to allocate the groups to the
task order (§4.5).

2We tried different mechanisms to stop the trials–e.g. pushing virtual
buttons–in the end an oral cue proved most foolproof.



4.7 Measures
We introduced objective and subjective measures used to evaluate the
differences between the platforms in terms of the hypotheses listed
in §4.1. The most basic of these were time to solve tasks (H1) and
accuracy (percentage of correct answers divided by the total number
of trials, H2).

Participants’ perception of their collaboration in the virtual environ-
ment was captured through a post-hoc survey. We adapted questions
from a study of presence in computer mediated collaboration [12], for
collaboration in VEs. Specifically, our questions aimed to measure
perceived communication, activity, capability and presence (the “feel-
ing of being there”). Responses were given on a Likert scale (Strongly
Disagree (1) - Strongly Agree (5)), except where explicitly stated oth-
erwise:

Strategies employed to solve the task (H3, open discussion question)
• “Can you describe the strategy that you used to count the trian-

gles?”
• “Can you describe the strategy that you used to find the shortest

path?”
Communication (H4.2) through the virtual reality set-up and infor-
mation sharing
• “I communicated frequently with my partner”
• “I openly shared all relevant information when completing the

tasks”
Organisational workload (H4.3)
• “Each of us had a clear sub-task”
• “There was conflict when determining the sub-tasks”
Evaluation of the perceived effort in completing tasks (H4.5)
• “My partner and I put a lot of effort into the tasks”
• “I was an active contributor when doing the tasks”
Usability (H5 and H6) of the system to perform the tasks
• “I felt comfortable using the virtual reality set-up”
• “I felt sick during the study”
• “I enjoyed myself”

In addition, we recorded head-tracked positions (6 DOF) (H4.5) and
audio (H4.2). We were then able to estimate degree of shared focus in
two ways (H4.1):

• Reported shared focus: through coding (by our first author) of par-
ticipant reported strategy (§5.2);

• Measured shared focus: a metric based on actual view frustum of
each participant.

Measured shared focus (H3) is determined from the positional and
rotational head movements provided by the VR tracking system
(recorded every 0.25 seconds). We assumed a vision cone to deter-
mine which portion of the 3D network the participants were viewing,
with an opening angle of 60◦ (corresponding to near peripheral vision,
also called the useful field of view, Fig. 2 (b)).

For each position and rotation recorded, we calculated the intersec-
tion of the vision cone with the nodes of the 3D network. Since we do
not know the depth of the participants’ focus we infer an approxima-
tion from the centre of gravity of these nodes. Thus, at each instant,
we have both participants’ spatial focus-points FP1,FP2 ∈ R3, Fig. 2
(b). Then dist(FP1,FP2) = |FP1−FP2| ∈R+0 gives an estimate of the
distance between these.

A minimum threshold of the focus-distance fd(platform) was used
to determine whether the participants were in focus on the same por-
tion of the 3D network or not. We take this from the diagonal dis-
tance of a single panel of the CAVE2’s tiled display wall such that
fd(CAVE) = 116cm. This was scaled proportionally to the model size
in the HMD condition, fd(HMD) = 11.6cm. The measured shared
focus value sf ∈ [0,1] gives the proportion of time the participants fo-
cused together over the total trial time (T):

sf =
1
T

T

∑
t=0

x
{

x = 1 if dist(FP1(t),FP2(t))≤ fd(platform)
x = 0 otherwise

5 RESULTS
In this section we present a set of quantitative results of groups of par-
ticipants’ accuracy, speed of task performance (completion time), pro-
portion of oral communication and amount of shared focus regarding
the two collaborative tasks in the CAVE2 and in the HMD set-up.

5.1 Functionality (RQ1)
In the following, we analyse the impact of the virtual environment on
functionality in terms of time and accuracy.
Accuracy (A) The mean rate of correct answers for the path task was
85% for the CAVE2 and 78% in the HMD. The distribution of correct
answers in both CAVE2 and HMD conditions were negatively skewed
(participants provided a high number of correct answers), thus as-
sumption of normality could not be met. We used the non-parametric
Mann-Whitney U test to compare the two path accuracy means. The
result of the test showed that there were no significant differences be-
tween the CAVE2 and the HMD average scores. The average amount
of correct answers for the triangle task was 69% for CAVE2 and 72%
for HMD, again with a negatively skewed histogram (participants also
provided a high amount of correct answers). The result of the Mann-
Whitney U test showed no significant differences between these two
means.
Completion Time (CT) The completion times for correct answers
from both groups were not normally distributed. The distribution of
completion time were slightly positively skewed, so we used a square-
root transformation (the Shapiro-Wilk test indicated that the trans-
formed distribution was normal, W = 0.99313, p = 0.5096).

The average completion time (Fig. 3(a)) for correct answers for the
path task was 107 seconds for the CAVE2, and 69 seconds for the
HMD. The result of the independent t-test showed that the average
completion time between the CAVE2 group and the HMD group were
significantly different (t(93) =−4.56, DF = 94 , p < .001).

The average completion time (Fig. 3(a)) for correct answers for the
triangle task was 102 seconds for the CAVE2, and 71 seconds for the
HMD. The result of the t-test showed that the average completion time
between the CAVE2 group and the HMD were significantly different
(t(89) =−5.85, DF = 94, p < .001).
Conclusion On both platforms groups of participants achieved a
high accuracy, and no significant differences were found between the
CAVE2 and the HMD condition (H1 was not confirmed). Rather, we
found a significant difference in completion time between the two plat-
forms for equivalent accuracy. For both tasks, participants were faster
to provide correct answers in the HMD than in the CAVE2 condition.
Participants were overall 40% faster to find correct answers for the
path task, and 30% faster for the triangles task (H2 was confirmed).

5.2 Collaboration (RQ2)
Here we discuss the results of the evaluation of collaboration in terms
of collaboration strategies and shared focus, proportion of oral com-
munication, tasks allocation and balance of physical movements be-
tween team members, in the two platforms.
Collaboration strategies (CS) Participants were asked to describe
their strategies in the survey. These were coded by two of the ex-
perimenters. Where there was inter-coder variability or uncertainty
categorisation was also informed by the observing experimenter. Fi-
nal strategy categorisation is summarised in Table 3.

Three main “path strategies” (PS) emerged to solve the shortest path
task in both virtual environments:

• PS1, Shared Focus: following paths, focused together on the same
edges and nodes.

• PS2, Divide and Conquer: participants agree on a mid-point node
to split the path and work independently to find the shortest sub-path
before combining their answer.

• PS3, Duplication: the participants worked independently before
comparing their answers.

We found that the emerged triangle count strategies (TS) followed
the same logic:



(a) Overall Time performance (b) Completion time by strategy (c) Vertical head movements in the CAVE2

Fig. 3: (a) Average completion time in seconds for the path and triangles tasks, and (b) per strategies. Average vertical head movements in
meters in the CAVE2 for the path and the triangle tasks (c) (HT in grey and non-HT in dark yellow).

• TS1, Shared Focus: explore the network together (with shared
focus), left-to-right or right-to-left, counting triangles at the same
time.

• TS2, Divide and Conquer: split the network into two parts, inde-
pendently count left and right triangles, and sum the result.

• TS3, Duplication: scan the network from one side to the other,
count the triangles independently, and compare and discuss the re-
sults at the end.

Strategy CAVE2 HMD
PS1 37.5%(3 groups) 11% (1 group)
PS2 25%(2 groups) 44.5%(4 groups)
PS3 37.5%(3 groups) 44.5%(4 groups)

TS1 75%(6 groups) 67%(6 groups)
TS2 0% 22%(2 groups)
TS3 25%(2 groups) 11% (1 group)

Table 3: Percentage and number of groups who employed the different
strategies

For path finding, the results suggest that the more independent
strategies (PS2 and PS3) were more popular in the HMD condition
while in the CAVE2 condition participants reported a balanced choice
of strategy. For the triangle task, participants in both environments
chose mostly a shared-focus strategy. However, these results have to
be treated with caution due to the small sample size.
Strategies, shared focus and completion time (CS,SF,CT) No dif-
ferences were found between the two VR platforms in terms of the
measured shared focus. We found that the mean shared focus for the
shortest-path task was 53% in the CAVE2 and 55% in the HMD con-
dition. The results for the triangle task were also close in both VR
platforms (63% in the CAVE2 condition, 61% in the HMD condition).

In the following we report the results of a deeper analysis which
aims to understand how different shared-focus strategies impacted
users’ performance in the two VR platforms. Our expectation was
that the reported shared focus strategies (PS1 and TS1, which we now
refer to as shared focus strategies) correspond to a higher degree of
measured shared focus, i.e. participants were more synchronously fo-
cused on the same nodes and edges to complete the tasks. Conversely,
we expected that PS2,3 and TS2,3 (which we collectively refer to as
defocused strategies) correspond to less measured shared focus.

We plotted these grouped strategies across the two platforms against
the proportion of measured shared focus (Fig. 4). The analysis of
the visualisation confirmed the correlation between the proportion of
shared focus and strategies. We also produced visualisations of par-
ticipants’ positions and focus points to assess reported strategies and
measured shared focus. The visualisations in Fig. 5 illustrate a TS2
strategy in (a) HMD, where green and magenta calculated focus points
are separated (low shared focus); and a TS1 strategy (b) in the CAVE2

Fig. 4: Shared focus distribution per strategies.

where the calculated focus points and head movements paths of the HT
and the non-HT participants are synchronous and close to each other
(higher shared focus).

We then analysed the impact of these strategies on user perfor-
mance. No evidence was found that strategies influenced the accu-
racy. We used the same square-root correction as in §5.1 for the com-
pletion time distribution. Two-way analysis of variance (ANOVA)
tests on task completion time do not show any significant interaction
between VR platforms and strategies. However, we did find a sig-
nificant main effect of the strategies on the completion time for the
path task (F(1,92) = 11, p = 0.001). Also, shared focus strategies
(PS1) elicited faster completion times than the defocused strategies
(M = 111s for PS1, M = 83s for PS2,3) (Fig. 3(b)). No significant
effect of strategy was found for the triangle task, probably because of
the smaller reported sample size (TS2,3). Still, Fig. 3(b) suggests a
trend of faster completion time with the two strategies in the triangle
task with the HMD.
Self-perception of collaboration (SPC) In order to analyse collabo-
ration, our questionnaire contained questions on perception of leader-
ship, workload and task division, along with ease of oral communica-
tion. Self-perception of leadership was measured with the open-ended
question “Was there one person who led the team? If so, who, and how
was this determined?”. We were expecting to observe biased results in
the CAVE2 with the HT participants. However, participants reported
ambiguous answers to this question so no conclusion could be made
on the influence of head-tracking on leadership. However, one non-HT
participant reported that her partner was leading because of the control
of the viewpoint.



Fig. 5: Two triangle count trials in the HMDs (a) (green and magenta)
and (b) in the CAVE2 (red (HT) and blue (non-HT)), showing horizon-
tal (1) and vertical (2) head movements. The horseshoe layout of the
3D network emerges from the position of the calculated focus points,
represented by circles. Colour saturation and line thickness increases
with time (1). Spatial scale of (a) is 10% of (b).

The questionnaire results scoring analysis of workload, contribution
to the tasks and organisational questions did not show a significant dif-
ference between the two environments. Yet, we found a trend for the
mean answers to the question “There was conflict when determining
the sub tasks”. In the CAVE2 the average score was 1.2, and 1.9 in the
HMDs. The Mann-Whitney U test results showed a trend of differ-
ence between the two average scores (W = 99.5, DF = 32, p = .06).
Although not statistically significant, this indicates that dividing work
could have been easier in the CAVE2, but the reported scores are low
in both environments.

To measure ease of communication, we posed the question “Discus-
sions using this virtual reality set-up tend to be more impersonal than
face to face discussions”. Participants reported that the discussion with
the HMD was more impersonal than in the CAVE2 environment. In
the latter, the average score was 2.2 compared to 3 for the HMD (16%
difference). Answers to this question were not normally distributed.
The Mann-Whitney U test indicated a significant difference between
these two average scores (W = 80.5, DF = 32, p = 0.023).
Analysis of oral communication (AOC) The participants conversa-
tions were recorded during the experiment. In order to evaluate the
influence of the VR platform on oral communication, we analysed the
proportion of oral communication (POC), i.e. the proportion of speak-
ing time between the two team members during the total completion
time per task. Audacity was used to blank the audio between the trials,
filter out the noise and isolate the two participants’ voices. No signif-
icant differences in POC between the two environments were found
(53% of POC for the path task in the CAVE2 and 54% in the HMDs;
60% in the CAVE2 for the triangles count task and 55% in the HMDs).
Balance of physical movements (BPM) The visualisation of the
recorded trial positions and rotations over time suggested that HT par-
ticipants in the CAVE2 were more likely to do vertical head move-
ments than non-HT participants (see a snippet in Fig. 5, right). The
histogram graph of the accumulated head vertical movements over
time indicated that the distribution of the movements was not normal,
hence we used the independent Mann-Whitney U test to analyse the
average movements between participants. In the CAVE2, we found
that HT participants moved their head vertically significantly more
than the non-HT participants did, to find the shortest path (Fig. 3(c),
HT in grey and non-HT in yellow). The average accumulated ver-
tical head movements over time for the shortest path task was 3.5m
for the HT participants and 1m for non-HT participants (W = 1852,
DF = 94, p < .001). HT participants vertically moved their heads 3.5
times more than the non-HT participants to solve this task. We also

found that the HT participants’ accumulated vertical head movements
were significantly higher than the non-HT participants’ ones for the
triangle task. On average, HT participants vertically moved their head
2.7 times more than non-HT participants, accumulating a total of 8.8m
vs. 3.2m for non-HT (W = 2046, p < .001, DF = 96). In the HMD
condition, no differences were found between the accumulated aver-
age vertical head movements over time of both participants for both
tasks. The average head vertical movements was 0.7m for the path
task, and 1.8m for the triangle task.
Conclusion HMD participants reported more defocused strategies, but
the analysis of shared focus did not reveal significant differences be-
tween the two environments for either task (H3 and 4.1 were not con-
firmed). We found that strategies had no impact on accuracy, but the
results indicated they influenced the path task in that focused strate-
gies were significantly slower than defocused strategies. The results
showed that within each strategy group, HMD participants were sig-
nificantly faster than the CAVE2 participants. No differences were
found in terms of the proportion of oral communication in the two
platforms thus H4.2 was not confirmed. Likert responses indicated
that participants’ allocation to subtask was less clear in the HMD than
in the CAVE-style environment (supporting H4.3). The questionnaire
did not reveal any difference in terms of leadership in the CAVE-style
environment (H4.4 was not confirmed). Finally, the analysis of phys-
ical movements showed an asymmetry between the participants of
the CAVE2 condition; HT participants moved their head significantly
more than the non-HT participants (H4.5 was confirmed).

5.3 User experience (RQ3)
We evaluated user experience between the two VR platforms and be-
tween the HT and the non-HT participants in the CAVE2, using a
Likert-scale (Strongly agree(1) - Strongly disagree(5)) questionnaire
and open-discussion questions.

We measured participants’ self-perception of capability of doing
the tasks, comfort of use, satisfaction and sickness induced by the dis-
play, for the two VR platforms. Participants in the CAVE2 reported
slightly higher usability scores for the usability criteria in Table 4. No
significant differences were found in terms of perceived capability of
completing the two tasks in the two environments. The investigation of
perception of usability of the CAVE2 between the HT and the non-HT
participants showed slightly more satisfaction for the non-HT partic-
ipants and slightly more comfort for the HT participants. However,
no statistical differences were found between the two participants an-
swers (Table 5).

Despite the lack of significant differences in these results, open dis-
cussions collected from the post-experiment questionnaire allowed us
to get more informal feedback on the usability of the two VR platforms
and between HT and non-HT participants.

One CAVE2 participant reported that the size of the display helped
them view a large quantity of data at once. Two participants reported
that they felt comfortable discussing the tasks; especially given they
share the same view of the data (3 participants), which supported the
decision taking process. However, one participant reported that step-
ping in front of each other while viewing the network can be disrup-
tive and generate frustration (display obstruction), and that the lack of
head-tracking for the non-head tracked collaborator made it difficult
to view complex parts of the network (4 participants). In addition, one
participant reported that the uncontrolled movement of the 3D visual-
isation is disturbing for the non-HT user. One HT participant reported
that physical interactions for this visualisation task were awkward.

Six HMD participants reported that being able to track the view-
point (the wire-frame pyramid) and finger position (the mini wand)
from their partners was a crucial and really useful cue to follow each
other’s gaze. One participant also reported that being able to share the
same position and view within the 3D network really helped commu-
nication while doing the tasks. Three participants reported that they
were focused on the visualisation and less on the environment, which
allowed them to concentrate more on the tasks (and one participant
reported that not being able to literally look at his partner helped in
focusing on the tasks).



CAVE2 HMD
Criterion/Question M σ M σ
Capability
I felt capable of doing the path task

4.5 0.8 5 0.6

Capability
I felt capable of counting the triangles

5 0.5 5 0.7

Satisfaction
I enjoyed myself

5 0.8 4 0.7

Comfort
I felt comfortable using the system

4 0.6 4 0.9

Sickness
I felt sick during the study

1 1.0 2 1.0

Table 4: Self-perception of usability of VR platforms, median M and
standard deviation σ .

HT Non-HT
Criterion/Question M σ M σ
Capability
I felt capable of doing the path task

4.5 0.7 4.5 0.9

Capability
I felt capable of counting the triangles

4.5 0.5 5 0.5

Satisfaction
I enjoyed myself

4.5 1.0 5 0.4

Comfort
I felt comfortable using the system

4.5 0.7 4 0.5

Table 5: Self-perception of usability for HT and non-HT participants

Conclusion No statistically significant differences were found in
terms of our quantitative measures of usability between the two VR
platforms and between the HT and non-HT participants in the CAVE2
(H5 and H6 were not confirmed). The overall reported strength of the
CAVE2 for collaboration was the sharing of the display and the ease of
communication. The lack of correct head-tracking viewpoint was the
most reported downside of the CAVE2. The main reported downside
of the HMD set-up was the Leap Motion tracker; participants felt frus-
trated when the sensor lost track of their finger position. Only three
HMD participants felt slightly disoriented during the experiment.

6 DISCUSSION
The analysis of functionality (RQ1) indicated high accuracy in both
environments (§5.1 – A). Participants achieved faster completion times
in both path and triangle tasks in the HMD than in the CAVE2 (§5.1
– CT). The lack of head-tracking for one of the two participants in the
CAVE2, occasional display obstruction and the size of the room are
possible factors for longer task execution times.

Participants reported more independent strategies in HMD, but the
shared-focus analysis showed no differences between the two VR plat-
forms (§5.2 – CS). However, a deeper analysis of their strategies
grouped by shared focus showed that strategies leveraging shared fo-
cus elicited faster completion times in the path task, independent of
the VR platforms (§5.2 – CS,SF,CT). Testing full independence be-
tween strategies and platform would require an equivalence test with a
significantly higher sample size, and is beyond the scope of this paper.

The investigation of collaboration only showed minor differences in
perceived communication (§5.3). HMD participants reported that they
found the platform impersonal for communication more frequently
than CAVE2 participants. Our initial thoughts were that the minimal
HMD set-up would prevent participants from communicating orally
and alter perception of presence, but the study of the POC did not
show any differences between the two VR platforms (§5.2 – AOC).
In the HMD condition, participants responded positively to “I felt that
my partner was really there with me inside the virtual visualisation”
with an average score of 4 over 5 (MD = 4, SD = .8).

The main collaboration difference found was the asymmetry of
physical movements in the CAVE2 (§5.2, (BPM)). HT participants
were more physically involved in analysing the graphics as they moved
vertically more than non-HT participants during the trials. The HMD

did not suffer from this asymmetry and the results showed that the
head movement quantities were balanced between the team members.

Overall then, the self-reported measures did not show significant
differences in user experience between the two VR platforms. Some
participants reported issues related to the finger-tracking sensor (Leap
Motion) in the HMD environment. New products shipping in 2016
such as the HTC VIVE and Oculus Touch offer hand-controllers with
very accurate tracking that may ameliorate this issue. In the CAVE2,
some participants were bothered by the asymmetry of the correct per-
spective being rendered for only one head-tracked participant.

7 LIMITATIONS

The results of this study are limited to sparse small and medium sized
networks (∼80 nodes). We used a force-directed layout that wraps
around the participants’ field of view and with limited depth (§4.2),
though enough to force the participants to move their heads to access
hidden nodes or crossed edges. This was chosen as a reasonable de-
fault use of space for the particular VEs used, but clearly there are
many other layout possibilities which may profoundly affect readabil-
ity and also participants’ task solution strategy.

Only the most commonly used set-up for multi-participant visual-
isation in the CAVE2 environment was tested. That is, only one par-
ticipant had saw the correct view-point for their head position. There
are other possibilities, such as allowing participants to interactively
switch the viewpoint or multiplexed viewpoints (as described above).
However, evaluation of these possibilities is left as future work.

Participants were randomly assigned to the virtual reality platforms,
but due to the small sample size, imbalance in demographic (in partic-
ular graph visualisation expertise, skill and gender) may have skewed
the results (§4.6). The two platforms were located on different cam-
puses and we could not easily re-assign participants to the platforms.
Finally, collaborative strategies were interpreted from coded question-
naire responses, precise head and view-frustum positions of partici-
pants over time, and analysis of amount of speech over time. Other
methods are of course possible, and may reveal more or different in-
sights, e.g. [37] used detailed talk aloud and video analysis.

8 CONCLUSION

In this paper we have presented the first formal user study into collabo-
rative analysis of abstract data in an immersive VR platform. In partic-
ular we compared two collaborative network connectivity tasks using
3D network visualisations on two popular but quite different VR plat-
forms: a CAVE-style environment and a modern HMD. Our aim was
to investigate whether the modern HMDs like the Oculus Rift meant
that collaborative immersive visualisation of abstract data no longer
required the use of expensive facilities like CAVEs.

Investigation of collaborative data visualisation is inherently com-
plex. Our study required developing an experimental platform that al-
lowed a fair comparison between the two environments without com-
promising the experience on either. We also needed to develop a mul-
tifaceted evaluation model. This comprised objective and subjective
measures such as accuracy, times, movements, oral communication,
tasks allocation and strategies, in order to highlight differences be-
tween the two VR platforms. The main results found were that partic-
ipants were:

• highly accurate in both environments, but
• substantially faster in the HMD, independent of the strategy em-

ployed to solve the tasks.

Somewhat to our surprise we found no major differences in terms of
oral communication and shared focus between the two platforms. In
addition, we found that physical engagement between the team mem-
bers in the HMDs were balanced, which makes the set-up more equi-
table. These results suggest that modern HMDs, such as the Oculus
Rift, provide a comparable experience for collaborative abstract data
analysis to more expensive purpose-built CAVE-style facilities, and
may even reduce the time required to complete the tasks.



REFERENCES

[1] M. Agrawala, A. C. Beers, I. McDowall, B. Fröhlich, M. Bolas, and
P. Hanrahan. The two-user responsive workbench: support for collab-
oration through individual views of a shared space. In Proceedings
of the 24th annual conference on Computer graphics and interactive
techniques, pages 327–332. ACM Press/Addison-Wesley Publishing Co.,
1997.

[2] B. Alper, T. Hollerer, J. Kuchera-Morin, and A. Forbes. Stereoscopic
highlighting: 2d graph visualization on stereo displays. IEEE Trans. Vi-
sual. Comput. Graphics, 17(12):2325–2333, dec 2011.

[3] R. Ball and C. North. Effects of tiled high-resolution display on basic vi-
sualization and navigation tasks. In CHI’05 extended abstracts on Human
factors in computing systems, pages 1196–1199. ACM, 2005.

[4] R. Bennett, D. J. Zielinski, and R. Kopper. Comparison of Interactive
Environments for the Archaeological Exploration of 3D Landscape Data.
In IEEE VIS International Workshop on 3DVis, 2014.

[5] C. Cruz-Neira, D. J. Sandin, T. A. DeFanti, R. V. Kenyon, and J. C. Hart.
The cave: Audio visual experience automatic virtual environment. Com-
mun. ACM, 35(6):64–72, June 1992.

[6] C. Donalek, S. Djorgovski, A. Cioc, A. Wang, J. Zhang, E. Lawler,
S. Yeh, A. Mahabal, M. Graham, A. Drake, S. Davidoff, J. Norris, and
G. Longo. Immersive and collaborative data visualization using virtual
reality platforms. In 2014 IEEE International Conference on Big Data
(Big Data), pages 609–614, 2014.

[7] S. Dredge. Facebook closes its $2bn Oculus Rift acquisition. What next?
https://goo.gl/I2i0pA, 2014. [Online; accessed 23-June-2016].

[8] T. Dwyer, Y. Koren, and K. Marriott. Ipsep-cola: An incremental proce-
dure for separation constraint layout of graphs. IEEE Trans. Vis. Comput.
Graph., 12(5):821–828, 2006.

[9] A. Febretti, A. Nishimoto, T. Thigpen, J. Talandis, L. Long, J. D. Pirtle,
T. Peterka, A. Verlo, M. Brown, D. Plepys, and others. CAVE2: a hybrid
reality environment for immersive simulation and information analysis.
In Proceedings IS&T / SPIE Electronic Imaging, volume 8649, pages
864903.1–12. SPIE, 2013.

[10] C. Greenhalgh and S. Benford. Massive: a distributed virtual reality sys-
tem incorporating spatial trading. In Distributed Computing Systems,
1995., Proceedings of the 15th International Conference on, pages 27–
34, May 1995.

[11] N. Greffard, F. Picarougne, and P. Kuntz. Beyond the classical mono-
scopic 3d in graph analytics: An experimental study of the impact of
stereoscopy. In 3DVis (3DVis), 2014 IEEE VIS International Workshop
on, pages 19–24, Nov 2014.

[12] C. N. Gunawardena and F. J. Zittle. Social presence as a predictor of satis-
faction within a computer-mediated conferencing environment. American
Journal of Distance Education, 11(3):8–26, 1997.

[13] J. Heer and M. Agrawala. Design Considerations for Collaborative Visual
Analytics. Information Visualization, 7(1):49–62, 2008.

[14] J. Heer, F. Ham, S. Carpendale, C. Weaver, and P. Isenberg. Informa-
tion visualization. chapter Creation and Collaboration: Engaging New
Audiences for Information Visualization, pages 92–133. Springer-Verlag,
Berlin, Heidelberg, 2008.

[15] C. Helbig, H.-S. Bauer, K. Rink, V. Wulfmeyer, M. Frank, and O. Kolditz.
Concept and workflow for 3D visualization of atmospheric data in a vir-
tual reality environment for analytical approaches. Environmental Earth
Sciences, 72(10):3767–3780, 2014.

[16] I. Heldal, M. Spante, and M. Connell. Are two heads better than one?:
object-focused work in physical and in virtual environments. In Proceed-
ings of the ACM Symposium on Virtual Reality Software and Technology,
pages 287–296. ACM, 2006.

[17] T.-J. Hsieh, Y.-L. Chang, and B. Huang. Visual Analytics of Terrestrial
Lidar Data for Cliff Erosion Assessment on Large Displays. In Proceed-
ings SPIE Satellite Data Compression, Communications, and Processing
VII, volume 8157, pages 81570D.1–17. SPIE, 2011.

[18] A. Irlitti, S. Von Itzstein, L. Alem, and B. Thomas. Tangible interaction
techniques to support asynchronous collaboration. In 2013 IEEE Interna-
tional Symposium on Mixed and Augmented Reality (ISMAR), pages 1–6,
2013.

[19] P. Isenberg, N. Elmqvist, J. Scholtz, D. Cernea, K.-L. Ma, and H. Hagen.
Collaborative visualization: Definition, challenges, and research agenda.
Information Visualization, 10(4):310–326, 2011.

[20] A. Kageyama, Y. Tamura, and T. Sato. Visualization of Vector Field by
Virtual Reality. Progress of Theoretical Physics Supplement, 138:665–

673, 2000.
[21] G. Kramida. Resolving the vergence-accommodation conflict in head-

mounted displays. Transactions on Visualization and Computer Graph-
ics, 22(7):1912–1931, 2016.

[22] G. Kurillo and M. Forte. Telearch – Integrated visual simulation environ-
ment for collaborative virtual archaeology. Mediterranean Archaeology
and Archaeometry, 12(1):11–20, 2012.

[23] O.-H. Kwon, C. Muelder, K. Lee, and K.-L. Ma. A study of layout, ren-
dering, and interaction methods for immersive graph visualization. IEEE
Trans. Visual. Comput. Graphics, pages 1–1, 2016.

[24] B. Lee, C. Plaisant, C. S. Parr, J.-D. Fekete, and N. Henry. Task taxon-
omy for graph visualization. In Proceedings of the 2006 AVI Workshop
on BEyond Time and Errors: Novel Evaluation Methods for Information
Visualization, BELIV ’06, pages 1–5, New York, NY, USA, 2006. ACM.

[25] K. M. Lee. Presence, explicated. Communication theory, 14(1):27–50,
2004.

[26] L. Leong. Viva la Vive: HTC is investing in VR in a big way. http:
//goo.gl/Z1aFWZ, 2016. [Online; accessed 23-June-2016].

[27] N. Li, A. S. Nittala, E. Sharlin, and M. Costa Sousa. Shvil: Collabo-
rative augmented reality land navigation. In Proceedings Conference on
Human Factors in Computing Systems (CHI 2014), pages 1291–1296.
ACM, 2014.

[28] N. Mahyar and M. Tory. Supporting communication and coordination
in collaborative sensemaking. IEEE Transactions on Visualization and
Computer Graphics, 20(12):1633–1642, 2014.

[29] P. Milgram and F. Kishino. A taxonomy of mixed reality visual dis-
plays. IEICE Transactions on Information and Systems, 77(12):1321–
1329, 1994.

[30] M. Narayan, L. Waugh, X. Zhang, P. Bafna, and D. Bowman. Quantify-
ing the Benefits of Immersion for Collaboration in Virtual Environments.
In Proceedings of the ACM Symposium on Virtual Reality Software and
Technology, pages 78–81. ACM, 2005.

[31] C. M. Oman. Motion sickness: a synthesis and evaluation of the sen-
sory conflict theory. Canadian Journal of Physiology and Pharmacology,
68(2):294–303, 1990. PMID: 2178753.

[32] S. A. Paul and M. C. Reddy. Understanding together: Sensemaking in
collaborative information seeking. In Proceedings of the 2010 ACM Con-
ference on Computer Supported Cooperative Work, CSCW ’10, pages
321–330, New York, NY, USA, 2010. ACM.

[33] D. Raja, D. A. Bowman, J. Lucas, and C. North. Exploring the benefits
of immersion in abstract information visualization. In In proceedings of
Immersive Projection Technology Workshop, 2004.

[34] L. Shupp, C. Andrews, M. Dickey-Kurdziolek, B. Yost, and C. North.
Shaping the display of the future: The effects of display size and cur-
vature on user performance and insights. Human–Computer Interaction,
24(1-2):230–272, 2009.

[35] N. G. Smith, K. Knabb, C. DeFanti, P. Weber, J. Schulze, A. Prudhomme,
F. Kuester, T. E. Levy, and T. A. DeFanti. ArtifactVis2: Managing real-
time archaeological data in immersive 3D environments. In Proceed-
ings Digital Heritage International Congress, volume 1, pages 363–370.
IEEE, 2013.

[36] Z. Szalavári, D. Schmalstieg, A. Fuhrmann, and M. Gervautz. Studier-
stube: An environment for collaboration in augmented reality. Virtual
Reality, 3(1):37–48, 1998.

[37] A. Tang, M. Tory, B. Po, P. Neumann, and S. Carpendale. Collaborative
coupling over tabletop displays. In Proceedings of the SIGCHI Confer-
ence on Human Factors in Computing Systems, CHI ’06, pages 1181–
1190, New York, NY, USA, 2006. ACM.

[38] C. Ware and P. Mitchell. Visualizing graphs in three dimensions. ACM
Trans. Appl. Percept., 5(1):2:1–2:15, Jan. 2008.

[39] H. Yang and G. M. Olson. Exploring Collaborative Navigation:: The
Effect of Perspectives on Group Performance. In Proceedings of the 4th
International Conference on Collaborative Virtual Environments, pages
135–142. ACM, 2002.

[40] P. YANG. The Untold Story of Magic Leap, the Worlds
Most Secretive Startup. http://www.wired.com/2016/04/
magic-leap-vr/, 2016. [Online; accessed 23-June-2016].

[41] S. Zhang, C. Demiralp, D. Keefe, M. DaSilva, D. Laidlaw, B. Greenberg,
P. Basser, C. Pierpaoli, E. Chiocca, and T. Deisboeck. An immersive vir-
tual environment for DT-MRI volume visualization applications: a case
study. In Proceedings Visualization 2001, pages 437–440. IEEE, 2001.

https://goo.gl/I2i0pA
http://goo.gl/Z1aFWZ
http://goo.gl/Z1aFWZ
http://www.wired.com/2016/04/magic-leap-vr/
http://www.wired.com/2016/04/magic-leap-vr/

	Introduction
	Related work
	Characteristics of the Virtual Platforms
	User Study
	Hypotheses
	Experimental design
	A minimal set-up for collaboration with two networked Oculus Rift DK2
	Stimuli and tasks
	Procedure
	Participants
	Measures

	Results
	Functionality (RQ1)
	Collaboration (RQ2)
	User experience (RQ3)

	Discussion
	Limitations
	Conclusion

