



































































Retrofitting Realities: Affordances and Limitations in Porting an Interactive Geospatial Visualization from Augmented to Virtual Reality


 

Retrofitting Realities: Affordances and Limitations in  
Porting an Interactive Geospatial Visualization from Augmented to Virtual Reality 

 
Matt Richardson, Derek Jacoby, Yvonne Coady 

University of Victoria 
Computer Science 

 
 
Abstract​—As Augmented Reality (AR) and Virtual Reality       
(VR) applications become more mainstream, developers      
now have a number of design decisions that must be          
carefully considered before choosing a device for an        
interactive visualization with big data. Unfortunately,      
understanding the true affordances and limitations of       
each device, and how these affect the resultant potential to          
support visual analytics, is still more of a black art than a            
science. In this paper, we highlight key design decisions         
and technical challenges in the context of a case study to           
port an interactive geospatial visualization from an AR        
device, the Microsoft Hololens, to a mobile VR device, the          
Google Daydream. Our results show that careful       
leveraging of backend cloud services can allow for        
interactive visualizations of big data to scale well across         
devices.  
 
Keywords—Augmented and Virtual Reality; interactive     
visualizations; geospatial datasets; data analytics;     
platform porting; cloud processing. 
 

 
Figure 1:​  Microsoft Hololens AR map view 

 
 

I.  INTRODUCTION 
The affordances of AR applications are very different        
than those of VR applications, as is the technical basis.          
For example, the Microsoft Hololens (Figure 1) enables        
visualizations to be painted on existing flat surfaces in         
the field of vision, and to use gestures and eye tracking           
to interact with those visualizations. This takes place in         
a fairly constrained field of view, however, and is         
generally not considered to be an immersive       
experience.  

   
 

Figure 2: ​VR Application 
 

In a VR application, particularly on a mobile        
platform like the Google Daydream (Figure 2), the        
input fidelity is much reduced—there is no eye tracking         
or gesture recognition—and because it is an immersive        
experience much more graphical context must be       
provided. Aside from through-the-viewer VR     
experiences, the mobile platform is also capable of AR         
on the phone screen, using the camera and global         
positioning system (GPS) to align to objects in the real          
world. This is more akin to the Pokemon GO style of           1
AR application.  

 
From a user interface perspective, there are many        

subtleties to creating a well-executed immersive VR       
experience. One of the foremost is response time. In         
order to avoid motion sickness, it has been suggested         
that head-motion-to-render times be under 25 msecs       
[1]. The processing power of the Google Daydream        
devices simply does not enable complex processing to        
occur on the device if this level of responsiveness is to           
be met in some application domains. For example,        
geographical information system (GIS) analytics.  

 

1 ​Pokemon GO! https://www.pokemongo.com/ 

978-1-5386-7266-2/18/$31.00 ©2018 IEEE 

1081



Although the Microsoft Hololens experience is      
generally a local processing approach, with the full        
power of a Windows PC, meeting this requirement is         
generally possible. Additionally, the response time      
requirements in AR applications may be different than        
in VR [2]. This is a reasonable supposition since in an           
AR application, the majority of the scene is viewed         
directly rather than rendered, so the motion sickness        
characteristics would be less affected by the rendered        
portion of the scene.  

 
A consequence of having less local processing       

capacity, and a greater system performance requirement       
in the VR setting, is that the greater computational         
capacity of a backend cloud-based system must be used         
to enable a performant Google Daydream GIS       
application. This system must include pre-processing      
of the data and since even the network transit time          
could easily be greater than 25 msec, intelligent        
pre-fetching schemes must also be implemented.  

 
Our work explores an initial investigation of the        

impact of pre-fetch and cloud intelligence in the context         
of a AR-to-VR port of an application for GIS analytics.          
Our core contributions are: 
 

1. A set of design decisions software engineers       
should consider in the domain of interactive       
visualizations involving big data on a spectrum       
of local resource rich and poor AR/VR       
devices. 

2. Implementation results in the form of      
microbenchmarks from an interactive GIS     
visualization. 

3. A prototype interactive geospatial    
visualization built on top of Microsoft Azure,       
extendable to other cloud platforms such as       
Amazon Web Services [3]. 

 
The remainder of the paper is organized as follows.         

In the related work section, we investigate the        
application of AR and VR in the GIS context, and          
identify some situations in which one may be preferable         
to another. In the experimental design section, the        
technical challenges of porting a Hololens AR       
application to a mobile device VR experience are        
highlighted. Our result section focuses on our       
experimental benchmarks on a variety of systems.       
Finally, future work outlines our goal to more fully         
explore progressive cloud architectures to facilitate this       
class of application.  

II.  RELATED WORK 

The use of three dimensional, interactive, GIS       
visualizations is well-established across a wide variety       
of domains and provides benefits in terms of user         
satisfaction and task success [4]. One example we        
have explored in previous work is the uses of         
immersive GIS in urban planning [5]. This domain        
requires familiarity with complex sets of geographical       
and construction information, along with     
neighbourhood planning and environmental concerns.     
More gamified uses of urban planning have also been         
explored, as well as other applications related to the         
environment and health geographics [6]. 

 
The ability of GIS applications to leverage the benefits         
of Mixed Reality (MR) environments, combining      
elements of AR and VR, to enhance embodied        
cognition [8, 9] has also lead us to study the persuasive           
power of immersion and engagement to cause       
behaviour changes in regards to improving our       
environment [7]. Part of persuasiveness is convincing       
the user that they belong to a group, and thus sharing           
group goals of preservation, both of the environment        
and culture. Work related to this includes       
device-mediated approaches to cultural computing [10],      
and many of the techniques are directly applicable to         
promoting environmental change. 

 
Situational awareness is a key aspect of many        

navigational tasks involving large geospatial datasets,      
including military situations. Although the immersion      
of a VR experience may be beneficial in understanding         
a large flow of information, in situations where the         
immediate environment poses dangers an AR approach       
becomes necessary and immersion is no longer a goal         
[11]. These GIS systems in MR are beneficial for         
battlefield command situations and for soldiers on the        
ground [12, 13] but also for individual wayfinding and         
hazard avoidance in non-battlefield conditions [14, 15]. 

 
Central to all of these scenarios is an understanding         

of how to display geographical data in different        
environments [16] and how to get those representations        
to the client. In the case of the Hololens, this can be            
largely a local processing experience [17] but on less         
capable devices a client-server model becomes essential       
[1, 18]. Currently, striking the right balance here is a          
painful process of trial and error.  

 
Since device capabilities differ in terms of many        

key resources, it is not always possible to rely on gaze           

 

1082



and gesture tracking in building user interfaces.       
Although the Hololens offers native Windows speech       
recognition, other devices do not necessarily have these        
capabilities. Speech recognition on it’s own is also not         
always sufficient to create a responsive query system.        
Some of the factors that go into such a system have           
been explored in an urban planning application in        
Singapore [19]. In the work presented here, our        
solution rests primarily on the speech and cognitive        
services provided by Microsoft Azure. 

III.  EXPERIMENTAL DESIGN 

The starting point for our experiment was an open         
source Hololens demo for visualizing terrain data       
developed by Richie Carmichael at the Esri Prototype        
Lab in 2016 . This application provided the viewing of         2
3D terrain maps with associated satellite imagery as a         
texture. It also facilitated the viewing of specific        
address information based upon where the gaze pointer        
was looking on the map. The primary map selection         
was based on system support from the       
Windows.KeywordRecognizer by saying “​Show    
{Location Name}​”. As long as the name was in a          
pre-set list of locations, it could generate the map         
dynamically by querying an ArcGIS web service for the         
height information, reverse geocoding (address data),      
and satellite imagery. Except for the actual map and         
geocode data, the entire application ran locally on the         
Hololens. 
 

Numerous design decisions and technical issues      
were encountered in attempting to adapt this starting        
point to a different platform. In order to systematically         
explore the affordances of particular hardware devices       
and consequent user interface approaches in different       
problem domains, we settled on the use of the Unity          
platform as a bridge across many of the devices. It is           3
important to note that some familiarity with native code         
was also generally still required.  

 
The remainder of this section starts by overviewing        

key elements of our experiment, including the       
Hololens, GoogleVR and the Daydream, the Unity       
Game Engine, and Microsoft Azure. We then focus on         
our methodology and data points collected.  

 
A. Microsoft Hololens 

The Microsoft Hololens is a revolutionary mixed       
reality device with a host of sensors, interactive tools,         
connectivity and adaptivity. It uses a pair of combiner         

2 https://github.com/Esri/hololens-terrain-viewer 
3 ​Unity https://unity3d.com/ 

lenses to display projected imagery in the users field of          
view as its primary output. Six speakers in two arrays          
around the users ears give binaural audio as well. To          
interact with it the user either makes pre-programmed        
hand gestures in the Hololens field of view, or utilizes a           
separate peripheral controller. It was released for       
developers in early 2016 by Microsoft and has the         
advantage of running a full Windows 10 environment,        
allowing it to be compatible with the vast majority of          
Universal Windows Platform applications.  

 
B.  GoogleVR and Daydream 

Google Inc. developed the Google Daydream      
platform and released it in late 2016 to consumers. It is           
considered a Virtual Reality device and usually requires        
two components; a Daydream-ready smartphone     
running Android Nougat 7.0 or later and a Daydream         
View or equivalent headset. The Daydream platform is        
built directly into the operating system and can be         
developed using multiple engines.  

 
GoogleVR is a project branch of Google Inc. that         

covers multiple devices and tools for developing and        
using virtual reality. The Daydream platform is the        
flagship of the project along with the original Google         
Cardboard and recently the addition of standalone VR        
devices. The tools for developing are offered for Unity,         
Unreal, Android, Android NDK, IOS, and browser       
based web applications.  

 
C.  Unity Game Engine 

Both the Daydream and Hololens can be developed        
for in the Unity Game Engine using SDKs available         
from both platform developers. The Unity Game       
Engine also has an editor-level player that permits        
quick testing. This combined with simple deployment       
tools for both devices made it our choice of editor. 

 
D.  Microsoft Azure 

In order to port the Windows speech recognition        
from the Esri Hololens demo to the Daydream, we used          
Microsoft Azure Speech. One potential advantage is       
the ability to use it’s other many cloud tools for the           
addition of other functionality. By using a cloud based         
service, it took platform dependence out of the equation         
for some elements of our implementation. 

 
E.  Methodology 

The original Esri Hololens demo had been       
developed in Unity and was available as a GitHub         
repository. It was cloned and re-opened in Unity for         
analysis. Key features were identified that either needed        
to be, (1) adapted to the Daydream or, (2) emulated on           
the Azure server. Specifically, design decisions      

 

1083



including data sourcing, visualization, interaction and      
testing are detailed. 

 
In terms of data sourcing, elevation data, reverse        

geocoding, and satellite imagery were dynamically      
loaded from an ArcGIS server when the user requested         
them. This functionality was identical the original       
Hololens application and functioned quite well. An       
additional data layer was added to the project in the          
form of gathering demographics such as population and        
the current temperature at that location. The       
demographic data was gathered using the “enrich” type        
query from ArcGIS. Current temperature information      
was gathered by using the NOAA National Weather        
Service API. 

 
For visualization, the original mixed reality      

application leveraged the user’s real-world environment      
as a backdrop for the visualized data. In virtual reality          
an environment had to be created consisting of a simple          
3D table and lamp, a plinth for the display of additional           
data, and a pleasant skybox of clouds. This gave a less           
stark and uncomfortable feeling to the user experience        
(see Figure 2). 

 
The terrain data and imagery gathered from       

ArcGIS arrives in the form of a point cloud, also called           
a terrain map. Although this can have a Unity game          
object created from it on the fly on a system with           
enough local resources, it is more conserving of        
processing resources to do this once and then store it as           
a terrain mesh, also called a Unity terrain object. The          
mesh was dynamically generated from the original       
ArcGIS data array after decompression and a satellite        
image was laid on top to provide a realistic         
representation. Although we were generating a terrain       
mesh from a terrain map, the technique could be used to           
visualize other elements of three-dimensional data,      
especially with the use of other data types. The         
generalizable concept here is to use native game object         
data types when possible, particularly in environments       
where local processing resources are limited. 

 
In terms of interaction, the Hololens uses gaze        

tracking, gesture recognition, speech recognition and a       
peripheral for interaction. While the Daydream does       
not use gaze tracking, it does use head tracking from an           
IMU (Inertial Measurement Unit) sensor suite in the        
smartphone. The GoogleVR SDK has prefabs for       
setting up a pointer at the center of the headset’s field           

of view. The user can use this pointer to select items           
and specific locations in the scene in the same way as           
the Hololens gaze tracker.  

 
Additionally, the Daydream platform has a      

peripheral controller with it’s own IMU and physical        
buttons. It can be set up in the scene to cast a pointer of              
its own using the SDK prefabs. In order to stay          
consistent with the original application, we chose to use         
the head tracked pointer to replace the Hololens gaze         
tracker and use the controller for it’s buttons only. 

 
Speech recognition was accomplished by calling an       

Azure based speech utility on Daydream and using        
local speech recognition on the Hololens and other        
devices. While there are several alternatives to       
processing speech on an Android device, on a less         
powerful processor local recognition was not an       
efficient use of resources that were otherwise needed to         
keep the frame rate within desired parameters.. 

 
Our experimental testing required deploying the      

application to the Daydream device, a Google PixelXL        
at various stages in the development for performance        
testing. To gather a performance benchmark a profiling        
timestamp was declared at the start of the main script,          
and and the start/end of each method. These were then          
output to the Android debug log (logcat) and viewed         
through the Android debugger. The current frame rate        
was also displayed in the headset to the user during          
testing to give a running indication of performance, and         
was logged. The data points we focused on were as          
follows: 

 
1. Voice interaction (Azure vs. local) 
2. Map generation with remote resource calls. 
3. Secondary data (population and temperature) 

 
IV.  RESULTS 

 
The results show how the devices compare in terms         

of microbenchmarks and load times. In Figure 3, the         
entirety of a typical 30 second run with the application          
was recorded. The X axis is unlabelled, because the         
times are mostly a function of the user interaction rather          
than system performance, for instance how long the        
user took in making a request. The critical times are the           
system measures from the completion of a request until         
the page is served, these times are shown as an average           
of all runs in Table I. 

 

  

 

1084



Figure 3.​  Load times and frame rates throughout a typical run on each device 

HoloLens 

 

HTC Vive 

 

 Oculus Rift 

 

Daydream 

 

 

TABLE I. Technologies and the load time implications of local/remote processing. These times represent an               
average of all testing runs on each device (10 runs each). Column 1 is a measure of the time in seconds between loading the                        
application and having a first fully loaded map. Column 2 is the time in seconds it takes to load a second map. Column 3 is the                          
length of time in seconds it takes for a speech query to complete. Column 4 is the length of time in seconds it takes to load a third                            
map. For the HoloLens, HTC Vive, and Oculus Rift, the speech query processing occurred locally, but for the Daydream the                    
speech processing was done with a Microsoft Azure remote cognitive service. 

 

Device First Map Second Map Query  Third Map 

HoloLens 0.99 0.76 0.01 0.74 

HTC Vive 0.18 0.17 0.06 0.19 

Oculus Rift 0.22 0.19 0.28 0.18 

Daydream 0.43 0.26 0.54 0.24 
 

 
 
  

 
 

 
 

 

1085



 
 

V.  ANALYSIS AND DISCUSSION 
 
Figure 3 is a depiction of the load times for each           

device, and shows the effect of framerate at each point          
in time. Recall from the introduction that for VR         
experiences, it is advised to keep lag under 25 msecs          
[1] so as to reduce the likelihood of motion sickness.          
In terms of frame rate, this corresponds to a minimum          
of 40 frames per second and the only place the frame           
rate drops below this is at the moment of a new map            
load. Benefits in comfort and feel of the application         
are seen well beyond these rates, though, and there is a           
difference in the platforms as to the minimum frame         
rate that is advised - the daydream and hololens         
suggest a minimum frame rate of 60Hz, the Vive         
suggests 90Hz, and the Oculus Rift can run at 120Hz.          
Aside from the ongoing lag, load time of individual         
maps is important as a measure of system        
responsiveness and is shown in table I. 

 
Table I describes some of the factors that are         

affected by a move of processing locally to        
cloud-based solutions. For instance, speech     
recognition is performed locally on the Hololens, but        
on the daydream the decision was made to offload this          
to a web service in order to conserve local processing          
resources. This table shows how these architectural       
decisions impact the user experience. 

 
Though the role of AR/VR within any platform        

designed to support visual analytics (VA) in general        
for big data has yet to be determined, we have shown           
that even low-end devices can utilize interactive       
visualizations to efficiently support analytical     
reasoning within a geospatial application. As AR/VR       
platforms continue to evolve, these advances will       
continue to provide new ways of visualizing and        
interacting with big data. Our results show that these         
environments can be effectively retrofitted for a range        
of devices, through careful leveraging of backend       
cloud-based services.  

 
VI.  FUTURE WORK AND CONCLUSION 

 
The application port described here is just an easy         

to grasp starting point. Many of the more complicated         
application areas described in the introduction and       
related work sections require much more complete       
applications. The Microsoft Azure platform contains      
many of the server capabilities needed to implement        
intelligent interactions with a GIS system. Our lab has         
been successful in acquiring the support of Microsoft        

under their AI for Earth platform and are at work on a            
project called the Coastal Climate Explorer (COAX)       
which is designed to motivate behaviour change in        
individuals with regard to human impact on climate        
change. 

 
Although visualization in AR and VR can be a         

user benefit, it is really the immersive power of these          
technologies that is most responsible for elucidating       
behaviour change [7]. The fundamental decisions      
involved in choosing a platform and adapting an        
application to it depend much on the specific use.         
Although mobile VR applications are more      
constrained in the capabilities of the system, they are         
more available, cheaper, and easier to set up so the          
reach of the application is greater than on more         
specialized pieces of hardware. The user interface       
concessions that must be made to these less capable         
platforms are generally a tradeoff between local and        
distributed processing. Intelligent client/server    
applications that can pre-process on the server based        
on predicted client actions can ease this transition. An         
understanding of the user ramifications of these       
decisions is essential to architecting the system to        
minimize the disruptions provided by network fetch       
times that exceed by many times the permissible        
delays imposed by a high refresh rate. 

 
 

VII. REFERENCES 
 

[1] Z. Lai, Y. C. Hu, Y. Cui, L. Sun, and N. Dai, “Furion:             
Engineering High-Quality Immersive Virtual Reality on      
Today’s Mobile Devices,” in Proceedings of the 23rd Annual         
International Conference on Mobile Computing and      
Networking, New York, NY, USA, 2017, pp. 409–421. 

[2] M. Walter, T. Wendisch, and K. Bengler, “In the Right Place           
at the Right Time? A View at Latency and Its Implications for            
Automotive Augmented Reality Head-Up Displays,” in      
Proceedings of the 20th Congress of the International        
Ergonomics Association (IEA 2018), 2018, pp. 353–358. 

[3] Wilms, Konstatin, “Building an Immersive VR Streaming       
Solution on AWS,” Amazon Web Services, 23-Feb-2018.       
[Online]. Available:  
https://aws.amazon.com/blogs/compute/building-an-immersive
-vr-streaming-solution-on-aws/. [Accessed: 18-Aug-2018]. 

[4] P. Millais, S. L. Jones, and R. Kelly, “Exploring Data in           
Virtual Reality: Comparisons with 2D Data Visualizations,” in        
Extended Abstracts of the 2018 CHI Conference on Human         
Factors in Computing Systems, New York, NY, USA, 2018, p.          
LBW007:1–LBW007:6. 

[5] T. Wei, Y. Coady, J. MacDonald, K. Booth, J. Salter, and C.            
Girling, “Dumb pipes for smart systems: How tomorrow’s        
applications can salvage yesterday’s plumbing,” in 2017 IEEE        
Pacific Rim Conference on Communications, Computers and       
Signal Processing (PACRIM), 2017, pp. 1–5. 

[6] M. N. Kamel Boulos, Z. Lu, P. Guerrero, C. Jennett, and A.            
Steed, “From urban planning and emergency training to        
Pokémon Go: applications of virtual reality GIS (VRGIS) and         

 

1086



augmented reality GIS (ARGIS) in personal, public and        
environmental health,” International Journal of Health      
Geographics, vol. 16, no. 1, p. 7, Feb. 2017. 

[7] D. Jacoby and Y. Coady, “Perspective Shifts in Mixed Reality:          
Persuasion through Collaborative Gaming,” Proceedings of the       
Personalization in Persuasive Technology Workshop, pp.      
84–90, 2017. 

[8] D. Gračanin, “Immersion Versus Embodiment: Embodied      
Cognition for Immersive Analytics in Mixed Reality       
Environments,” in Augmented Cognition: Intelligent     
Technologies, 2018, pp. 355–368. 

[9] J. Tham, A. H. Duin, L. Gee, N. Ernst, B. Abdelqader, and M.              
McGrath, “Understanding Virtual Reality: Presence,     
Embodiment, and Professional Practice,” IEEE Transactions      
on Professional Communication, vol. 61, no. 2, pp. 178–195,         
Jun. 2018. 

[10] M. K. Bekele, R. Pierdicca, E. Frontoni, E. S. Malinverni, and            
J. Gain, “A Survey of Augmented, Virtual, and Mixed Reality          
for Cultural Heritage,” J. Comput. Cult. Herit., vol. 11, no. 2,           
pp. 7:1–7:36, Mar. 2018. 

[11] R. P. Spicer, S. M. Russell, and E. S. Rosenberg, “The mixed            
reality of things: emerging challenges for human-information       
interaction,” in Next-Generation Analyst V, 2017, vol. 10207,        
p. 102070A. 

[12] M. Jenkins, A. Wollocko, A. Negri, and T. Ficthl, “Augmented          
Reality and Mixed Reality Prototypes for Enhanced Mission        
Command/Battle Management Command and Control     
(BMC2) Execution,” in Virtual, Augmented and Mixed       
Reality: Applications in Health, Cultural Heritage, and       
Industry, 2018, pp. 272–288. 

[13] X. You et al., “Survey on Urban Warfare Augmented Reality,”          
ISPRS International Journal of Geo-Information, vol. 7, no. 2,         
p. 46, Jan. 2018. 

[14] F. Wiehr, F. Daiber, F. Kosmalla, and A. Krüger, “ARTopos:          
Augmented Reality Terrain Map Visualization for      
Collaborative Route Planning,” in Proceedings of the 2017        
ACM International Joint Conference on Pervasive and       
Ubiquitous Computing and Proceedings of the 2017 ACM        
International Symposium on Wearable Computers, New York,       
NY, USA, 2017, pp. 1047–1050. 

[15] T. Butkiewicz, “Designing augmented reality marine      
navigation aids using virtual reality,” in OCEANS 2017 -         
Anchorage, 2017, pp. 1–9. 

[16] W. Wang et al., “Holo3DGIS: Leveraging Microsoft HoloLens         
in 3D Geographic Information,” ISPRS International Journal       
of Geo-Information, vol. 7, no. 2, p. 60, Feb. 2018. 

[17] Y. Yang, B. Jenny, T. Dwyer, K. Marriott, H. Chen, and M.             
Cordeil, “Maps and Globes in Virtual Reality,” Computer        
Graphics Forum, vol. 37, no. 3, pp. 427–438, Jun. 2018. 

[18] D. Zhou et al., “The Analysis of Task and Data Characteristic           
and the Collaborative Processing Method in Real-Time       
Visualization Pipeline of Urban 3DGIS,” ISPRS International       
Journal of Geo-Information, vol. 6, no. 3, p. 69, Mar. 2017. 

[19] T. Y. Shin, Y. Zihong, N. W. Siong, Z. Yangfan, and V.             
Phangt, “Towards a deep learning powered query engine for         
urban planning,” in 2017 International Conference on Asian        
Language Processing (IALP), 2017, pp. 99–102. 

 

 

1087


