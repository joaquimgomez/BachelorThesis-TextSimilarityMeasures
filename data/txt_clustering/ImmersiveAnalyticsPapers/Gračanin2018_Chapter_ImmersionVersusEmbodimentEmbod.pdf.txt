















































































Immersion Versus Embodiment: Embodied Cognition for Immersive Analytics in Mixed Reality Environments


Immersion Versus Embodiment:
Embodied Cognition for Immersive

Analytics in Mixed Reality Environments

Denis Gračanin(B)

Department of Computer Science, Virginia Tech,
Blacksburg, VA 24060, USA

gracanin@vt.edu

Abstract. Visualization techniques are used to analyze and understand
data within the context of the underlying conceptual and physical model.
We collect and generate data at an increasingly fast rate, but visual anal-
ysis capabilities are lagging. The challenges of visual data analysis and
exploration are associated with very large data sets, increased dimension-
ality, and the consideration of data semantics, including features, focus
and context. Typically, visual analysis is done using Coordinate Multi-
ple Views (CMV) tools that support linking and brushing (selection) of
data in multiple synchronized views. The recent advances in MR tech-
nologies provide a great opportunity to support deployment and use of
MR applications for visualization and visual analytics. Direct mapping
of CMV tools to an MR environment arguably creates more problems
than it solves. Embodied interactions and embodied user interfaces lead
towards invisible user interfaces and move the visualization and anal-
ysis from a computer screen to physical space and place. It is neces-
sary to explore various interaction and visualization modalities in MR
environments to identify best practices to leverage embodied cognition
and interactions. Such explorations can benefit from a framework and
an evaluation testbed for embodied interactive immersive analysis. The
framework provides services for data access, data visualization (views),
both traditional two-dimensional view and a three-dimensional equiva-
lent, views assembly, gestures, and interaction devices. An Internet of
Things based Smart Built Environment example is used to illustrate the
proposed approach.

Keywords: Mixed reality · Immersive analytics

1 Introduction

Visualization techniques help us analyze and understand data within the context
of the underlying conceptual and physical model. However, there is still a cog-
nitive gap that needs to be closed to facilitate better interactive visual analysis.
More sophisticated approaches use immersive virtual environments [34], game
c© Springer International Publishing AG, part of Springer Nature 2018
D. D. Schmorrow and C. M. Fidopiastis (Eds.): AC 2018, LNAI 10915, pp. 355–368, 2018.
https://doi.org/10.1007/978-3-319-91470-1_29

http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-91470-1_29&domain=pdf


356 D. Gračanin

engines [5], and augmented/mixed reality [11]. Those approaches focus mostly
on the visual aspects without taking into account the importance of the user’s
interactions with the physical world.

Typically, visual analysis is done using a coordinate multiple views (CMV)
tool that supports linking and brushing (selection) of data in a variety of view
types [35]. The synergy of multiple views, each showing the brushed data from
different perspective, facilitates insight into a complex data set.

Mixed reality (MR) [7] incorporates digital artifacts in the surrounding physi-
cal world creating an MR environment that implicitly leverages embodiment and
affordances of the physical world while providing the visualization capabilities of
the digital world. If those digital artifacts, or virtual objects, are data represen-
tatives, we can use their placements in the physical world to reduce the cognitive
gap and enhance the user’s ability to analyze the data. In other words, we can
bring the CMV paradigm into an MR world.

MR technologies allow for providing users with an environment that blends
the physical surroundings with virtual objects. The recent advances in MR tech-
nologies provide a great opportunity to support the deployment and use of MR
applications for visualization, simulation, training, and education, in single-user
and collaborative settings. Users can interact with virtual objects that can help
them be more engaged and acquire more information compared to the more
traditional approaches.

The most obvious approach is to directly map the CMV tool views to “float-
ing” digital screens in an MR environment and implement brushing using a
simple point and click interaction paradigm. While such an approach is rel-
atively easy to implement, it arguably creates more problems than it solves.
The problems include technical (MR device resolution, field of view, spatial
mapping, occlusion), usability (two-dimensional views in a three-dimensional
space) and cognitive (switching between the views, grasping the overall view
of data). Immersion in an MR environment does not necessarily benefits from
embodiment.

Therefore, the challenge is to explore various interaction and visualization
modalities in an immersive MR environment to identify what are the best prac-
tices to leverage embodied cognition and interactions to support immersive visual
analytics. In order to address and explore that challenge, we developed a frame-
work and its initial reference implementation as an evaluation testbed for embod-
ied interactive immersive analysis. The goal is to take advantage of affordances
and embodied cognition in an MR environment.

The framework provides services for data access, data visualization (views),
both traditional two-dimensional view and a three-dimensional equivalent, views
assembly, gestures, and interaction devices. The implementation is based on a
Microsoft HoloLens device combined with eye-tracking, tracking, and biometric
wrist band devices to provide for accurate monitoring of user actions, eye gaze
and physiological signals.

We use an Internet of Things (IoT) [36] based Smart Built Environment
(SBE) physical system to illustrate the framework and to evaluate the testbed.
The historical and current sensor data are stored in a data repository and



Immersion Versus Embodiment: Embodied Cognition 357

accessed based on the context and user interest. Visual analysis can be con-
ducted in the original SBE or in any other space. A pilot user study is under
way to test the functionality and services provided by the framework as well as
to refine and extend the provided services. The survey data, task performance,
and the physiological data will be used to evaluate the cognitive gap and its
effects based on the level of embodiment.

2 Related Work

One of the challenges of visual analytics is how to analyze a huge amount of het-
erogeneous data [37]. Interactive visual analytics is often limited by our inability
to fully grasp the data presented on a computer screen due to the cognitive over-
load. User interaction and ability to quickly search, filter, visualize, and analyze
data is essential. Supporting a high-level information processing requires that a
user can filter, visualize, and navigate data [8].

Interactive brushing or dynamic querying enables iterative on-the-fly for-
mulation and refinements of data analysis tasks based on the visual feedback
provided by multiple views [15]. CMVs are frequently used for interactive visual
analysis because they provide multiple simutaneous perspectives or view of the
data [35]. There are many available CMV tools tailored for different application
domains [29].

Our cognitive processes depend on how our body interacts with the physical
world (affordances) and how we off-load cognitive work onto our physical sur-
rounding (embodied cognition). Embodiment cognition [39] leverages the notion
of affordances, potential interactions with the environment, to support cogni-
tive processes. Embodied interactions [12] and embodied user interfaces [16] lead
towards invisible user interfaces moving the computation and analysis from com-
puter screen to a physical environment [13,38]. Embodied interactions demon-
strate the importance of the body’s interactions with the physical world.

Three-dimensional (3D) Virtual Reality (VR) environments allow users to
explore virtual worlds without actually “being there.” VRs have been used in a
variety of applications including education, training, architectural walkthroughs,
scientific visualization, art, and entertainment. Although VR applications can be
used for many different purposes, there are some fundamental interaction tasks
used in VR applications.

VR applications can be used to map or represent real-world sensors and
embedded devices to virtual devices and objects. VR-based user interface and
visualization are used to evaluate IoT computing applications and to interac-
tively test various IoT configurations [14,30,33].

Experience from 3D user interfaces and interactions with VR environments
can be a starting point for interaction with the real world and architectural
artifacts [21,26]. By providing an MR environment that uses the surrounding
architectural space as a context for visualization and is connected to controls,
sensors, and actuators in the architectural space, we can use human-computer
interaction based approaches and interaction techniques to support MR interac-
tive visualization.



358 D. Gračanin

MR environments are also well suited for tangible visual analysis because
the physical objects (props) can be used to interact with the physical world and
with the virtual objects, thus leveraging embodiment and context awareness
[19,24,25]. New MR technologies (e.g., Microsoft HoloLens device) can augment
the user experience and provide affordances for brushing by allowing the user to
define the context using semantic representatives (virtual/tangible and 2D/3D).
We can use the concept of tangible brushing for exploratory analysis in a MR
environment using tangible (semantic) representatives for data dimensions [20].
Manipulation of representatives selects a subset of data providing the users with
immediate feedback and a powerful support for data comprehension.

MR environments can be used in a single-user and in a collaborative, group
settings. MR systems are well suited for collaborative and distributed work
because users interact in face-to-face mode with the real world as well as the vir-
tual objects even if they are not co-located. These results are reflected in collabo-
rative and standalone MR applications. The applications range from robotics [17]
and manufacturing [18] to gaming [23] and visualization [40].

However, a collaboration of multiple users who are not necessarily co-located
introduces additional challenges. Billinghurst and Kato explored the notion of
functional and cognitive seams in collaborative MR systems [6] and reviewed
MR techniques for developing collaborative interfaces.

Contextual information about the physical surrounding can help improve MR
experience. The IoT paradigm provides integration between the physical and the
digital worlds and allows for new applications that can benefit from connecting
everyday objects to the internet. In 2008, the US National Intelligence Council
(NIC) included IoT in a list of six disruptive civil technologies with potential
impact on US interests.

The IoT architecture and the corresponding implementation [10] differ from
the traditional network architecture. A large number of devices are connected,
most of them with limited computing and networking capabilities. The IoT
devices are deployed in various contexts [1], including wearable devices, house
appliances/sensors, embedded devices/smartphones, SBEs, and environmental
sensors. They can span large urban areas to support “smart cities” [27]. With
an intelligent infrastructure core, large number of sensors, and mobile, ubiqui-
tous access, IoT provides many opportunities for innovation. Examples include
MR human-human, human-device and device-device collaborations, personal-
ized healthcare/medicine, intelligent transportation with autonomous and semi-
autonomous vehicles, and SBEs for various living and work settings.

3 Visual Analytics, Immersion, and Embodiment

Thomas and Cook [37] define visual analytics as “the science of analytical rea-
soning facilitated by interactive visual interfaces.” It is a wide-ranging field of
science that involves visualization and interaction methods combined with ana-
lytical reasoning, data representation and transformation as well as production
and presentation of the results.



Immersion Versus Embodiment: Embodied Cognition 359

We are able to collect and generate data at an increasingly fast rate, but
capability of analyzing the collected data lags behind. We focus on how analysts
gain insight into data, find expected and unexpected features, and make deci-
sions using visual tools. The analysts’ main goal is always to explore phenomena
and test hypotheses or to discover unexpected results that question established
assumptions or the validity of the data acquisition process. That can lead to the
generation of new hypotheses.

The challenges of data analysis and exploration are associated with very
large data sets, increased dimensionality and the consideration of data semantics,
including features, focus, and context. Therefore, a visualization tool should be
designed in close collaboration with potential users.

When dealing with a large amount of information, we first find and under-
stand individual pieces of information and then develop a combined understand-
ing of a data set containing many heterogeneous pieces of information. Such
activities provide insight formation (comprehension, making sense, storytelling,
or interpretation). The goal is to get new insights through the novel combina-
tion, organization, or structuring of known information. This insight formation
is crucial for individual users as well as for groups of users.

It is important to explore and understand how analysts cope with complex
models in various tasks and how can they benefit from collaboration. However,
not all analysts should be visualization experts. Rather, they should be provided
with semantic interfaces that can adjust to their needs and common knowledge.

Insight formation is affected by the organization and coordination of views
and user interactions with those views. Therefore, view management and input
modalities play an important role in both traditional visualization and VR/MR
visualization. Most of the visual analytical systems use either a traditional desk-
top or large scale displays with direct touch.

However, in VR/MR view management involves maintaining visual con-
straints on the ‘data’ objects, locating related objects near each other, or pre-
venting occlusion [4]. Affordances of different input modalities (touch, speech,
proxemics, gestures, gaze, and wearable) can be described in terms of direct
interactions (no mediator) between a human body and ‘data’ objects [3]. For
example, spatial immersion introduces challenges such as depth perception, data
localization and object relations [28].

Immersive analytics could provide the ultimate user interface for insight for-
mation within integrated immersive data worlds. For that, it is essential to pro-
vide a mix of creative user interactions, support for collaboration, and insightful
learning algorithms [22].

There is a lack of methods and practical guidelines for the development of
embodied user interfaces, especially in the context of MR environments. Much
has been written about embodiment theory, and there are examples of effective
embodied interfaces, but how can we approach the design and implementation
of a user interface or an interaction technique that would enable users to make
use of their powerful embodied resources in an MR environment? The ability



360 D. Gračanin

to combine immersion and embodiment in MR environments is critical to the
success of the MR-based immersive analytics.

We can make embodiment more concrete by using the theory of affor-
dances [32]. We can investigate the affordances provided by various user interface
designs, and to evaluate which affordances lead to a higher degree of “embodied
behavior,” a behavior that indicates the user is taking advantage of embodied
resources to gain better insight, especially in a collaborative setup [9].

In order to investigate impact of embodiment and immersion on visual ana-
lytic in MR environments, we need to developed the supporting infrastructure.
This infrastructure include a testbed to develop and deploy applications and a
framework that provides data collection, fusion and evaluation services.

Since the future visual analytics infrastructure will be distributed and collab-
orative [31], we need to develop collaborative frameworks that will bring together
geographically distributed and co-located analysts. Only through gradual refine-
ment and improvements that takes advantage of recent immersive technologies,
we can provide new immersive analytics, control, interaction, and visualization
modalities.

However, most MR-enabled collaborative work done over the network has
seams, namely spatial, functional, and cognitive seams [6]. Spatial seams are a
consequence of geographically separated and technologically asymmetric spaces
of individual users. Functional seams steam from different functional workspaces,
forcing the user to change operation. Cognitive seams result from differences
between existing and new work practices. Therefore, it is necessary to address
those seams and enhance the collaborative experience in order for the MR sys-
tems to go beyond what current systems could do.

4 Immersive Analytics Frameworks and Services

The need for new immersive analytics, control, interaction, and visualization
modalities is only one side of the coin. The data about the analysts, especially
cognitive state and stress level, can direct how the data objects are presented
and interacted with. An instrumented physical space enriched with a variety of
sensors can provide a lot of data about the user and the surrounding physical
environment. The environmental and physiological data can be combined with
self-reported data to create a contextualized data about the user. This contex-
tualized data then informs the stimuli that is provided to the user (Fig. 1).

The capabilities are provided through basic environmental, physiological and
cognitive data retrieval service. The derived composite service implement algo-
rithms and techniques to provide activity, stress and affect related information.
The services help us explore how to design human-space interaction, how to
identify the modalities of interactions, and how to inform the overall design of
space when it is superimposed with technology.

The challenge is how to effectively collect, analyze and fuse data while man-
aging heterogeneous and geographically distributed users. Creating a MR collab-
orative system that takes advantage of fused data and orchestrated interactions



Immersion Versus Embodiment: Embodied Cognition 361

Fig. 1. Contextual physical environment: an instrumented physical space.

can improve quality of experience and interactions, and reduce spatial, functional
and cognitive seams for more effective and efficient human networks.

The present extent of context-aware applications leaves a lot to be desired.
The problem is that context awareness shown by human beings is based on a
radically different paradigm as compared to the one used by the computational
infrastructure.

Figure 2 shows that a typical physical space (a classroom) can be quickly
transformed in a hospital room, bathroom or a model of an SBE. Due to the
immersion and embodiment of MR environments, the context changes dramati-
cally due to a closed loop from the user to stimuli and then back to the user.

Fig. 2. Transforming an instrumented physical space into a MR environment Left:
Physical environment, a classroom. Right: MR environment, a hospital room.

Creative user interactions [22] must take advantage of the human body as
an interaction devices. However, the presence of tangible, reconfigurable smart
objects, provide tremendous opportunities to address the needs of the immersive
analytics process in MR environments.



362 D. Gračanin

Unlike traditional graphical user interfaces (WIMP paradigm), tangible user
interfaces are not well integrated. There is a cognitive and contextual gap due
to the space separation between the user input and display. Various modalities
of tangible inputs (e.g., AR markers, smart objects) must be integrated with a
display within the user interface space. Visually overlaying tangible input with
visual analysis display within an MR environment must address issues such as
occlusion, limited embodiment and limited user interface space [20]. Figure 3
shows how tangible interactions for MR analytics can be created by combining
a traditional CMV tool with tangible markers to provide a tangible input space.

Fig. 3. Tangible interactions for MR analytics. Left: A traditional CMV tool. Center:
The user interface space is a combination of a tangible input space and a visual analysis
display space. Right: The implementation of the user interface space by integrating
the output of the traditional CMV tool within an MR environment.

We have to move beyond the 2D displays and 2D representatives to lever-
age benefits of embodied interactions. MR devices with integrated cameras and
environment mapping capabilities can provide support for flexible 3D tangible
and 3D virtual representatives. In that context, a smart tangible object can
serve as prop for a number of interactions. Figure 4 shows such a smart, recon-
figurable tangible object representing an adjustable money bill. Using smart
tangible objects reduces the need for artificial, learned gestures and provides
affordances for embodied interactions. The smart, tangible object becomes an
intermediary for interactions between the user and the avatar or between geo-
graphically distributed users who see each other as avatars.

Using the physiological bio-sensing devices (wearable sensors for galvanic
skin-response, heart-beat, blood volume pulse, etc.), arguably captures subcon-
scious processes in the human body. Therefore, such approach is unlikely to
interfere with the user’s activities due to social or other considerations. That, in
turn, allows exploration of social and technology interactions in information-rich
physical spaces and places that characterize MR environments.

Figure 5 shows a local user (left), a remote user (center) and an avatar con-
trolled by the application (right). The environmental data (tracking) is used to
position the remote use in the local user’s space. The avatar representation of the
remote user mirrors the remote user’s body movement. A simple visualization
of the remote user’s biometric data, blood volume pulse (BVP), galvanic skin



Immersion Versus Embodiment: Embodied Cognition 363

Fig. 4. Left: A view of a smart, tangible object representing an adjustable money bill.
The user can set the amount using the ‘-’ and ‘+’ icons. Right: The avatar reaching
towards the smart object to indicate setting the return value.

Fig. 5. Remote collaboration. Left: A local user in a distributed MR environment.
Center: A remote user is tracked and represent by an avatar that replicates, in real-
time, the remote user’s body movements and displays biometric data. Right: An avatar
that is controlled by the application to interact with the user.

resistance (GSR), and temperature (TMP) provides an additional context and
possible indication of the remote user’s affect.

4.1 Implementation

The described characteristics of immersive analytics frameworks and related ser-
vices demonstrate the current stage in the development of a framework and an
evaluation testbed for embodied, MR-based immersive analysis. The framework
provides services for data access, data visualization (views), both traditional two-
dimensional view and a three-dimensional equivalent, views assembly, gestures,
and interaction devices. The provided services include, among others:

– Collection and visualization of body tracking data using an avatar in an MR
environment.

– Visualization of sensor data (environmental and biometric) within the MR
environment.

– Data exchange among multiple sites that are aligned in a common MR envi-
ronment.

– Support collaborative interactions and manipulation in the MR environment.



364 D. Gračanin

The collaboration and access to the remote tracking and sensor data is imple-
mented using MQTT, a popular lightweight M2M communication protocol for
exchange of telemetry data [2]. The data is stored in a data repository (OSIsoft
PI system). The collected longitudinal data can be accessed, viewed and analyzed
using the framework services.

4.2 Example: Smart Built Environments

A smart environment is a physical space enriched with smart objects that work
continuously to make residents lives more comfortable SBEs incorporate sensors
and actuators into the built space to provide new functionalities or enhance the
current ones.

User tasks in built environments usually involve interaction with physical
objects that respond to user actions with some kind of feedback. In addition,
the user can view and analyze SBE data. Services allow users to both issue
commands and receive feedback. The services exist on multiple scales ranging
from the design of door knobs to the placement of house modules.

SBE augments traditional home by adapting new technology into the existing
patterns of use to provide a rich computational and communication infrastruc-
ture. This infrastructure includes smart things, devices and sensors that can
observe the physical environment and interact with the inhabitants in novel
ways. However, the physical and social structures within a built environment
are subject to continuous change that creates the need for reconfigurable spaces
and places in SBEs.

An ongoing FutureHAUS project focuses on the design and construction of
a smart, modular house. The framework was used to develop and instrument a
supporting hybrid testbed for the visualization and analysis of the smart house’s
modules. Among others, testbed was used to implement and explore new modal-
ities in using lighting in built environments. The conducted user studies investi-
gated the impact of visual stimuli on cognitive states and emotions.

Figure 6 left shows visual representation of an SBE (a kitchen module) in a
large room to provide for a realistic walkthrough (in actual size). Figure 6 right
shows positioning an avatar in a public space (library).

Fig. 6. Positioning virtual object in physical space: Left: A model of a kitchen module
(actual size) placed in the Cube facility space. Right: An avatar positioned in a library.



Immersion Versus Embodiment: Embodied Cognition 365

Fig. 7. Kitchen module: Left: The constructed kitchen module, a part of a modular
smart house (SBE). Right: The VR representation of the kitchend module.

Figure 7 shows the completed kitchen model (left) and its VR equivalent
(right).

5 Conclusion

Immersive analytics has tremendous potential in addressing visual analytics chal-
lenges. However, that will require new immersive analytics, control, interaction,
and visualization modalities. The challenge is to identify what are the best prac-
tices to leverage embodied cognition and interactions to support immersive visual
analytics.

Immersion alone is not sufficient and must be complemented by embodiment.
Fully immersive VR environments isolate the user from the physical world and
drastically reduce affordances for embodied interactions. MR environments pro-
vide sufficient immersion while maintaining the affordances for embodied interac-
tions. The presented framework and the corresponding testbed provide services
for embodied interactive immersive analysis. The goal is to take advantage of
affordances and embodied cognition in an MR environment.

Using the framework based tools in a real-world context, where computer
generated stimuli are blended with the real-world stimuli through the use MR
technologies, will provide necessary affordances in support of higher-fidelity,
embodied interactions that will, in turn, result in more effective visualization
and analysis.

References

1. Anderson, J., Rainie, L., Duggan, M.: The internet of things will thrive by 2025.
Technical report, Pew Research Center, Washington, D.C., May 2014

2. Arlitt, M., Marwah, M., Bellala, G., Shah, A., Healey, J., Vandiver, B.: MQTT
version 3.1.1 plus errata 01. Standard, OASIS, 10 December 2015



366 D. Gračanin

3. Badam, S.K., Srinivasan, A., Elmqvist, N., Stasko, J.: Affordances of input modal-
ities for visual data exploration in immersive environments. In: Proceedings of the
Workshop on Immersive Analytics: Exploring Future Interaction and Visualization
Technologies for Data Analytics (#Immersive 2017) – IEEE VIS (2017)

4. Bell, B., Steven Feiner, S., Höllerer, T.: View management for virtual and aug-
mented reality. In: Proceedings of the 14th Annual ACM Symposium on User
Interface Software and Technology, pp. 101–110. ACM, New York (2001)

5. Bijl, J.L., Boer, C.A.: Advanced 3D visualization for simulation using game
technology. In: Proceedings of the Winter Simulation Conference, pp. 2815–2826
(2011)

6. Billinghurst, M., Kato, H.: Collaborative mixed reality. In: Ohta, Y., Tamura,
H. (eds.) International Symposium on Mixed Reality (ISMR 1999), pp. 261–284.
Springer, Heidelberg (1999)

7. Bimber, O., Raskar, R.: Spatial Augmented Reality: Merging Real and Virtual
Worlds. A K Peters, Wellesley (2005)

8. Card, S.K., Mackinlay, J., Shneiderman, B. (eds.): Readings in Information Visu-
alization: Using Vision to Think. Interactive Technologies, Morgan Kaufmann, San
Francisco (1999)

9. Clark, A.: Embodied, situated, and distributed cognition. In: Bechtel, W., Graham,
G., Balota, D.A. (eds.) A Companion to Cognitive Science, pp. 506–517. Blackwell
Publishing Ltd., Oxford (2017)

10. daCosta, F.: Rethinking the Internet of Things: A Scalable Approach to Connecting
Everything. Apress L. P., Berkeley (2013)

11. Dong, S., Kamat, V.R.: Collaborative visualization of simulated processes using
tabletop fiducial augmented reality. In: Proceedings of the Winter Simulation Con-
ference, pp. 828–837 (2011)

12. Dourish, P.: Where the Action Is: The Foundations of Embodied Interaction. The
MIT Press, Cambridge (2001)

13. Dourish, P.: Re-space-ing place: “place” and “space” ten years on. In: Proceedings
of the 2006 20th Anniversary Conference on Computer Supported Cooperative
Work (CSCW 2006), pp. 299–308. ACM, New York, 4–8 November 2006

14. Eastman, C., Teicholz, P., Sacks, R., Liston, K.: BIM Handbook: A Guide to Build-
ing Information Modeling for Owners, Managers, Architects, Engineers, Contrac-
tors, and Fabricators. Wiley, Hoboken (2008)

15. Fishkin, K., Stone, M.C.: Enhanced dynamic queries via movable filters. In: Pro-
ceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp.
415–420. ACM Press/Addison-Wesley Publishing Co., New York (1995)

16. Fishkin, K.P., Gujar, A., Harrison, B.L., Moran, T.P., Want, R.: Embodied user
interfaces for really direct manipulation. Commun. ACM 43(9), 74–80 (2000)

17. Frank, J.A., Krishnamoorthy, S.P., Kapila, V.: Toward mobile mixed-reality inter-
action with multi-robot systems. IEEE Robot. Autom. Lett. 2(4), 1901–1908
(2017)

18. Gonzalez-Franco, M., Pizarro, R., Cermeron, J., Li, K., Thorn, J., Hutabarat, W.,
Tiwari, A., Bermell-Garcia, P.: Immersive mixed reality for manufacturing training.
Front. Robot. AI 4, 3 (2017)

19. Gračanin, D., Eck II, T., Silverman, R., Heivilin, A., Meacham, S.: An approach
to embodied interactive visual steering: bridging simulated and real worlds. In:
Proceedings of the 2014 Winter Simulation Conference (WSC), pp. 4073–4074,
December 2014



Immersion Versus Embodiment: Embodied Cognition 367

20. Gračanin, D., Tasooji, R., Handosa, M., Matković, K., Waldner, M.: Tangible visual
analysis: brushing in a mixed-reality environment. In: Puig, A., Isenberg, T. (eds.)
Proceedings of the 19th EG/VGTC Conference on Visualization (EuroVis 2017) –
Posters. The Eurographics Association, 12–16 June 2017

21. Gračanin, D., Zhang, X.: CaffeNeve: a podcasting-capable framework for creating
flexible and extensible 3D applications. ACM Comput. Entertain. 11(1), 5:1–5:30
(2013)

22. Hackathorn, R., Margolis, T.: Immersive analytics: building virtual data worlds for
collaborative decision support. In: Proceedings of the 2016 Workshop on Immersive
Analytics (IA), pp. 44–47, March 2016

23. Jacoby, D., Coady, Y.: Perspective shifts in mixed reality: persuasion through col-
laborative gaming. In: Proceedings of the Personalization in Persuasive Technology
Workshop, Persuasive Technology 2017 (PPT 2017), pp. 84–90 (2017)

24. Kirsh, D.: Embodied cognition and the magical future of interaction design. ACM
Trans. Comput. Hum. Interact. 20(1), 3:1–3:30 (2013)

25. Klemmer, S.R., Hartmann, B., Takayama, L.: How bodies matter: five themes
for interaction design. In: Proceedings of the 6th ACM Conference on Designing
Interactive systems (DIS 2006), pp. 140–149. ACM Press, New York (2006)

26. Lazem, S., Gračanin, D.: Social traps in Second Life. In: Debattista, K., Dickey, M.,
Proenc̃a, A., Santos, L.P. (eds.) Proceedings of the 2nd International Conference
on Games and Virtual Worlds for Serious Applications (VS GAMES 2010), pp.
133–140, 25–26 March 2010

27. Lea, R., Blackstock, M.: Smart cities: an IoT-centric approach. In: Proceedings
of the 2014 International Workshop on Web Intelligence and Smart Sensing, pp.
12:1–12:2. ACM, New York (2014)

28. Luboschik, M., Berger, P., Staadt, O.: On spatial perception issues in augmented
reality based immersive analytics. In: Proceedings of the 2016 ACM Companion
on Interactive Surfaces and Spaces, pp. 47–53. ACM, New York (2016)

29. Matković, K., Freiler, W., Gračanin, D., Hauser, H.: ComVis: a coordinated mul-
tiple views system for prototyping new visualization technology. In: Proceedings
of the 12th International Conference on Information Visualisation (IV 2008), pp.
215–220, 9–11 July 2008

30. McGlinn, K., Hederman, L., Lewis, D.: SimCon: a context simulator for supporting
evaluation of smart building applications when faced with uncertainty. Pervasive
Mob. Comput. 12, 139–159 (2014)

31. Nguyen, H., Marendy, P., Engelke, U.: Collaborative framework design for immer-
sive analytics. In: Proceedings of the 2016 Big Data Visual Analytics (BDVA), pp.
1–8, November 2016

32. Norman, D.A.: The Design of Everyday Things. Currency/Doubleday, New York
(1990)

33. Prendinger, H., Brandherm, B., Ullrich, S.: A simulation framework for sensor-
based systems in Second Life. Presence Teleop. Virtual Environ. 18(6), 468–477
(2009)

34. Renambot, L., Bal, H.E., German, D., Spoelder, H.J.W.: CAVEStudy: an infras-
tructure for computational steering in virtual reality environments. In: Proceedings
of the Ninth International Symposium on High-Performance Distributed Comput-
ing, pp. 239–246, 1–4 August 2000

35. Roberts, J.C.: State of the art: coordinated multiple views in exploratory visual-
ization. In: Proceedings of the Fifth International Conference on Coordinated and
Multiple Views in Exploratory Visualization (CMV 2007), pp. 61–71. IEEE, 2 July
2007



368 D. Gračanin

36. Sinclair, B.: IoT Inc.: How Your Company Can Use the Internet of Things to Win
in the Outcome Economy. McGraw-Hill Education, New York (2017)

37. Thomas, J.J., Cook, K.A. (eds.): Illuminating the Path: The Research and Devel-
opment Agenda for Visual Analytics. IEEE Computer Society, Los Alamitos (2005)

38. Williams, A., Kabisch, E., Dourish, P.: From interaction to participation: con-
figuring space through embodied interaction. In: Beigl, M., Intille, S., Rekimoto,
J., Tokuda, H. (eds.) UbiComp 2005. LNCS, vol. 3660, pp. 287–304. Springer,
Heidelberg (2005). https://doi.org/10.1007/11551201 17

39. Wilson, M.: Six views of embodied cognition. Psychon. Bull. Rev. 9(4), 625–636
(2002)

40. You, S., Thompson, C.K.: Mobile collaborative mixed reality for supporting sci-
entific inquiry and visualization of earth science data. In: Proceedings of the 2017
IEEE Virtual Reality Conference, pp. 241–242, March 2017

https://doi.org/10.1007/11551201_17

	Immersion Versus Embodiment: Embodied Cognition for Immersive Analytics in Mixed Reality Environments
	1 Introduction
	2 Related Work
	3 Visual Analytics, Immersion, and Embodiment
	4 Immersive Analytics Frameworks and Services
	4.1 Implementation
	4.2 Example: Smart Built Environments

	5 Conclusion
	References




