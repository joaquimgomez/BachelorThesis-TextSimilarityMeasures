














































































8. Collaborative Immersive Analytics

Mark Billinghurst1, Maxime Cordeil2, Anastasia Bezerianos3, and Todd
Margolis4

1 School of ITMS, University of South Australia, Mawson Lakes, Australia
mark.billinghurst@unisa.edu.au
2 Monash University, Australia

max.cordeil@monash.edu
3 Univ. Paris-Sud, CNRS, Inria, Université Paris-Saclay, France

anastasia.bezerianos@u-psud.fr
4 Qlik, California USA

tmargo@gmail.com

Abstract. Many of the problems being addressed by Immersive An-
alytics require groups of people to solve. This chapter introduces the
concept of Collaborative Immersive Analytics (CIA) and reviews how
immersive technologies can be combined with Visual Analytics to facil-
itate co-located and remote collaboration. We provide a definition of
Collaborative Immersive Analytics and then an overview of the different
types of possible collaboration. The chapter also discusses the various
roles in collaborative systems, and how to support shared interaction
with the data being presented. Finally, we summarize the opportunities
for future research in this domain. The aim of the chapter is to provide
enough of an introduction to CIA and key directions for future research,
so that practitioners will be able to begin working in the field.

Keywords: collaborative immersive analytics, immersive analytics, immersion,
human-computer interaction, visual analytics, collaboration, collaborative visual-
ization, shared interaction

8.1. Introduction

In a world of increasing computing power and sensing, there is more data being
generated than ever before. Over the last ten years Big Data [95] has become an
important field of research with data scientists exploring many different ways
to transform terabytes of information into valuable insights. One of the popular
methods for creating understanding is to generate visual representations of the
data, as explored in the area of Visual Analytics [125], defined as “the science
of analytical reasoning facilitated by interactive visual interfaces”. In this case a
wide variety of visualization techniques are used to explore complex datasets. For
example, 2D bar charts can be used to understand detailed election results and
3D graphics to show sensor data on terrain models. Visualization tools have been
shown to improve performance time and productivity on a range of different tasks
c© Springer Nature Switzerland AG 2018
K. Marriott et al. (Eds.): Immersive Analytics, LNCS 11190, pp. 221–257, 2018.
https://doi.org/10.1007/978-3-030-01388-2_8



such as software analysis [50], data analysis [118], and information retrieval [130],
among others. Sun et al. [119] provide an excellent summary of visual analytics
techniques and applications, and their benefits.

Most recently, researchers have begun to explore how novel immersive display
technology can be used to enhance visual analytics solutions (Chapter 1). The
term Immersive Analytics [20] was coined to describe “an emerging research
thrust investigating how new interaction and display technologies can be used to
support analytical reasoning and decision making”. These display technologies
range from room scale immersive CAVE projection systems [30] to Virtual Reality
head mounted displays (HMDs) [31], and from interactive walls, tables and multi-
display environments [124] to portable head worn Augmented Reality displays [3].
Klapperstück et al. [72] proposed ContextuWall, a system for interactive local
and remote collaboration using touch and mobile devices as well as displays of
various sizes. Emerging interaction technologies include devices like the Microsoft
Kinect [71] or Leap Motion [74] which support natural gesture input, eye-tracking
devices [99] for collecting awareness information, and even Brain Computer
Interfaces that respond to thoughts [87].

Previous research has shown that using Immersive Analytics technologies
can enable people to be significantly more effective at understanding data vi-
sualizations than more traditional interface technologies. For example, Belcher
et al. found that using Augmented Reality (AR) improved user performance
at finding node connections in complex graph visualizations compared to a 2D
desktop presentation [8]. Similarly, Ware [135] found that using head-coupled
stereo viewing enabled a person to understand an abstract graph three times the
size of a graph viewed on a normal non-stereo monitor.

However, effective presentation on immersive displays is only one way to gain
an understanding of complex data. Research stretching back over decades has
found that collaborative decision making is often more effective than working on
problems alone. Hill [51] provides a good review of early research comparing group
versus individual performance on different tasks, finding that group performance
is generally superior to that of an average individual. Similarly, in comparing
collaborative to single user performance on an information visualization task,
Mark et al. [83] found that groups worked slower, but produced more accurate
results. Recently, Woolley et al. [137] have argued for a group collective intelligence
factor that predicts performance on collaborative tasks, and could be improved
by using collaborative tools. However, supporting effective collaboration can
be challenging and was identified as one of the Grand Challenges for Visual
Analytics [27].

Co-located collaboration provides important benefits. Sawyer et al. showed
that team rooms supporting face-to-face activities helped focus the activities of
work groups and removed them from interruptions [108]. Most recently, Teasley
et al. found that co-located software teams working in “war rooms” with access
to tools such as computers, whiteboards and flipcharts were twice as productive
as the similar teams working in a traditional office environment [123]. Some

222 Mark Billinghurst et al.



visualization systems, such as CoVis [33] have specific tools for supporting
co-located collaboration.

Significant benefits can be found from teams working together in an Immersive
Analytics setting. Marai et al. [82] review three Immersive Analytics projects
undertaken by research teams using the CAVE2 immersive projection environment
(see Figure 1). The feedback was overwhelmingly positive, with the authors saying,
“At the end of the meeting one of the team members said that the team got more
done in 2 days than in 6 months of email, Skype, and Google Hangout.” They
were able to make such rapid progress because the CAVE2 environment enabled a
group of people to work together face to face while seeing multiple representations
of the data across a large amount of screen real-estate all at the same time. This
research builds on their earlier work with the Cyber-Commons [76] which was a
100 megapixel tiled display wall that allowed small groups of multidisciplinary
researchers to work together.

Fig. 1: EVL researchers at work in the CAVE2 at UIC, from [82]

In this chapter we provide an overview of and introduction to Collaborative
Immersive Analytics. Although there has been a significant amount of research
on how immersive technologies can support collaboration, there has been very
little research in the newer area of Collaborative Immersive Analytics. Our aim
is to provide an introduction to researchers to this field, describe some example
applications, provide design guidelines, and clearly identify directions for future
research. This should be helpful for guiding researchers wanting to enter this
emerging field of research or for teams requiring new means of analysis for complex
data.

In the remainder of the chapter we first provide a definition of Collaborative
Immersive Analytics, and various roles people have when using collaborative
systems. We then provide an overview of collaborative interaction methods, before

Collaborative Immersive Analytics 223



identifying promising areas for research. Finally, we summarize the chapter and
outside areas for future work. Overall, the aim of this chapter is to provide enough
of an introduction to Collaborative Immersive Analytics that interested readers
could begin to do their own work in the field.

8.2. Definition and Scope

Before we can discuss Collaborative Immersive Analytics (CIA) we need to define
what the term means and describe the overall scope of the chapter. CIA is so
new that there is no well accepted definition. However Collaborative Immersive
Analytics is related to Collaborative Visualization, a term that Isenberg et al. [58]
define as:

The shared use of computer-supported, (interactive,) visual representations
of data by more than one person with the common goal of contribution to joint
information processing activities.

Using this definition, Isenberg points out that collaborative visualization lies
at the intersection of the two major research fields of visualization and computer
supported collaborative work (CSCW), each of which have made significant
research contributions that could benefit collaborative visualization.

Considering this and returning to the original definition of Immersive Analyt-
ics, we define Collaborative Immersive Analytics as: The shared use of immersive
interaction and display technologies by more than one person for supporting
collaborative analytical reasoning and decision making.

The main difference between Collaborative Immersive Analytics and Collabo-
rative Visualization is the focus in CIA on the use of immersive interaction and
display technologies. Hackathorn and Margolis [45] point out that Immersive
Analytics environments can span most of Milgram’s Reality-Virtuality (RV)
continuum [86]. The RV continuum is a well-known way of classifying interface
technologies in terms of how they connect the physical and virtual worlds, from
the fully physical world (on the left) to the fully virtual world (on the right)
(Figure 2). Within this continuum is the class of Mixed Reality technologies
which merge real and virtual worlds. This ranges from tabletop displays and
Augmented Reality environments to fully immersive virtual reality head mounted
displays and rooms.

Fig. 2: Milgram’s Mixed Reality Continuum, from [86].

224 Mark Billinghurst et al.



Collaborative Immersive Analytics explores how Mixed Reality technologies
such as these can be used in a collaborative setting. In contrast, Collaborative
Visualization is concerned more with shared visualization in general, so the
field of CIA can be viewed as a subset of the broader field of Collaborative
Visualization. Just as Collaborative Visualization lies at the intersection of the
field of visualization and CSCW, Collaborative Immersive Analytics lies at the
intersection of Collaborative Visualization and Mixed Reality (see Figure 3).

Fig. 3: The relationship between Collaborative Immersive Analytics, Collaborative
Visualization and Immersive Analytics.

By its very nature, CIA is a multi-disciplinary research field. In addition to
Collaborative Visualization, it is also related to the fields of Scientific Visualisation
(SciVis) [68], Information Visualisation (InfoVis) [17] and Visual Analytics [125],
all of which are well established with ongoing conferences and journals over the
past 20 years. However, in these fields relatively little attention has been paid to
the study of collaborative visualisation, especially using advanced interface tools.
For example, Isenberg points out that of the nearly 1600 papers published in the
three main IEEE visualization conferences from 1990 to 2010, only 34 focused
on collaborative visualisation, less than three percent of the total [58]. According
to Google Scholar, until now there have been no papers published using the term
“Collaborative Immersive Analytics”.

This lack of research means there are significant opportunities for research
in the field of Collaborative Immersive Analytics. The focus of this chapter
highlights these areas and provides some of the background material to help inter-
ested researchers. We provide some important background work in collaborative
systems, discuss multi-user interaction modalities, highlight lessons learned from
collaboration with immersive technologies such as Virtual and Augmented Real-
ity displays and interactive walls and tables, and specifically identify important
research topics. We should note that this chapter is not an exhaustive literature

Collaborative Immersive Analytics 225



review, nor a tutorial on collaborative technologies or immersive interfaces in
general.

In the next section we begin by introducing a taxonomy of collaborative
systems based on space and time, and examples of collaborative immersive
technologies that fit into this taxonomy. Then, we discuss the different roles
that people have in collaborative systems and how they can interact with the
datasets. Finally, we provide a high-level overview of some of the possible research
opportunities that could be explored in the future.

8.3. Collaboration Over Space and Time

Using Immersive Analytics technology there are many different types of collabora-
tion possible. For example, analysts can work together face to face in a CAVE [82]
or a multi-display environment [124], come together from remote locations into
the same shared VR environment [77], or leave annotations in datasets to be
viewed at different times [35]. Collaborative scenarios like these can be classified
according to where they occur in space (distributed vs. co-located) and time
(synchronous vs. asynchronous) [65] (see Figure 4). For example, people working
together in a lab space are in a Synchronous/Co-located configuration, while
those exchanging email over time are working in a Distributed/Asynchronous
configuration.

Fig. 4: A space-time taxonomy of collaborative visualisation, from [58].

226 Mark Billinghurst et al.



In this section we describe this classification in more detail and review several
examples of CIA systems at different points in the space/time collaborative matrix.
We also describe some of the lessons that have been learned from previous research
that can be used as guidelines for developing collaborative systems.

8.3.1. Co-Located Synchronous Collaboration

There are many examples of Immersive Analytics applications where collaborators
are in the same physical space and working together at the same time. Marai
et al. [82] describes three Immersive Analytics projects where a small group of
researchers work face-to-face together inside the CAVE2 environment surrounded
by immersive projection screens. In this case researchers brought their own laptop
computers into the CAVE2 and were able to work on their private displays as
well as use the shared immersive display. Other environments include shared
visualizations on interactive tabletops where people stand around the table
surface [70] [1], responsive wall displays that multiple people can interact with at
the same time [23] [29] [2] [5], and face to face collaborative AR solutions [109] [12].
Figure 5 shows a typical use of interactive tables and walls to support face-to-face
collaboration.

Fig. 5: Examples of using interactive wall displays and tabletops for co-located
synchronous collaborative visualization.

Co-located synchronous collaboration has a number of advantages. Collabo-
rators can directly see one another and work together at the same time and so
changes they make to the data set can be easily seen by their co-workers [115].
This makes it very easy to have awareness of others which can improve com-
munication, and to move between focusing on their individual work and group
work. Collaborators can also easily bring their own externalisation tools (such
as computers and notes) into the meeting and share them amongst each other,
which helps establish a common ground.

The different immersive technologies available for face-to-face collaboration
may also provide unique benefits. For example, with a co-located Augmented

Collaborative Immersive Analytics 227



Reality interface each user has their independent view into the shared dataset
that can be customized according to their role [109]. A virtual terrain model could
be overlaid with sensor data for an environmental engineer, but an urban designer
looking at the same AR model might see traffic information overlaid on the
terrain. Similarly, using a tablet at the same time as an interactive wall [97, 140]
or table [134] allows the user to have their own custom private view and input
into the shared display space.

8.3.2. Distributed Synchronous Collaboration

Many collaborative analytics tasks are performed by distributed teams, so there
can be a need for Collaborative Immersive Analytics applications that support
remote synchronous collaboration. For example immersive virtual reality displays
can be used to allow analysts working at different locations to come together in the
same virtual space to jointly explore complex datasets. Donalek et al. [31] describes
how the OpenSim framework was used to create an immersive collaborative
visualization space that could be explored in VR head mounted displays (Figure
6). Similarly, high-speed networking can be used to bring remote collaborators
into an environment with interactive walls or tabletop displays. For example,
the Hugin framework [70] allows the creation of visualization systems where a
group of people around an interactive table at one location can connect and
collaborate with users around a similar table at a remote location. Other systems
have explored the use of mobile devices [88], web browsers [5], CAVEs [66] and
Augmented Reality [138] for supporting remote collaboration for information
visualization.

Fig. 6: Using VR to support remote collaborative data visualization, from [31].
User in VR HMD interacting with the system (b) VR view showing remote
collaborator as virtual person.

228 Mark Billinghurst et al.



The main advantage of synchronous remote collaboration tools is that they
allow remote individuals to connect and collaborate together. In some cases this
can produce a similar performance as face-to-face collaboration. For example, in a
study comparing performance on a collaborative visual analytics task in a CAVE
(face-to-face) or VR HMDs (remote), researchers found that searching in the
HMD condition was as accurate as in the CAVE, and was completed faster [28].

However, there are significant challenges that need to be addressed around
awareness and representation of each of the collaborators. For example, traditional
video conferencing does not produce the same conversation style as face-to-face
interaction [47]. This is because video conferencing cannot adequately transmit the
rich non-verbal signals so vital in face-to-face communication and this introducing
a communication seam between the participants [61]. In interactive walls and
tables, the presence of remote collaborators are often reduced to a pointer icon [43],
or virtual shadow of a hand [122]. Collaborative virtual environments immerse
users in the same virtual space, but even here the remote participants may be
reduced to simple video textures [46] or avatars that cannot convey subtle body
motions [31].

Gutwin and Greenberg point out that designing for collaborative systems is
difficult because of having to support two goals; designing for individual control
over the application and designing for group awareness [43]. They say that
collaboration in remote groupware tools is different from co-located collaboration
for the following reasons: Groupware systems show far less of the workspace than
what can be seen in a physical environment. Manipulation techniques in virtual
workspaces are not bound by the physical constraints that exist in physical
workspaces. Virtual workspaces can represent and display artifacts in more ways
than physical workspaces allow.

8.3.3. Distributed Asynchronous Collaboration

For Immersive Analytics most collaborative applications involve synchronous
collaboration. Viégas and Wattenberg point out that in general ”synchronous
collaborative scenarios have been much more widely explored than asynchronous
visualization based communication” [131]. However, distributed asynchronous
collaboration involves capturing input from people at different times and different
places and so can provide some unique benefits. For example, Benbunan-Fich et al.
found that asynchronous collaboration can produce broader discussions and more
complete reports from group discussions than their face-to-face counterparts [9].
Other benefits include enabling people to contribute whenever they have time
to provide input [104], they can work on the part of the problem that they feel
most qualified to address [129], and can combine information from a variety of
sources [52].

Some efforts have been made to add support for asynchronous collaboration
to CAVE and immersive Virtual Reality experiences, mostly through simple
recording and playing back of messages. For example, Imai’s V-Mail system allows
people to send and view asynchronous messages in VR [54]. In this case the user
can record a voice message along with their virtual avatar body movements and

Collaborative Immersive Analytics 229



gestures for later playback. Later, this was extended to include a VR-annotator
tool for attaching 3D recordings to objects, and a VR-vcr streaming recorder
for recording all actions occurring in a collaborative session [55]. Similarly, the
Virtue immersive visualization environment provided support for multimedia
annotations that allowed users to mark temporal and spatial points for later
replay and sharing [111]. More recently, the vAcademia Virtual World shows
how 3D recording can be used for asynchronous collaboration in desktop VR
environments [89].

There are other examples of non-immersive asynchronous Analytics interfaces
that can provide valuable lessons for extending this work. Zimmer and Kerren
introduced OnGraX [143], a collaborative synchronous and asynchronous web
based tool to support visual analysis of networks. Willet et al. have developed
CommentSpace, a collaborative visual analysis tool that allows analysts to anno-
tate information visualisations [136]. The tool allows analysts to add comments
or view tags to the visualization over time and link the tags together to aid
understanding of the document. In this way Collaborators gain the benefit of
seeing the tagged classifications that people are applying to the text, and the
semantic links being created. Similarly, Chen et al. [22] developed ManyInsights,
a web-based tool for asynchronous collaborative analysis of multidimensional
data (see Figure 7). Using this tool people can record their own insights from
data, and read the insights of others over time. They found in an evaluation
that this led to the generation of more shared insights, and being able to group
insights by data similarity was a particularly powerful way to understand the
data.

Heer and Agrawala [48] provide an excellent description of design charac-
teristics for asynchronous collaborative visual analytics interfaces, drawing on
web-based interfaces. In particular they list design considerations that should
be taken into account, such as the value of supporting freeform annotations,
using visualization bookmarks, linking to specific views, and clearly showing past
actions that have taken place, among others. They designed the sense.us web
application for social visual data analysis, showing how these design guidelines
can be used [49]. In this case the interface supports view sharing, doubly linked
discussions, graphical annotation, collecting and linking of views, awareness and
social navigation, and unobtrusive collaboration. Figure 8 shows the sense.us
interface and collaboration features.

Viegas and Wattenberg highlight the research opportunities available in
the Distributed Asynchronous Collaboration Space [131]. They say “By not
fully exploring asynchronous communication of visualization discoveries and
processes, the research community is missing an opportunity to make important
contributions to visualization research”. In particular, they identify three features
that are important; (1) Playback: Allowing users to edit a visualization session,
picking out only a few key frames, and sharing it with someone else who can
rewatch it. (2) Annotation: Enabling users to point to objects and add information.
Tracking changes in annotations over time, and supporting group creation. (3)
Information Foraging: Supporting how users spread their attention over the

230 Mark Billinghurst et al.



Fig. 7: ManyInsights User Interface [22].

data space, and encouraging users to look at little viewed parts of the dataset.
We discuss more about the research opportunities in Distributed Asynchronous
Collaboration in Section 8.6.

8.3.4. Co-Located Asynchronous Collaboration

Another area which has not been well studied for information visualization is
co-located asynchronous collaboration. One common example of an asynchronous
collaboration tool is a shared public display that people can view at different
times. For example, Carter et al. [19] describe a public display interface that they
developed for showing common topics of interest among emails and the current
locations of the email writers. This display was designed to encourage more
collaboration between co-workers. Other examples of using collaborative public
displays include Groupcast [84], CWall [114], and the Notification Collage [40].
Figure 9 shows the Notification Collage interface with a variety of different media
items (web pages, notes, video feeds, desktop views, etc.) posted on a shared
public display.

The availability of public displays has prompted researchers to experiment
with asynchronous, co-located visualizations, for example, in the form of ambient
information displays [113]. In this case, public displays in the physical environment
are used for information visualization. Pousman and Stasko [100] provide a good
overview about how such systems can be used for casual information visualization
in a public setting, such as showing bus timetable information, social networks,

Collaborative Immersive Analytics 231



Fig. 8: Sense.us interface [49]; (a) an interactive visualization applet, (b) annota-
tion tools, (c) a bookmark trail of saved views, (d) text-entry field for adding
comments, (e) threaded comments attached to the current view, (f) URL for the
current state of the application, updated automatically as the state changes.

or photo collections. Similarly, Vogel provides an overview and design guidelines
for interacting with co-located displays [133].

Co-located asynchronous collaborative systems have many of the benefits
of distributed systems, with the additional benefit of collaborators viewing
the same physical space. Viegas and Wattenberg [131] mention that this offers
some interesting design opportunities for the physical surroundings around the
visualization, such as providing pens and paper for people to add their own notes
around the display. Heer and Agrawala [48] point out the value of collaborators
being able to see the same collaboration space, and other researchers have
identified the value of having externalization tools that help people create and add
their own insights [81]. Overall, having a common physical reference space with
shared visualization and external objects should significantly help collaborators
achieve common ground and shared contextual understanding [25].

8.3.5. Mixed-Presence Collaboration

So far we have talked about collaborative experiences that fit into only one
quadrant of the time and space taxonomy. However, it is also possible to create
experiences that link together two or more quadrants. For example, Mixed
Presence Groupware (MPG) systems are collaborative systems that connect
both co-located and distributed collaborators [53] [120]. As such Mixed-Presence

232 Mark Billinghurst et al.



Fig. 9: The Notification Collage interface [40].

systems support synchronous collaboration between people in the same location
and a remote location (see Figure 10). For example, the NICE Project supported
collaboration between groups in one immersive CAVE with remote collaborators
in other VR environments in an educational setting [67]. Other examples include
interactive tables that include local and remote participants [106], and combining
a VR conferencing space with a physical interactive table [103]. Figure 11 shows
a typical mixed presence tabletop application where remote users are represented
by video shadows of their arms (from [Tuddenham 2007]).

MPG systems combine the advantages of co-located and distributed syn-
chronous collaborative interfaces, allowing distributed groups of people to work
together. However there are a number of design challenges that need to be ad-
dressed. For example it can be challenging to enable co-located and remote users
to have the same level of mutual awareness, and there is a need to provide some
representation of the remote users into the local users space [98]. For example,
video arms [121], as seen in Figure 11, can show where the remote users are
reaching into the shared interactive space. It is also difficult to enable users to
share notes, their own device screen and other local physical artifacts with remote
users. Robinson and Tuddenham provides a list of design guidelines to address
some of these challenges [106].

Collaborative Immersive Analytics 233



Fig. 10: Mixed Presence Groupware systems combining co-located and remote
groups.

Fig. 11: Mixed Presence Tabletop System [106].

8.3.6. Lessons Learned

A number of lessons have been learned from this research. From the co-located
synchronous research (e. g. [82]) the following features were found to be important:

– Supporting different independent viewpoints
– Enabling the use of different tools for different data
– Supporting face-to-face group work
– Using different data representations

Churchill et al. [24] reviewed a wide range of collaborative settings and state that
the following should be provided in order to support good group communication
and problem solving:

– Shared context - the shared knowledge and context around the data
– Awareness of others - users being aware of others’ actions

234 Mark Billinghurst et al.



– Negotiation and communication - people being able to freely talk with each
other

– Flexible and multiple viewpoints - showing different viewpoints depending
on roles

As can be seen from the reviewed systems, there are a wide range of different
ways to achieve these requirements. In addition, previous research has shown that
groups need to be able to have access to externalization tools, such as notes or
laptop computers that enable them to record their insights and organize the results
of their analysis [80] [48]. It is also important to enable collaborators to have
access to both individual and shared workspaces [43] [60], and to enable them to
easily shift their focus from their own work to the work of their collaborators [32].
Marai et al. point out that people from different backgrounds want to see data
represented in different ways, so there should be multiple representations available
and a variety of tools for interacting with the data [82].

In this section we have categorized collaborative systems according to how
they support collaboration through space and time. Another important element
of these systems are the various roles that the users have, and how the technology
can support these roles. In the next section we describe the different roles used
in CIA systems.

8.4. Types of Participants in CIA

In collaborative systems participants may have different roles or types of engage-
ments with the system. For example, during a data driven presentation there are
typically one or more people presenting while other constituents are audience
members that generally do not interact directly with the presentation material.
The roles of the people in the collaborative setting define the level of engagement
that they have. Isenberg et al. [58] discuss how there are three different levels of
engagement with collaborative visualization systems:

– Viewing: people are consuming a data presentation without interacting with
the data, such as in a lecture presentation.

– Interacting/exploring: people have the means to choose alternate views or
explore the data.

– Sharing/creating: people are able to create and distribute new datasets and
visualizations to be explored.

Zhu [142] discusses role-based collaboration and points out that for efficient
collaboration there needs to be support for asymmetric expertise and authority.
For example, in a meeting the collaborative system needs to allow a presenter to
control the image presentation, while the images shown should be made visible
to all of the viewers in the audience. Similar to theater acting, storytelling can
be enhanced by adjusting your presentation to suite your audience and react to
their response. Sole [116] points out that effective knowledge-sharing requires
storytellers to streamline their narrative by removing (or temporarily hiding)

Collaborative Immersive Analytics 235



preparatory parts of the analysis that may overwhelm or confuse the audience.
Sole asserts that the other most essential part of a good narrative is to provide
a surrogate experience for the recipient. Immersive Analytics uniquely offers a
sensorial advantage for enveloping recipients in a story thereby increasing the
effect of Suspension of Disbelief. For example, analysts within a CIA willingly
interact with abstract visualizations of data that may not realistically represent
the original source of that data.

Collaborative Immersive Analytics builds upon these archetypal roles, but
also enables the ability to mix roles. Given that CIAs are necessarily more highly
mediated and interactive than traditional visual analytics platforms, it may be
quite natural for participants to switch back and forth between passive viewer
and active explorer. In fact, Heer et al. [49] discuss how their sense.us platform
was a social space for information visualization designed to facilitate data driven
discussion and debate. CIA offers a unique opportunity for example in Command
and Control scenarios where users may spend most of their time monitoring
data feeds and then switch contexts into active analyst and explorer roles as
problems arise. It is thus important that data representations and interfaces of
CIAs can support fluid transitions between these roles. Another consideration
when designing Collaborative Immersive Analytics is the number of creators
and consumers. Here are some of the most common formats for collaborative
analytics:

– One to self (e. g. data discovery that might be shared later)
– One to some (e. g. reporting findings to colleagues)
– One to many (e. g. presenting reports to stakeholders)
– Many to many (e. g. department heads presenting at employee town halls)
– Many to one (e. g. department heads presenting to CEO)
– Machine to one, some or many (e. g. AI returning results to analyst)

Up until about 10 years ago, the predominant method for analysis was through a
guided analytics approach whereby data scientists created visualizations that were
presented to and utilized by other users. In the Business Intelligence industry, this
meant a very small group of individuals produced highly structured dashboards
and reports which were consumed by specific departments within large organiza-
tions. Over the last 10 years though, there has been a massive shift to what is
referred to as Self-Service Analytics in which users now have the ability to input,
access, and analyze data directly by themselves. This transformation has enabled
a new form of data discovery which allows for more people to ask more questions
and solve more problems. This trend towards creating new collaborative analytics
platforms is nicely articulated by the ManyEyes project [132] mentioned earlier.
ManyEyes attempted to “democratize” visualization by not only serving as a
discovery tool for anyone with data, but also as a platform to prompt discussions
around data for large groups of users. This example best typifies the “many to
many” form of collaboration.

Machine learning and artificial intelligence (AI) are being applied more and
more towards analytics. There are many advanced analytics systems which employ

236 Mark Billinghurst et al.



AI to automate various aspects of the analysis process including everything from
data cleansing, preparation and modeling, to dynamically creating visualizations
or data-driven narratives through natural language generation, to alerting users
of “unusual activity” and predicting future trends. This new “analytics agent” can
be considered as a third axis in a collaborative user matrix in Figure 12. Within
a CIA, this collaboration with the AI could be represented through a friendly
3D avatar such as those we already interact with aurally through automated
voice systems. However, ultimately AI systems will likely mature sufficiently so
that they will replace some human knowledge workers through new forms of deep
learning combined with expert systems.

This possible introduction of machine intelligence in the analysis process
complicates further the design of CIA, as it adds a new level of complexity in
the form of an additional actor or role (beyond the existing human ones), whose
actions need to be made aware and interpreted by the human collaborators.

Fig. 12: Collaborative User Matrix.

Given that CIAs can offer more interactivity and potential for greater contri-
bution by more participants, it can also encourage dialogue and debate around
data analysis. If a chart is presented in 3D and one can easily move around it and
interrogate it, will that simple shift in perspective also transform users into active

Collaborative Immersive Analytics 237



listeners? Or can we go beyond simply allowing for the ability of multiple users
with various roles within an Immersive Analytics environment to also enable
them to have suitable means to effectively collaborate? In the next section we will
explore how different types of interaction modalities enable participants within a
CIA to engage in various analytic tasks.

8.5. Interaction in CIA Environments

Deciding on appropriate interactions to support any visual analysis task is
important, as interactions facilitate the dialogue between analysts and their
data [126]. When considering collaborative environments in particular, analysts
are in dialogue not only with the data, but also with each other, and the choice of
interaction technique can influence the nature of collaboration and the awareness
of others [32,44]. For example, Prouzeau et al. [102] observed that when analyzing
graphs on a wall display, pairs using techniques with a large visual footprint
adopted tighter coordination strategies than pairs using techniques that were
more visually localized.

Interaction design in Collaborative Immersive Analytics (CIA) environments
faces similar challenges to other visual analysis environments [126]. For example
they require interactions to support potentially complex analytic tasks, such as
defining and filtering unwanted data, requesting new representations of the data,
etc. The chapter on interaction covers these extensively (Chapter 4). Thus the
interaction vocabulary in such environments needs to be rich enough to go
beyond simple actions such as pointing (a task studied in detail in immersive
environments). Moreover, collaborative environments must also take into account
other challenges and opportunities identified in previous work on collaborative
immersive environments more generally.

This section discusses preliminary synchronous interaction (co-located or
distributed), as it directly impacts collaboration aspects such as awareness and
coordination.

8.5.1. Synchronous Co-located CIA

Synchronous co-located CIA happens historically in shared physically large
immersive environments such as CAVEs, walls and multi-display environments
(MDEs) that surround the viewer, where analysts can move freely around the
space (such as the ones seen in Figures 1 and 5). Movement has been linked to
benefits such as avoiding occlusion by others in CAVEs [21], and correcting for
possible visual distortion in walls [13].

Movement: While collaborative interaction from stationary positions with
mice and keyboards is still possible [16,57], it does not leverage the full rendering
capacities of these immersive environments. In CAVEs analysts can move inside
the data itself and view it from different perspectives. With walls they can see data
at different scales, coming close to the display to see details and further back to
get an overview [7]. With tables, they can see data from different angles by moving

238 Mark Billinghurst et al.



Fig. 13: Interaction using a tangible prop of a brain to rotate brain images, and a
stick that defines the camera position (left), and close-up (right) from [39] (used
with permission, ©CNRS-Phototheque - Cyril FRESILLON).

around the table. In all cases, movement can be seen as an implicit interaction
for navigating in the data space. Movement has also been used as an explicit
interaction, for example to trigger dynamic changes in content rendering [7], or to
invoke actions such as dynamic filtering depending on the analyst’s position and
movement [63]. While such approaches can increase the interaction vocabulary,
they need to be carefully considered in collaborative environments, where implicit
actions as simple as moving may cause large scale visual changes that could disturb
other analysts. This issue is particularly prominent in large-scale stereoscopic
environments (such as CAVEs), where almost always only a single person controls
perspective and interaction, even though multiple people can move in the space.

Pointing and Gestures: Existing work has looked at different interaction
alternatives that support free movement. Research on CAVEs, walls and multi-
display environments have considered extensively pointing techniques (using
hands or dedicated devices) [4, 92, 93], that combined with on-screen menus
and widgets, could cover the complex interaction needs in CIA environments.
Pointing actions can be easily seen by colleagues, increasing the awareness of
others. Nevertheless, the existence of persistent on-screen menus on the shared
collaboration space takes up space that could be used for displaying data, while
menus invoked on the fly can be disruptive to collaborators sharing the space.

As an alternative, hand or full-body gestures can activate commands (e. g.,
[117]) and provide a rich interaction vocabulary without taking up screen space.
They provide awareness of the actions of others, given that these gestures are

Collaborative Immersive Analytics 239



Fig. 14: Multi-touch input on a tablet to zoom and scroll through a stack of
brain-scans [97] (used with permission).

Fig. 15: A user sketching the sliders they need for their exploration [127] (used
with permission).

easily seen by colleagues sharing the space. Nevertheless, full-body gestures
can prove tiring during long periods of work [139] required by some analytics
tasks [126], and gestural vocabularies can be large thus coming with a learning
cost [69].

Dedicated Devices: Researchers have also examined the use of personal,
dedicated, or generic, mobile devices. For example, in order to analyze brain
scans on a wall display, Gjerlufsen et al. [39] suggested using a dedicated tangible
prop of a brain to rotate and translate the scans (Figure 13), and physical tools
to indicate camera viewing directions; Olwal et al. [97] suggested using dedicated
software running on tablets to provide interaction tools and different views of
the brain between colleagues in medical teams (Figure 14).

These examples illustrate the large possible range of such mobile devices in
terms of both generalizability across analytic tasks, and awareness of others.
Interaction with dedicated tangible props (such as a model of a brain), comes with
a high degree of awareness since props used by colleagues are easy to identify, but
their utility is limited to specific data domains and analysis tasks (exploring brain
scans). On the other hand, more general purpose devices can be adapted across
analysis tasks and data. For example, devices with multiple degrees of freedom
(e. g., [11, 37]) allow for a rich input vocabulary while at the same time being
general purpose. Existing mobile devices, such as smartphones and tablets, can
also be appropriated, adapting their content to provide a personalized interface

240 Mark Billinghurst et al.



Fig. 16: A user throws an item and another performs a gesture to make it “fly”
to their location in CoReach [79] (used with permission).

for analysis tasks or personal views of the data [97, 134, 140]. Mobile devices can
also be combined with physical widgets [64] or be customized by analysts, for
example allowing them to sketch their own interfaces based on their analysis
needs [127] (Figure 15). Because these more general purpose personal devices can
be adapted to help analysts perform many tasks, they usually provide limited
awareness of the actions of others. In this case, additional awareness information
needs to be given during collaboration, for example in the form of colored cursors,
or other highlights to indicate the work of others [44]. Mobile devices, dedicated
or general purpose, need to be carried around while analysts are moving, which
could prove cumbersome in long duration analysis tasks.

Touch and other modalities: Physically large immersive environments
often include displays that can act as touch surfaces. In the past researchers
have considered direct touch interaction as input in collaborative analytics on
very large surfaces such as walls [62, 102] or tabletops [60, 91]. Direct touch
provides immediate awareness information about where others are working in
the shared space, and allows for direct manipulation [112]. Nevertheless, touch
technology may not be always available, and even if it is, it requires users to
be close to the display, missing the benefits of movement discussed before. This

Collaborative Immersive Analytics 241



is where multi-modal interaction can prove useful; for example Liu et al. [79]
use a collection of collaboration techniques based on direct touch on a wall and
distant interaction using mobiles. More generally, multimodal interaction can
prove beneficial for collaborative settings for diverse reasons. For example, adding
voice commands on a touch tabletop can enhance the gesture vocabulary and
potentially increase awareness of others’ actions [128]. Or providing personal
devices coupled with the shared interactive surface, allows colleagues to fluidly
move between individual and team work [110], and chose when to share their
individual analysis results with others [85].

Collaborative actions: Co-located collaborative settings also allow the
creation of new collaborative interaction techniques, where users combine their
actions in order to create a single more powerful interactions. Morris et al. [90]
present actions by multiple colleagues that the system interprets as a single
command, in order to pass ownership or trigger global changes for example. Tse
et al. [128] allow colleagues to combine verbal commands on a tabletop to group
objects. Isenberg and Fisher [59] provide collaborative brushing and linking
interactions on tabletop visualizations and Liu et al. [78,79] look at how multiple
collaborators can share actions such as moving objects across different areas on a
wall display (Figure 16).

8.5.2. Synchronous Distributed and Mixed Presence CIA

Synchronous distributed immersive technology, such as HMDs provides ana-
lysts with the ability to chose whether to share exactly the same view or completely
different ones. As with individual analysis in these environments (see Chapter 4),
a rich input vocabulary can be achieved using pointing and virtual menus [15],
more elaborate gestures [6,26,74], or even mobile devices [36]. What is unique
about this setting is that, contrary to physically large environments where aware-
ness of others often comes from seeing the location and actions of others within
a shared physical space, here analysts may be unaware of the focus and view
of their colleagues. In order to maintain awareness of others, interaction needs
to provide feedback not only to the user taking an action, but also to their
colleagues. For example, researchers have proposed the use of avatars to repre-
sent their fellow analysts within the virtual world, starting from very abstract
representation of others [10] to full 3D body models [101]. Analysts’ actions need
also be accompanied by feedback presented to their colleagues, for example in
the form of highlights of their area of focus [38] that can be very fine grained
(e. g., through eye-tracking [42]), and the ability to share views with others [96].

Given the introduction of augmented reality technology, it is possible to
have scenarios where analysts are physically co-located, but have personal views
of their data superimposed on the virtual environment. In these mixed reality
environments, most interactions that are possible in physically large environments
are possible here as well (with the exception possibly of touch interactions on
shared surfaces). Nevertheless, in this context, awareness of actions can prove
problematic. Even if the physical position of analysts and their physical actions
(e. g., gestures) are visible, there is no guarantee that they are transparent to

242 Mark Billinghurst et al.



their colleagues since they are not sharing a common view. This problem is
similar to the awareness challenges present in distributed environments. It may
even be more pronounced in mixed reality settings: in distributed situations,
representations of colleagues can be artificially placed close to their areas of focus;
in a mixed reality setting, we have a strong visual presence of colleagues moving
inside an analyst’s field of vision (but not necessarily sharing a common view),
that may be hard to override in order to represent their virtual focus.

Designing interaction for situations where some colleagues are co-located
and some distributed, i.e., mixed-presence collaboration, presents possibly
the biggest challenge. The analysis environment needs to provide awareness
support for the actions of co-located colleagues, while at the same time providing
representations of the distributed partners and/or their actions. All this, in an
environment that is likely already occupied by large amount of visualized data.

8.5.3. Asynchronous CIA

So far, we have considered only synchronous interaction in collaborative en-
vironments. Nevertheless, analysis tasks can be long, requiring several work
sessions [126], or analysts may work individually but share their analysis later on.
These asynchronous analysis sessions can be done either distributed, or even co-
located (working in shifts). In these case, some interaction challenges are similar
to single user interaction. Nevertheless, there are new problems that arise when
analysts work asynchronously, such as understanding where their colleagues
left-off with their analysis, where they focused on, etc. More general, supporting
hand-off in asynchronous analysis [141], both co-located and distributed, becomes
crucial. Here, we require techniques that leave a trace of the analysis work and
progress of colleagues. These can be explicit, for example annotations or sum-
maries left behind for others [94, 105]; implicit, such as summaries of all past
interactions (as is done in [14]); or a combination of both, where the analysis
system can help the analyst to tell a story [18,34,75, 107] of their work to bring
their colleagues up-to-speed (see Chapter 6).

8.6. Opportunities for Research

Collaborative Immersive Analytics is an emerging research field and it is still
underexplored. As it is challenging to identify a pertinent path to explore in CIA,
this section aims to provide a non-exclusive list of important research questions
to guide research in this new domain. Previous work has identified important
characteristics of Collaborative Visual Analytics. Heer [48] identified the following
topics to explore:

– Division and allocation of work
– Common ground and awareness
– Reference and deixis 1

1 Deixis refers to words and phrases, such as “me” or “here”, that cannot be fully
understood without additional contextual information. Source: Wikipedia.

Collaborative Immersive Analytics 243



– Incentives and engagement
– Identity, trust, and reputation
– Group dynamics
– Consensus and decision making

Virtual/Augmented/Mixed reality, wall-size displays and tabletops can lead
to more immersion and presence. The nature and affordances of such immersive
environments may have an influence on how a group of users work together.
This chapter presents a set of questions related to the space/time matrix and
important topics previously identified in Collaborative Visual Analytics, with
respect to immersive environments.

8.6.1. Virtual reality for Immersive Collaborative Analytics

CAVE-style setups and large display or multi-display environments are a natural
space for collaboration. CAVE-style setups are rooms with screens surrounding
the users. Collaborators view and talk to each other, and can, for example have
face-to-face discussions. Collaboration in Virtual Reality can also be alternatively
supported by networked head-mounted displays, or fishtanks. Cordeil et al. [28]
found that performances were comparable between CAVE-style displays (Figure
17) and HMDs (Figure 18), but raised some usability issues for collaboration.
However, there are other Virtual Reality displays that need to be compared for
collaboration. Hence it is important to conduct research with different display
form factors.

8.6.2. HMDs and face-to-face communication

While HMDs offer an alternative to wall displays and other CAVE-style environ-
ments, they obscure the face of collaborators. This can potentially be a major
impairment as facial expressions are used as visual cues for different reasons,
including establishing a common ground or assessing and discussing findings [48].
The study of Cordeil et al. [28] did not reveal a major issue with this impairment
but they tested only low-level graph visualization tasks and did not test analyti-
cal reasoning. In this condition, using only conversation and gaze was probably
sufficient to complete the tasks. An interesting opportunity for research is to
measure the effect of not seeing the face expressions of collaborators:

– When they are sharing insights: how does it affect engagement?
– On the group dynamics: how does it affect leadership?

8.6.3. CIA and scale: number of users

Research has often studied collaborative tasks with head-mounted displays or
shared large displays, involving only two collaborators. In real-life applications,
collaboration potentially involves larger team sizes. There is a compelling case to
study the impact of using both HMDs and shared displays, such as walls and
tabletops, with large groups in terms of situation awareness. In order to design

244 Mark Billinghurst et al.



and develop future techniques, systems and platforms that support immersive and
collaborative analytics, it is crucial to study how different group sizes collaborate
with visualizations. A possible (non-restricted) list of research questions related
to group size, could be:

– How do collaborators locate each other’s positions in collaborative virtual
environments?

– How do collaborators share insights with others (either in virtual or collo-
cated environments)?

– How do collaborators share/find each other’s cues?

8.6.4. Distant Collaborative Immersive Analytics

Little research has been focused on remote collaboration using immersive display
technologies. Some studies have been carried out in the scientific visualization
domain [31] and the robotics domain [73]. Those studies often investigate the
ability of users to perform their tasks in such environments, and often use
artificial or abstract tasks such as puzzle solving. However, collaborative analytics
requires a higher level of engagement from collaborators when analyzing and
understanding data. The study of engagement of team members in remote,
immersive and collaborative data analytics remains widely unexplored. To this
regard, the following research questions should be investigated:

– Does immersive and collaborative analytics bridge the distance gap in
remote collaboration scenarios?

– What is the impact of immersion in collaborative analytics quality?
– What is the effect of immersion on remote collaboration: engagement, group

dynamics?

8.6.5. The design space of Immersive and Collaborative Analytics

Immersive display technology is very heterogeneous. It includes head-mounted
displays, CAVE-style environments, wall-sized displays, tabletops, and multi-
display environments. Those displays also vary widely in size, resolution, and
modality of interaction. Choosing which technology is more appropriate for the
given tasks and needs of analysts, and designing for them, is a challenge. There is a
great opportunity for research to define the design space of collaborative immersive
analytics in order to guide the design of immersive collaborative platforms.
Moreover, we can envision situations where a mix of these technologies can come
in play, for example a group of analysts working in a CAVE and a colleague
connecting remotely using a HMD. Understanding how collaborative analysis is
affected by analysts using asymmetric technology (in terms of awareness, deixis,
group dynamics), and how to design for it, is an important research direction.

8.6.6. Asynchronous collaboration

Collaboration can occur asynchronously for various reasons: collaborators work in
shifts, in different time zones, they have different schedules, etc. In this situation,

Collaborative Immersive Analytics 245



users view, analyze and understand data at different moments in time. We can
identify two roles:

– A “hand over” role: users who have finished working with the visualizations
– A “take over” role: users who come after the previous group to keep working

with the data visualization.
It is important to understand what the users’ needs in those two roles are

and how immersion can be used to better support hand-overs.
On one end, immersive technology can help preparing high fidelity handovers

to communicate the results of data analysis. There are various ways to create
a handover for the next users. For example, motion-tracking technology can be
used to record precise hand gestures and interactions with the visualizations, and
stereo cameras can record a user’s body and face. Such recording can be used to
facilitate telling a story (Chapter 6) about the work done with the visualizations.
However, telling a story is only one form of asynchronous collaboration handover.
A group of users may require the help and draw the attention of another user or a
group of users to examine a specific issue. On the other end, a user who enters a
collaborative space needs to be updated with the situation. The tasks of this user
can be to consult the handover, resolve issues, answer questions, provide input for
a specific problem, etc. In the context of Collaborative Immersive Analytics, it is
unclear which techniques are better suited for specific scenarios. Hence, there is a
research opportunity to study how to convey the state of progress of collaborative
visualization in immersive and asynchronous collaborative analytics.

8.6.7. Channels for collaboration

Collaboration between members of a team is supported by various channels of
communication. Those channels include (but are not limited to) oral communi-
cation, gaze direction, deixis (e. g. pointing) or feedback from other members’
actions. Since CIA can simultaneously occur across different types of immersive
technology, it seems important to evaluate, according to the users’ tasks, which
are the essential channels to support efficient group work.

8.6.8. Evaluation of CIA systems

The evaluation of CIA systems, as with all collaborative visual analysis systems
is a challenging matter, as it needs to consider both task-related and team-
work related aspects [56]. Because collaborative environments incorporate a
large number of variables to consider, they are often evaluated through field
and laboratory observation studies [56]. The variables to consider increase in
immersive environments, as colleagues may have asymmetric collaboration, be
joining the work with different immersive technology, making evaluation even
more challenging. Recent years, have also seen an increase in controlled experiment
evaluations in collaborative immersive environments (e. g, [79], [102], [20], [78]),
where researchers try to tease out effects of particular factors, including immersive
technology. For example, Cordeil et al. [28] compared CAVE environments to
Head-mounted displays in a graph analysis task (Fig. 18,17). Clearly, it is a mix of

246 Mark Billinghurst et al.



Fig. 17: Two users visualize a 3D network of abstract data in the CAVE2. The
3D immersive visualization can only be presented from the user wearing the
head-tracker (left) [28] (used with permission).

Fig. 18: Two users visualize a 3D network of abstract data in a network application
with head-mounted displays. This platform allows independent head-tracking.
Gaze cues and interactions are communicated on the network [28].

both open-ended and controlled evaluations that can provide practitioners with
holistic insights. Hence, it is important to build evaluation frameworks for CIA,
based on subjective, objective and collective measures to quantify the efficiency
of collaboration. Such frameworks can certainly be built upon existing ones, for
example ones coming from social presence [41].

8.7. Conclusion

This chapter introduced the concept of Collaborative Immersive Analytics (CIA),
defined as the shared use of new immersive interaction and display technologies
by more than one person, to support collaborative analytical reasoning and
decision making. As shown in the introduction, CIA is at the intersection of
the fields of Mixed Reality, Visual Analytics and CSCW and as such there are
many technologies and approaches available for building CIA systems. These

Collaborative Immersive Analytics 247



were classified according to a space-time taxonomy, and examples of systems in
each quadrantof the taxonomy were provided.

From these examples we see some of the benefits of CIA systems, including
enabling teams to work together, using emerging immersive technologies for
intuitive data explorations, supporting multiple devices and tools, and seeing
multiple representations of data. Perhaps most importantly CIA systems have
the potential to enable teams to overcome the barriers of distance and time to
collaborate together in the most effective way possible.

To fully realize this potential CIA systems must be carefully designed. The
chapter covers the different roles collaborators have in such systems, such as
viewing, interacting and sharing which should be accommodated in the system
development. It also reviews the types of input modalities that could be used
to design intuitive interaction. These include devices for capturing pointing
and gestures, dedicated handheld devices, support for touch, and the use of
collaborative actions involving input from multiple people.

While the individual domains of visual analytics, immersion, and computer
mediated collaboration each have a long and rich research history, their combina-
tion that gives rise to CIA is very new and so there are significant opportunities
for future work. As discussed, fewer than three percent of the papers published
in the main visualization conferences even address collaboration. The chapter
conclusion identifies a number of topics for research including using VR for
CIA, developing methods for distant CIA, exploring the CIA design space, asyn-
chronous collaboration, and methods for evaluating CIA systems. Perhaps most
importantly is research in the area of multi-user interaction. Interaction modali-
ties that may work well in immersive environments in general, are not necessarily
appropriate for complex collaborative analysis work, that may last over several
sessions, with participants connecting using different technologies. If and how
technological asymmetry between colleagues can affect the quality of analysis
work is still an open question.

Overall, Collaborative Immersive Analytics is an exciting new field with
significant potential for improving how teams problem-solve, and presents many
opportunities for ongoing research. CIA is at the same point that the field of
visualization was at twenty years ago, and in the same way that visualization
has become a key tool in the years since, we expect that CIA will dramatically
change, and challenge, problem solving in the future.

References

1. Anslow, C.: Reflections on collaborative software visualization in co-located environ-
ments. In: IEEE International Conference on Software Maintenance and Evolution
(ICSME). pp. 645–650. IEEE (2014)

2. Anwar, A., Klein, B., Berger, M., Arisona, S.M.: Value Lab Asia: A space for physical
and virtual interdisciplinary research and collaboration. In: 19th International
Conference on Information Visualisation (IV). pp. 348–353. IEEE (2015)

3. Arglasses(2016), http://arglassesguide.com/

248 Mark Billinghurst et al.

http://arglassesguide.com/
http://arglassesguide.com/
http://arglassesguide.com/
http://arglassesguide.com/
http://arglassesguide.com/
http://arglassesguide.com/
http://arglassesguide.com/
http://arglassesguide.com/


4. Argelaguet Sanz, F., Andujar, C.: A survey of 3D object selection techniques
for virtual environments. Computers and Graphics 37(3), 121–136(2013), https:
//hal.archives-ouvertes.fr/hal-00907787

5. Badam, S.K., Elmqvist, N.: Polychrome: A cross-device framework for collaborative
web visualization. In: Proceedings of the Ninth ACM International Conference on
Interactive Tabletops and Surfaces. pp. 109–118. ACM (2014)

6. Bai, H., Lee, G., Billinghurst, M.: Free-hand gesture interfaces for an augmented
exhibition podium. In: Proceedings of the Annual Meeting of the Australian Special
Interest Group for Computer Human Interaction. pp. 182–186. OzCHI ’15, ACM,
New York, NY, USA (2015)

7. Ballendat, T., Marquardt, N., Greenberg, S.: Proxemic interaction: Designing for a
proximity and orientation-aware environment. In: ACM International Conference
on Interactive Tabletops and Surfaces. pp. 121–130. ACM, New York, NY, USA
(2010)

8. Belcher, D., Billinghurst, M., Hayes, S., Stiles, R.: Using augmented reality for
visualizing complex graphs in three dimensions. In: Proceedings of the Second
IEEE and ACM International Symposium on Mixed and Augmented Reality. pp.
84–93. IEEE (2003)

9. Benbunan-Fich, R., Hiltz, S.R., Turoff, M.: A comparative content analysis of
face-to-face vs. asynchronous group decision making. Decision Support Systems
34(4), 457–469 (2003)

10. Benford, S., Fahlén, L.: A spatial model of interaction in large virtual environments.
In: Proceedings of the Third Conference on European Conference on Computer-
Supported Cooperative Work. pp. 109–124. ECSCW’93, Kluwer Academic Pub-
lishers, Norwell, MA, USA (1993)

11. Benko, H., Holz, C., Sinclair, M., Ofek, E.: NormalTouch and TextureTouch:
High-fidelity 3D haptic shape rendering on handheld virtual reality controllers.
In: Proceedings of the 29th Annual Symposium on User Interface Software and
Technology. pp. 717–728. UIST ’16, ACM, New York, NY, USA (2016)

12. Benko, H., Ishak, E.W., Feiner, S.: Collaborative mixed reality visualization of an
archaeological excavation. In: Proceedings of the 3rd IEEE/ACM International
Symposium on Mixed and Augmented Reality. pp. 132–140. IEEE Computer Society
(2004)

13. Bezerianos, A., Isenberg, P.: Perception of visual variables on tiled wall-sized displays
for information visualization applications. IEEE Transactions on Visualization and
Computer Graphics 18(12), 2516–2525 (2012)

14. Bezerianos, A., Dragicevic, P., Balakrishnan, R.: Mnemonic rendering: An image-
based approach for exposing hidden changes in dynamic displays. In: Proceedings
of the 19th Annual ACM Symposium on User Interface Software and Technology.
pp. 159–168. UIST ’06, ACM, New York, NY, USA (2006)

15. Bowman, D.A., Wingrave, C.A.: Design and evaluation of menu systems for immer-
sive virtual environments. In: Proceedings of the Virtual Reality 2001 Conference
(VR’01). pp. 149–. VR ’01, IEEE Computer Society (2001)

16. Bradel, L., Endert, A., Koch, K., Andrews, C., North, C.: Large high resolution
displays for co-located collaborative sensemaking: Display usage and territoriality.
International Journal of Human-Computer Studies 71(11), 1078–1088 (2013)

17. Card, S.K., Mackinlay, J.D., Shneiderman, B.: Readings in Information Visualiza-
tion: Using Vision to Think. Morgan Kaufmann (1999)

18. Carpendale, S., Diakopoulos, N., Henry Riche, N., Hurter, C.: Data-Driven
Storytelling. Dagstuhl Reports 6(2), 1–27(2016), https://hal-enac.archives-
ouvertes.fr/hal-01348422

Collaborative Immersive Analytics 249

https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal.archives-ouvertes.fr/hal-00907787
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422
https://hal-enac.archives-ouvertes.fr/hal-01348422


19. Carter, S., Mankoff, J., Goddi, P.: Building connections among loosely coupled
groups: Hebb’s rule at work. Computer Supported Cooperative Work (CSCW)
13(3-4), 305–327 (2004)

20. Chandler, T., Cordeil, M., Czauderna, T., Dwyer, T., Glowacki, J., Goncu, C.,
Klapperstueck, M., Klein, K., Marriott, K., Schreiber, F., et al.: Immersive analytics.
In: Big Data Visual Analytics (BDVA), 2015. pp. 1–8. IEEE (2015)

21. Chen, W., Ladeveze, N., Clavel, C., Mestre, D., Bourdot, P.: User cohabitation in
multi-stereoscopic immersive virtual environment for individual navigation tasks.
In: 2015 IEEE Virtual Reality (VR). pp. 47–54 (2015)

22. Chen, Y., Alsakran, J., Barlowe, S., Yang, J., Zhao, Y.: Supporting effective
common ground construction in asynchronous collaborative visual analytics. In:
IEEE Conference on Visual Analytics Science and Technology (VAST). pp. 101–110.
IEEE (2011)

23. Chung, H., North, C., Self, J.Z., Chu, S., Quek, F.: VisPorter: facilitating infor-
mation sharing for collaborative sensemaking on multiple displays. Personal and
Ubiquitous Computing 18(5), 1169–1186 (2014)

24. Churchill, E.F., Snowdon, D.N., Munro, A.J.: Collaborative virtual environments:
digital places and spaces for interaction. Springer Science & Business Media (2012)

25. Clark, H.H., Brennan, S.E., et al.: Grounding in communication. Perspectives on
Socially Shared Cognition 13(1991), 127–149 (1991)

26. Colaço, A., Kirmani, A., Yang, H.S., Gong, N.W., Schmandt, C., Goyal, V.K.:
Mime: Compact, low power 3D gesture sensing for interaction with head mounted
displays. In: Proceedings of the 26th Annual ACM Symposium on User Interface
Software and Technology. pp. 227–236. UIST ’13, ACM, New York, NY, USA
(2013)

27. Cook, K.A., Thomas, J.J.: Illuminating the path: The research and development
agenda for visual analytics. Tech. rep., Pacific Northwest National Laboratory
(PNNL), Richland, WA (US) (2005)

28. Cordeil, M., Dwyer, T., Klein, K., Laha, B., Marriott, K., Thomas, B.H.: Immersive
collaborative analysis of network connectivity: CAVE-style or head-mounted dis-
play? IEEE Transactions on Visualization and Computer Graphics 23(1), 441–450
(2017)

29. Craig, P., Huang, X., Chen, H., Wang, X., Zhang, S.: Pervasive information visual-
ization: Toward an information visualization design methodology for multi-device
co-located synchronous collaboration. In: IEEE International Conference on Com-
puter and Information Technology; Ubiquitous Computing and Communications;
Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Com-
puting (CIT/IUCC/DASC/PICOM). pp. 2232–2239. IEEE (2015)

30. Cruz-Neira, C., Sandin, D.J., DeFanti, T.A.: Surround-screen projection-based
virtual reality: the design and implementation of the CAVE. In: Proceedings of the
20th Annual Conference on Computer Graphics and Interactive Techniques. pp.
135–142. ACM (1993)

31. Donalek, C., Djorgovski, S.G., Cioc, A., Wang, A., Zhang, J., Lawler, E., Yeh, S.,
Mahabal, A., Graham, M., Drake, A., et al.: Immersive and collaborative data
visualization using virtual reality platforms. In: IEEE International Conference on
Big Data (Big Data). pp. 609–614. IEEE (2014)

32. Dourish, P., Bellotti, V.: Awareness and coordination in shared workspaces.
In: Proceedings of the 1992 ACM Conference on Computer-supported Coop-
erative Work. pp. 107–114. CSCW ’92, ACM, New York, NY, USA(1992),
http://doi.acm.org/10.1145/143457.143468

250 Mark Billinghurst et al.

http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468
http://doi.acm.org/10.1145/143457.143468


33. Edelson, D., Pea, R., Gomez, L., et al.: Constructivism in the collaboratory. Con-
structivist Learning Environments: Case Studies in Instructional Design 151 (1996)

34. Elias, M., Aufaure, M.A., Bezerianos, A.: Storytelling in visual analytics tools for
business intelligence. In: IFIP Conference on Human-Computer Interaction. pp.
280–297. Springer (2013)

35. Ellis, S.E., Groth, D.P.: A collaborative annotation system for data visualization.
In: Proceedings of the Working Conference on Advanced Visual Interfaces. pp.
411–414. ACM (2004)

36. Feiner, S., Macintyre, B., Höllerer, T.: A touring machine: Prototyping 3D mobile
augmented reality systems for exploring the urban environment. In: Personal
Technologies 1(4). pp. 74–81 (1997)

37. Flystick: http://www.ar-tracking.com/products/interaction/flystick2/ ([Online; ac-
cessed 05-Feb-2017])

38. Fraser, M., Benford, S., Hindmarsh, J., Heath, C.: Supporting awareness and
interaction through collaborative virtual interfaces. In: Proceedings of the 12th
Annual ACM Symposium on User Interface Software and Technology. pp. 27–36.
UIST ’99, ACM, New York, NY, USA (1999)

39. Gjerlufsen, T., Klokmose, C.N., Eagan, J., Pillias, C., Beaudouin-Lafon, M.: Shared
substance: Developing flexible multi-surface applications. In: Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems. pp. 3383–3392.
CHI ’11, ACM, New York, NY, USA (2011)

40. Greenberg, S., Rounding, M.: The notification collage: posting information to
public and personal displays. In: Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. pp. 514–521. ACM (2001)

41. Gunawardena, C.N., Zittle, F.J.: Social presence as a predictor of satisfaction within
a computer-mediated conferencing environment. American Journal of Distance
Education 11(3), 8–26 (1997)

42. Gupta, K., Lee, G.A., Billinghurst, M.: Do you see what I see? the effect of gaze
tracking on task space remote collaboration. IEEE Transactions on Visualization
and Computer Graphics 22(11), 2413–2422 (2016)

43. Gutwin, C., Greenberg, S.: Design for individuals, design for groups: tradeoffs be-
tween power and workspace awareness. In: Proceedings of the 1998 ACM Conference
on Computer Supported Cooperative Work. pp. 207–216. ACM (1998)

44. Gutwin, C., Greenberg, S.: A descriptive framework of workspace awareness for
real-time groupware. Computer Supported Cooperative Work 11(3), 411–446 (2002)

45. Hackathorn, R., Margolis, T.: Immersive analytics: Building virtual data worlds for
collaborative decision support. In: 2016 Workshop on Immersive Analytics (IA).
pp. 44–47(March 2016) doi: 10.1109/IMMERSIVE.2016.7932382

46. Hauber, J., Regenbrecht, H., Billinghurst, M., Cockburn, A.: Spatiality in video-
conferencing: trade-offs between efficiency and social presence. In: Proceedings of
the 2006 20th Anniversary Conference on Computer Supported Cooperative Work.
pp. 413–422. ACM (2006)

47. Heath, C., Luff, P.: Disembodied conduct: communication through video in a
multi-media office environment. In: Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems. pp. 99–103. ACM (1991)

48. Heer, J., Agrawala, M.: Design considerations for collaborative visual analytics.
Information Visualization 7(1), 49–62 (2008)

49. Heer, J., Viégas, F.B., Wattenberg, M.: Voyagers and voyeurs: supporting asyn-
chronous collaborative information visualization. In: Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems. pp. 1029–1038. ACM (2007)

Collaborative Immersive Analytics 251

http://www.ar-tracking.com/products/interaction/flystick2/
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
http://www.ar-tracking.com/products/interaction/flystick2/
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382
https://doi.org/10.1109/IMMERSIVE.2016.7932382


50. Hendrix, T.D., Cross II, J.H., Maghsoodloo, S., McKinney, M.L.: Do visualizations
improve program comprehensibility? experiments with control structure diagrams
for java. In: ACM SIGCSE Bulletin. vol. 32, pp. 382–386. ACM (2000)

51. Hill, G.W.: Group versus individual performance: Are N+ 1 heads better than one?
Psychological Bulletin 91(3), 517 (1982)

52. Hiltz, S.R.: The virtual classroom: Learning without limits via computer networks.
Intellect Books (1994)

53. Hutterer, P., Close, B.S., Thomas, B.H.: Supporting mixed presence groupware
in tabletop applications. In: First IEEE International Workshop on Horizontal
Interactive Human-Computer Systems (TableTop 2006). pp. 8–pp. IEEE (2006)

54. Imai, T., Johnson, A.E., Leigh, J., Pape, D.E., DeFanti, T.A.: The virtual mail
system. In: Proceedings of IEEE Virtual Reality. p. 78. IEEE (1999)

55. Imai, T., Qiu, Z., Behara, S., Tachi, S., Aoyama, T., Johnson, A., Leigh, J.: Over-
coming time-zone differences and time management problem with tele-immersion.
In: The Proceedings of INET. pp. 18–21 (2000)

56. Isenberg, P., Bertini, E., Lam, H., Plaisant, C., Carpendale, S.: Empirical studies
in information visualization: Seven scenarios. IEEE Transactions on Visualization
& Computer Graphics 18, 1520–1536 (2012)

57. Isenberg, P., Bezerianos, A., Henry, N., Carpendale, S., Fekete, J.D.: CoCoNutTrix:
Collaborative retrofitting for information visualization. IEEE Computer Graphics
and Applications 29(5), 44–57(2009), https://hal.inria.fr/hal-00690020

58. Isenberg, P., Elmqvist, N., Scholtz, J., Cernea, D., Ma, K.L., Hagen, H.: Col-
laborative visualization: definition, challenges, and research agenda. Information
Visualization 10(4), 310–326 (2011)

59. Isenberg, P., Fisher, D.: Collaborative brushing and linking for co-located visual
analytics of document collections. In: Proceedings of the 11th Eurographics / IEEE
- VGTC Conference on Visualization. pp. 1031–1038. EuroVis’09, The Eurographs
Association &#38; John Wiley &#38; Sons, Ltd., Chichester, UK (2009)

60. Isenberg, P., Fisher, D., Ringel Morris, M., Inkpen, K., Czerwinski, M.: An ex-
ploratory study of co-located collaborative visual analytics around a tabletop display.
In: Proceedings of Visual Analytics Science and Technology (VAST). pp. 179–186.
IEEE, Salt Lake City, UT, United States(2010), https://hal.inria.fr/inria-00587236,
received an honorable mention at VAST 2010

61. Ishii, H., Kobayashi, M., Arita, K.: Iterative design of seamless collaboration media.
Communications of the ACM 37(8), 83–97 (1994)

62. Jakobsen, M.R., Hornbaek, K.: Up close and personal: Collaborative work on a
high-resolution multitouch wall display. ACM Transactions on Computer-Human
Interaction 21(2), 11:1–11:34 (2014)

63. Jakobsen, M.R., Sahlemariam Haile, Y., Knudsen, S., Hornbæk, K.: Information
visualization and proxemics: Design opportunities and empirical findings. IEEE
Transactions on Visualization and Computer Graphics 19(12), 2386–2395 (2013)

64. Jansen, Y., Dragicevic, P., Fekete, J.D.: Tangible remote controllers for wall-
size displays. In: Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems. pp. 2865–2874. CHI ’12, ACM, New York, NY, USA (2012)

65. Johansen, R.: Groupware: Computer support for business teams. The Free Press
(1988)

66. Johnson, A., Leigh, J.: Tele-immersive collaboration in the CAVE research network.
In: Collaborative Virtual Environments. pp. 225–243. Springer (2001)

67. Johnson, A., Roussos, M., Leigh, J., Vasilakis, C., Barnes, C., Moher, T.: The nice
project: Learning together in a virtual world. In: Proceedings of the IEEE Virtual
Reality Annual International Symposium. pp. 176–183. IEEE (1998)

252 Mark Billinghurst et al.

https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/hal-00690020
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236
https://hal.inria.fr/inria-00587236


68. Johnson, C.: Top scientific visualization research problems. IEEE Computer Graph-
ics and Applications 24(4), 13–17 (2004)

69. Keates, S., Robinson, P.: The use of gestures in multimodal input. In: Proceedings
of the Third International ACM Conference on Assistive Technologies. pp. 35–42.
Assets ’98, ACM, New York, NY, USA (1998)

70. Kim, K., Javed, W., Williams, C., Elmqvist, N., Irani, P.: Hugin: A framework for
awareness and coordination in mixed-presence collaborative information visualiza-
tion. In: ACM International Conference on Interactive Tabletops and Surfaces. pp.
231–240. ACM (2010)

71. Kinnect(2016), https://developer.microsoft.com/en-us/windows/kinect
72. Klapperstueck, M., Czauderna, T., Goncu, C., Glowacki, J., Dwyer, T., Schreiber,

F., Marriott, K.: ContextuWall: Multi-site collaboration using display walls. Journal
of Visual Languages and Computing 46, 35 – 42(2018) doi: 10.1016/j.jvlc.2017.10.
002

73. Kratz, S., Ferriera, F.R.: Immersed remotely: Evaluating the use of head mounted
devices for remote collaboration in robotic telepresence. In: 2016 25th IEEE
International Symposium on Robot and Human Interactive Communication (RO-
MAN). IEEE (aug 2016)

74. Leapmotion(2016), https://www.leapmotion.com/
75. Lee, B., Henry Riche, N., Isenberg, P., Carpendale, S.: More than telling a story:

A closer look at the process of transforming data into visually shared stories. IEEE
Computer Graphics and Applications 35(5), 84–90 (2015)

76. Leigh, J., Brown, M.D.: Cyber-commons: merging real and virtual worlds. Commu-
nications of the ACM 51(1), 82–85 (2008)

77. Leigh, J., Johnson, A.E., Brown, M., Sandin, D.J., DeFanti, T.A.: Visualization in
teleimmersive environments. Computer 32(12), 66–73 (1999)

78. Liu, C., Chapuis, O., Beaudouin-Lafon, M., Lecolinet, E.: Shared interaction
on a wall-sized display in a data manipulation task. In: Proceedings of the 34th
International Conference on Human Factors in Computing Systems. pp. 2075–2086.
CHI ’16, ACM (2016)

79. Liu, C., Chapuis, O., Beaudouin-Lafon, M., Lecolinet, E.: CoReach: Cooperative
gestures for data manipulation on wall-sized displays. In: ACM (ed.) Proceedings of
the 35th International Conference on Human Factors in Computing Systems. CHI
’17, Denver, United States(2017), https://hal.archives-ouvertes.fr/hal-01437091

80. Mahyar, N., Sarvghad, A., Tory, M.: Note-taking in co-located collaborative visual
analytics: Analysis of an observational study. Information Visualization 11(3),
190–204 (2012)

81. Mahyar, N., Tory, M.: Supporting communication and coordination in collaborative
sensemaking. IEEE Transactions on Visualization and Computer Graphics 20(12),
1633–1642 (2014)

82. Marai, G.E., Forbes, A.G., Johnson, A.: Interdisciplinary immersive analytics at
the electronic visualization laboratory: Lessons learned and upcoming challenges
(2018)

83. Mark, G., Carpenter, K., Kobsa, A.: Are there benefits in seeing double? A study of
collaborative information visualization. In: CHI’03 Extended Abstracts on Human
Factors in Computing Systems. pp. 840–841. ACM (2003)

84. McCarthy, J.F., Costa, T.J., Liongosari, E.S.: Unicast, outcast & groupcast: Three
steps toward ubiquitous, peripheral displays. In: International Conference on Ubiq-
uitous Computing. pp. 332–345. Springer (2001)

Collaborative Immersive Analytics 253

https://developer.microsoft.com/en-us/windows/kinect
https://developer.microsoft.com/en-us/windows/kinect
https://developer.microsoft.com/en-us/windows/kinect
https://developer.microsoft.com/en-us/windows/kinect
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/10.1016/j.jvlc.2017.10.002
https://www.leapmotion.com/
https://www.leapmotion.com/
https://www.leapmotion.com/
https://www.leapmotion.com/
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://developer.microsoft.com/en-us/windows/kinect
https://developer.microsoft.com/en-us/windows/kinect
https://developer.microsoft.com/en-us/windows/kinect
https://developer.microsoft.com/en-us/windows/kinect
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/10.1016/j.jvlc.2017.10.002
https://doi.org/10.1016/j.jvlc.2017.10.002
https://www.leapmotion.com/
https://www.leapmotion.com/
https://www.leapmotion.com/
https://www.leapmotion.com/
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091
https://hal.archives-ouvertes.fr/hal-01437091


85. McGrath, W., Bowman, B., McCallum, D., Hincapié-Ramos, J.D., Elmqvist, N.,
Irani, P.: Branch-explore-merge: Facilitating real-time revision control in collabora-
tive visual exploration. In: Proceedings of the 2012 ACM International Conference
on Interactive Tabletops and Surfaces. pp. 235–244. ITS ’12, ACM, New York, NY,
USA (2012)

86. Milgram, P., Kishino, F.: A taxonomy of mixed reality visual displays. IEICE
Transactions on Information and Systems 77(12), 1321–1329 (1994)

87. Millán, J.d.R.: Brain-computer interfaces. Tech. rep., The MIT Press (2002)
88. Moraes, A.C., Eler, D.M., Brega, J.R.: Collaborative information visualization using

a multi-projection system and mobile devices. In: 18th International Conference on
Information Visualisation (IV). pp. 71–77. IEEE (2014)

89. Morozov, M., Gerasimov, A., Fominykh, M.: vAcademia–educational virtual world
with 3D recording. In: 2012 International Conference on Cyberworlds (CW). pp.
199–206. IEEE (2012)

90. Morris, M.R., Huang, A., Paepcke, A., Winograd, T.: Cooperative gestures: Multi-
user gestural interactions for co-located groupware. In: Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems. pp. 1201–1210. CHI ’06,
ACM, New York, NY, USA (2006)

91. Morris, M.R., Lombardo, J., Wigdor, D.: Wesearch: Supporting collaborative search
and sensemaking on a tabletop display. In: Proceedings of the 2010 ACM Conference
on Computer Supported Cooperative Work. pp. 401–410. CSCW ’10, ACM, New
York, NY, USA (2010)

92. Nacenta, M.A., Sallam, S., Champoux, B., Subramanian, S., Gutwin, C.: Per-
spective Cursor: Perspective-based interaction for multi-display environments. In:
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
pp. 289–298. CHI ’06, ACM, New York, NY, USA (2006)

93. Nancel, M., Pietriga, E., Chapuis, O., Beaudouin-Lafon, M.: Mid-air pointing on
ultra-walls. ACM Transactions on Computer-Human Interaction 22(5), 62 pages
(2015)

94. Nassani, A., Bai, H., Lee, G., Billinghurst, M.: Tag It!: AR annotation using
wearable sensors. In: SIGGRAPH Asia 2015 Mobile Graphics and Interactive
Applications. pp. 12:1–12:4. SA ’15, ACM, New York, NY, USA (2015)

95. Nature specials “big data” (2008)
96. Nguyen, T.T.H., Duval, T., Fleury, C.: Guiding Techniques for Collaborative

Exploration in Multi-Scale Shared Virtual Environments. In: GRAPP Interna-
tional Conference on Computer Graphics Theory and Applications. pp. 327–336.
Barcelona, Spain(2013), https://hal.archives-ouvertes.fr/hal-00755313

97. Olwal, A., Frykholm, O., Groth, K., Moll, J.: Design and evaluation of interac-
tion technology for medical team meetings. In: Human-Computer Interaction –
INTERACT 2011. pp. 505–522. Springer Berlin Heidelberg (2011)

98. Pauchet, A., Coldefy, F., Lefebvre, L., Picard, S., Bouguet, A., Perron, L., Guerin,
J., Corvaisier, D., Collobert, M.: Mutual awareness in collocated and distant
collaborative tasks using shared interfaces. Human-Computer Interaction–Interact
2007 pp. 59–73 (2007)

99. Poole, A., Ball, L.J.: Eye tracking in hci and usability research. Encyclopedia of
Human Computer Interaction 1, 211–219 (2006)

100. Pousman, Z., Stasko, J.: A taxonomy of ambient information systems: four patterns
of design. In: Proceedings of the Working Conference on Advanced Visual Interfaces.
pp. 67–74. ACM (2006)

254 Mark Billinghurst et al.

https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313
https://hal.archives-ouvertes.fr/hal-00755313


101. Prince, S., Cheok, A.D., Farbiz, F., Williamson, T., Johnson, N., Billinghurst, M.,
Kato, H.: 3DD Live: Real time interaction for mixed reality. In: Proceedings of the
2002 ACM Conference on Computer Supported Cooperative Work. pp. 364–371.
CSCW ’02, ACM, New York, NY, USA (2002)

102. Prouzeau, A., Bezerianos, A., Chapuis, O.: Evaluating multi-user selection for
exploring graph topology on wall-displays. IEEE Transactions on Visualization
and Computer Graphics p. 14 pages(2016), https://hal.archives-ouvertes.fr/hal-
01348578

103. Regenbrecht, H., Haller, M., Hauber, J., Billinghurst, M.: Carpeno: interfacing
remote collaborative virtual environments with table-top interaction. Virtual Reality
10(2), 95–107 (2006)

104. Rice, R.E.: Computer-mediated communication and organizational innovation.
Journal of Communication 37(4), 65–94 (1987)

105. Robinson, A.C.: Collaborative synthesis of visual analytic results. In: 2008 IEEE
Symposium on Visual Analytics Science and Technology. pp. 67–74 (2008)

106. Robinson, P., Tuddenham, P.: Distributed tabletops: Supporting remote and
mixed-presence tabletop collaboration. In: Second Annual IEEE International
Workshop on Horizontal Interactive Human-Computer Systems (TABLETOP’07).
pp. 19–26. IEEE (2007)

107. Satyanarayan, A., Heer, J.: Authoring narrative visualizations with ellipsis. Com-
puter Graphics Forum 33(3), 361–370 (2014)

108. Sawyer, S., Farber, J., Spillers, R.: Supporting the social processes of software
development. Information Technology & People 10(1), 46–62 (1997)

109. Schmalstieg, D., Fuhrmann, A., Hesina, G., Szalavári, Z., Encarnaçao, L.M., Ger-
vautz, M., Purgathofer, W.: The studierstube augmented reality project. Presence:
Teleoperators and Virtual Environments 11(1), 33–54 (2002)

110. Seifert, J., Simeone, A., Schmidt, D., Holleis, P., Reinartz, C., Wagner, M.,
Gellersen, H., Rukzio, E.: Mobisurf: Improving co-located collaboration through
integrating mobile devices and interactive surfaces. In: Proceedings of the 2012
ACM International Conference on Interactive Tabletops and Surfaces. pp. 51–60.
ITS ’12, ACM, New York, NY, USA (2012)

111. Shaffer, E., Reed, D.A., Whitmore, S., Schaeffer, B.: Virtue: Performance visual-
ization of parallel and distributed applications. Computer 32(12), 44–51 (1999)

112. Shneiderman, B.: Direct manipulation: A step beyond programming languages.
Computer 16(8), 57–69 (1983)

113. Skog, T., Ljungblad, S., Holmquist, L.E.: Between aesthetics and utility: designing
ambient information visualizations. In: IEEE Symposium on Information Visualiza-
tion (INFOVIS 2003). pp. 233–240. IEEE (2003)

114. Snowdon, D., Grasso, A.: Diffusing information in organizational settings: learning
from experience. In: Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems. pp. 331–338. ACM (2002)

115. Soares, A.G.M., dos Santos, C.G.R., Mendonça, S.D.P., Carneiro, N.J.S., Miranda,
B.P., de Araújo, T.D.O., de Freitas, A.A., de Morais, J.M., Meiguins, B.S.: A
review of ways and strategies on how to collaborate in information visualization
applications. In: 20th International Conference on Information Visualisation (IV).
pp. 81–87. IEEE (2016)

116. Sole, D., Wilson, D.: Storytelling in organizations: The power and traps of using
stories to share knowledge in organizations. LILA (2002)

117. g speak: http://www.oblong.com/g-speak ([Online; accessed 05-Feb-2017])

Collaborative Immersive Analytics 255

https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
http://www.oblong.com/g-speak
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
https://hal.archives-ouvertes.fr/hal-01348578
http://www.oblong.com/g-speak


118. Stasko, J., Catrambone, R., Guzdial, M., McDonald, K.: An evaluation of space-
filling information visualizations for depicting hierarchical structures. International
Journal of Human-Computer Studies 53(5), 663–694 (2000)

119. Sun, G.D., Wu, Y.C., Liang, R.H., Liu, S.X.: A survey of visual analytics tech-
niques and applications: State-of-the-art research and future challenges. Journal of
Computer Science and Technology 28(5), 852–867 (2013)

120. Tang, A., Boyle, M., Greenberg, S.: Understanding and mitigating display and
presence disparity in mixed presence groupware. Journal of Research and Practice
in Information Technology 37(2), 193–210 (2005)

121. Tang, A., Neustaedter, C., Greenberg, S.: Videoarms: embodiments for mixed
presence groupware. In: People and Computers XX—Engage, pp. 85–102. Springer
(2007)

122. Tang, J.C., Minneman, S.: VideoWhiteboard: video shadows to support remote
collaboration. In: Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems. pp. 315–322. ACM (1991)

123. Teasley, S., Covi, L., Krishnan, M.S., Olson, J.S.: How does radical collocation
help a team succeed? In: Proceedings of the 2000 ACM Conference on Computer
Supported Cooperative Work. pp. 339–346. ACM (2000)

124. Terrenghi, L., Quigley, A., Dix, A.: A taxonomy for and analysis of multi-person-
display ecosystems. Personal and Ubiquitous Computing 13(8), 583 (2009)

125. Thomas, J.J.: Illuminating the path: the research and development agenda for
visual analytics (2005)

126. Thomas, J.J., Cook, K.A.: A visual analytics agenda. IEEE Computer Graphics
and Applications 26(1), 10–13 (2006)

127. Tsandilas, T., Bezerianos, A., Jacob, T.: SketchSliders: Sketching widgets for visual
exploration on wall displays. In: Proceedings of the 33rd Annual ACM Conference
on Human Factors in Computing Systems. pp. 3255–3264. CHI ’15, ACM, New
York, NY, USA (2015)

128. Tse, E., Greenberg, S., Shen, C., Forlines, C., Kodama, R.: Exploring true multi-
user multimodal interaction over a digital table. In: Proceedings of the 7th ACM
Conference on Designing Interactive Systems. pp. 109–118. DIS ’08, ACM, New
York, NY, USA (2008)

129. Turoff, M., Hiltz, S.R., Bahgat, A.N., Rana, A.R.: Distributed group support
systems. MIS quarterly pp. 399–417 (1993)

130. Veerasamy, A., Belkin, N.J.: Evaluation of a tool for visualization of information
retrieval results. In: Proceedings of the 19th Annual International ACM SIGIR
conference on Research and Development in Information Retrieval. pp. 85–92.
ACM (1996)

131. Viegas, F.B., Wattenberg, M.: Communication-minded visualization: A call to
action. IBM Systems Journal 45(4), 801 (2006)

132. Viegas, F.B., Wattenberg, M., Van Ham, F., Kriss, J., McKeon, M.: Manyeyes:
a site for visualization at internet scale. IEEE Transactions on Visualization and
Computer Graphics 13(6) (2007)

133. Vogel, D., Balakrishnan, R.: Interactive public ambient displays: transitioning
from implicit to explicit, public to personal, interaction with multiple users. In:
Proceedings of the 17th Annual ACM Symposium on User Interface Software and
Technology. pp. 137–146. ACM (2004)

134. Voida, S., Tobiasz, M., Stromer, J., Isenberg, P., Carpendale, S.: Getting practical
with interactive tabletop displays: Designing for dense data, "fat fingers," diverse

256 Mark Billinghurst et al.



interactions, and face-to-face collaboration. In: Proceedings of the ACM Interna-
tional Conference on Interactive Tabletops and Surfaces. pp. 109–116. ITS ’09,
ACM, New York, NY, USA (2009)

135. Ware, C., Franck, G.: Evaluating stereo and motion cues for visualizing information
nets in three dimensions. ACM Transactions on Graphics (TOG) 15(2), 121–140
(1996)

136. Willett, W., Heer, J., Hellerstein, J., Agrawala, M.: CommentSpace: structured
support for collaborative visual analysis. In: Proceedings of the SIGCHI conference
on Human Factors in Computing Systems. pp. 3131–3140. ACM (2011)

137. Woolley, A.W., Chabris, C.F., Pentland, A., Hashmi, N., Malone, T.W.: Evidence
for a collective intelligence factor in the performance of human groups. Science
330(6004), 686–688 (2010)

138. Yasojima, E.K.K., Meiguins, B.S., Meiguins, A.S.G.: Collaborative augmented
reality application for information visualization support. In: 16th International
Conference on Information Visualisation (IV). pp. 164–169. IEEE (2012)

139. Yoo, B., Han, J.J., Choi, C., Yi, K., Suh, S., Park, D., Kim, C.: 3D user interface
combining gaze and hand gestures for large-scale display. In: CHI ’10 Extended
Abstracts on Human Factors in Computing Systems. pp. 3709–3714. CHI EA ’10,
ACM, New York, NY, USA (2010)

140. von Zadow, U., Büschel, W., Langner, R., Dachselt, R.: SleeD: Using a sleeve
display to interact with touch-sensitive display walls. In: Proceedings of the Ninth
ACM International Conference on Interactive Tabletops and Surfaces. pp. 129–138.
ITS ’14, ACM, New York, NY, USA (2014)

141. Zhao, J., Glueck, M., Isenberg, P., Chevalier, F., Khan, A.: Supporting Handoff
in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs.
IEEE Transactions on Visualization and Computer Graphics 24(1), 340–350(2018),
https://hal.inria.fr/hal-01565560

142. Zhu, H.: From WYSIWIS to WISINWIS: role-based collaboration. In: 2004 IEEE
International Conference on Systems, Man and Cybernetics. vol. 6, pp. 5441–5446.
IEEE (2004)

143. Zimmer, B., Kerren, A.: Ongrax: A web-based system for the collaborative visual
analysis of graphs. Journal of Graph Algorithms and Applications 21(1), 5–27
(2017)

Collaborative Immersive Analytics 257

https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560
https://hal.inria.fr/hal-01565560

	8. Collaborative Immersive Analytics
	Abstract
	Keywords
	8.1. Introduction
	8.2. Definition and Scope
	8.3. Collaboration Over Space and Time
	8.4. Types of Participants in CIA
	8.5. Interaction in CIA Environments
	8.6. Opportunities for Research
	8.7. Conclusion
	References




