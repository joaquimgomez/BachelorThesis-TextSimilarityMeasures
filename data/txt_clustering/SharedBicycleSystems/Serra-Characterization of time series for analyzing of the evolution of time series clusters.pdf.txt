








































Expert Systems with Applications 42 (2015) 596–611
Contents lists available at ScienceDirect

Expert Systems with Applications

journal homepage: www.elsevier .com/locate /eswa
Characterization of time series for analyzing of the evolution of time
series clusters
http://dx.doi.org/10.1016/j.eswa.2014.08.012
0957-4174/� 2014 Elsevier Ltd. All rights reserved.

⇑ Corresponding author. Tel.: +55 31 3319 4117; fax: +55 31 3319 4001.
E-mail addresses: serra.anapaula@yahoo.com.br (A.P. Serra), zarate@pucminas.

br (L.E. Zárate).
Ana P. Serra, Luis E. Zárate ⇑
Department of Computer Science, Applied Computational Intelligence Laboratory-LICAP, Pontifical Catholic University of Minas Gerais, Av. Dom José Gaspar 500, Coração
Eucarístico, Belo Horizonte 30535-610, MG, Brazil
a r t i c l e i n f o

Article history:
Available online 21 August 2014

Keywords:
Time series
Temporal database
Cluster analysis
Multivariate database
Data mining
a b s t r a c t

This work proposes a new approach for the characterization of time series in databases (temporal
databases – TDB) for temporal analysis of clusters. For the characterization of time-series it were used
the level and trend components calculated through the Holt-Winters smoothing model. For the temporal
analysis of those clusters, it was used in a combined manner the AGNES (Agglomerative Hierarchical
Cluster) and PAM (Partition Clustering) techniques. For the application of this methodology an R-based
script for generating synthetic TDBs was developed. Our proposal allows the evaluation of the clusters,
both in the object movement such as in the appearance or disappearance of clusters. The model chosen
to characterize the time-series is adequate because it can be applied for short periods of time in situations
where changes should be promptly detected for quick decision making.

� 2014 Elsevier Ltd. All rights reserved.
1. Introduction

It is currently observed an accelerated growth in data storage
capacity. This increase has allowed the registration of data for long
periods of time introducing a temporal character in it. It is then
passed to handle huge volumes of historical data that should be
exploited in the temporal point of view for a better understanding
of a problem. This is also a characteristic scenario in Big Data.

Within this context there was an extension of conventional data
mining for Temporal Data Mining (TDM), which in essence is based
on mining of sequential data. According to Roddick and
Spiliopoulou (2002), TDM brought the ability to mine activities
and trajectories more than simply defined states or moments of
time. TDM makes the extracted knowledge more complete.

Among the initial studies of manipulation of historical time ser-
ies, the work of Last, Klein, Kandel, and Abraham (2001) stands as
an important contribution to present a methodology for the appli-
cation of data mining in time series. This methodology suggests
that the conventional steps of a data mining process, applied to a
database containing data collected over a particular time or time
interval, called by many authors as a static database, were restruc-
tured for data mining on a temporal database (TDB).

As exposed previously, the need arises for algorithms, tech-
niques, procedures and methods for data mining capable of dealing
with temporal information. The conventional mining algorithms
need to be adapted to treat temporal databases, or temporal data
needs to be pre-processed and converted into ‘‘frozen’’ or summa-
rized point values for an instant of time ‘t’ before the application of
conventional data mining techniques.

With the growing interest of the scientific community in recog-
nition of the time value of data, several studies from TDM have
emerged addressing not only the sequential ordering of data, but
the time value itself contained in the historical data.

The work of Lin, Orgun, and Williams (2002) provides a simpli-
fied view of the TDM process and foundations to manipulate tem-
poral data. The authors discuss the two fundamental problems of
TDM which are to calculate the similarity between time series
and the identification of periodicities in historical data. At the
end, a challenge was launched for the search for a general theory
for TDM which would represent a milestone in this area, since
the work has not previously had an established theoretical founda-
tion. In Last, Kandel, and Bunke (2004), the authors presented an
important contribution to structuring the area containing an over-
view of relevant articles that present proposals for the biggest
challenges of TDM. From this work, new categorizations arise
where researchers eventually end up inducing the appearance of
subareas within the TDM.



A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611 597
On the other hand, in mining works of time series using cluster-
ing techniques is possible to observe the emphasis in the discus-
sion of the best way to calculate similarity/dissimilarity
measures. Approaches that generalize this issue are still under dis-
cussion, and the conclusion that most of the studies reached is that
this choice depends on the domain and the database structure
(Liao, 2005).

As contextualized previously, when clustering techniques are
applied on the TDB, the main aspect to be considered is the char-
acterization of the series and therefore it is necessary to extract
features that represent the essence of a series. The best measures
for the characterization of a series are extracted from structures
present in a generalized model of a time series, which is essen-
tially made up of the following components: level, trend, season-
ality and periodicity. This study contributes in this direction
presenting a methodology for clustering time series by temporal
windows through the characterization of the series, based on the
structural components of a generalized model, in this paper the
Holt-Winters model. This model is easy to understand, has low
computational cost, it is applicable in series with few observations
and it allows the adjustment of the values of the smoothing con-
stants for each component in the model, making it a flexible
model. It is important to note that the ability of this model to deal
with a few observations allows the selection of temporal windows
with lower amplitudes, restricted to a minimum of three observa-
tions necessary to adjust the model. This is a useful feature when
one wishes to apply this model in a historical data that is still rel-
atively small and when there is a need to obtain information for
decision making.

In this work the similarity shall be based on the characteristics
(level and trend) extracted from each series so far from Holt-
Winters model. With this, it is possible to understand the dynamics
of moving objects and clusters over time, providing subsidies for
explanation and prediction of behaviors and phenomena.

Due to the lack of real databases that are complete and compre-
hensive that allow analysis of proposal, a generator for a synthetic
multivariate TDB was built. The generator allows the setting of the
number of attributes in the database, how many observation
points in time and how many records will be considered. For the
generation of the synthetic database, the R1 environment was used.

This paper is organized as follows: in Section 2 a brief review of
the literature about TDM is presented; in Section 3, the formaliza-
tion of our proposal and the problem of characterization of series
are presented; Section 4, shows how the time series generator used
to evaluate the proposal was developed; in Section 5, simulation
results are presented and discussed; in Section 6, the final com-
ments and conclusions are presented.
2. A brief review of the literature

As it can be observed through the extensive literature, the dis-
cussion of methodologies for handling temporal data is a latent
area of research and with a strong tendency of growth especially
motivated by new paradigms and trends in Big Data. Among con-
tributions more recent one can cite: Xiong and Yeung (2004),
Liao (2005), Gardner and Diaz-Saiz (2008), Zhang, Chen, Brijs,
and Zhang (2008), Böttcher, Spott, Nauck, and Kruse (2009),
Gullo, Ponti, Tagarelli, and Greco (2009).

In Roddick and Spiliopoulou (2002), the authors proposed tax-
onomy for TDM considering three dimensions: (i) the type of data;
(ii) the ordering of the data; and (iii) the mining paradigm used. As
for the type data dimension, three divisions were considered: (a)
1 www.r-project.org.br.
approaches that deal with scalar values; (b) approaches that deal
with events; (c) approaches that deal with mining results. The lat-
ter one also called mining high-order (Roddick, Spiliopoulou, Lister,
& Ceglar, 2008) is considered by the authors as a challenge in the
TDM. To the dimension corresponding to the ordering of the data
(ii), two divisions were defined: (a) works dealing with temporally
ordered data and (b) works that deal with data where there is no
sort order. For the mining paradigm dimension (iii), two divisions
were created: (a) work for discovering temporal association rules
(Zhang et al., 2008); and (b) work classification which may be
supervised or unsupervised, the latter one corresponding to the
clustering technique (Böttcher et al., 2009).

Considering the taxonomy presented by Roddick and
Spiliopoulou (2002) this work is inserted in the categories type data
(i) ‘mining result’ (c), and in the mining paradigm (iii) – ‘classification
work’ (b). In this paper, a methodology to monitor the evolution of
cluster models and their objects, after the application of an unsuper-
vised classification on a TDB from a characterization of the time ser-
ies, via Holt-Winters smoothing model (Gardner & Diaz-Saiz, 2008;
Winters, 1960), that make up the database is proposed.

In Liao (2005) the author has reviewed the methodologies
applied in the cluster analysis over time series. Most methodolo-
gies are restricted to a univariable time series, and the difference
between them is how to calculate the similarity/dissimilarity mea-
sure between series, which depends on the type (regular and irreg-
ular) and the specific characteristics of TDB.

The approaches raised by Liao (2005) were organized into three
categories: (1) cluster analysis applied directly to the TDB, with
some modifications in conventional data mining algorithms; (2)
cluster analysis on features extracted from the time series (Gullo
et al., 2009); and (3) cluster analysis based on models built from
the TDB (Xiong & Yeung, 2004). In the first category, it proposes a
direct manipulation of the original data, resulting in a very high
computational cost in implementing TDM techniques. In the third
category, the proposal was to use information models (coefficients,
residue, etc.), which is not very consistent since these coefficients
have no direct relation with the problem domain. Many of these fac-
tors are merely adjustments of the models for the historical data.
Among the three categories the second, which covers the applica-
tions of cluster analysis based on features extracted from the time
series, are the most interesting, because with the data synthesis, it
is possible to gain in computational cost and if well chosen, these
extracted features can correctly represent the information con-
tained in the TDB. For example, in Gullo et al. (2009) the authors pro-
pose a representation of time series based on dynamic time warping
(DTW). The new approach permits to capture the main trends of
time series and data compress, which can be used for similarity
detection of time series. DTW is a method that searches an optimal
match between two given sequences which may vary in time or
speed. Other alternative ways of dealing with the problem of com-
putational cost in processing a TDB consist in the use of techniques
to reduce the dimensionality. Amongst the most relevant works
Wang, Wirth, and Wang (2007), Wang and Megalooikonoumou
(2008) and Al-Naymat and Taheri (2008) can be mentioned.
3. Characterization of time-series for temporal evolution of
clusters

Fig. 1 illustrates the proposal of this work. Consider a multivar-
iate TDB containing records that contain variables (attributes)
expressed through temporal series. Then it is possible to character-
ize these series extracting the level and trend components and
applying a clustering technique chosen for two time intervals,
Window 1 and Window 2. Each point on the graph represents a
record of the database shown for example through the first two



A
t
t
r
i
b
u
t
e
1

Attribute 2

Temporal window
A
t
t
r
i
b
u
t
e
1

Attribute 2

Temporal window

Fig. 1. Example 1 of cluster monitoring in two temporal windows.

598 A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611
main components. Looking at the graphs in Fig. 1 it is possible to
note that from Window 1 to Window 2 there was a movement of
objects between clusters and there were also changes in the cluster
forms (size and position). The proposed method allows the detec-
tion and tracking of these movements and changes in the cluster
shape, allowing from a further analysis the explanation of what
motivated the evolution of clusters over time.

Fig. 2 shows the situation where there are moving objects
between clusters from one temporal window to another and the
emergence of a new cluster. This means that the patterns discov-
ered for Window 1 were valid only within the time interval of
Window 1. This situation leads to a new problem that is about
the length of the extracted knowledge. In this case the choice of
a ‘temporal window’ size enabling to observe changes in the pat-
terns discovered are critical for determining the regularity in the
temporal changes of the patterns discovered. This is a main aspect
discussed in this work.

By monitoring the cluster evolution in a multivariate TDB
between ‘temporal windows’, it would be possible to answer
through a subsequent analysis to questions such as:

� What are the reasons for changes in records/objects between
clusters?
� What reasons lead to the creation or extinction of clusters

within a temporal window to another?
A
t
t
r
i
b
u
t
e
1

A
t
t
r
i
b
u
t
e
1

Attribute 2

Temporal window

Fig. 2. Example 2 of cluster monito
� What are the attributes that most influence significant changes
in measures of similarity?
� What is the expiration life of the knowledge? That is, by default

when an extracted clustering analysis is valid.

In this section, the process of characterization of a time series in
a multivariate TDB, subdivided into pre-defined ‘temporal windows’
for a later application of cluster analysis is presented. This paper
considers that the TDB is formed by time series whose data is col-
lected in the same time periods and may be regular or irregular.

The basis of multivariate temporal data considered in this study
is expressed as follows:

½Z� ¼

Zt11 Zt12 � � � Zt1M
Zt21 Zt22 � � � Zt2M
..
. ..

. . .
. ..

.

ZtN1 ZtN2 � � � ZtNM

2
66664

3
77775

for; i ¼ 1; . . . ;N; j ¼ 1; . . . ;M

ð1Þ

with Ztij ¼ fZ1ij ; Z2ij ; . . . ; ZTijg.
Where N represents the number of instances (records or exam-

ples) and M the number of variables (attributes), each correspond-
ing to a time series. Each Ztij element corresponds to an observation
of example i, attribute j and each value of t = 1, . . . ,T corresponds to
Attribute 2

Temporal window

ring in two temporal windows.

Alexandra
Resaltado

Alexandra
Resaltado



A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611 599
the periods of observation in the series. Note that the number of
elements in the database |Z| is given by NxMxT.

The approach proposed in this work consists in dividing the Ztij
series in jh ‘temporal windows’ of window set J ¼ fj1; j2; . . . ; jHg, and
then characterizing them and then applying the technique of clus-
tering for evolutionary monitoring. Each Ztij series will be divided
into ‘temporal windows’, which may be of the same or different
amplitude. Within the range of the original interval of the series,
t = 1, . . . ,T, there will be cut-off points defined by the window set
J. The characteristics to be extracted from these series to compose
the characteristics vector Z�ijh that represents them, will be deter-
mined for each window jh, where h corresponds to the h-th tempo-
ral window.

The set of these vectors Z�ijh will compose the input data for the
application of clustering techniques. The clustering technique is
applied to the set of vectors for the ‘H’ windows, which allows
the follow-up of the evolution of the clusters. As exposed, three
procedures need to be adopted: (1) define the amplitude of the
temporal window; (2) choose the time series model; (3) character-
ize the time series in the TDB; and (4) choose the clustering
technique.
3.1. Defining the amplitude for the temporal window

The way to determine the amplitude of the jh windows may
vary according to the data variation, according to the problem
domain and the availability of information concerning the data-
base in question. Given that the objective of this stage in the TDB
subdivision in windows, is the search for cutoff points in time
where there are important changes that are of interest for the anal-
ysis, to have information about the context of the problem can give
an indication of the frequency of variations and therefore the
amplitude of the windows.

To illustrate one possible selection of the window amplitudes
through the data variation, to consider one TDB containing the ser-
ies, Series 1, Series 2, Series 3 and Series-4, see Fig. 3. A criterion for
the selection of jh can be established through the identification of
the series with less variability (in our example the Series-3) and
define as the window amplitude the time period until the next
change in this series. This case assumes that a change in series with
little variation, and possibly depending on the domain, it may
involve changes in other attributes (series) in the TDB that are
more sensitive to changes over time.
Fig. 3. Example of wi
Another criterion for the selection of temporal windows
through the analysis of the problem domain and availability of
information about the database can be given by fixing an ampli-
tude for all windows that is consistent with the context. As an
example, in the case of the TDB with monthly values for a period
greater than two years, it is consistent the choice of semester win-
dows, since it is considered to be a sufficient period to occur signif-
icant variations. In case of having information available about the
problem domain, indicating a better subdivision of these series
according to their periods of change, this will lead to a better
exploitation of the database.

As mentioned earlier, for this work it was used a generator of
synthetic multivariate TDBs, reason for which the strategy to
define the window size at a monthly basis was used in the gener-
ated database. Therefore, window amplitudes annual and semester
will be considered.

3.2. Selection of the model and characterization of the time series

In the area of time series analysis, there are several types of
models that describe the behavior of a time series. The choice of
the best model varies with the objectives, the properties of the data
and the number of series to be manipulated. Basically, these mod-
els fall into two categories according to the number of parameters
involved: (i) parametric models that have a finite number of
parameters, for which the analysis is done in the time domain;
and (ii) non-parametric models involving an infinite number of
parameters and analyze the series in the frequency domain. Non-
parametric models are used in the investigation of the mechanisms
of time series and it is very helpful in identifying relevant period-
icities in the data. While parametric models are best suited to char-
acterize and make predictions of a time series (Morettin & Toloi,
1960).

Within the category of parametric models, the most commonly
used ones are error models (or regression), the ARIMA models
(Box and Jenkins), exponential smoothing models, structural mod-
els and nonlinear models. Except for exponential smoothing mod-
els, all others assume that a time series has the general form given
by Zt = f(t) + at, composed by a time function f(t) added to a noise at.
The variation in the methodologies is that the assumptions about
the data do imply that in the function definition f(t), assumptions
such as probability distributions and independence between
observations. These assumptions end up introducing limitations
on the validity of the models.
ndow definition.

Alexandra
Resaltado

Alexandra
Resaltado

Alexandra
Resaltado



600 A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611
On the other hand, the exponential smoothing models make
no assumption about the type of data that make up the series.
These are based solely on the idea that past data form the pat-
tern of behavior of the series, and this pattern is found by
smoothing the randomness of the series. This means that if it
is possible to remove all interference from the random noise of
the time series, one can observer the basic pattern inserted into
the data.

One of the most comprehensive and widely used smoothing
models in practice is the Holt-Winters (Winters, 1960), which
associates to each component of the standard series (level (l),
trend (T) and seasonality (F)) be a smoothing constant.

Zt ¼ ltFt þ Tt þ at ð2aÞ
Zt ¼ lt þ Ft þ Tt þ at ð2bÞ

The Holt-Winters model can be additive or multiplicative depend-
ing on the behavior of seasonality, Eqs. (2a) and (2b) respectively.
This work is not considered the seasonality (Ft), since the character-
ization of the series will be held for short periods of time, defined by
the amplitudes of the time frames jh. In series with few observa-
tions, there is no way to identify the seasonality component, which
is a feature that needs a long history to be observed. Thus, the Holt-
Winters model considered in this work will have the smoothing
parameter of the seasonal component removed.

According to the chosen model the level Zt and trend T̂ t compo-
nents in the time series will be estimated by Eqs. (3) and (4).

Zt ¼ aZt þ ð1� aÞðZt�1 þ T̂t�1Þ; 0 < a < 1 ð3Þ
T̂t ¼ bðZt � Zt�1Þ þ ð1� bÞT̂t�1; 0 < b < 1 ð4Þ

There are two ways of selecting the smoothing constants a and b.
This can be by minimizing the quadratic sum of the forecast
errors, where the selection of the constants is based on the com-
bination of possible values for these constants that generate the
lowest prediction error. Or simply by setting values for these con-
stants arbitrarily or taking into account a prior knowledge of the
history in the data. These parameters serve as factors of forgetting
the past.

From Eqs. (3) and (4), it is observed that the smaller the con-
stant values a and b to calculate the level and trend of most impor-
tance is given to those who passed the current values. Note that
level and trend are directly associated. The higher the tendency
value the more changes will occur at the series level over time.
In the case of series with a greater structural variability, that is,
where level and/or tendency are irregular over time, it is more
appropriate for the observation of the last number to have a higher
weight on the estimation of these components, in this case the con-
stants a and b may have values close to one.

According to Chatfield and Yar (1988), by means of empirical
results the parameter values a and b often exceed 0.3 in applica-
tions where it is necessary to fix the value of these constants.
The authors recommend that both parameters should be set at
0.4. In this study, the determination of these parameters was per-
formed using the method of minimum squares, which minimizes
the squared sum of the prediction errors from the Holt-Winters
model according to Eq. (5).

S ¼
XT

t¼‘þ1
ðZt � Ẑt�1ð1ÞÞ

2
ð5Þ

where ‘ corresponds to the index up to where the initial observa-
tions used to calculate the components of the model in the series
go according to Eqs. (3) and (4). The adjusted model is responsible
for initiating the process of calculating the prediction error and the
values of the observed series are compared to the values predicted
by the model.
3.3. Characterization of temporal series in the TDB

After defining the Holt-Winters model, the next step is the iden-
tification of the characteristics vector of the time series that com-
prise the TDB. Considering the Eqs. (3) and (4), the level and trend
components are defined as:

� Level: Zijh corresponds to the series level from example i, attri-
bute j, in the temporal window h.
� Trend: T̂ ijh corresponds to the series trend of example i, attri-

bute j, in temporal window h.

Each pair of these components will represent and characterize a
series Ztij . Therefore, the characteristics vector defined as Z

�
ijh is

defined by Eq. (6).

Z�ijh ¼ ½Zijh;; T̂ ijh� i ¼ 1; . . . ;N; j ¼ 1; . . . ;M and h ¼ 1; . . . ;H;
ð6Þ

where H represents the total number of Windows for analysis in the
TDB.

Using the representation given by Eq. (6) for the time series
defined in Eq. (1), the transformed database for a future application
of the clustering technique is defined as:

ð7Þ

where N is the total number of examples, M of attributes and H of
windows. The cardinality of the matrix [Z⁄] corresponds to
NxMxHx2.
3.4. Determining the number of clusters for analysis of the temporal
evolution

After defining the [Z⁄] matrix, the next step corresponds to the
analysis of the clusters applied to each temporal window jh for
h = 1, . . . ,H. For monitoring the evolution and analyzing the clus-
ters, the number of clusters should be determined in advance, later
fixed or not. The procedures adopted in this work are presented
below:



A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611 601
1. To set the optimal window size (main parameter of the pro-
posed analysis methodology) for the observation and analysis
of significant changes the agglomerative hierarchical clustering
(AGNES – AGglomerative NEsting) technique can be applied to
different window sizes, for example: sizes 6 and 12 months.
Further details on these procedures will be presented in
Section 5.4.

2. To set the initial number of clusters that is most appropriate for
the first window of the TDB, the hierarchical agglomerative
clustering (AGNES), density-based as DBSCAN (Ester, Kriegel,
Sander, & Xu, 1996) and OPTICS (Ankerst, Breunig, Kriegel, &
Sander, 1999), or partitioning k-medoids (PAM) techniques
can be applied. All techniques have positive and negative
aspects. The agglomerative hierarchical clustering technique is
more natural because it does not require a prior definition of
the number of clusters, although exist difficulty handling differ-
ent sized cluster and convex shapes. Furthermore, it is sensitiv-
ity to noise and outliers. With density-based algorithms as
DBSCAN and OPTICS is not necessary to specify the number of
clusters a priori, and furthermore, they can find clusters with
arbitrary shape, which is a very interesting aspect for real prob-
lems. DBSCAN requires two parameters to be adjusted: distance
that defines the concept of density reachability e (eps), and the
minimum number of points required to form a dense region
(minPts). The major weaknesses of DBSCAN, it cannot detect
meaningful clusters in data with varying density, since the
parameters (minPts, e) cannot be chosen appropriately for all
clusters. The OPTICS algorithm, considered a generalization of
DBSCAN, addresses the major weaknesses of DBSCA. It replaces
the e parameter (search radius) with a maximum value that
mostly affects performance. Notice that, for minPts = 2, the
result will be the same as of hierarchical clustering with the
dendrogram cut at height e. For the partitioning k-medoids
technique, it requires an iterative search process by the number
k of ideal clusters which may require a large computational
effort. It is important to observe that to set the initial number
of clusters is the main objective in this stage. It is possible to
apply any clustering technique that permit to obtain that num-
ber. The density-based algorithms, as DBSCAN, would be inter-
esting to use, but this algorithm requires two parameters to be
adjusted, which together determine the number of clusters to
be found. Furthermore, as in this work, synthetic temporal dat-
abases are considered, which do not represent a particular
problem domain and have not outliers or noise, is not necessary
to use those algorithms which have the higher capacity to
detect convex regions, noise and outliers. For this stage, the
agglomerative hierarchical technique will be considered. In
order to have a comparison point the DBSCAN algorithm was
applied in this stage too.

3. To follow the movement of objects between clusters and
between the windows the initial number of k clusters must be
fixed and applied the technique of partitioning k-medoids
(PAM).

4. To follow the evolution of the number of clusters varying the k
number of initial clusters, determined in step 2, the partitioning
k-medoids technique could be applied within an iterative pro-
cess in finding the optimal number of clusters. This process
must be repeated for each temporal window.

As previously mentioned, in the agglomerative hierarchical
clustering technique it is not necessary to define in advance
how many clusters should be formed for the database. This num-
ber may be identified through analysis of the cluster tree (den-
drogram) and the height range graph proposed in Kaufman and
Rousseeuw (1990). The number of clusters is defined by deter-
mining a threshold, such as setting a limit on the maximum dis-
tance between clusters. The agglomerative hierarchical AGNES
method starts considering each object as a cluster and then
aggregates clusters of two in pairs forming increasingly larger
clusters until all objects are in a single cluster. The criterion in
choosing the clusters to be merged is defined by Ward’s method
(Ward, 1963), which fuses the two clusters which result in the
smallest increase of the squares sum variance between the two
clusters, done over all the variables. At each clustering step, all
possible joints are considered. The sum variance of the squares
is calculated for each of these unions and the lowest value should
be the chosen one.

The technique of partitioning k-means (MacQueen, 1967) is a
simple applying technique, relatively scalable and efficient in
the processing of large databases. For this reason, it has been used
in clustering analysis (Kakizawa, Shumway, & Taniguchi, 1998;
Wilpon & Rabiner, 1985). In this study a more robust version of
the method of k-means, k-medoids discussed and implemented
in Kaufman and Rousseeuw (1990) through (Partitioning Around
Medoids) MAP algorithm will be used. In general, the partitioning
techniques work as follows: Given n unlabeled tuples, the parti-
tioning method constructs k partitions where each partition rep-
resents a cluster containing at least one observation and the
number of clusters must be less than or equal to the number of
observations. In the k-medoid technique, each cluster is repre-
sented by the observation of most central location in the cluster,
also called the medoid. The main idea of the method is to find k
representative objects or medoids among the observations of the
clusters. After finding a set of k medoids, k clusters are con-
structed by allocating each object to the nearest medoid. In
choosing the best cluster, the goal is to find k representative
objects which minimize the sum of the dissimilarities between
observations and their closest medoid. Several dissimilarity mea-
sures can be used.

It is important to note that in database with high dimension-
ality the data can be sparse. In this kind of database, density and
distance measurements between two points becomes less signif-
icant. In other words, the relative difference of the distances
between the closest and farthest data points from an indepen-
dently selected point tends to 0 as the dimensionality increases,
limd!1ððmaxDist �minDistÞ=minDistÞ ¼ 0, where d corresponds
to the database dimension. In Hinneburg, Aggarwal, and Keim
(2000), it was shown that the absolute difference
maxDist �minDist, depends on the distance measure considered.
For the L1 metric (Manhattan distance), the absolute difference
increases with dimensionality; for the L2 metric (Euclidean dis-
tance), the absolute difference remains relatively constant, and
for the Ld metric, d P 3, the absolute difference goes to 0 as the
dimensionality increases. In conclusion, metrics with d P 3 are
not recommended for highly dimensional data. For the clustering
process in this work, the L2 metric (Euclidean distance) was con-
sidered, since that the TDB considered in the experiments has not
high dimensionality.
4. Generator for synthetic temporal databases

For this work, it was developed a time series generator which
allows building multivariate TDBs. The R language was used as
platform for the development of scripts that allows the generation
of synthetic TDBs.

After defining the number of examples, the number of attri-
butes and the number of observations in the time series, both



Script 1 – Algorithm for the TDB generation

1: N <- 50 # number of examples from the database
2: for (h in 1:N){
#————————————————————————————————— Sets initial parameters
3: AT = 5 # number of temporal attributes (AT > 0)
4: n <- 48 # amount of time points
5: freq <- 12 # intervals within the year in the series
#———————————————————————————————————— Generation of time series
6: for (i in 1:AT){
#———————————————————————————————————— Generates standard normal regular time series (SR)
7: X <- ts(rnorm(n), frequency = freq)
#———————————————————————————————————— Generates irregular series to introduce tendency in the SR
8: Int <- vector() # vector series of irregular intervals (SI)
9: Int[1] <- 1 # first time of the irregular series
10: k = 1 # points counter of SI time
11: while (Int[k] < n){
12: Tam <- as.integer(runif(1, min = 6,

max = 12))
13: Int[k + 1] = Int[k] + Tam
14: k = k + 1
15: } # creates the vector of points in time from SI
16: Int[k] = n #last vector of int from SI = to the last from SR
17: library (zoo) #SI creation package
18: (Y <- zoo(rnorm(k), Int)) # creates a standard normal distribution with SI
#———————————————————————————————————— Differentiates the SI to calculate trends
19: YY <- diff(Y) #differentiates the SI
20: tt <- diff(Int) #calculates the size of the time intervals from SI
21: B <- vector () #stores the values of the slopes
22: for (v in 1:length(tt)){ #creates the series with trends added to the SR
23: B[v] = (YY[v]/tt[v])
24: }
#———————————————————————————————————— Generates vector results of the line equation
25: XR <- vector () #creates the vector where the results of the line equations

will be stored
26: XR[1] = Y[1] #1st vector element of the lines = to the 1st series element
27: a = 2
28: d = 2
29: for (c in 1:length(tt)){
30: for (z in a:Int[d]){
31: XR[z] = B[c] * z + Y[c] � B[c] * Int[c]
32: }
33: a = Int[d] + 1
34: d = d + 1
35: }
36: Xf <- X
37: Xf = Xf + XR #adds to the original series the line equation
38: if (i > 1) XX <- cbind(XX,Xf) else XX <-

Xf
39: }
40: if (h < 2) XX1 <- XX else XX1 <- rbind

(XX1,XX)
41: }

602 A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611
regular and irregular series can be generated. These series com-
prise the multivariate TDB. Script 1 shows the instructions for
generating a TDB.
To monitor the evolution of clusters over time, it is necessary to
segment the generated TDB series by setting the window size to be
considered for analysis. Script 2 shows the instructions to segment



A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611 603
TDB. The process of characterizing the TDB series as defined in Sec-
tion 3 was implemented using Script 3.
Script 2 – Algorithm for the segmentation of the TDB
1:
 n <- 48

2:
 TJan <- 6

3:
 NJan <- n/TJan

4:
 LimInf <- 1

5:
 LimSup <- TJan

6:
 MAT <- array(dim = c(NJan,N,2))

7:
 for (k in 1:NJan){

8:
 MAT[k,1,1] = LimInf

9:
 MAT[k,1,2] = LimSup

10:
 for (i in 2:N){

11:
 MAT[k,i,1] = MAT[k,i�1,1] + n

12:
 MAT [k, i, 2] = MAT[k, i, 1] + (TJan-1)

13:
 i = i + 1

14:
 }

15:
 LimInf = LimInf + (TJan-1)

16:
 LimSup = LimSup + (TJan-1)

17:
 k = k + 1

18:
 }

19:
 VFINAL <- array(dim = c(N,(AT * 2),NJan))
Script 3 – Algorithm for the characterization of the TDB
1:
 for (m in 1:NJan){

2:
 for (i in 1:N){

3:
 j = 1

4:
 for (a in 1:AT){

5:
 JAN = XX1[MAT[m,i,1]:MAT[m,i,2], a]

6:
 MODELO <- HoltWinters(JAN,gamma = FALSE)

7:
 MODELO

8:
 COF <- coef(MODELO)

9:
 VFINAL [i,j,m] = COF[1]

10:
 VFINAL [i,j+1,m] = COF[2]

11:
 j = j + 2

12:
 }

13:
 }

14:
 }
For the last stage corresponding to the cluster analysis which will
happen for each temporal window defined in Script 2, the methods
used are the AGNES method for hierarchical clustering (Script 4a)
and PAM method for clustering by partitioning (Script 4b). The
AGNES method is used for items 1 and 2 in the procedure indicated
in Section 3.4 and the PAM method for items 3 and 4. Script 4c
allows finding the k number of optimal clusters as proposed in item
4 of Section 3.4.
Script 4a – Algorithm for applying the AGNES technique
1:
 Library (cluster)

2:
 for (n in 1:NJan){

3:
 GRUPOS <- agnes(VFINAL[,,n],diss = FALSE,

metric = ‘‘euclidean’’,stand = TRUE)

4:
 plot(GRUPOS)

5:
 }
Script 4b – Algorithm for applying the PAM technique
1:
 library (cluster)

2:
 k = 3
 # initial parameter

for the number of
clusters
3:
 for (n in 1:NJan){

4:
 GRUPOS <- pam(VFINAL[,,n],k,

diss = FALSE,
metric = ‘‘euclidean’’,stand = TRUE)
5:
 plot(GRUPOS)

6:
 }
Script 4c – Algorithm for optimal cluster search with the
PAM technique
1:
 library (cluster)

2:
 for (n in 1:NJan){

3:
 asw <- numeric(7)

4:
 for (k in 2:7) {

5:
 asw[k] <- pam(VFINAL[,,n],k, diss = FALSE,metric =

‘‘euclidean’’, stand = TRUE) $ silinfo $ avg.width

6:
 k.best <- which.max(asw)
}

7:
 cat(‘‘silhouette-optimal number of clusters:’’, k.best,

‘‘nn’’)

8:
 plot(1:7, asw, type = ‘‘h’’, main = ‘‘pam() clustering

assessment’’,

9:
 xlab = ‘‘k (# clusters)’’, ylab = ‘‘average silhouette

width’’)

10:
 axis(1, k.best, paste(‘‘best’’,k.best,sep = ’’nn’’),

col = ‘‘red’’, col.axis = ‘‘red’’)

11:
 windows()

12:
 }
5. Simulations and experimental procedures

5.1. Generation of the TDB

For generating the TDB, Eq. (1), the Script 1 was used with the
following input parameters: number of samples N = 50; number
of attributes M = 5 (AT variable in script 1) and number of observa-
tions per time series n = 48. All series that will comprise the TDB
correspond to regular series.

In a data analysis, it is known that the greater the number of
records (N) the more information is available about the domain,
especially if they compose a representative sample of the universe
which is to be analyzed. Since the database is synthetic and there is
no consistent interpretation with a real situation, the variation of
this parameter only brings changes in processing time. Thinking
generally, it is expected that in real databases the larger the actual
number of records, the better it is the understanding of the prob-
lem domain.

The number of attributes M also influences the amount of infor-
mation about the problem domain, but since it is an analysis based
on synthetic data, this value was set in 5 without any damage for
further analysis. In real problems, it is important to think that
the information gain must compensate the increase in processing
time. It is possible to find correlated attributes implying in duplica-
tion of information, or even attributes that do not add value to the



604 A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611
analysis which can be discarded. This attribute selection process
needs to be performed, if possible, prior to data collection through
a previous knowledge of the problem domain. Other means of
selecting the key variables would be to use multivariate statistical
methods such as principal component analysis.

The variation in the number of observations (n) of the time ser-
ies can also follow the same reasoning: the higher the number of
observations, the greater the TDB will be and the greater will be
the time spent in processing. What sets the optimal number of
observations or at least enough for a more complete analysis is
the granularity and the frequency found in the data. The granular-
ity depends on the variability of the data in time, and this can be
set in hours, days, months or years. As for the periodicity, this
can be observed by the existing natural cycle in the context. For
example, if the data is collected on a monthly basis, it is natural
that a 12-month period closes a cycle in most contexts. For a con-
sistent analysis, in this work it was determined 48 points of obser-
vations in time, which correspond to four years, sufficient for
monitoring the evolution of clusters.
5.2. Segmentation of the TDB prior to the selection of the ideal window
size

The window size (TJan – according to Script 1) is the main
parameter of the analysis methodology proposed in this paper.
This choice is related to the number of observations (n) in the
TDB series and also with a comprehension concerning the problem
domain. Optimal cutoff points are the points where it is expected
that there are significant changes from one window to another.
According to the previous discussion about the influence of the
window size, values for the TDB construction and analysis were
set (see Table 1). For the segmentation of series, Script 2 should
be run. This procedure is performed in order to obtain various sce-
narios to identify the optimal window size in accordance with item
1 in Section 3.4.
5.3. Extraction of the series characteristics

After the generation and segmentation of the synthetic data-
base, running Script 3 prepares the data for later cluster analysis.
The feature extraction is carried out using the Holt-Winters tech-
nique identifying the level components and trend of each TDB time
series (Eqs. (3) and (4)), considering a specific window size. This
procedure was discussed in Section 3.3 and the transformed data-
base will contain the shape defined by Eq. (7). In this work and in
this step, the series were characterized considering the window
Table 2
Simulation 2 (window size = 12).

AGNES DB

Windows # of Clusters Height (threshold) Ag

Window 1 7 8.29 0.
Window 2 8 8.58 0.
Window 3 7 7.94 0.
Window 4 7 8.11 0.

Table 1
Input parameters for simulations.

Simulation Records Attributes

1 50 5
2 50 5
size of 6 and 12, for subsequent selection of the optimal window
size.
5.4. Identifying the optimal window size

To identify the optimal window size over which the analysis of
the clusters will be performed, the agglomerative hierarchical clus-
tering (AGNES) method and the density-based (DBSCAN) method
were applied.

Considering the TDB containing 48 observations in each series,
it is possible to set various window sizes. For example, 16 windows
with size 3 (J3i , for i = 1, . . . ,16), 8 windows of size 6 (J

6
i , for

i = 1, . . . ,8), 4 windows of size 12 (J12i , for i = 1, . . . ,4) etc.. The selec-
tion of the optimal window size obeyed the following criteria:

1. To follow the evolution of the number of clusters between
windows of the same size (item 4 in Section 3.4), proceed as
follows: Given two distinct window sizes that contain the high-
est variability in the number of clusters, consider the size of the
smaller window for the fact that it contains a higher detail on
the evolution of changes. The selection of a higher window size
would lead to an attenuation of information included in the
series, provided that the Holt-Winters model is a smoothing
model.

2. For moving objects between clusters by windows of the same
size (item 3 in Section 3.4), proceed as follows: Choose the win-
dow size with the least variability in the number of clusters
obtained. This helps to identify the optimal number of clusters
(k) for an analysis of the entire TDB. Note that fixing k it is pos-
sible to track the movement of these objects.

Applying the proposed methodology to synthetic TDB, the pro-
cess starts off by applying the clustering technique on a window of
size 12 (simulation 2 in Table 1), obtaining 4 time slots for the ser-
ies with 48 months, as shown in Table 2. Table 3 shows the results
of the second experiment, where the series were segmented at a
shorter interval of time (6 months, simulation 1 in Table 1), result-
ing in 8 time slots presented.

In order to identify the number of clusters, using AGNES, this
work considered the distance in which two points or clusters were
merged (Height measure). As a threshold, it was defined the arith-
metic average of these distances (Height measures) with an added
standard deviation. In Figs. 4a and b, it is possible to identify a
threshold of Height = 8.29 to 7 clusters.

The agglomerate coefficient measures the cohesion of clustered
objects. The higher the value of this index, the better defined are
SCAN

glomerative coefficient # of Clusters Silhouette index

75 12 �0.09
78 4 0.14
75 7 �0.01
75 7 0.10

Series size Window size # of Windows

48 6 8
48 12 4



Fig. 4a. Banner for window 1 with size = 12.

Fig. 4b. Dendogram for window 1 with size = 12.

Table 3
Simulation 1 (window size = 6).

AGNES DBSCAN

Windows # of Clusters Height (threshold) Agglomerative coefficient # of Clusters Silhouette Index

Window 1 8 7.84 0.75 7 �0.03
Window 2 10 7.84 0.76 10 0.03
Window 3 9 7.99 0.74 9 �0.04
Window 4 8 7.99 0.76 14 0.12
Window 5 7 8.08 0.78 5 �0.07
Window 6 9 7.87 0.72 9 �0.01
Window 7 7 8.32 0.78 7 �0.02
Window 8 7 7.87 0.78 8 �0.03

A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611 605



Table 6
Descriptive measures of clusters per temporal window.

Cluster Size max_diss av_diss Diameter Separation

Window 1
1 11 4.74 3.35 7.22 2.29
2 17 5.37 3.13 7.28 2.29
3 9 5.26 3.02 7.03 2.81
4 5 4.57 2.94 6.54 3.50
5 6 4.69 2.89 6.48 3.16
6 1 0.00 0.00 0.00 7.64
7 1 0.00 0.00 0.00 6.05

Window 2
1 11 5.46 2.88 8.26 2.23
2 10 4.20 3.14 6.97 3.04
3 12 5.10 2.99 7.19 2.23
4 1 0.00 0.00 0.00 9.64
5 7 4.80 3.04 7.11 2.96
6 2 3.46 1.73 3.46 3.14
7 7 5.78 3.65 8.62 3.12

Window 3

Table 5
Entropy in object movement.

Examples Entropy

1 0.000
2 0.000
3 0.452
4 0.452
5 0.602
6 0.602
7 0.452
8 0.602
9 0.244

10 0.452

606 A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611
the clusters with low intra-cluster dissimilarity and high dissimi-
larity between the different clusters. Also, the lower the value of
this index, the lower is the cluster definition and the higher the
number of clusters with few observations. This coefficient varies
between 0 and 1.

Regarding the DBSCA algorithm, in Tables 2 and 3, it is possible
to observe that the number of clusters is similar to the number
found by AGNES algorithm. Notice that, the temporal database
corresponds to a synthetic database, where the time series were
generated with normal distribution of amplitude �1 to +1. There-
fore, the characteristic vectors are very close. This situation led to
the DBSCAN algorithm to generate few groups with large number
of objects and many groups with a single object. If only one object
belongs to a cluster, then the silhouette index value (measure of
overall quality considered) is 0, reducing the average value of the
silhouette index of the cluster. It can be observed in Tables 2 and
3. Therefore, in this case the DBSCAN algorithm is not
recommended.

Comparing Tables 2 and 3, it is possible to note that for win-
dows of size 6, more changes in the number of clusters occurs from
one window to another. Therefore, according to criterion 1
(Section 3.4), this size of this window is considered optimal for
an evolutionary tracking in the number of clusters. For the analysis
of moving objects between clusters according to criterion 2, the
size of the window equal to 12 will be considered.

5.5. Tracking of movements in objects between clusters

Identifying the optimal window size for monitoring the move-
ment of objects between clusters in the TDB (in this work, window
size 12), the second stage of the analysis will be to define the most
appropriate initial number of clusters for the first temporal win-
dow in the TDB. The ideal number of clusters is identified by Table 2
and corresponds to value with higher frequency, k = 7, the number
of which presented an agglomerative clustering coefficient of 0.75.

After defining the appropriate number of clusters for the TDB, it
is possible to start tracking the movement of objects between clus-
ters and between the windows by setting the initial number (k = 7)
of clusters and applying the k-medoids (PAM) partitioning tech-
nique. To monitor the movement of objects between clusters,
one can analyze the series of observations per temporal window
or compare the descriptive measures of the clusters.

Table 4 shows the results of the first ten objects from the data-
base, where the first column is the identification of the object’s
record in the database and has the following columns identify
the cluster to which each observation was allocated to each
window.

With the information in Table 4, it is possible to track objects
identifying which objects changed clusters from one window to
another, and through the windows, which records moved the most.
The objects that presented little movement between clusters
(examples 1 and 2) are probably those with little variation in their
Table 4
Sample from object movement between windows.

Examples Window 1 Window 2 Window 3 Window 4

1 1 1 1 1
2 2 2 2 2
3 2 3 3 1
4 3 4 1 3
5 3 5 2 1
6 2 6 3 1
7 2 5 2 4
8 2 3 4 5
9 2 2 5 2

10 1 2 3 3
attribute values. As for those which have migrated from one cluster
to another without showing a predominance in any cluster are
objects which were more dynamic with greater variation in their
attributes (examples 5, 6 and 8). These objects are likely to have
different characteristics from all the clusters or they cannot prop-
erly belong to any of them.

By analyzing the variations in the attribute values in the search
of the causes in the object transition, it is possible to identify two
situations:

1. The Movement between clusters may be caused by the varia-
tion of only one or a few attributes, and these attributes are
the most influential in determining the allocation of the object.

2. The movement can be caused by the variation of values consid-
ered in all of the attributes in the cluster, a situation that does
not directly permit, by means of a quick inspection, finding
the responsible attributes.
1 4 4.55 2.83 5.54 2.93
2 8 5.63 3.50 7.36 2.94
3 13 4.65 2.82 6.80 2.61
4 3 4.69 2.82 5.45 4.16
5 12 4.73 3.25 8.16 2.54
6 9 4.25 2.95 5.97 2.54
7 1 0.00 0.00 0.00 5.75

Window 4
1 12 4.52 2.83 6.42 2.26
2 5 3.56 2.63 4.68 2.43
3 6 4.12 3.22 6.40 3.11
4 3 4.47 2.50 5.02 3.41
5 11 4.11 2.76 6.39 2.26
6 8 5.73 3.66 8.70 2.34
7 5 5.89 3.87 7.82 3.36



Fig. 5a. Graph of PAM cluster for Window 1 with size 12 with two principal components.

Fig. 5b. Graph of PAM cluster for Window 1 with size 12 – Silhouette Index.

A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611 607
In order to measure in the most appropriate manner the vari-
ation degree in the movement of objects between clusters the
informational entropy can be calculated for each sample, consid-
ering all temporal windows. The amount of information or
entropy of the random variable E (example) probability pe, is
defined by Eq. (8):
HðEÞ ¼ �
X
e2E

pe logðpeÞ ð8Þ

To illustrate this, the entropy of the examples in Table 4 were
calculated, as seen in Table 5 examples 5, 6 and 8 were those with
the highest entropy, which shows that the objects are the ones
moving the most along the temporal windows.

Another way to evaluate changes in the clusters is given by
descriptive measures of the clusters formed by windows. These
measures are: number of objects (size), maximum dissimilarity
between objects of the same cluster (max_diss), average dissimilar-
ity within each cluster (av_diss), the cluster diameter (diameter)
and minimum dissimilarity between objects of different clusters
(separation). Through these measures, it is possible to follow the
evolution of the clusters with respect to their positioning and their
scope in space. Follow the descriptive measures by the window,
the window size 12, seen in Table 6.



Fig. 6. Graphs presenting the silhouette amplitudes measured by cluster number.

608 A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611
Through the analysis of Table 6 is possible to detect the pres-
ence of outliers. This can occur when an object cannot belong to
any of the clusters, forming a cluster with a single element. You
can see that windows 1, 2 and 3 has been typical examples of



A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611 609
outliers, where the cluster is formed only by an object and this
object has a separation measure (minimum dissimilarity between
objects of different clusters) higher than the values found for other
clusters.

In the execution of the PAM algorithm, through Script 4b, two
types of graph are generated summarizing the results, making it
easier to read and interpret. The first graph crosses the two main
components from this attribute analysis through the multivariate
statistical PCA technique (Principal Component Analysis). This
chart type allows the overview of the data in the space between
their attributes, since it would be impossible to visualize the origi-
nal data in more than three dimensions. In this graph, the percent-
age of total variability explained by the two components is
presented.

The second type of graph is built based on an index to assess the
clustering quality, called ‘‘amplitude silhouette’’ s(i) (Silhouette
Width). This index is calculated for each observation (example) in
order to evaluate its allocation in a given cluster (segmentation).
The index s(i) has the following definition: with a(i) being the aver-
age dissimilarity between the observation i and all the other obser-
vations of the cluster to which i belongs (if i is the only observation
of their cluster then s(i) = 0). In relation to the other C clusters,
given that d(i, C) = represents the average dissimilarity of i to all
observations of a C cluster. The lowest d(i, C) is b (i) = min_C d(i,
C) and it can be seen as the dissimilarity between i and neighboring
clusters. In other words, b(i) corresponds to the nearest neighbor-
ing cluster. This index is defined by Eq. (9).
sðiÞ ¼
bðiÞ � aðiÞ

maxðaðiÞ; bðiÞÞ
ð9Þ

Observations with a high s(i) (close to 1) value are objects that
are very well allocated in their clusters, as for a low s(i) value (near
0) means that the observations are undefined between two clus-
ters. And the observations with a negative s(i) are probably allo-
cated in a wrong cluster.

As the measure of overall quality of the clustering the ampli-
tude of average silhouette (Average Silhouette Width) can be used,
Table 8
Cluster comparison when applying the AGNES (threshold = height’s average +2 std.
deviations) and PAM techniques.

Windows AGNES (# of clusters) PAM (# of clusters)

Window 1 4 5
Window 2 4 7
Window 3 3 6
Window 4 4 2
Window 5 4 7
Window 6 4 5
Window 7 3 6
Window 8 3 2

Table 7
Cluster comparison when applying the AGNES (threshold = height’s average +1 std.
deviation) and PAM techniques.

Windows AGNES (# of clusters) PAM (# of clusters)

Window 1 8 5
Window 2 10 7
Window 3 9 6
Window 4 8 2
Window 5 7 7
Window 6 9 5
Window 7 7 6
Window 8 7 2
which is nothing more than the arithmetic mean of the s(i) indices
calculated for all observations of the cluster.

In Figs. 5a and b the graphs representing the result of the seg-
mentation in k = 7 clusters using the PAM algorithm with the data
of window 1 are shown.

5.6. Monitoring the evolving clusters

Having as the last analysis proposal the monitoring of the evolu-
tion in the number of clusters (see item 4 of Section 3.4), it was tested
several values for the number of k clusters in all temporal windows
and it was evaluated what value of k is the most adequate in each
window. For the evaluation index the average amplitude value of sil-
houettes s(i) will be used, calculated from the application of the k-
medoids partitioning technique. The number of k clusters that
obtains the highest index will be deemed the most appropriate.

Fig. 6 corresponds to the graphs of the amplitude averages of
the silhouettes by the k number of clusters ranging from 1 to 7.
The highest index is highlighted in red and it is called the ‘best’,
which means that with that number of k clusters obtained the
cluster with the best evaluation index. Note that if two or more k
values obtain the same level of review and if this is the highest
value on the chart, it will be considered as the best the cluster with
the highest k value.

Comparing the application of clustering technique by partition
(PAM) with the hierarchical clustering (AGNES) technique, it
obtained different results in the number of clusters, as seen in
Table 7. This was due to the threshold set for the definition of
the k number of clusters in the result analysis of applying the
AGNES technique, which was defined as the average Height’s value
with an added standard deviation. This threshold does not take
into account any index that can measure the clustering quality.
As for the application of the PAM technique, the average of the
silhouettes was used to evaluate the clusters.

It is noted from Table 8 that the number of clusters found with
AGNES technique is higher, and that if the threshold is increased to
the average value of the Heights added in two more deviations, it is
possible to decrease the number of clusters, approaching the
results obtained with the PAM technique, as seen in Table 7.

6. Conclusions

This study proposes a methodology for the characterization of
time series in multivariate TDBs for cluster analysis. Our method-
ology allows monitoring the evolution of the number of clusters
and the movement of objects between clusters. In the characteriza-
tion of the time series, the level and trend components were used,
calculated by the Holt-Winters smoothing model. The seasonality
component was disregarded by the fact that its characterization
would require a long history to be seen, being our proposal the
characterization of series for short periods of time. This situation
is present when it is necessary to analyze problem domains where
the changes should be promptly detected for fast decision making.
One direction of research could be to consider time series for long
periods in order to consider the seasonality parameter. That new
approach would permit to model and understand the dynamic
and the preferences of the stock market applicators or the levels
of fidelity of the clients in a telecommunications enterprise.

The Holt-Winters model is a model that is easy to understand
and apply, flexible and that can handle various series sizes. This
last model feature comes from joining the approach of series seg-
mentation for the characterization and monitoring over time. This
allows the identification of the changing time of the series clusters,
which generates the discovery of temporal knowledge.

As for the clustering techniques, the AGNES (Agglomerative
Hierarchical Cluster) and DBSCAN (density-based) techniques were



610 A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611
compared to set the initial number of clusters that was most
appropriate for the first window of analysis of the TDB and the
PAM (Partition Clustering) technique to monitor the movement
of objects between clusters and between the windows. Regarding
the evolution of the number of clusters between windows, both
the PAM and AGNES techniques can be used. However, for the
AGNES technique, attention should be given to the (Height’s)
threshold set to define the number of clusters (k), as long as the
threshold does not directly measure the clustering quality. For
the PAM technique, the average silhouettes can be used to evaluate
the cluster quality in the search of the ideal number of clusters.

Regarding the clustering techniques, there are also challenges
to be tested and evaluated, as there are several techniques and
algorithms developed that can be tested or even tailored to
improve the analysis methodology of a TDB proposed in this study.
As example it is possible to consider the algorithms: DBSCAN,
OPTICAL and HDBSCA (Campello, Moulavi, & Sander, 2013) an
improved version of the first two, which can automatically deter-
mine the number of clusters.

It is important to note that the stage of cluster analysis the char-
acteristics of the series (level and trend) were considered as scalar
measures. However, in essence these components should be
addressed jointly. This limitation can be solved using vector dis-
tance measures, since each pair of level and trend form a vector
of size two.

For applying the experiments, a synthetic TDB was generated by
running a set of scripts developed by the R package. It is important
to note that the choice for a synthetic TDB does not interfere with
the proposed methodology. The use of TDB from real domains can
help contextualize the movement of objects between clusters and
between windows. In these applications, the model can be further
explored in the use of its flexibility of smoothing constants, since
these constants work in the forgetting factor of the level and trend
components. Another direction of research would be to test other
models of time series such as the error models and those working
in the frequency domain.

Real TDBs may also contribute to the definition stage of the
amplitude in the temporal windows that can go beyond a fixed
value or be defined by the problem domain. If it is possible to
observe some correlation between the attributes taking into
account the variability in time, this can be used for determining
the optimum cut for evolutionary analysis of the clusters.

Within the new landscape of Big Data, our proposal allows the
evaluation of the cluster development, both in the movement of
objects such as the appearance or disappearance of clusters. The
model used to characterize the time series becomes interesting
because it can be applied for long or short periods of time.

Finally, as additional future research directions, associated to
characterization of time series of temporal databases for applica-
tion of TDM and for analyzing of the evolution of the time series
clusters, it is possible highlight the followings:

1. As mentioned, temporal clustering allows tracking moving
objects and the appearance and disappearance of clusters. It is
possible to develop intelligent systems that can work with
alerts, if the domain is financial risk or credit, or with recom-
mendations, if the problem domain is to capture, customize
and fidelize customers, as some examples.

2. When patterns are discovered on databases for a specific time
period, in which the temporal aspect is not taken into account,
the question arises, What is the validity of the extracted knowl-
edge?, How long the pattern is valid?. The TDM research area
should to answer these questions proposing new theories,
approaches and criteria with restrictions in order to estimate
or predict the lifetime of the extracted pattern. For this, the time
series prediction models can be considered. Some authors call
this challenge of ‘‘Change Mining’’.

3. By tracking the evolution of groups is possible to observe the
dynamics of change, which, over time can show seasonal pat-
terns. So the concept of TDM can be attended in its broadest
way, ie, the search for patterns and its temporal changes, which
may be called Temporal Change Mining (TCM).

4. Propose a methodology to follow the evolution of the number of
clusters and the movement of objects between clusters by
means of temporal windows of different sizes, adapting TCM
for real condition of the problems.
Acknowledgments

The authors acknowledge the financial support received from
the Foundation for Research Support of Minas Gerais State-FAP-
EMIG, through Project CEX PPMVI 107/12 and National Council
for Scientific and Technological Development – CNPq, Brazil.

References

Al-Naymat, G., & Taheri, J. (2008). Effects of dimensionality reduction techniques on
time series similarity measurements. IEEE Transactions on Knowledge and Data
Engineering, 5, 188–195.

Ankerst, M., Breunig, M. M., Kriegel, H.-P., & Sander, J. (1999). OPTICS: Ordering
points to identify the clustering structure. In Proc. ACM SIGMOD international
conference on management of data (pp. 49–60). ACM Press.

Böttcher, M., Spott, M., Nauck, D., & Kruse, R. (2009). Mining changing customer
segments in dynamic markets. Expert Systems with Applications, 36(1), 155–164.

Campello, R. J. G. B., Moulavi, D., Sander, J. (2013). Density-based clustering based
on hierarchical density estimates. In Proc. 17th Pacific-Asia conference on
knowledge discovery in databases, PAKDD 2013. Lecture Notes in Computer
Science 7819 (p. 160).⁄⁄⁄

Chatfield, C., & Yar, M. (1988). Holt-Winters forecasting: Some practical issues. The
Statistician, 37(2), 129–140.

Ester, M., Kriegel, H-P., Sander, J., & Xu, X. (1996). A density-based algorithm for
discovering clusters in large spatial databases with noise. In Proceedings of the
second international conference on knowledge discovery and data mining (KDD-96)
(pp. 226–231). AAAI Press.

Gardner, E., & Diaz-Saiz, J. (2008). Exponential smoothing in the
telecommunications data. International Journal of Forecasting, 24, 170–174.

Gullo, F., Ponti, G., Tagarelli, A., & Greco, S. (2009). A time series representation
model for accurate and fast similarity detection. Pattern Recognition, 42(11),
2998–3014.

Hinneburg, A., Aggarwal, Ch. C., & Keim, D. A. (2000). What is the nearest neighbor
in high dimensional spaces?. In Proceedings 26th international conference on very
large data bases, VLDB 2000 (pp. 506–515). Cairo, Egypt.

Kakizawa, Y., Shumway, R., & Taniguchi, N. (1998). Discrimination and clustering for
multivariate time series. Journal of the American Statistical Association, 93(441),
328–340.

Kaufman, L., & Rousseeuw, P. J. (1990). Finding groups in data: An introduction to
cluster analysis. New York: Wiley.

Last, M., Kandel, A., & Bunke, H. (2004). Data mining in time series databases. Series
in machine perception and artificial intelligence (Vol. 57). Singapore: World
Scientific.

Last, M., Klein, Y., Kandel, A., & Abraham, K. (2001). Knowledge discovery in time
series databases. IEEE Transactions on Systems, Man, and Cybernetics – Part B:
Cybernetics, 31, 160–169.

Liao, T. W. (2005). Clustering of time series data – a survey. Pattern Recognition, 38,
1857–1874.

Lin, W., Orgun, M. A., & Williams, G. J. (2002). An overview of temporal data mining.
In Proc. the australian data mining workshop (ADM’02)(pp. 83–131).

MacQueen, J. (1967). Some methods for classification and analysis of multivariate
observations. In: Proc. 5th berkeley symposium on mathematical statistics and
probability, Vol. 1 (pp. 281–297).

Morettin, P., & Toloi, C. (1960). Análise de séries temporais (2nd ed.). São Paulo:
Blucher.

Roddick, J. F., & Spiliopoulou, M. (2002). A survey of temporal knowledge discovery
paradigms and methods. IEEE Transactions on Knowledge and Data Engineering,
14(4), 750–767.

Roddick, J. F., Spiliopoulou, M., Lister, D., & Ceglar, A. (2008). Higher order mining.
SIGKDD Explorations, 10(1), 5–17.

Wang, X., Wirth, A., & Wang, L. (2007). Structure-based statistical features and
multivariate time series clustering. In Proc. 7th IEEE international conference on
data mining (pp. 351–360).



A.P. Serra, L.E. Zárate / Expert Systems with Applications 42 (2015) 596–611 611
Wang, Q., & Megalooikonoumou, V. (2008). A dimensionality reduction technique
for efficient time series similarity analysis. Information Systems, 33, 115–132.

Ward, J. (1963). Hierarchical grouping to optimize an objective function. Journal of
the American Statistical Association, 58, 236–244.

Wilpon, J., & Rabiner, L. (1985). Modified k-means clustering algorithm for use in
isolated word recognition. IEEE Transactions on Acoustics, Speech, and Signal
Processing, 33(3), 587–594.
Winters, P. (1960). Forecasting sales by exponentially weighted moving average.
Management Science, 6, 324–342.

Xiong, Y., & Yeung, D.-Y. (2004). Time series clustering with ARMA mixtures. Pattern
Recognition, 37(8), 1675–1689.

Zhang, L., Chen, G., Brijs, T., & Zhang, X. (2008). Discovering during-temporal
patterns (DTPs) in large temporal databases. Expert Systems with Applications,
34(2), 1178–1189.




