coverag charact base neural machin translat tecnica cobertura y caracter integrado en la traduccion automatica basada en aprendizaj profundo m.bashir kazimi marta r. costa-jussa talp research center universitat politecnica catalunya, barcelona abstract: recent years, neural machin translat (nmt) achiev state- of-th art perform translat language; sourc language, another; target language. however, propos method us word embed techniqu repres sentenc sourc target language. charact em- bed techniqu task suggest repres word sentenc better. moreover, recent nmt model us attent mechan relev word sourc sentenc gener target word. problem approach word translat multipl times, word translated. address problem, coverag model integr nmt track already-transl word focu untransl ones. research, present new architectur us charact embed repres sourc target languages, us coverag model certain word translated. experi perform compar model coverag charact model result model perform better models. keywords: machin learning, deep learning, natur languag processing, neu- ral machin translat resumen: en lo ultimo anos, la traduccion automatica basada en el aprendizaj profundo ha conseguido resultado estado del arte. sin embargo, mucho lo metodo propuesto utilizan espacio palabra embebido para representar una oracion en el idioma origen y destino y esto genera mucho problema nivel cobertura vocabulario. avanc recient en la traduccion automatica basada en aprendizaj profundo incluyen la utilizacion caracter que permit reducir la palabra fuera vocabulario. por otro lado, la mayora algoritmo traduccion automatica basada en aprendizaj profundo usan mecanismo atencion dond la palabra ma relevant en la oracion fuent se utilizan para generar la traduccion destino. el problema est enfoqu es que mientra alguna palabra se traducen varia veces, alguna otra palabra se traducen. para abordar est problema, usamo el modelo cobertura que realiza seguimiento la palabra ya tra- ducida y se centra en la traducidas. en est trabajo, presentamo una nueva arquitectura en la que utilizamo la incorporacion caracter para representar el lenguaj origen, y tambien usamo el modelo cobertura para asegurarno que la frase origen se traduc en su totalidad. presentamo experimento para comparar nuestro modelo que integra el modelo cobertura y modelo caracteres. lo resultado muestran que nuestro modelo se comporta mejor que lo otro do mod- elos. palabra clave: aprendizaj automatico, aprendizaj profundo, procesado del lenguaj natural, traduccion automatica 1 introduct machin translat (mt) task us- ing softwar translat text languag another. natur languag world complex fact word differ mean base context in, differ gram- matic categori (e.g. match noun verb). therefore, main challeng mt fact correct translat word, requir differ factor considered; grammat struc- ture, context, preced succeed- ing words. years, research devel- op differ method order reduc manual work human in- tervention, increas auto- matic work, machin depend transla- tion. main method mt sta- tistic machin translat (smt) data-driven approach produc trans- lation base probabl sourc target language. goal maxim condit probabl p(y|x) target sentenc y given equival sourc sentenc x base set pre- design featur (koehn, 2009). nmt recent approach ma- chine translat pure base larg neural network train learn translat text sourc tar- language. unlik smt, re- quir pre-design featur function train fulli base train data (lu- ong manning, 2015). nmt attract attent research re- cent years. us neural network translat baidu (zhongjun, 2015), attent googl nmt (wu et al., 2016), facebook automat text trans- lation, industri given urg research nmt push. research, studi state art nmt, propos novel approach combin recent model nmt; coverag (tu et al., 2016) charac- ter model (costa-jussa fonollosa, 2016), hope achiev state art results. rest paper organ follows. section 2 studi relat work nmt, section 3 explain propos model studi point contribut research, section 4 explain exper- iment perform result obtained, final section 5 summar thesi point possibl futur research. 2 relat work nmt achiev state art result mt, nmt model re- current neural network (rnn) encod de- coder architectur (sutskever, vinyals, le, 2014; cho et al., 2014). ap- proach, input sentenc encod encod fixed-length vector ht recurr neural network (rnn), fixed-length vector decod decoder; rnn, gener output sen- tence. word-embed (mandelbaum shalev, 2016) representa- tion sourc target words. main issu simpl rnn encod decod model encod vector fix length, repres long sentenc completely. address issue, attent model introduc simpl rnn encod decod model (bahdanau, cho, bengio, 2014). at- tention model us bi-direct recurr neural network store inform memori cell instead fixed-length vector. small neural network call attent mechan us input inform memori cell inform pre- viousli translat word decod or- der focu relev input word translat specif output word. model mention above, word em- bed word representa- tions. perform well, limit nmt model fixed-s vocabulary. model train larg set vocabularies, vocabulari lim- ited, model face problem rare out-of-vocabulari (oov) word (yang et al., 2016; lee, cho, hofmann, 2016). word morphologi- cal forms, affixes, word- embed model abl dis- tinguish word train affix ad differ morphologi- cal form word (chung, cho, bengio, 2016). address problems, propos us charact embed- ding word embedding, result fulli character-level nmt (lee, cho, hofmann, 2016), charact base nmt model us charact embed sourc languag (costa-jussa fonollosa, 2016; kim et al., 2015), character-level decod us charact embed target languag (chung, cho, bengio, 2016). addit ad- vantag charact embed nmt usabl multilingu translation, result abil identifi share morpholog structur lan- guages, fact oppos word embed models, text segmenta- tion required, enabl learn map sequenc charac- ter overal mean represent au- tomat (lee, cho, hofmann, 2016). prove charact nmt mod- el produc improv perform attent model (costa-jussa fonollosa, 2016; yang et al., 2016; lee, cho, hof- mann, 2016; chung, cho, bengio, 2016). issu model mention earlier; specif case atten- tion model, track translat histori hence, word translat time word translat translat falsely. address problem, differ model coverag propos track translat history, avoid translat word multipl time focu word translat (tu et al., 2016; mi et al., 2016). author claim achiev better result compar attent base model. 3 coverag charact base neural machin translat 3.1 contribut research base model rnn encod decod (sutskever, vinyals, le, 2014; cho et al., 2014) attent model (bahdanau, cho, bengio, 2014), produc charact model (costa-jussa fonollosa, 2016; yang et al., 2016; kim et al., 2015; lee, cho, hof- mann, 2016) coverag model (tu et al., 2016; mi et al., 2016) achiev state art results, model address issu earlier model sep- arately. charact model address problem rare, oov words, word morpholog structures, us charact embed word em- bedding, coverag model address problem word trans- late multipl time rest fals translated. research, propos jointli address im- portant problem tradit nmt model introduc coverag charact model achiev state art result nmt. charact embed sourc words, target word us word embedding. 3.2 architectur propos nmt model backbon propos architectur attent model propos bahdanau et al. (bahdanau, cho, ben- gio, 2014) word embed input languag replac charact embed propos costa-jussa fonollosa (costa-jussa fonollosa, 2016). thus, all, encod comput input sentenc summari ht = [ h t; h t] concaten h t h t t = 1, 2, ...., t . h t h t hidden state forward backward rnn encod read inform input sen- tenc forward revers order, re- spectively. hidden state calcul follows. h t = f (xt, h t1) (1) h t = f (xt, h t1) (2) h t1 h t1 denot previou hidden state forward backward rnn, f f recurr activ functions, xt embed represen- tation t-th input word. atten- tion model, xt simpl word embed represent word sourc lan- guage, case, xt charact embed calcul propos costa- jussa fonollosa (costa-jussa fonol- losa, 2016) explain follows. all, sourc word k repre- sent matrix ck sequenc vector repres charact embed- ding charact sourc word k. then, n convolut filter h length w, w rang 1 7, appli ck order obtain featur map fk sourc word k follows. fk[i] = tanh(ck[, : i+w1], h+b) (3) b bia i-th element featur map. convolut filter h, output maximum valu select max pool layer order captur import feature. ykh = max fk[i] (4) concaten output valu n convolut filter h; yk = [ykh1, y k h2, ...., y k hn], represent sourc word k. addit highwai network layer prove bet- ter represent sourc word (kim et al., 2015). layer highwai network perform follows. xt = t g(whyk + bh) + (1 t) yk (5) g nonlinear function, t = (wty k + bt ) transform gate, (1 - t) carri gate, xt charact em- bed equat 1 2. decod gener summari zt target sentenc follows. zt = f(zt1, yt1, st) (6) st represent sourc word calcul follows. st = t t=1 ttht (7) ht calcul encod ex- plain earlier, tt comput fol- lows. tt = exp(ett)t k=1 exp(etk) (8) ett = a(zt1, ht, ct1t) (9) call attent mechan align model score relev input word posit t output word posit t, ct1t previou cov- erag coverag model propos tu et al. (tu et al., 2016) calcul follows. ctt = f(ct1t, tt, ht, zt1) (10) then, output sentenc gener comput condit distribut possibl translation. log p(y|x) = p(yt |y<t , x) (11) y x output input sen- tences, respectively, yt t -th word sentenc y. condit proba- biliti term p(yt |y<t , x) comput feed forward neural network follows. p(yt |y<t , x) = softmax(g(yt1, zt , st)) (12) g nonlinear function, zt de- code state equat 6, st context vector equat 7 overal architectur propos model illustr figur 1, 2, 3. figur 1 illustr charact base word embed- ding model take input embed- ding charact sourc word xt, output final word level represent it. output fed encoder; depict figur 2 output context vector st base attent mechan coverag model. context vector st fed decod illustr figur 3 gener target translation. highwai network highwai network e c h t xt charact embed multipl convolut differ length max pool layer select maximum output filter layer highwai network figur 1: charact base word embed echt dick kist charact base word embed x1 x2 x3 h1 h2 h3 + c1 c2 c3 attent layer t = 1 st coverag bidirect rnn figur 2: encod coverag & align 4 experi order evalu perform model, experi data set z2z1 echt dick kist charact base word embed x1 x2 x3 encod + attent layer s1 s2 awesom y1 sauc y2 figur 3: decod perform charact model costa-jussa fonollosa (costa-jussa fonollosa, 2016), coverag model tu et al.(tu et al., 2016), final pro- pose model study; coverag char- acter model. section divid subsections. subsect 4.1 explain data set preprocess per- form data, subsect 4.2 elab- orat evalu method re- sult obtained. 4.1 data data set experi kindli provid costa-jussa (costa-jussa, 2017) includ subset larger data set includ set paper edit 10 year bilingu catalan news- paper , el periodico, addit cor- pu medic domain provid universal- doctor project1. preprocess task, data set token dic- tionari 10 thousand frequent word prepar train system. detail inform data set list tabl 1 languag set # sentenc # word # vocab ca train dev test 83.5k 1k 1k 2.9m 27.7k 27k 83.5k 6.9k 6.7k es train dev test 100k 1k 1k 2.7m 25k 24.9k 90k 7k 7k tabl 1: spanish-catalan dataset statist 1 model bleu score charact 53.30 coverag 53.76 character+coverag 54.87 tabl 2: bleu score nmt model 4.2 evalu result evalu model, bleu (bilingu evalu understudy) evalu method propos papineni et al.(papineni et al., 2002) used. main idea evalu method closer human translat machin translat is, better model performs. result experi perform data set mention section 4.1 list tabl 2. observ tabl 2, propos model outperform model achiev state art performance. main motiv studi try ad- dress main issu attent model. first, attent model us word embed- ding languag representation, suffer rare, oov word problems, problem identifi differ mor- pheme ad word. second issu attent model fo- cuse relev input sen- tenc order translat gener output sentence, track already-transl words, lead mul- tipl translat word rest fals translated. is- sue individu tackl charac- ter model (costa-jussa fonollosa, 2016; yang et al., 2016; lee, cho, hofmann, 2016; kim et al., 2015), coverag mod- el (tu et al., 2016; mi et al., 2016), respec- tively. research, tri improv state art introduc coverag charact model nmt. experi perform data set shown tabl 1 clearli show model outperform earlier models, shown tabl 2. under- stand contribut propos model combin charact coverag model compliment model perform better models, list tabl 3 manual analysi sampl translat mod- el tested. martaruiz highlight martaruiz cross-out martaruiz insert text us subset larg corpu extract paper edit bilingu catalan newspaper, el peridico (costa-juss`a et al., 2014). spanish-catalan corpu partial avail elda (evalu languag resourc distribut agency) catalog number elra-w0053. reference: marta r. costa-juss`a, march poch, jose a.r. fonollosa, mireia farrus, jose b. marino. 2014. larg spanish-catalan parallel corpu releas machin translation. comput informat journal, 33 martaruiz cross-out martaruiz sticki note repeat surnam times... "by tu et al. (2016)" previous... refer paper! martaruiz cross-out martaruiz insert text word martaruiz cross-out martaruiz insert text martaruiz cross-out martaruiz cross-out martaruiz insert text martaruiz sticki note need list refer here... relat work, enough! martaruiz sticki note exampl model capabl keep best translat character-bas (exampl x) coverage-bas (exampl y) baselin system add new improv (exempl xxx) (you write better ;-), idea... martaruiz cross-out martaruiz cross-out 1 src tgt ch cov ch+cov do regidor es presenten al comicis. do concejal se pre-sentan alo comicios. do edil se presentan en lo comicios. do concejal sepresentan lo comicios. do concejal se presentan lo comicios. 2 src tgt ch cov ch+cov la falta public l ha condemnat mort en una zona clau l oci barcelon que , pel que es veu , te me poder convocatoria. la falta publico lo ha condenado muert en una zona clave del ocio barcelon que , por lo que se ve , tien ma poder convocatoria. palma alguna publico le ha condenado muert en una zona clave del ocio barcelon que , por lo que se ve , tien ma poder convocatoria. falta publico al ha condenado muert en una zona clave del ocio barcelon que , por lo que se ve , tien mejor convocatoria. la falta publico le ha condenado muert en una zona clave del ocio barcelon que , por el que se ve , tien adema poder convocatoria.a. 3 src tgt ch cov ch+cov una firma austraca va voler vendr sang amb sida l asia.. una firma austriaca quiso vender sangr sida en asia. una firma unk quiso vender sangr sida en asia. una seguidor unk quiso vender sangr sida en asia. unafirma unk quiso vender sangr consida en asia 4 src tgt ch cov ch+cov com consequencia la progressiva reduccio del marges. como consecuencia la progresiva reduccion lo margenes. consecuencia la unk reduccion lo margenes. consecuencia la unk reduccion lo margenes. como consecuencia la progresiva reduccion lo margenes. 5 src tgt ch cov ch+cov ... requereix esforc que involucri departa turism , joventut educacio , tamb coordinacio en l ambit europeu ... ... requier esfuerzo que involucr departamento turismo , juventud y educacion , y tambien coordinacion nivel europeo ... ... requier esfuerzo que unk departamento turismo , joventut y educacion , y que tien que unk en el ambito europeo ... ... requier esfuerzo que unk departamento turismo , joventut y educacion , y que tien que unk en el ambito europeo ... ... requier esfuerzo que unk departamento turismo , juventud y educacion , y tambien coordinacion en el ambito europeo ... tabl 3: manual analysis. src tgt repres sourc target sentences, ch, cov, ch+cov repres translat character, coverage, propos moedl, respectively. exampl 1 2, propos model behav like coverag model, exampl 3, behav like charact model, exampl 4 5, perform better models. 5 summari recent model; attention, propos bahdanau et al.(bahdanau, cho, ben- gio, 2014) tackl problem fixed- length encod vector rnn en- coder decod model sutskev et al.(sutskever, vinyals, le, 2014) cho et al. (cho et al., 2014). give nmt abil abl translat sentenc length. face main problems; rare, oov word problem problem differ possibl morphem singl word, problem over- translat under-translation. char- acter model us charact embed (costa-jussa fonollosa, 2016; kim et al., 2015; yang et al., 2016; lee, cho, hof- mann, 2016) coverag models, track translat histori (tu et al., 2016; mi et al., 2016) individu ad- dress issues, respectively. research, coverag intro- duce charact model aim address main issu mention earlier al- together, improv state art nmt. corpu shown tabl 1 martaruiz cross-out martaruiz insert text model martaruiz cross-out martaruiz insert text una firma martaruiz sticki note mark bold correct translat martaruiz sticki note encoder-decod martaruiz sticki note italics? why? martaruiz sticki note please, add list references!!! martaruiz cross-out martaruiz cross-out martaruiz sticki note italics? experi result list tabl 2. clearli observ model studi outperform pre- viou model achiev state art perform nmt. case charact model, charact embed sourc language, target languag limit word embedding. re- search requir order studi char- acter embed ad target lan- guag impact perform model, left investig factor af- fect perform nmt systems. acknowledg work support ministerio economa y competitividad fondo eu- ropeo desarrollo regional, con- tract tec2015-69266-p (mineco/feder, ue) postdoctor senior grant ramon y cajal. refer bahdanau, d., k. cho, y. bengio. 2014. neural machin translat jointli learn align translate. corr, abs/1409.0473. cho, k., b. van merrienboer, c. gulcehre, d. bahdanau, f. bougares, h. schwenk, y. bengio. 2014. learn phrase represent rnn encoder-decod statist machin translation. arxiv preprint arxiv:1406.1078. chung, j., k. cho, y. bengio. 2016. character-level decod explicit segment neural machin transla- tion. corr, abs/1603.06147. costa-jussa, m. r. 2017. catalan- spanish neural machin translation? anal- ysis, comparison combin standard rule phrase-bas technolo- gies. a: workshop nlp similar languages, varieti dialects. pro- ceed fourth workshop nlp similar languages, varieti di- alect (vardial), page 5562. costa-jussa, m. r. j. a. r. fonollosa. 2016. character-bas neural machin translation. corr, abs/1603.00810. kim, y., y. jernite, d. sontag, a. m. rush. 2015. character-awar neu- ral languag models. arxiv preprint arxiv:1508.06615. koehn, p. 2009. statist machin transla- tion. cambridg univers press. lee, j., k. cho, t. hofmann. 2016. fulli character-level neural machin translat explicit segmentation. corr, abs/1610.03017. luong, m.-t. c. d. manning. 2015. stanford neural machin translat sys- tem spoken languag domains. mandelbaum, a. a. shalev. 2016. word embed us sentenc clas- sific tasks. corr, abs/1610.08229. mi, h., b. sankaran, z. wang, a. it- tycheriah. 2016. coverag embed- ding model neural machin translation. corr, abs/1605.03148. papineni, k., s. roukos, t. ward, w.- j. zhu. 2002. bleu: method auto- matic evalu machin translation. proceed 40th annual meet associ comput linguis- tics, page 311318. associ com- putat linguistics. sutskever, i., o. vinyals, q. v. le. 2014. sequenc sequenc learn neural networks. advanc neural informa- tion process systems, page 31043112. tu, z., z. lu, y. liu, x. liu, h. li. 2016. coverage-bas neural machin transla- tion. corr, abs/1601.04811. wu, y., m. schuster, z. chen, q. v. le, m. norouzi, w. macherey, m. krikun, y. cao, q. gao, k. macherey, et al. 2016. googl neural machin translat sys- tem: bridg gap human machin translation. arxiv preprint arxiv:1609.08144. yang, z., w. chen, f. wang, b. xu. 2016. character-awar encod neu- ral machin translation. coling. zhongjun, h. 2015. baidu translate: re- search products. acl-ijcnlp 2015, page 61.