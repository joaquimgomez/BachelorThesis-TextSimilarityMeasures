novemb 21, 2014 appli artifici intellig (aai) journal main appear appli artifici intellig (aai) journal vol. 00, no. 00, month 20xx, 116 random algorithm exact solut transduct support vector machin g. espositoa m. martina universitat politecnica catalunya, barcelona, spain; (v1.0 releas octob 2014) random sampl effici method deal constrain optim problems. comput geometry, method successfulli applied, clarkson algo- rithm (clarkson, 1996), solv gener class problem call violat spaces. machin learning, tsvm learn method small fraction label data available, impli solv non convex optim problem. approxim method propos solv it, usual suboptim solutions. however, global optim solut obtain exact techniques, cost suffer exponenti time complex respect number instances. paper, interpret tsvm term violat space given. hence, random method present extend us exact method reduc time complex exponen- tial w.r.t. number support vector optim solut instead exponenti w.r.t. number instances. keywords: transduction, semi-supervis learning, support vector machine, violat spaces, branch bound 1. introduct comput geometry, random sampl effici method deal con- strain optim problems. firstli find optim solut subject random subset constraints. likely, expect number constraint violat solut significantli smaller overal number remain constraints. lucki cases, solut violat remain constraints. hence, exploit properti build simpl random algorithm. clarkson algorithm (clarkson, 1996) two-stag random sampl techniqu abl solv linear pro- gram problems, appli gener framework violat spaces. violat space framework well-establish tool field geometr optimization, develop subexponenti algorithm start random- iz variant simplex method. class violat space includ problem comput minimum-volum ball ellipsoid enclos given point set rd, problem find distanc convex polytop rd computa- tional geometri problem (gartner, j. matousek, p.skovron, 2008). gener violat space problem make applic number non-linear geometr problems. clarkson algorithm stage base random sampl conceptu simple. shown particular optim problem correspond author. email: 1 novemb 21, 2014 appli artifici intellig (aai) journal main regard violat space problem, certain algorithm primit implement it, clarkson algorithm immedi applicable. machin learning, transduct support vector machin (tsvm ) (vapnik, 1995) extend know support vector machin handl partial label datasets. tsvm learn maximum margin hyperplan classifi label train data present unlabel data insid margin. unfortunately, deal tsvm impli solv non convex optim problem. wide spectrum approx- im techniqu appli solv tsvm problem (chapelle, 2008), guarante find optim global solution. fact, state-of-the-art approxim tsvm method appli differ benchmark problems, far optim solut (chapelle, 2008). unfortun exact method appli small dataset exponenti time complex cost respect number instances. (j., y., j., o., 2008) balcazar et alt. sug- gest hard margin svm belong class violat space, propos random sampl techniqu determin maximum margin separ hyperplane. note problem solv svm convex solv tsvm non convex problem, differ nature. paper prove global optim solut tsvm total reli knowledg support vector set, gather size smaller set instances. moreover, demonstr tsvm formul violat space problem allow us clarkson algorithm optim global solution. foster tsvm sparsiti property, introduc random algorithm abl reduc time complex- iti exact methods, scale exponenti w.r.t. number support vector optim solut instead exponenti w.r.t. number instances. method exact solut independ number instanc problem support vectors. includ problem dimension featur space rel small. 2. violat space random algorithm violat space problem introduc abstract framework random al- gorithm abl solv linear program variant simplex algorithm. com- putat geometry, exampl problem solv method smallest enclos ball (seb). goal comput smallest ball enclos set n point d dimension space (fig. 1). follow introduc main tool abstract framework violat space order random- iz method devis comput geometri appli solv tsvm problem. detail proof report properti (gartner et al., 2008). definit 1: abstract lp-type problem consist finit set h, repres constraint problem, weight function w give g h cost w(g) w optimum solution, w linearli order (w,). (h,w,w,) satisfy: monotonicity: f g h w(f ) w(g) locality: f g h h h w(f ) = w(g) w(g) < w(g {h}) w(f ) < w(f {h}) g h basi inclusion-minim subset b h w(g) = w(b). combinatori dimens lp-type problem size largest basis. violat g addit constraint h h w(g) 6= w(g h). 2 novemb 21, 2014 appli artifici intellig (aai) journal main extrem violat figur 1. smallest enclos ball: extrem (red encircl red) point essenti solution, violat (in blue) point ly outsid ball, basi minim set point have ball element h extrem g w(g {h}) 6= w(g). definit h violat g h extrem g{h}. set constraint violat g call violat map v (g) := {h h : w(g) < w(g {h})} . violat map possibl phrase term violat test lp-type problem neglect explicit evalu w: definit 2: violat space pair (h,v ) h finit set v mapping. (h,v ) satisfy: consistency: g v (g) = hold g h locality: f g h g v (f ) = v (g) = v (f ). given lp-type problem (h,w,w,) pair (h,v ) violat space gener framework appli random algorithm describ later. violat space (h,v ) combinatori dimens , order setup basi evalu algorithm need defin follow primit operation: primit 1: (violat test) (h,v ) violat space implicitli defin prim- itive: given g h h h\g decid h v (g) consid suitabl violat space defin v (r) := {h g\r | w(r {h}) 6= w(r)} set violat r x(r) := {h r | w(r\{h}) 6= w(r)} set extrem r. follow result holds: lemma 2.1 (sampl lemma): set r size r uniformli chosen random set ( g r ) r-element subset g (|g| = n), defin random variabl vr : r 7 |v (r)| xr : r 7 |x(r)| expect valu vr := e(vr), xr := e(xr). 0 r n vr = xr+1 nrr+1 . 3 novemb 21, 2014 appli artifici intellig (aai) journal main violat space |h| = n combinatori dimens choos subset r, sampl lemma bound expect number violat trough vr nr r+1 . clarkson (clarkson, 1996) envisag smart random algorithm abl solv viola- tor space problem reli expect number violat bound accord sampl lemma. seb case, w repres radiu ball, h set constraint requir point insid ball, violat point outsid ball. prove seb violat space problem clarkson algorithm solv it. algorithm proce round maintain vote box initi contain vote slip point. round, set r r vote slip drawn random repetit vote box seb correspond set r computed. new round number vote slip violat point doubled. algorithm termin soon violat (no point outsid ball). r d2, expect number round o(log n) reduc problem size n o(log n) problem size o(d2). clarkson algorithm problem find basi solv trivial algorithm abl solut subset size . clarkson basis2 algo- rithm call trivial algorithm subroutine, increas probabl obtain base iterations. consid g multiset (h) (initi set one) de- note multipl h. set f g compound multipl element f (f ) := hf (h). sampl g envisag contain (h) copi element h. round, violat thresh- old, basis2 doubl multipl (weight) violat points, increment probabl select base rounds. converg basis2 algorithm reli fact trivial correct. iter loop success chang weight elements. estim unsuccess iter pass success ones, sampl lemma bound used. hence, shown k success iter hold 2k e[(b)] n ek/3 basi b g |g| = n particular k < 3 log n. summar clarkson algorithm basis2 comput basi g expect number 6n log n call primit 1, expect number 6 log n call trivial set size 62. 3. transduct support vector machin tsvm describ train set l label exampl {(xi, yi)} xi rd label yi = {1} = 1, ..., l i.i.d. accord unknown distribut p(x, y) u unlabel exampl {xk} k = l + 1, ..., n n = l + u total number examples, distribut accord distribut p(x). consid w vector normal hyperplan constant b, problem formul find vector label yu = {yl+1, ..., yn} (yi = {1}) have maxim geometr margin separ hyperplan (w, b) solving: 4 novemb 21, 2014 appli artifici intellig (aai) journal main i(w, b, yu) = min (w,b,yu) w22 2 + c l i=1 p + c n k=l+1 p k (1) subj. yi(w t(xi) + b) 1 0 1 l yk(w t(xk) + b) 1 k k 0 l + 1 k n p = 1 2 respect linear (l1) loss quadrat (l2) losses. term control gener capac others, slack variabl i, number misclassifi samples. regular paramet (c c ) akin confid known labels. decis function hypothesi space f f repres f(xi) = w t(xi) + b yi = sign (f(xi)) assum exist given hilbert h space map : rd h. map send exampl data featur space gener kernel satisfi mercer conditions. work refer quadrat losses, fix yu, associ hessian matrix posit definit bring object function uniqu strictli convex. moreover, optim problem consid computation stabl l2 losses. however, main result appli l1 loss case. earli on, main strategi adopt minim i(w, b, yu): local approxim methods: start tent label yu perform local search space labelings, effect heuristics, maxim i(w, b, yu). method sensibl local minima. instance, svm ligth method (joachims, 1999) us local search algorithm solut fail deliv solut close global one. exact combinatori methods: fix unlabel vector yu i(w, b, yu) con- vert optim (w, b) standard svm. combinatori method global optim solut search entir space possibl label yu svm maximum margin. focus exact combinatori optim j (yu) = min(w,b) i(w, b, yu), ob- jectiv minim j (yu) set binari variables. optim non convex belong comput class np-hard. solved, instance, branch bound (bb) (chapelle, sindhwani, keerthi, 2006) integ program (ip) (bennett demiriz, 1998), computation demand larg number differ possibl label unlabel instances. 3.1. critic analysi tsvm exact global solut examin optim problem show optim solut depend label unlabel data, label support vector optim solut sparsiti properti svms. observ 1: assum have set l label r unlabel examples. yr vector label solv tsvm cost j (yr) (i.e. label instanc r lower cost). consid svmyr induct svm label (ylyr). 5 novemb 21, 2014 appli artifici intellig (aai) journal main therefore, ad unlabel point x set l r, ly outsid margin svmyr, induc optim label transduct problem l r {x} yr{x} = (yr {yx}) (yx label induc svmyr x). proof. assum exist differ label yr{x} better yr{x} j (yr{x}) < j (yr{x}) lead contradiction. consequ yr{x} optim labelling. fact, assumpt j (yr{x}) < j (yr{x}) fact ad point x ly outsid margin svm yr chang svm solut follow j (yr{x}) = j (yr). finally, stem j (yr) j (y r{x}) ad point given label increas leav unchang cost. j (yr) j (y r{x}) < j (yr{x}) = j (yr) consequ j (y r) < j (yr) contradiction, hypothesi yr repres optim label set r. observ 2: assum have set l label u unlabel examples. consid subset r u given vector label yr solv tsvm set l r svmyr induct svm label (yl yr). assum dont point u \ r ly insid margin svmyr. case, optim label tsvm lu repres ylu = (yl yu ), yu vector label induc solut svmyr set u . proof. prove appli observ 1 point u \ r. start label vector yr, notic point u \ r li outsid margin svm yr iter appli observ 1 end global solut l u . observ 3: necessari condit global optim solut set point l u subset l r r contain support vector global optim solut l u proof. prove appli observ 2. note optim solut l u built l r. condit point u \ r lie insid margin. henceforth, wont support vector u \r r. accord observ come follow conclusion: given tsvm, data point unnecessari build global optim solution. set support vector complet defin global optim solut (tsvm sparsiti property). suggest feasibl obtain tsvm optim global solut set l r includ set support vectors. question right subset points. claim tsvm solut obtain work reduc subset exampl l r comput leverag |r| |u|. noticeably, tsvm build violat space solv problem random algorithms. worth note method practic us number support vector solut tsvm problem scale o(log n), usual case. however, method appli solut support vector dimens featur space problem rel small. 6 novemb 21, 2014 appli artifici intellig (aai) journal main 4. random algorithm tsvm section readi tsvm problem belong class violat space. therefore, clarkson algorithm optim global solution. case, weight function w repres i(w, b, yu) evalu subset constraint l r l u combinatori dimens depend number support vectors. given subset partial label points, tsvm global optim solut (a basis) obtain exact method like ip bb (our trivial algorithm). need defin violat test clarkson algorithm reli probabl select basi increas weight violat points. violat easili detect remain point ly tsvm separ margin. tsvm arous formul term violat space problem. endow constrain formul tsvm problem violat space definit formal propose: proposit 4.1: given tsvm constraint hf , wf (gf ) : 2 hf wf map- ping defin subset gf hf wf (gf ) = minhf f (gf ) = f (wg, bg, yg) wf bound linearli order , quadrupl (hf , wf ,wf ,) repres associ violat space problem. order verifi local monoton need prove lemma: lemma 4.2: given tsvm constraint hf subset gf hf global optimum f (gf ), ad constraint hf hf gf chang global optimum accord f (gf ) f (gf hf ) proof. lemma easili prove contradiction. suppos lemma true f (gf ) > f (gf hf ). solut tsvm problem set constraint gf hf feasibl solut tsvm problem set constraint gf . hypothesi f (gf ) > f (gf hf ) contradict minim tsvm solut f (gf ). however, need provid primit build violat space, need construct proof lemma follow increment solut tsvm. consid subset constraint gf hf , get object f (gf ) mean global optim solut tsvm problem subset label unlabel data lr lu . exact method quest global optimum subset constraint gf entail 2 r possibl configur vector label yr, cast correspond svm minimum object f (gf ) solv dual problem written w (s, b) . ad constraint gf hf vector label yr+1 mean 2r+1 possibl configurations, select correspond svm minimum object f (gf hf ) solv dual express w ( s, h, b ). hence, relationship object valu consid increment train analysi svm report (cauvembergh poggio, 2001) differ case gather quadrat losses. svm dual problem obtain introduc r+ lagrangian multi- plier consid optim solut thekkt condit satisfied. elimin w, (note quadrat loss 6= 0 6= 0) dual problem w (, b) = max (r+,br) s i=1 1 2 s i,j=1 yiyjij(k(xi,xj) + ij c ) + b s i=1 iyi 7 novemb 21, 2014 appli artifici intellig (aai) journal main (where c = c c accordingly) defin = [ij ] = [yiyjk(xi,xj) + ij c ] (ij = 1 = j, 0 otherwise), kkt condit support vector (i 6= 0) s i=1 yii = 0 gs = yif(xi) + 1 = 0 defin vector y equival express compact form [ 0 y t y ] [ b ] = [ 0 1 ] (2) kkt non support vector (i = 0) express gr = yif(xi) 1 0. final solut dual problem s give maximum valu w (s) substitut kkt condit defin s = ( 0 y ts ys s ) written w (s) = 1 2 ts ss. increment train ad new point h consist adiabat increment h start h = 0 chang paramet kkt condit point h fulfilled. increment h ensur kkt condit point maintained. following, defin differ new alpha valu old valu . kkt condit h updat connect variat support vector s, b ad h accord to: 0 gs gr gh = 0 yt ys ss yr rs yh hs [ b s ] + yh th thr thh h support vector gs = 0 [ b s ] = [ 0 yt ys ss ]1 [ yh th ] h = q1 zhh (3) gather [ gr gh ] = [ yh hs yr rs ] [ b s ] + [ thr thh ] h tackl equat infer chang presenc new data function h. unfortunately, gener valu h directli us system obtain new state, possibl chang composit set support vector non support vector h increases. handl situat demand book keep possibl adiabat modifications. so, procedur consist increment h maximum follow case arise: problem solved, keep kkt condit point forc 8 novemb 21, 2014 appli artifici intellig (aai) journal main chang set support vectors1. particular deal increment h > 0 bring chang support vector composit depend h. possibl situations: h = 0 gh 0, new point chosen label yh wont chang set support vector solut w (s) h > 0 gh = 0 (g valu increment h), new point chosen label yh new support vector support vector set chang composit (no migrations) new kkt written 0 yt yhy ss th yh sh hh bs h = 01 1 ensu connect old solut sherman-morison-woodburi formula block matrix invers 0 yt yhy ss th yh sh hh 1 = [q1 0 0 0 ] + q1h [ q1 zh 1 ] [ zthq 1 s 1 ] qh = [ qs zh 1 ]t 0 yt yhy ss th yh sh hh [q zh 1 ] = hh zthq 1 s zh (4) qh 0 semidefinit posit kernel wherefrom express updat valu h = h = max(0, 1 yhb shs) hh zthq 1 s zh = 1 yhf(xh) qh = h qh [ b s ] = [ b s ] q1 zhh h = max(0, 1 yhf(xh)) optimum updat w ( s, h) = w (s) + 1 2 2hqh = w (s) + 1 2 2h qh entail w (s) w ( s, h) h > 0 gh < 0, new point chosen label yh new support vector support vector set chang composition. quadrat loss 1note case quadrat loss need divid set point sets: support (s) (o) , case linear losses, point belong differ sets: support (s), error (e) (o). 9 novemb 21, 2014 appli artifici intellig (aai) journal main mean possibilities: weather support vector, k k > 0 gk = 0, non support vector k = 0 g k 0 viceversa (from k = 0 gk 0 k > 0 g k = 0). case g h < 0 condit holds. set support vector chang increment h (it chang increment), global optimum increment accord evalu as: w ( s, h) = 1 t s + h 1 2 [ s h ]t [ ss sh hs hh ] [ s h ] w (s, h) = 1 ts + h 1 2 [ s h ]t [ ss sh hs hh ] [ s h ] s = s + s h = h + h allow evalu global optimum increment w = w ( s, h)w (s, h) . equat (3) express chang global optimum function h written as: g h = 1 + b yh + s j=1 hj j + hh h < 0 gh = 1 + byh + s j=1 hjj + hhh < 0 easili w = 1 2 qh 2 h gh h 0 gh neg (see assumpt case) qh positive. assum set equat describ evolut point chang step. however, increas h, check kkt condit verified. consequently, condit longer fulfil given point, statu chang (from support non support vector vice versa depend case) equat modifi accord bookkeeping. final remain object function monoton chang equat support vector set. easi l2 loss bookkeep chang valu object function. fact bookkeep necessari support vector non-support vector wai round. cases, k point chang statu 0 (if point new support vector k increas 0, leav statu support vector, alpha valu decreas 0). k = 0 ad remov modifi object function. consequ object function chang bookkeeping. 10 novemb 21, 2014 appli artifici intellig (aai) journal main end conclus increment svm optimum grow monoton- ically. henceforth, exact method solv tsvm gf mean look global optim solut 2r svms, correspond possibl label f (gf ) = min{yr 1 : 2r | w (s, b)} gf hf look global optim solut 2r+1 svms, correspond possibl label f (gfhf ) = min{yr+1 1 : 2r+1 | w ( s, h, b )} realiz label global optimum gf , new constraint hf , differ label correspond global optimum gf hf . therefore, thank fact svm global optim solut monoton increas move gf gf hf global optim solut exact method seek minimum possibl svm follow f (gf ) f (gf hf ). lemma provid monoton increment optim solut tsvm allow infer (hf , wf ,wf ,) repres violat space (from lp-type) problem report proof proposit 4.1. proof. need monoton local hold: monotonicity: given subset constraint ff gf ff gf hf build gf ff incremen- tal process ad new constraint h hf\ff . lemma (4.2) follow f (ff ) f (gf ) locality: given subset constraint ff gf ff gf hf constraint h hf . f (ff ) = f (gf ) set entail global solution. ad increment constraint h hf subset gf f (gf ) f (gf {h}) convei new constraint chang global solution. consequ ad h hf ff chang global solut f (gf ) = f (ff ) f (ff {h}) face definit associ violat space problem tsvm, basi repres set support vector global solution, cardin largest combinatori dimension. stage shown feasibl formul tsvm violat space problem. addition, adopt clarkson algorithm us violat map violat test. primit 2: ( tsvm violat test) given tsvm global solut f (gf ) subset constraint gf decis function fgf (xh), constraint h hf\gf violat solut (h vf (gf )), h = max(0, 1 yhfgf (xh)) 0 label obtain yh = sign(fgf (xh)). shown associ violat space problem tsvm deal exact method optim global solution. remarkably, clarkson random method viabl solv tsvm acquir basi violat test primit 2. effici reli sparsiti properti tsvm. 5. algorithm implement exact method like integ program (ip) (bennett demiriz, 1998) branch bound (bb) (chapel et al., 2006) investig tsvm. basic 11 novemb 21, 2014 appli artifici intellig (aai) journal main algorithm 1 stsvm (xl, yl, xu,maxit,r) require: r = 2go |xl xu| repeat choos xr random accord distribution: xr ( u r ) base(objr, yr, ,w, b) := bb(xl, xr, yl) // branch bound yu\r := sign(f(w,b)(xu\r)) // comput labels... 2 u\r := max(0, 1 yu\rf(w,b)(xu\r)) 2 // ... slack vbase := {h u\r : u\r 6= 0} // check violat (vbase) (u)/g0 (h) := 2(h), h vbase // updat weight violat end objr := objr + 1/2 u\r 2 u\r/qu\r // updat best... basebest := minobj(base(objr, yr, ,w, b), basebest) // ...known solut maxit vbase = vbase = return base(objr, yr, ,w, b) // global optimum return basebest // best solut end idea ip tsvm add integ decis variabl indic class point work set. solut mix integ program code provid resourc suitabl size problem. branch bound method abl solv combinatori optim problem enum techniqu implicitli effici search entir space solutions. clearli methods, exact tsvm implement trivial algorithm. practic reason follow focu bb imple- mentat indic bbtsvm devis chapel tsvm. solv problem set binari variabl evalu j (yu) embodi induct svm. bbtsvm minim 2u possibl choic yu. node binari search tree, partial label data set gener corre- spond son cast label unlabel data. core bbtsvm subroutin akin primit evalu svm data label proce search tree. svm usual solv constrain quadrat program- ming problem dual, work primal unconstrained. bbtsvm us method requir iter approxim solut (worst case) complex o(n3). svm convex quadrat program problem polynomial-tim solvable, current best method rais (worst case) complex o(n3/2d2). enumer process bbtsvm reach complex o(2nn3) o(2nn3/2d2) depend svm solver used. 5.1. implement random tsvm have set matter way, readi encompass detail implement random version exact method solv tsvm refer stsvm. algorithm (1) implement stsvm slightli revis clarkson scheme. gener ignor combinatori dimens correspond violat space. fact, apart linear case, theoret bound number support vector practic use. henceforth, start fix given 12 novemb 21, 2014 appli artifici intellig (aai) journal main valu ro. accord basis2, stress probabl select given violat round doubl correspond weight slack u\r 6= 0. quadrat loss implement example, u\r = u\r mean point chang margin ad basis. basic implement trivial make us implement exact method work random sampl l r l u . envisaged, practic implement trivial us bb. however, differ exact algorithm provid implement trivial chang converg stsvm affect performances. stop condit foreseen: (1) vbase = (no violat condition: return global optimum) (2) max number iter (return best known solution). case best known solut repres minimum fbest = minobjr bb(lr) objr = objr + (vr) = objr + 1/2 u\r 2 u\r/qu\r, qu\r correct factor increment updat equat 4. criterium take account objr best result bb, minim violat contribut (vr). implement bb abl optim undergo svm primal dual formul quadrat linear losses. 5.2. converg complex stsvm appli sampl lemma, expect valu e[vbase] element u (with u = |u|) violat random set r, (cast r = 2g0 < rmax 1 g0 ) bound e[|vbase|] (u) r r + 1 < (u) 2g0 = (u) 2g0 markov inequ predict pr(|vbase| (u) g0 ) pr(|vbase| 2e[|vbase|]) 1/2 impli expect number round twice larg number success ones. let sv set support vector tsvm, success iter xi sv wont l r (xi) get doubled. |sv| mean xi get doubl successfulli round k round (xi) 2k/. hand, success round rais total weight (1 + r ) = (1 + 1 2go ) give bound 2k/ e[(u)] u(1 + 1 2g0 )k n e k 2g0 n e k 2 . k success iter lower bound exce upper k 2log n. final expect number violat test o(n log n) expect number call bbtsvm g0log n = o(log n) set r averag size r = 2g0 = o( 2) (at rmax). time complex stsvm show sparsiti properti allow rel gain respect us bb set data o(2nn3/2d2) 13 novemb 21, 2014 appli artifici intellig (aai) journal main 4 3 2 1 0 1 2 3 4 2.5 2 1.5 1 0.5 0 0.5 1 1.5 2 2.5 x 1 x 2 sbbtsvm twomoon complet 0 500 1000 1500 2000 2500 3000 3500 4000 4500 0 100 200 300 400 500 600 sv reduc index s v r e d u ce d f q u e n cy (a) (b) figur 2. (a) distribut exact solut moon data set (4.000 unlabelled, 2 label triangl cross). (b) weight distribut set point final round. support vector optim solut encircled. o( log n 2r (r+ l)3/2d2). method effect number support vector lower number instances. 6. experi section briefli result obtain propos random method known benchmark problem previous solv exact methods. stsvm stand known benchmark problem moon com- pose 4.000 unlabel exampl (figur 2). moon problem report burdensom solv state-of-the-art transduct method (chapelle, 2008). tabl 1 show error rate produc method small data set contain 200 unlabel points. method fall local minima far global one. however, problem solv previous appli bb space possibl label instanc (chapel et al., 2006). unfortunately, albeit method abl optim solution, exponenti time complex number unlabel instances, consequently, abl solv moon data set hundr unlabel instances. method describ paper find optim solut instead scale number instances, scale number support vectors. case moons, number support vector scale number points. henceforth, face 4.000 unlabel exampl stsvm allow exact solut minutes. stsvm abl exact solut known benchmark problem previous solv exact methods: coil20 data set (describ (nene, nayar, murase, 1996)). contain 1440 imag 20 object photograph differ perspect (72 imag object) 2 label imag object. coil20 belong multi-class problem commonli solv one- vs-all scheme. scheme, build svm class assign label accord class return maximum output given point, measur confid classif point. tabl 1 show success state-of-the-art transduct 14 novemb 21, 2014 appli artifici intellig (aai) journal main data set svm s3vm cs3vm cccp s3vmlight da newton stsvm twomoon 50.2 61 37.7 63.1 68.8 22.5 11 0 coil3 66.6 61.6 61 56.6 56.7 61.6 61.5 0 coil20 24.1 25.6 30.7 26.6 25.3 12.3 24.1 1.4 tabl 1. error rate dataset supervis svm, state art transduct algorithm (a report (chapelle, 2008)), stsvm. algorithm data set describ chapel (2008). bbtsvm abl solv problem, larg number instances, albeit manag coil3 pictur sole 3 class hard discriminate. stsvm find solut 0 error coil3 data set tabl 1 illustr error produc state- of-the-art transduct method (from chapel (2008)). column error rate l2 loss (use search tree stsvm ) appli label exampl reported, result obtain stsvm one-vs- scheme act label unlabeled. remarkably, differ baselin result shown chapel (2008) ours, come differ loss svm (l2 case, l1 others). stsvm differ losses, stress error reduct (from 10.9% coil20, 0% coil3), absolut value. however, examin 20 tsvm produc one-vs-al scheme (one class), notic tsvm 0 violat 100 iter solut 0 violat found, right balanc posit neg examples. side, tsvm return solut 0 violat right rate posit neg examples. consid tsvm good predictor class repres them. point label posit class remov data set, deliv smaller one. afterwards, tri solv smaller data set appli one-vs-al scheme remain classes. problem simplified, appear new good predictor class procedur repeat data set complet solved. final error follow procedur 20 error 1400 unlabel initi exampl reduc error coil20 1.4%. remark irreduc error produc class 5 19, class method good predictor. 2 class hold maximum margin hypothesi addit constraint correctli classifi them; fact modifi ratio label unlabel larg increas formers, problem correctli solved. 7. conclus futur work paper, shown origin interpret tsvm problem term violat space, abl extend us exact method optim solut scale time complex number support vector instead number points. suitabl situat method appli dataset entail small support vectors, independ size data set. limit approach appear size support vector set higher hundreds, common situat real data sets. future, plan investig implement spars svm formulations, allow extend applic method larger datasets. also, preliminari experi larger benchmark dataset (where number support vector rang hundreds) method is, expected, 15 novemb 21, 2014 appli artifici intellig (aai) journal main abl optim solut reason time. however, case kept best solut describ section 5.1. cases, return solut better return svm label examples. experi encourag explor error bound propos method try appli good approxim optim solution. finally, consid interest possibl interpret weight obtain exampl random method. sampl high weight usual appear violat help identifi point interest final solut method abl exact one. acknowledg work partial support fi-dgr programm agaur eco/1551/2012. refer k. bennett a. demiriz. semi supervis support vector machines. adv. neural infor- mation process systems, 12:368374, 1998. g. cauvembergh t. poggio. increment decrement support vector machin learning. adv. neural inform. procesing, mit press(13):409415, 2001. o. chapelle. optim techniqu semi supervis support vector machines. journal machin learn research, 9:203233, 2008. o. chapelle, v. sindhwani, s. keerthi. branch bound semi-supervis support vector machines. adv. neural inform proc. systems, 2006. k. l. clarkson. la vega algorithm linear integ program dimens small. journal acm, 42, 1996. b. gartner, l.rust j. matousek, p.skovron. violat spaces: structur algorithms. discret appli mathematics, 156(11), 2008. balcazar j., dai y., tanaka j., watanab o. provabl fast train algorithm support vector machines. theori comput systems, 2008. t. joachims. transduct infer text classif support vector machines. intern confer machin learning, 1999. s. a. nene, s. k. nayar, h. murase. columbia object imag librari (coil-20). technic report, columbia university, usa, 1996. v. vapnik. natur statist learn theory. springer, new york, 1995. 16