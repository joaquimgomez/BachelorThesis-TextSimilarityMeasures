scalabl inexact balanc domain decomposit constraint overlap coarse/fin correct santiago badiaa,b, alberto f. martna,b, javier principea,b acentr internacion metod numer en enginyeria (cimne), parc mediterrani la tecnologia, upc, estev terrada 5, 08860 castelldefels, spain buniversitat politecnica catalunya, jordi girona 1-3, edifici c1, 08034 barcelona, spain abstract work, analyz scalabl inexact two-level balanc domain decomposit constraint (bddc) precondition krylov subspac it- er solvers, highli scalabl asynchron parallel implementa- tion fine coars correct comput overlap time. way, coarse-grid problem fulli overlap fine-grid comput (which embarrassingli parallel) wide rang cases. further, con- sider inexact solver reduc comput cost/complex memori consumpt coars local problem boost scalabl solver. numer experimentation, conclud bddc precondi- tioner insensit inexact solvers. particular, cycl algebra multigrid (amg) attain algorithm scalability. further, clear reduct comput time memori requir inexact solver com- pare spars direct on make possibl scale far state-of-the-art bddc implementations. excel weak scalabl result obtain propos inexact/overlap implement two-level bddc preconditioner, 93,312 core 20 billion unknown juqueen. fur- ther, appli propos set unstructur mesh partit pressur poisson solver backward-fac step bench- mark domain. keywords: domain decomposition, inexact solvers, bddc, parallelization, overlapping, scalabl 1. introduct order deal increas level complex simul phe- nomena govern partial differenti equat (pdes), comput engi- neer scienc advanc develop numer algorithm implement effici exploit ever-increas preprint submit parallel comput septemb 24, 2015 comput resources. growth comput power result moor law pass increas number core chip, instead make core faster. result, gener supercom- puters, abl reach 1 exaflop/s, expect reach billion cores. effici exploit billion-fold level concurr big challenge. advanc larg scale scientif comput strongli relat abil effici exploit extrem core count [1]. time spent implicit simul linear solver rel overal execut time grow size problem number core [2]. extrem scale implicit simulations, massiv parallel linear solver kei component. scenario exacerb need highli scal- abl algorithm implementations. numer algorithm compon scalabl effici run extrem scale supercomputers. ex- treme scale solver develop assumpt local flop cheap commun expensive. extrem core counts, reduc commun synchron processors, overlap commun computation. largest scales, linear solver base precondit krylov subspac methods. algorithm scalabl precondition includ (algebraic) multigrid (mg) [3] domain decom- posit (dd) algorithm [4]. however, theoret properti practic weak scalability, precondition allow massiv scalabl implementation. todai scalabl algorithms/imple- mentat present practic limit parallelism, e.g., small, coars problem solv hierarch process dd/amg, loss sparsiti denser commun pattern coarser level amg. dd precondition explicit us partit global mesh, e.g., finit element (fe) integration, sub-mesh (subdomains) pro- vide natur framework develop fast parallel solver tailor distributed-memori machines. one-level dd algorithm involv solut local problem nearest-neighbor communications. (second level) coars correct (coupl subdomains) requir algorithm scalability, harm practic (cpu time) weak scalability. two-level dd al- gorithm includ balanc neumann-neumann precondition (bnn) [5], balanc dd constraint precondition (bddc) [6], feti-dp precondition [7]. cases, positive-definit matrices, poly- logarithm express condit number precondit = 1 + log2 ( h h ) proved, h h mesh subdomain characterist sizes, respectively, d space dimension; (h h )d local problem size. consequently, weak scale scenarios, i.e., increas linear size number processor keep h h fixed, number itera- tion precondit conjug gradient (pcg) solver (asymptotically) independ number processors. practic scalabl limit two-level dd implement deter- mine coars solver computation, size increas (at best) linearli respect number subdomains. coars problem rapidli be- 2 come bottleneck algorithm increas number processors, reduc weak scalabl [8]. coars problem order magni- tude smaller origin global system, small portion comput core effici exploit (assum parallel coars solver). typic dd implementations, produc unaccept parallel effici loss, core involv coars solver comput idl (see figur 1). obviou strategi improv scalabl reduc wall-clock time spent coars solver using, e.g., mpi-distribut spars direct solver like mump [9] (see [10] bddc [11] feti-dp). however, approach mitig problem. 2. motiv bddc precondition salient properti permit over- come parallel overhead, make excel candid extrem scale solver design: (p1) allow mathemat support extrem aggress coarsening. ratio size global coars problem order local problem size, i.e., (h h )d. memory-constrain supercom- puters, order 105 spars direct method [12] 106 inexact solver (see section 6). (p2) coars matrix similar sparsiti pattern origin matrix. (p3) constrain neumann dirichlet local problems, coars problem, comput inexact way, e.g., amg cycl affect algorithm scalabl method [13]. (p4) fact coars matrix similar structur origin matrix, multilevel extens algorithm possibl [14, 15]. (p5) coars fine compon comput parallel [6], basi coars space construct wai orthogon fine compon space respect inner product induc unassembl matrix [6, 16]. properti (p1) (p2) readili exploit bddc implementation. properti (p3), i.e., algorithm scalabl bddc inexact solvers, prove dohrmann [13]. similar inexact precondition present [17]. inexact bddc method easili increas parallel efficiency, linear complex coars solver, especi larg core counts. however, far know, practic weak scalabl analysi inexact bddc method (at larg scales) carri far. besides, feti-dp, cycl mpi-distribut amg solver boomeramg [18] inexact coars solver [19, 20] 2d elast problems. regard (p4), multilevel bddc algorithm propos [15], coars problem bddc level approxim bddc 3 approximation. way, cpu cost coars problem reduced, condit number bound increas number level [15]. high- perform implement multilevel bddc method [10]. effici exploit (p5), i.e., orthogon coars fine spaces, trivial. however, properti make possibl parallel comput coars fine corrections, i.e., overlap time. [12], classifi duti exact (i.e., spars direct solvers) bddc-pcg algorithm fine coars duties. duti re- schedul achiev maximum degre overlap preserv data dependencies. actual implement idea requir signific code refactoring, involv switch data parallel task paral- lelism paradigm, divid processor have fine grid duti have coars grid duties. clearly, approach reduc synchron processors, overlap communications/computations, follow exascal solver paradigm [1]. exploit [12], perform scalabl analys 3d poisson linear elast problem pair state-of-the-art multicore-bas distributed-memori machin (helio curie). excel weak scalabl attain 27k core rea- sonabl high local problem sizes, e.g., (h h ) = 30 mean 27k elements/cor 3d poisson problem; local coars problem solv multi-thread spars direct solver pardiso [21]. hardwar front, current trend hpc increas core count node reduc memori avail core. hand, reduc synchronization, overlap bddc implement [12], crucial extrem core counts. further, overlap implementa- tion allevi memori requirements, fine (resp., coarse) processor perform/stor coars (resp., fine) solver duties/matrices. direction, linear complex inexact solvers, memori intens spars di- rect methods, certainli favored. favor larg core counts, potenti loss scalabl coars solver dramatic. current state-of-the-art dd implement supercomput trend reach exascal motiv combin overlapped/inexact bddc implement propos work. article, extend overlap implement [12] exact solver inexact bddc method propos [13] (with slight modifica- tions). first, analyz effect perturb isol problem bddc preconditioner. next, propos differ inexact methods, combin differ number amg cycl intern problem. comprehens weak scalabl analysi result overlapped/inexact bddc implemen- tation perform 93,312 core 20 billion unknown juqueen, julich supercomput center (jsc). test perform structur mesh partit constant physic coeffi- cients. far know, largest scale scalabl analys simul perform far dd methods. 4 propos implement inexact bddc method code fempar, massiv parallel finit element solver devot le simu- lation incompress turbul flow mhd unstructur mesh (see [22, 23, 24, 25]). problems, typic approach consid pres- sure segreg techniqu (see [26, 27]), lead momentum equat usual integr explicitli pressur poisson solver, bottle- neck simulations. effici scalabl pressur poisson solver must. result, appli propos set unstructur mesh partit pressur poisson problem backward-fac step benchmark domain. note free flow solver lead poisson prob- lem constant coefficients, main differ respect flow highli heterogen porou media. situations, arise, e.g., subsurfac modelling, advanc implement base adapt bddc coars space [10] need (see [28] effici techniqu problems). further, us inexact solver problem requir local amg solver suitabl highli vari coefficients. work structur follows. section 3 devot non-overlap dd bddc precondition section 4 devot intro- duction inexact variants. section 5, extend highli scalabl paral- lel distributed-memori implement bddc algorithm [12], overlap fine coars computations, inexact variant. section 6, report comprehens set numer experi structur mesh includ studi influenc approxim solv intern problem isol weak scalabl analysis. numer exampl unstruc- ture mesh partit provided. finally, section 7, draw conclus defin futur line work. 3. balanc domain decomposit 3.1. problem set let consid bound polyhedr domain rd d = 2, 3 quasi-uniform fe partit (mesh) characterist size h. model problem, studi poisson problem , arbitrari forc term boundari condit (a soon problem well-posed). let v h1() c0- continu fe space. galerkin approxim poisson problem respect v lead linear equations: ax = f. (1) further, consid quasi-uniform partit global mesh nsbd local meshes, induc non-overlap domain decomposit subdomain i, = 1, . . . , nsbd (of characterist size h). interfac defin := \ interfac (skeleton) domain decomposit := nsbd i=1 i. subdomain i, introduc local fe space function vi. v := v1 . . . vnsbd denot global fe space 5 function discontinu ; clearly, v v. obviously, fe space isomorph real vector spaces. let defin restrict oper ri : v vi, appli vector v provid restrict i, r := r1 . . .rnsbd : v v. let defin oper ei := r t idi : vi v, di : vi vi weight operator. weight oper repres partit unity, sens rtdr = i, d := d1 . . .dnsbd : v v. further, let e := r td. subdomain fe matrix correspond vi denot k (i), size denot ni. k := diag ( k(1), . . . ,k(nsbd) ) global sub-assembl fe matrix v. (along paper, denot letter k (partially) sub- assembl matric fulli assembl ones.) analogously, defin local sub-assembl right-hand g(i) global counterpart g. matrix right-hand f obtain assembl k = rtkr g f = rtg. non-overlap partit induc reorder fe vector inte- rior interfac nodes, i.e., u = [ui , u] t. defin interior restrict oper riu := ui . lead follow block reorder structur global assembled, global sub-assembl local matrices: = [ aii ai ai ] , k = [ aii ki ki k ] , k(i) = [ (i) ii (i) (i) k (i) ] , respectively. matric aii , ai, ai andk present block diagon structur (veri amen parallelization), e.g., aii = diag ( (1) ii , (2) ii , . . . , (nsbd) ii ) . matric ki ki simpli extens zero ai ai , re- spectively. 3.2. bddc precondition bddc precondition two-level domain decomposit method local fine-grid correct global coarse-grid correct (that coupl subdomain make precondition scalabl optimal) combined. idea bddc precondition approxim origin fe problem relax continu conditions, drastic reduc size modifi schur complement, combin initi final interior correction. construct bddc precondition requir partit degre freedom (dofs) objects, corners, edg faces. next, associ (or all) object coars dof. coars dof valu function corners, mean valu function edges/faces. defin bddc fe space v subspac function v continu coars dofs; clearly, v v v. common variant bddc method refer bddc(c), bddc(ce) bddc(cef), enforc continu corner coars dofs, corner edg coars dofs, corner, edg face coars dofs, respectively. denot k fe matrix relat v. 6 formal obtain partial assembl (at coars dof only) global sub-assembl matrix k, implement way. invert matrix depend definit coars dofs. let define: pi := r t ia 1 ii ri , pfc := ek 1et, h := ipia = [ 0 a1ii ai 0 ] . (fc denot fine/coars correct h so-cal discret harmon extens operator.) bddc precondition m consist multipl combin pi , pfc , pi . fact piapi = pi , obtain: m = pi +hpfch t. practic implement bddc correct pfc requir elab- oration. let consid decomposit bddc space v fine space vf vector vanish coars dof k-orthogon complement vc , denot coars space. result, bddc fe problem de- compos fine coars components, i.e., x = k1etr = xf + xc . fine coars space k-orthogon definition, comput parallel. fine space function vf vanish coars dof (which dof involv continu subdomains). k-orthogonality, fine compon defin xf := efk 1 f e t f , kf galerkin project k vf , i.e., function vanish coars dofs, ef restrict e vf . order comput fine correct practice, defin local matrix constraint ci that, given local vector unknowns, provid local coars dof values. refer [29] detail implement ci. result, fine correct xf comput involv constrain neumann problems:[ k(i) cti ci 0 ] [ x (i) f ] = [ etir 0 ] . (2) describ [29], solut constrain neumann prob- lem perform appli permut separ coars corner dof (denot c) rest dof (denot r), i.e., x(i) = [x (i) c , x (i) r ] t vi. further, defin restrict rr,i rr,ix (i) = x (i) r . corner dof explicitli elimin (in fact x (i) c = 0 fine correction), lead [ k (i) rr c t r,i cr,i 0 ] [ x (i) r r,i ] = [ rr,ie t ir 0 ] . (3) solv comput schur complement edge/fac lagrang multipli cr,i(k (i) rr ) 1ctr,i (i) r = cr,i(k (i) rr ) 1rr,ie t ir. (4) 7 result, fine correct involv comput invers global matrix krr := diag(k (1) rr , . . . ,k (nsbd) rr ). exist mechan modifi definit object order enforc krr invert (see [30, 6]). coars space vc v built vc = span{1,2, . . . ,ncts}, coars function associ coars dof. denot matrix column i. coars basi (the matrix column i) solut multipl right-hand global system. valu coars dof prescrib rest dof local, coars space comput (parallel) local constrain neumann problems, i.e.,[ k(i) cti ci 0 ] [ (i) (i) ] = [ 0 ] . (5) (5) solv wai (2), getting: (i) = [ (i) c (i) r ] , (i)c = [ 0 ] , (i)r = (k (i) rr ) 1ctr,i r [ k (i) rc 0 ] , (6) cr,i(k (i) rr ) 1ctr,i (i) r = cr,i(k (i) rr ) 1 [ k (i) rc ] . (7) let note function associ object support set subdomain share object. thus, subdomain comput non-zero restrictions, i.e., coars space basi function relat local coars dofs. comput coars matrix kc kc = tk = nsbd i=1 rtc,i (i)tk(i)(i)rc,i. rc,i coars matrix assembl operator, i.e., local-to-glob cor- respond coars dofs. subdomain contribut (i) t k(i)(i) readili comput (in parallel) assembled, e.g., processor. coars residu rc = tetr comput analog (see [29]). kc rc assembled, coars correct obtain xc = k 1 c rc . bddc precondition final state as: m = pi +h(ek 1 c tet + efk 1 f e t f )h t. remark 3.1. consid exact dirichlet solvers, initi pre- correction, easili check ri = 0 krylov iterations. case, pi h comput elimin modifi method, i.e., mr = h(ek1c tet + efk 1 f e t f )r, lead interior (dirichlet) correct iterations, instead two. 8 refer [31] proof follow theorem, condit number bddc-precondit matrix. theorem 3.1. maximum minimum eigenvalu bddc precon- dition matrix are: min(ma) 1, max(ma) , := ( 1 + log2 ( h h )) , bddc(c) bddc(ce) 2d, bddc(ce) bddc(cef) 3d, > 0 depend (h,h). bddc(c) precondition 3d, min(ma) 1, max(ma) h h , > 0 independ (h,h). remark 3.2. bddc precondition quasi-optim algorithm scalable, condit number precondit matrix depend local size, fix weak scale scenario. fur- ther, condit number poly-logarithm function h h , except bddc(c) 3d. case, condit number affect addit h h factor, larg (e.g., 60 numer ex- periment section 6). justifi larg iter count method (compar bddc(ce) bddc(cef)). 4. inexact bddc exact bddc precondition involv linear system solved. action k1rr , i.e., local constrain (on coars corner dof only) neumann problems, requir comput coars basi fine correction, action a1ii , i.e., local dirichlet problems, requir interior corrections, action k1c , i.e., coars problem, solv comput coars correction. problem tradition solv spars direct method [6]. however, motiv introduc- tion, us inexact solver appeal large-scal simul supercomputers, increas memori restrict higher core counts. dohrmann propos analyz [13] inexact version bddc method, local/coars problem replac preconditioners. let assum dispos approxim krr krr that: fx tkrrx xtkrrx fxtkrrx, x. (8) (along section, assum () () posit constant independ (h,h) vector space x infer matric inequalities.) further, let introduc approxim krr aii krr aii , respectively, follow global matrices: k := [ krr krc kcr kcc ] , k := [ aii ki ki k ] . (9) 9 assum x tkx xtkx xtkx, ixtkx xtkx ixtkx, x. (10) (we omit reorder oper brevity.) order matric (9) semi-posit definite, assum xtkrrx xtkrrx, xtaiix xtaiix, x. note [13], k singular, kernel k, k, k ident satisfi (10). let w ker(k) range(w ), wi = riw . given arbitrari approxim aii , build aii solv exactli range(wi) (see [13]), i.e., a1ii := wi(w t 1 ii wi) 1w ti+eia 1 ii e t , ei := iaiiwi(w t 1 ii wi) 1w ti . block-diagon natur k, correct local. definit kernel-correct krr defin analogously. finally, approx- imat coars matrix kc studi differ options. differ respect previou problem fact kc avail inexact solvers. (it involv exact comput .) option assembl galerkin project k inexact coars basi , i.e., tk, consid approxim matrix that: cx t(tk)x xtkcx cxt(tk)x, x. (11) approach, [13], consid approxim coars matrix tk: cx t(tk)x xtkcx cxt(tk)x, x. (12) remark 4.1. general, inexact matric krr, krr, aii ex- plicitli built, action invers approxim algo- rithm, e.g., one/sever amg cycles. further, krr, aii dens matrices, kernel correction. however, coars problem approxim (12), krr explicitli needed. fortunately, inexact version (5) (equat (15)-(16) below) easili check k (i) rr (i) = ct, make possibl comput tk tct. case, k1rr stand precondit krylov solver tolerance, approach lead gener nonsymmetr indefinit matrix. sit- uations, better us (11). besides, (11) section 6 slightli better performance. finally, inexact bddc precondition read as: m = pd + h(ek 1 c tet + ef k 1 f e t f )h t, (13) 10 h := [ 0 a1ii ai 0 ] , pi = r t ia 1 ii ri , (14) inexact coars basi comput (i) = [ (i) c (i) r ] , (i)c = [ 0 ] , (i)r = (k (i) rr ) 1ctr,i r [ k (i) rc 0 ] , (15) cr,i(k (i) rr ) 1ctr,i (i) r = cr,i(k (i) rr ) 1 [ k (i) rc ] . (16) theorem 4.1. let assum (8)-(10) hold. kc satisfi (11), have: min(ma) min(1, 1 f , 1 c ) , max(ma) max(ma) 2i max(1, 1 f , 1 c ) 2 . alternatively, kc satisfi (12), get: min(ma) min(1, 1 f , 1 c ) , max(ma) max(ma) 2i max(1, 1 f , 1 c ) 2 . proof. proof result readili follow analysi [13], differ fact fine correct gener comput differ preconditioners. easili handl fact kf galerkin project k result like (11) matrix. further, coars precondition built tk, result readili obtain fact c xttkx xtkcx c xttkx, obtain combin (10)-(11). remark 4.2. inexact precondition m replac local/coars problem optim approximations. block-diagon structure, (possibl different) precondition k1rr k 1 rr local built k (i) rr , obtained, e.g., amg cycl matrix. analogously, a1ii built local approxim (i) ii . remark 4.3. precondition comput dirichlet problems, i.e., krr kii , includ kernel correct (see [13]). hand, need fine coars correct precondition krr kc . 11 remark 4.4. comput , comput (k (i) rr ) 1ctr,i. impli ap- ply subdomain local precondition vector local coars corners. comput reus build schur complement matrix edge/fac constrain neumann problem (4), soon krr = krr. otherwise, comput (k (i) rr ) 1ctr,i. make suitabl consid precondition (with kernel correction) fine correction. 5. highli scalabl distributed-memori implement section adapt highli scalabl distributed-memori implemen- tation method propos [12] consid inexact solvers. global linear (1) solv mean krylov subspac method, inexact bddc precondition m global matrix precondi- tioner (see section 4). implement consid differ krylov subspac method (e.g. pcg, ipcg [32], fgmre [33]) solut global problem bddc precondition solut local problem amg precondition [34, 3, 35]. featur exploit test combin report here, e.g., pcg- amg method local problem (with coars tolerances), ipcg [32] global system. however, inexact variant base krylov method turn effici fix number amg cycl cases. result report sake brevity. distributed-memori implement bddc precondit krylov subspac solver, data structur (i.e., matric vectors) comput split distribut mpi task concord underli non-overlap partit domain. refer reader [29] com- prehens coverag implement aspects. rest section, identifi briefli comput commun requir implement bddc precondition inexact solvers. initi set-up bddc precondition turn split symbol numer phase algorithm 1 2, respectively, applic residu depict algorithm 3. commun stage label gc lc depend global (i.e., mpi task involved) local (i.e., mpi task commun subset tasks) nature, respectively. algorithm 1, 2, 3 requir global gather/scatt communication, local exchang nearest neighbors. symbol set-up bddc precondition present al- gorithm 1, adjac graph (denot g) matric (i) ii k (i) rr requir dirichlet constrain neumann problem comput line 12 13. coars solver task line 1- 11 ident exact bddc method [12]. numer set-up bddc precondition present algo- rithm 2. oper requir phase depend inexact solver used. e.g., solver set-up incomplet numer factoriza- tion ilu methods, involv construct hierarchi 12 algorithm 1: m set-up (symbolic) 1: identifi count (nicts) local coars dof 2: gather nict gc 3: gather global identifi geometr entiti correspond coars dof gc 4: comput global order coars dof (defin rc,i transpose) 5: scatter global order coars dof gc 6: fetch nict of/from neighbor lc 7: fetch global identifi coars dof neighbor lc 8: comput row count gkc correspond local coars dof 9: gather row count gkc gc 10: comput adjac list gkc correspond local coars dof 11: gather adjac list gkc gc 12: construct g k (i) rr gk(i) 13: construct g (i) ii gk(i) amg. task algorithm 2 subdivid fine task (line 1-6) coars task (line 7-9). fine mpi task includ extract (i) ii k (i) rr line 1 2 set-up approximations, e.g., amg hierarchi (possibly) kernel-correct set-up, line 3 4, respec- tively. fine duti involv comput coars space matrix line 5 coars matrix coeffici line 6. mpi task (or tasks) charg coars problem gather contribut perform matrix assembl correspond rc,i order build kc line 7 8, respectively. finally, mpi task charg coars problem perform coars preconditioner, e.g., amg hierarchi set-up inexact coars matrix (see line 9). algorithm 2: m set-up (numerical) 1: extract (i) ii (i) k (i) 2: extract k (i) rr k (i) rc k (i) 3: set-up (i) ii 4: set-up k (i) rr (possibly) k (i) rr 5: comput (i) (15)-(16) 6: comput k (i) c ( (i))tk(i)(i) (or alternatively, k (i) c ( (i))tc(i)i) 7: gather k (i) c gc 8: comput kc nsbd i=1 r t c,ik (i) c rc,i 9: set-up kc 13 algorithm 3 describ algorithm appli bddc precondition residual. first, comput interior precorrection, corre- spond residu updat perform line 1 2, respectively. then, updat residu extend bddc space et (see line 3). hand, fine-grid task includ comput fine correct mean constrain local neumann problem (see line 6). contribu- tion subdomain coarse-grid residu comput line 4, mpi task charg coars problem gather contribut perform vector assembl associ rc,i order build rc line 5 7, respectively. next, coars problem solv inexact way, e.g., one/sever amg cycle(s). finally, solut scatter task subdomains, subdomain coarse-grid correct local coars dofs. finally, correct inject v project e, correct interior line 12. algorithm 3: z := mr 1: comput (i) (a (i) ii ) 1r (i) 2: comput r (i) r (i) (i) (i) 3: comput r(i) etr lc 4: comput r (i) c ( (i))tr(i) 5: gather r (i) c gc 6: comput x (i) f (3)-(4) 7: comput rc nsbd i=1 r t c,ir (i) c 8: solv zc = k 1 c rc 9: scatter zc z (i) c , = 1, 2, . . . , nsbd 10: comput s (i) c (i)z (i) c 11: comput z(i) e(s(i)f + s (i) c ) lc 12: comput z (i) = (a (i) ii ) 1a (i) iz (i) + (i) typic implement bddc precondition [36, 37] illus- trate figur 1 (a), one-to-on map subdomains, mpi tasks, comput core used. fine coars duti serialized. vast major core fine duties, core fine coars duties. dramat reduct size origin coars matrix. consequence, tremend parallel overhead caus idling, i.e., wall-clock time requir solv coars problem tc time number core fine duti only. further, core coars fine duti requir memori resources. problem current multicore-bas distributed-memori architectur (in rang 1-4 gbyte core); memori limit expect restrict futur exascal supercomput [1]. alternative, propos [12] highli scalabl implementa- 14 tion exact bddc method solv aforement problem exploit algorithm properti make possibl comput coars fine duti parallel. techniqu illustr figur 1 (b). global set mpi task (i.e., global mpi communicator) split fine coars mpi tasks, i.e., fine duti (fine mpi communicator), coars duti (coars mpi communicator), compu- tation fine coars correct overlap time. possibl approach parallel solut coarse-grid problem propos [12]: openmp coarse-grid solver dedic node, shown figur 1 (b), gener mpi-bas solut distribut coarse-grid problem. global commun fine-grid correct coarse-grid correct c o 1 c o 2 c o 3 c o 4 c o p tc tf time idl main mpi commun c o 1 c o 2 c o 3 c o 4 c o p f global commun fine-grid mpi commun c o 1 c o 2 c o p c coarse-grid mpi commun tf tc pc openmp-bas coarse-grid solut (a) (b) figur 1: comparison (a) typic parallel distributed-memori implement algo- rithm 1, 2 3 (b) highli scalabl propos [12] implement multi- threading. effici exploit idea requir import remap re-schedul commun comput code refactoring, comprehens describ [12]. final result de- pict tabl 1, similar [12] includ modi- ficat need us inexact solvers. tabl 1 clearli evid area region (three exact version [12], symbol factorization), separ global commun stages, overlap fine coars du- ti possible: gather k (i) c line 7 algorithm 2 gather r (i) c line 5 algorithm 3. stress fact coars duties, produc sever idl and, result, loss parallel efficiency, overlap fine duties. tabl 1 consid m set-up stage header krylov phase. krylov loop, overlap fine-grid/coarse-grid duti present ap- plicat preconditioner, depict region tabl 1 dash horizont line. 15 fine-grid mpi task coarse-grid mpi task identifi count (nicts) local coars dof gather nict gather global identifi geometr entiti correspond coars dof comput global order coars dof (defin rc,i transpose) scatter global order coars dof fetch nict of/from neighbor fetch global identifi coars dof neighbor comput row count gkc correspond local coars dof gather row count gkc comput adjac list gkc correspond local coars dof gather adjac list gkc construct g (i) ii g k(i) construct g k (i) rr g k(i) construct k (i) rr k (i) rc k (i) set-up (k (i) rr ) (possibly) k (i) rr comput (i) (15)-(16) comput k (i) c ((i))tk(i)(i) (or k (i) c ((i))tc(i)i) gather k (i) c construct (i) ii (i) k(i) comput kc nsbd i=1 (rc,i) tk (i) c rc,i set-up (i) ii set-up kc r0 := f ax0 x0 := x0 a1ii r0 r0 := f ax0 comput (i) (a(i) ii )1r (i) comput r (i) r(i) a(i) (i) comput r(i) etr comput r (i) c (i)tr(i) gather r (i) c comput x (i) f (3)-(4) comput rc nsbd i=1 r t c,i r (i) c solv kczc = rc scatter zc z (i) c , = 1, 2, . . . , nsbd comput s (i) c (i)z(i) c comput z(i) e(s(i) f + s (i) c ) solv (i) ii z (i) = a(i) z (i) z (i) := z (i) + (i) tabl 1: map pcg-bddc algorithm fine-grid coarse-grid mpi task achiev maximum degre overlap time. 16 6. numer experi main goal underli numer experi section paper comprehens assess, state-of-the-art supercomputers, weak scala- biliti overlap implement two-level bddc precondition equip machineri allow inexactli solv intern problem (i.e., comput coarse-grid space basis, local dirichlet, constrain neumann problems, global coarse-grid problem) preserv pre- condition optim (see [13] section 4). benefit techniqu view light futur parallel architectures: trend scalabl architectur (e.g., ibm bluegene) limit memori core. studi present paper complement mathemat- ical analysi [13] answer far overlapped/inexact bddc code number core scale problem reason rang efficiency. section structur follows. section 6.1 briefli introduc paral- lel codes, software/hardwar stack supercomput tested. section 6.2 comprehens analyz perform scal- abil juqueen overlap implement inexact bddc precondit code appli discret 3d poisson problem uni- form mesh partitions, constant physic coefficients. section 6.2.1 particular describ target problem, map parallel code underli supercomputer. prior actual raw weak scalabl study, section 6.2.2, evalu effect inexact solut intern problem isol precondit effici (i.e., number pcg it- erations). view result evaluation, section 6.2.3, defin set inexact bddc variant differ particular solver intern problem, lead differ trade-off total comput time versu precondition efficiency. then, weak scalabl variant comprehens studi order meet object section. finally, section 6.3 evalu applic platform marenostrum iii al- gorithms/cod 2d 3d (pressure) poisson equat aris pressur segreg solut backward-fac step benchmark discret unstructur mesh split mean automat partitioners. 6.1. code parallel framework inexact/overlap implement bddc precondition studi paper implement fempar (finit element multiphys massiv parallel) numer software. fempar in- hous developed, parallel hybrid openmp/mpi, object-ori (oo) frame- work which, features, provid basic tool effici parallel distributed-memori implement substructur dd solver [29]. parallel code fempar heavili us standard comput kernel pro- vide (highly-effici vendor implement of) bla lapack. besides, proper interfac parti libraries, local fine-grid global coarse-grid problem two-level dd method 17 solv spars direct approxim solvers. work, explor hsl mi20 [38] softwar packag approxim solut prob- lems. hsl mi20 serial implement classic ruge-stuben amg precondition (a described, e.g., [39] [40]) converg acceler krylov subspac solvers. amg preconditioners, robust spars direct method general, particularli well-suit system aris discret poisson problem. indeed, amg precondit lead optim converg rate (i.e., independ mesh characterist size) linear arithmetic/memori complex number applications. experi report section obtain pair prace tier-0 supercomputers, marenostrum iii (mn-iii), locat barcelona (spain), barcelona supercomput center (bsc), juqueen, lo- cate julich (germany) julich supercomput center (jsc). mn-iii compos 3,056 ibm dx360 m4 comput node interconnect fdr10 infiniband high perform network. comput node equip pair 8-core intel e5-2670 sandybridge-ep cpu (i.e., 16 cores/node), run- ning 2.6ghz, 32 gbyte ddr3 memory. comput node run standard linux suse distribut (v11, sp3). hand, juqueen belong gener ibm blue gene famili supercomputers, so-cal bg/q supercomputer. juqueen configur 28-rack sys- tem, featur total 28,672 comput node interconnect extrem low-lat five-dimension (5d) toru interconnect network. comput node equip 16-core, 64-wai threaded, ibm power pc a2 proces- sor, 16 gbyte sdram-ddr3 memori (i.e., 1gbyte/core), run lightweight proprietari cnk linux kernel. code compil intel fortran compil (14.0.2) ibm xlf fortran compil bg/q (v14.1) mn-iii juqueen, respectively, recommend optim flags. openmpi (1.8.1) custom mpich2 message-pass mn-iii juqueen, respectively. code link blas/lapack avail single-thread intel mkl (11.1.2) ibm essl librari bg/q (v5.1), respectively, hsl mi20 (v1.5.1), hsl ma87 (v2.1.1). 6.2. inexact bddc 3d poisson structur mesh juqueen 6.2.1. problem parallel set-up consid benchmark solut poisson problem rect- angular prism = [0, 2] [0, 1] [0, 1] homogen dirichlet boundari condit constant forc term domain. global conform uniform mesh (partition) hexahedra trilinear fe dis- cretiz (i.e., q1 fes) continu equation. 3d mesh partit cubic grid p = 4m 2m 2m cubic subdomains. subdomain handl mpi task subdomains, distribut m3 = 23, 33, . . . , 183 comput node (128, 432, . . . , 93, 312 cores), 4 2 2 subdomains/mpi task comput node mpi task physic core. 18 addit special mpi task spawn order perform coarse-grid relat computations. task map addit comput node, al- access core 1 gbyte memory. limit hardware/softwar stack juqueen, al- low mix differ execut mode differ comput node (e.g., 16 mpi tasks/1 thread task fine-grid node 1 mpi task/16 thread task coarse-grid node). despit this, glance, sever restriction, demonstr resourc suffici solv large-scal problems.1 quotient subdomain mesh characterist sizes, i.e., h h , pro- vide measur local problem size. number fe (i.e., hexahedra) local cubic subdomain h h h h h h , global mesh given 4mh h 2mh h 2mh h . experi perform section select order evalu rate comput time evolv fix h h increas number core (within aforement range). trade-off factor determin scalabl code depend h h , perform studi pair valu fix problem size h h = 40 60. point, worth note effort set hsl mi20 paramet reach fastest solut times. particular, subset valu paramet consid [38] tested, = 0.67, rs1 coarsening, damp jacobi smooth winner combin intern problems. besides, parameter- valu combination, mesh independ converg rate achieved. 6.2.2. impact approxim solv intern problem section evalu effect inexact solut in- ternal problem isol effici bddc preconditioner. object section two-fold. first, confirm experiment result mathemat analysi present [13] section 4. special attent paid precondition optim preserv (i.e., number pcg iter asymptot constant fix local problem size increas number subdomains) matter intern problem perturbed. second, determin extent margin improve- ment (in term number pcg iterations) usag accur solver intern problems. cases, possibl practic scenario reach trade-off total comput time precondition effici lead faster solut problem. figur 2, 3, 4 compar number pcg iter exact bddc precondition inexact variant, bddc(c), bddc(ce) bddc(cef), respectively. figur provid impact perturba- 1one wai deal restrict distribut coarse-grid problem mpi coarse-grid tasks, possibl span multipl comput nodes. explor here, left futur work. 19 tion intern problem isol number pcg iterations. example, figur 2 (a), 3 (a), 4 (a) focus impact inexact solut coarse-grid problem, rest intern problem solv exactly. appli (b), (c), (d), solu- tion dirichlet problem, comput coarse-grid basis, solut constrain neumann problem, respectively. experiment, differ inexact solver considered, amg(1), amg(2), amg(4), stand single, amg cycles, respectively. amg cycles, accur solut correspond in- ternal problem expect be, result benefici impact number inexact bddc-pcg iterations. figur 2, 3, 4, global prob- lem size scale linearli number subdomain local problem size h/h = 40, i.e., 64k fe core; largest local problem size solv provid exact bddc precondition implement base spars direct solvers, 1gbyte/cor memori constrain juqueen. result obtain smaller local problem size (h/h = 10, 20, 30) omit brevity; similar conclus on h/h = 40 raised. 0 50 100 150 200 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(c) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 50 100 150 200 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(c) (h/h)3=403 (64k) fes/cor bddc inexact coars exact bddc 0 50 100 150 200 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(c) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 50 100 150 200 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(c) (h/h)3=403 (64k) fes/cor bddc inexact dirichlet exact bddc (a) (b) 0 50 100 150 200 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(c) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 50 100 150 200 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(c) (h/h)3=403 (64k) fes/cor bddc inexact coars basi exact bddc 0 50 100 150 200 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(c) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 50 100 150 200 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(c) (h/h)3=403 (64k) fes/cor bddc inexact neumann exact bddc (c) (d) figur 2: sensit number outer bddc(c)-pcg iter presenc per- turbat solut (a) coarse, (b) dirichlet, (c) coarse-grid basis, (d) neu- mann problems. differ intern solvers, amg(1), amg(2) amg(4), test solut problems. 20 0 5 10 15 20 25 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(ce) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 5 10 15 20 25 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(ce) (h/h)3=403 (64k) fes/cor bddc inexact coars exact bddc 0 5 10 15 20 25 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(ce) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 5 10 15 20 25 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(ce) (h/h)3=403 (64k) fes/cor bddc inexact dirichlet exact bddc (a) (b) 0 5 10 15 20 25 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(ce) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 5 10 15 20 25 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(ce) (h/h)3=403 (64k) fes/cor bddc inexact coars basi exact bddc 0 5 10 15 20 25 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(ce) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 5 10 15 20 25 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(ce) (h/h)3=403 (64k) fes/cor bddc inexact neumann exact bddc (c) (d) figur 3: sensit number outer bddc(ce)-pcg iter presenc per- turbat solut (a) coarse, (b) dirichlet, (c) coarse-grid basis, (d) neu- mann problems. differ intern solvers, amg(1), amg(2) amg(4), test solut problems. figur 2, 3, 4 overal confirm mathemat analysi [13] section 4. particular, provid inexact bddc precondition equip spectral equival approxim neumann coars problems, spectral equival kernel preserv approxim dirichlet problem constrain neumann problem comput coars basi functions, precondition optim preserved. observ figur 2, 3, 4 number pcg iter asymptot constant matter intern problem perturbed, extent perturbed. true, worth note impact inexact solut intern problem number pcg iter highli depend constraint consid coars space, intern problem perturbed. this, y-axi plot figur 2, 3, 4 scale accordingli correspond intern problem perturb largest impact number pcg iterations. focu figur 2 (a) (c), observ inexact solution/comput coarse-grid problem/coarse-grid basi mild impact number pcg iterations. indeed, number pcg 21 0 5 10 15 20 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(cef) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 5 10 15 20 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(cef) (h/h)3=403 (64k) fes/cor bddc inexact coars exact bddc 0 5 10 15 20 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(cef) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 5 10 15 20 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(cef) (h/h)3=403 (64k) fes/cor bddc inexact dirichlet exact bddc (a) (b) 0 5 10 15 20 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(cef) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 5 10 15 20 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(cef) (h/h)3=403 (64k) fes/cor bddc inexact coars basi exact bddc 0 5 10 15 20 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(cef) (h/h)3=403 (64k) fes/cor amg(1) amg(2) amg(4) 0 5 10 15 20 128 1k 2k 3.5k 5.5k 8k n u m b e r o f p c g e ra tio n s #core weak scale bddc(cef) (h/h)3=403 (64k) fes/cor bddc inexact neumann exact bddc (c) (d) figur 4: sensit number outer bddc(cef)-pcg iter presenc perturb solut (a) coarse, (b) dirichlet, (c) coarse-grid basis, (d) neumann problems. differ intern solvers, amg(1), amg(2) amg(4), test solut problems. iter inexact bddc(c) precondition singl amg cycl close exact bddc(c). increas number amg cycl leads, expected, reduct number pcg iterations, neglig case. consid nice properti bddc precondit approach provid coarse-grid problem main scalabl bottleneck, overal solut approach immedi benefit save memory/tim achiev algorithm. however, figur 2 (b) (d), reveal high impact inexact solut dirichlet neumann problems, respectively, higher impact case. example, amg(1), roughli 50% 100% increas number pcg iter respect exact bddc(c) precondition observed, respectively. addit amg cycl approxim dirichlet problem, precondition effici exact bddc(c) precondition rapidli recovered. however, case neumann problem, 50% increas observ amg(4). pictur differ case bddc(ce) preconditioner. inexact solut coarse-grid problem mild impact number pcg iter (see figur 3 (a)), inexact comput 22 coarse-grid basi highest impact (a roughli 80% increas number pcg iter amg(1)) intern problems. impact inexact solut dirichlet neumann problem milder inexact comput coarse-grid basis, similar other, 50% increas amg(1) (compar figur 3 (b) (c)). figur 4 (b) (c) reveal close respons bddc(cef) precondi- tioner bddc(ce) precondition presenc perturb dirichlet problem comput coarse-grid basis, respectively. however, figur 4 (a) reveal higher sensit bddc(cef) precondi- tioner perturb solut coarse-grid problem.1 indeed, amg(1), 60% increas number pcg iter observ respect exact bddc(cef) preconditioner. hand, fig- ur 4 (d) reveal mild impact inexact solut neumann problem, amg(1) recov precondition effici exact bddc(cef) preconditioner. 6.2.3. scalabl overlap implement inexact solver previou section shown margin improve- ment (at term number pcg iterations) usag accur solver amg(1) intern problems. light observa- tion, tabl 2 present set select inexact variant bddc precon- ditioner. column label , dirichlet, neumann, coars refer comput coarse-grid basi vectors, dirichlet, neumann, coarse-grid intern problems, respectively. amg(1), amg(2) stand single, pair amg cycles, respectively. dirichlet neumann coars var. 1 amg(1) amg(1) amg(1) amg(1) var. 2 amg(1) amg(2) amg(1) amg(1) var. 3 amg(2) amg(1) amg(2) amg(1) var. 4 amg(2) amg(2) amg(2) amg(1) tabl 2: set select inexact variant two-level bddc method. stress inner solver combin shown tabl 2 on possible, on select wider set comprehens experimentation. first, observ pai accur solver coarse-grid problem (e.g., amg(2) intern pcg-amg iteration), reduct number outer pcg iter compens decreas scalabl larg core count caus costli solut coarse-grid problem. second, 1we remind bddc(cef) coars matrix denser bddc(ce). amg approxim effect fact. 23 consid variant krr 6= krr, e.g., amg(2) neumann prob- lem, amg(1) comput coarse-grid vector vice versa. state remark 4.4, scenario comput (k (i) rr ) 1ctr,i preserv symmetry, instead re-us (k (i) rr ) 1ctr,i constrain neu- mann problem requir comput coarse-grid basi vectors. involv solut extra linear multipl right-hand sides, local coars constraints, precondition set-up. ex- periment observ extra comput significantli outweigh gain deriv usag variants. pair detail underli inexact variant tabl 2 worth noting. first, coarse-grid problem built galerkin project k inexact coars basi (see (11)) instead approach [13], build coarse-grid problem tk (see (12)). consist observ approach lead number pcg iter latter, 25% reduct case (in particular, inexact bddc(cef) precondition largest local problem size h/h = 60). second, var 3. 4 effort var. 1 2, respectively, (more accurate) comput coarse-grid basi vector neumann problem, note var. 3 4 potenti overlap fine-grid coarse-grid computations, particular precon- dition applic bottom-most overlap area tabl 1. studi extent properti var. 3 4 lead increas scalabl reduc comput time compar var. 1 2. figur 5 (a), (b), (c) provid compar view weak scalabil- iti total comput time (in seconds) inexact variant bddc(c), bddc(ce) bddc(cef) solvers, respectively. local problem size went 403=64k (left side) 603=216k fe core (right side), number core 168 93,312 (see section 6.2.1). variants, local problem size combin high degrad weak scalabl observ 43.9k cores, run code limit consumpt underli parallel resources. hand, figur 6 (a), (b), (c) report number pcg (outer solver) iter variants, local problem size combin figur 5. set initi solut vector guess x0 = 0 outer iterations, stop residu rk given iter k satisfi rk2 106r02. shape differ scalabl curv shown figur 5 depend particular balanc fine-grid coarse-grid comput achieved, inexact variant, overlap area shown tabl 1, precondition effici achiev variant, de- termin number extern outer solver iterations. example, inexact bddc(ce), bddc(cef) variants, load core 64k fes/- core, total comput time domin coarse-grid solver 16k 8k cores, respectively, render overlap techniqu success (i.e., given load core limit potenti overlap- 24 0 50 100 150 200 250 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(c) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 50 100 150 200 250 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(c) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 200 400 600 800 1000 1200 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(c) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 0 200 400 600 800 1000 1200 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(c) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 (a) 0 10 20 30 40 50 60 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(ce) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 10 20 30 40 50 60 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(ce) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 20 40 60 80 100 120 140 160 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(ce) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 0 20 40 60 80 100 120 140 160 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(ce) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 (b) 0 10 20 30 40 50 60 70 80 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(cef) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 10 20 30 40 50 60 70 80 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(cef) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 20 40 60 80 100 120 140 160 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(cef) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 0 20 40 60 80 100 120 140 160 1k 16k 27.6k 43.9k 65.5k 93.3k t o ta l w ll cl o ck t im e ( se cs .) #core approxim bddc(cef) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 (c) figur 5: weak scalabl total comput time inexact variant (a) bddc(c), (b) bddc(ce), (c) bddc(cef) solver 3d poisson problem juqueen. left: h h = 40. right: h h = 60. solut coarse-grid linear map addit blade. 25 0 50 100 150 200 250 300 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(c) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 50 100 150 200 250 300 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(c) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 50 100 150 200 250 300 350 400 450 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(c) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 0 50 100 150 200 250 300 350 400 450 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(c) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 (a) 0 5 10 15 20 25 30 35 40 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(ce) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 5 10 15 20 25 30 35 40 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(ce) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 5 10 15 20 25 30 35 40 45 50 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(ce) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 0 5 10 15 20 25 30 35 40 45 50 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(ce) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 (b) 0 5 10 15 20 25 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(cef) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 5 10 15 20 25 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(cef) (h/h)3=403 (64k) fes/cor var. 1 var. 2 var. 3 var. 4 0 5 10 15 20 25 30 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(cef) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 0 5 10 15 20 25 30 1k 16k 27.6k 43.9k 65.5k 93.3k n u m b e r o f p c g e ra tio n s #core approxim bddc(cef) (h/h)3=603 (216k) fes/cor var. 1 var. 2 var. 3 var. 4 (c) figur 6: weak scalabl number pcg iter inexact variant (a) bddc(c), (b) bddc(ce), (c) bddc(cef) solver 3d poisson problem juqueen. left: h h = 40. right: h h = 60. 26 ping). observ left-hand figur 5 (b), (c) total comput time variant increas number cores. expected, degrad weak scalabl linear (with higher slope case inexact bddc(cef) larger, denser stencil, coarse-grid problem) number subdomains, given linear arithmet complex- iti amg preconditioning. however, local problem size increased, overlap fine-grid/coarse- grid duti progress success tackl bottleneck as- sociat coarse-grid problem. example, inexact bddc(c), var. 3, load core 64k fe core, variant case largest load core 216k fes/core, weak scalabl sole determin fast outer precondition solver achiev asymptot constant converg rate fix problem size increas number cores, mean overlap techniqu success complet overlap coarse-grid relat comput 128-93.3k core range; right- hand figur 5 (a), 6 (a). shown right-hand figur 5 (b), hold inexact bddc(ce), 128-43.9k core range; similar observ inexact bddc(cef) tighter core rang figur 5 (c), larger, denser stencil, coarse-grid problem algorithm. 43.9k cores, comput time coarse-grid problem relat comput overlap area #1 #2 (see tabl 1) start exceed fine-grid relat comput inexact bddc(ce) variants, justifi (very) mild degrad roughli 20% 50% observ var. 3 1, respectively, 43.9k-93.3k core range, render level hierarchi necessary. turn attent figur 6 (a), (b), (c), observ var. 4 consist lead smaller number iterations, follow var. 3, 2 1. certainli make sens given var. 4 solv accur intern problems. however, term comput times, provid total comput time domin fine-grid relat computations, rel rank variant subject studi change, vars. 1 3 faster, var. 4 slowest; figur 5 (a), (b), (c). interest observ made, e.g., left hand figur 5 (a) figur 5 (b), var. 3 faster var. 1 suffici larg core counts, extra comput time incur amg(2) comput coarse-grid basi solut neumann problem. mention above, var. 3 put comput effort solut neumann problem. increas potenti implement fulli overlap solut coarse-problem precondition applic pcg phase, result increas scalability. further, import note overal effect accur comput coars matrix scalability. coars basi comput amg(2) (vars. 3 4) instead amg(1) (vars. 1 2), scalabl loss severe. expected, degrad linear, 27 linear complex amg. however, slope notic wors vars. 1 2 vars. 3 4. conclusion, larg core counts, reduc number pcg iterations, increas scalability, lower degrad var. 3 win choic larg core counts. conclud study, report tabl 3, exact inexact vari- ant bddc preconditioner, increas valu local problem size h h , memori consumpt figur fine-grid precondit level; tabl 4 report coarse-grid precondit level. 1 exact vari- ant suppli hsl ma87 [41], highly-effici parallel multi-thread dag-bas code implement supernod spars direct choleski solver. note figur report tabl 3 4 correspond (permanent) memori consum precondition com- puted, (temporary) memori computation. h h solver var. 10 20 30 40 60 bddc(c) inexact 20.8m 38.2m 81.2m 157.1m 516.5m exact 21.9m 64.2m 218.4m 613.6m o.m. bddc(ce) inexact 22.6m 42.3m 86.7m 158.9m 522.2m exact 21.7m 64.0m 219.9m 618.8m o.m. bddc(cef) inexact 22.6m 42.3m 86.7m 160.6m 527.5m exact 21.7m 64.3m 219.6m 625.2m o.m. tabl 3: memori consumpt highest memori consum fine-grid task exact inexact variant 2-level bddc preconditioner. o.m.: memory. m (#subdomains=16m3) solver 6 8 10 12 14 16 18 inexact bddc(c) 17.1m 23.1m 33.9m 51.5m 63.5m 94.2m 133.9m inexact bddc(ce) 30.3m 56.2m 100.3m 167.8m 263.5m 392.7m 582.7m inexact bddc(cef) 54.9m 118.9m 228.5m 396.6m 604.0m o.m. o.m. tabl 4: memori consumpt coarse-grid task inexact variant 2-level bddc preconditioner. o.m.: memory. expected, inexact variant bddc precondition memori demand exact one. clearli justifi linear order memori complex amg solver inexact variant fulli exploits. absolut terms, consum moder memory, roughli half gbyte largest local problem size. 50% memori avail juqueen, mean larger problem size solv machin (i.e., reach memori limit propos solver machinery), enabl improv scalabl results. (mild) increas 1memori consumpt obtain juqueen malloc stat right precondition set-up. 28 memori consumpt addit constraint easili explain fact extra number coarse-grid basi vector store memory. hand, look memori consumpt coarse- grid solver tabl 4 observe, expected, moder (linear) increas number subdomains, higher number constraints, higher slope. absolut terms, observ largest number subdomain test (i.e., 93.3k), inexact bddc(ce) precondition consum roughli 58% (i.e., 582.7mbytes) memori available, mean solver machineri propos solv larger problem larger number subdomains. 6.3. inexact bddc complex domain unstructur mesh mn-iii section appli overlap implement inexact bddc precondition solut poisson problem unstructur meshes, requir numer simul (turbulent) incompress- ibl flow typic pressur segreg techniqu [26, 27]. particular, analyz applic inexact bddc method solut pressur poisson equat classic benchmark incompress flow backward face step. unstructur fe mesh split mean automat mesh (actual dual graph) partitioner. combin factors, extent increas degre irregular address (e.g., automat partition typic lead irregular decomposit domain subdomains), poss challeng techniqu precon- dition local dirichlet/neumann problems, global coarse-grid (in case, classic ruge-stuben amg), inexact bddc precondit approach whole. stress, however, purpos section comprehens assess weak scalabl code structur test case section 6.2, grasp expect algorithm machineri subject studi appli complex test cases. comprehens assess require, hand, consid wider rang test cases. hand, requir explor wider rang number subdomain scale problem on consid here. nevertheless, abl compar per- formanc exact inexact variant provid exact bddc precondition fit avail memori number core scale problem consid (in contrast experi section 6.2). 6.3.1. 2d experi code subject studi appli linear fe discret (i.e., p1-elements) 2d poisson problem backward-fac step domain depict figur 7. boundari condit set homogen dirichlet boundari left-most (perpendicular x-axis) edg boundary, unknown constrain equal constant function gd = 1 (i.e., non-homogen dirichlet boundari conditions). finally, set f = 1 (i.e., constant forc term) domain. 29 consid local problem size l1 20.9k, l2 39.2k, l3 80k, l4 157.8k triangl subdomain, respectively. local problem size, consid meshes, correspond 64, 256, 1024 subdomains. mesh partit shared-memori multipro- cessor 256gbyte main memori multilevel graph partit algorithm avail meti 5.1.0 [42]. (irregular) partit (result metis) mesh depict figur 7 (b) 64 part shown figur 7 (a) color repres subdomain load core l1 l2 l3 l4 #core #node #fe #node #fe #node #fe #node #fe 16 168k 334k 316k 628k 643k 1.28m 1.26m 2.52m 64 671k 1.34m 1.26m 2.51m 2.56m 5.12m 5.07 10.1m 256 2.68m 5.35m 5.03m 10.0m 10.3m 20.5m 20.2m 40.4m 1024 10.7m 21.4m 20.1m 40.2m 41m 82.0m 80.7m 161m tabl 5: number node triangl (#fes) unstructur comput mesh scale study. mesh partit 16, 64, 256, 1024 subdo- main meti 5.1.0 [42], differ comput load subdomain l1 20.9k, l2 39.2k, l3 80k, l4 157.8k triangl subdomain consid study. (a) (b) figur 7: (a) 2d backward face step domain partit 64 subdomain metis. (b) zoom view unstructur mesh area delimit grei colour contour figur 7 (a). tabl 6 compar number pcg iter (exact) 2-level bddc precondition variant inexact precondi- tioner sketch tabl 2. number core size problem 30 scale describ tabl 5. effort set hsl mi20 pa- ramet reach best memory/tim trade-off particular problem, = 0.25, rs1 coarsening, damp jacobi smooth winner combin intern problem (see [38] details). comparison tabl 6 reflects, hand, exact 2-level bddc precondition reach faster (with number cores) asymptot- ical constant number iter compar inexact variants. hand, observ that, expected, asymp- totic constant number iter reach smaller bddc inexact bddc. case, stress differ number iter exact inexact bddc precondition depend number cores, provid method subject evalu final reach asymptot constant number iterations. besides, consid remark properti inexact variant moder loss precon- dition robust came benefit reduc order arithmet memori complex (in particular, o(n1.5) order arithmet complex spars direct solver versu o(n) amg precondition set-up, o(nlogn) versu o(n) precondition application). turn attent rel merit inexact variant tabl 6, observ var 1. var 3. converg (almost) number iterations. observ reveal accur comput coarse-grid basi vectors, solut neumann problem iter (amg(2) var. 3 versu amg(1) var. 1), littl impact precondition robust particular problem. confirm compar number iter vars. 2 4, (almost) coincident. however, compar number iter vars. 1. 3, vars. 2. 4, observ signific reduct number iter variants, reveal that, problem subject study, margin improv (in term number iterations) accur solut dirichlet problem (amg(2) vars. 2 3 versu amg(1) vars. 1 4). higher load core, number cores, larger improvements. figur 8 report weak scalabl comput time over- lap implement (exact) 2-level bddc precondition (see [12]) inexact variants, l1 l2 (figur 8 (a)), l3 l4 fix load core (figur 8 (b)). y-axi plot right figs. 8 (a) (b) scale match correspond on left simplifi comparison bddc(c) bddc(ce)-bas variants. point, import stress that, precondition subject study, load core explored, time spent coarse-grid problem fulli overlap 16-1024 core range. observ justifi exact inexact variant equip corner edg constraint faster counterpart equip corner constraint (com- pare left right part figur 7, complement comparison number iter bddc(c) versu bddc(ce) tabl 7). overall, figur 7, remark time scalabl observ 16-1024 core rang 31 solver var. load core l1 l2 l3 l4 #core #core #core #core 16 64 256 1k 16 64 256 1k 16 64 256 1k 16 64 256 1k bddc(c) exact * 10 14 16 17 10 14 16 17 11 15 18 18 12 16 18 19 inexact 1 20 34 40 41 23 41 49 50 28 48 60 60 31 58 75 76 2 13 21 23 24 15 24 26 26 17 25 28 29 17 28 31 30 3 21 33 38 40 25 41 48 49 30 50 61 61 35 62 75 77 4 12 18 21 24 13 21 22 23 16 22 25 26 16 24 26 27 bddc(ce) exact * 8 8 8 8 9 8 8 8 9 8 9 9 9 9 9 9 inexact 1 21 25 26 27 24 29 31 32 30 36 38 39 34 45 46 48 2 13 14 15 16 14 14 15 15 16 15 16 16 16 16 16 17 3 21 25 25 27 24 30 32 33 30 37 39 40 34 46 48 49 4 12 13 15 15 14 14 15 16 15 14 15 16 15 15 16 17 tabl 6: weak scalabl number pcg iter exact inexact variant (see tabl 2) bddc(c), bddc(ce) precondition 2d poisson problem backward face step domain (see figur 7). despit high irregular problem hand. 0 0.1 0.2 0.3 0.4 0.5 16 64 256 1024t o ta l w ll cl o ck t im e ( se cs .) #subdomain var. 1 var. 2 var. 3 var. 4 exact 0.5 0.6 0.7 0.8 0.9 1 16 64 256 1024t o ta l w ll cl o ck t im e ( se cs .) weak scale bddc(c) l1 l2 0 0.1 0.2 0.3 0.4 0.5 16 64 256 1024t o ta l w ll cl o ck t im e ( se cs .) #subdomain var. 1 var. 2 var. 3 var. 4 exact 0.5 0.6 0.7 0.8 0.9 1 16 64 256 1024t o ta l w ll cl o ck t im e ( se cs .) weak scale bddc(ce) l1 l2 (a) load core l1 l2. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 16 64 256 1024t o ta l w ll cl o ck t im e ( se cs .) #subdomain var. 1 var. 2 var. 3 var. 4 exact 3.0 4.0 5.0 6.0 7.0 8.0 16 64 256 1024t o ta l w ll cl o ck t im e ( se cs .) weak scale bddc(c) l3 l4 0.0 0.5 1.0 1.5 2.0 2.5 3.0 16 64 256 1024t o ta l w ll cl o ck t im e ( se cs .) #subdomain var. 1 var. 2 var. 3 var. 4 exact 3.0 4.0 5.0 6.0 7.0 8.0 16 64 256 1024t o ta l w ll cl o ck t im e ( se cs .) weak scale bddc(ce) l3 l4 (b) load core l3 l4. figur 8: weak scalabl mn-iii total comput time exact inexact variant (see tabl 2) bddc(c) (left) bddc(ce) (right) precondition 2d poisson problem backward face step domain (see figur 7). 32 compar time inexact variant figur 8, recurr observ var. 2 fastest variant, (closely) follow var. 4, (further) var. 1, and, finally, (further) var. 3. surpris glanc provid that: (1) vars. 2 4 took significantli iter converg vars. 1 3 (see tabl 6); (2) vars. 4 3 perform extra amg cycl comput coarse-grid basi vector solut neumann problem pcg iter (compar vars. 2 1, respectively). however, note despit var. 2 converg iter var. 1, var. 2 involv applic extra amg cycl solut (pre post) dirichlet correct pcg iteration. therefore, conclud (remarkable) trade-off reach reduct number iter incur extra pair amg cycl (at iteration) compens extra time requir applic (at iteration). compar comput time fastest inexact variant, i.e., var. 4, exact bddc preconditioner, observ that, cases, method achiev similar trade-off number iter time spent solut intern problems. reflect that, rang local problem subject study, gain deriv costli set inner solver inexact variant outweigh extra number outer iter result lost robustness. (we expect, however, signific gain inexact solver far local problem size scale further.) case, inexact variant save signific memori compar exact counterparts, reduc order memori complex amg solver spars direct solvers. example, l4 1k cores, exact bddc(ce) precondition consum 411 mbyte highest memori demand fine-grid task, inexact variants, 309 mbytes. 6.3.2. 3d experi section appli inexact bddc precondition solut three-dimension extens problem target section 6.3.1. comput domain problem shown figur 9, unstructur comput mesh tetrahedra scale study, partit 64 subdomains. mesh gener keep ratio total number fe number subdomain approxim equal 20k, 128, 256, 512, 1024, 2048 subdomains, respectively. tabl 7 minimum, maximum, averag number node subdomains. observ maximum number node slightli increases, reach asymptot regime. appli factor (significant) impact perfor- mance, averag number coars dof subdomain. hand, limit moder load core consid section 6.3.1, given constraint inher (mesh partitioning) code, limit memory/disk capac avail underli shared-memori com- puter (recal code mesh partit reli metis, i.e., distributed-memori parallelization). load core consid 33 figur 9: 3d backward face step domain, unstructur comput mesh tetrahedra partit 64 subdomain metis. largest section 6.3.1 code abl partit mesh requir perform scalabl studi 4k cores. tabl 7 compar number pcg iter (exact) 2-level bddc precondition inexact bddc variant subject study. set valu hsl mi20 paramet consid sec- tion 6.3.1 consid here. present shorter, omit result correspond bddc(c)-base preconditioners. similar observ section 6.2 case, bddc(c)- base precondition (significantly) robust bddc(ce/cef)-bas ones. solver var. #core 16 32 64 128 256 512 1k 2k 4k bddc(ce) exact * 13 16 16 14 16 18 20 21 23 inexact 1 19 22 24 23 27 35 45 49 52 2 16 20 21 19 23 28 35 37 39 3 20 22 24 24 28 35 46 51 53 4 16 18 19 18 21 27 34 37 39 bddc(cef) exact * 13 16 16 13 16 17 19 21 23 inexact 1 19 22 24 23 27 35 44 49 52 2 16 20 22 21 25 31 37 41 40 3 20 22 24 24 28 35 45 50 53 4 16 19 22 20 23 31 36 40 41 tabl 7: weak scalabl number pcg iter exact inexact variant (see tabl 2) bddc(ce) bddc(cef) precondition 3d poisson problem backward face step domain (see figur 9). overall, close observ tabl 6 deriv tabl 7. exact 2-level bddc precondition reach faster (with number 34 cores) asymptot constant number iter compar inexact variants, besides, constant reach smaller former. stress differ asymptot constant number core (confirm precondition optim inexact variants). load core explored, maximum increas factor 2.3 1.7 seen vars. 1 3, 2 4, respectively. compar inexact variants, accur solut dirichlet problem signific posit impact precondition qualiti (compar vars. 2 1, 4 3), comput coarse-grid basi vector constrain neumann problem iter (compar vars. 3 1, 4 2). finally, result tabl 7 confirm that, exact inexact variants, gain deriv addit face constraint coarse-grid space, number iter bddc(cef)-bas precondition (at most) bddc(ce)-bas counterparts. figur 10 report weak scalabl comput time over- lap implement (exact) 2-level bddc(ce) (left) bddc(cef) (right) precondition (see [12]) inexact variants, mod- erat load 20k fes/core. y-axi plot right fig. 10 scale match correspond left simplifi com- parison bddc(ce) bddc(cef)-bas variants. 0 5 10 15 20 25 30 16 256 512 1k 2k 4k t o ta l w ll cl o ck t im e ( se cs .) #subdomain weak scale bddc(ce) var. 1 var. 2 var. 3 var. 4 exact 0 5 10 15 20 25 30 16 256 512 1k 2k 4k t o ta l w ll cl o ck t im e ( se cs .) #subdomain weak scale bddc(cef) var. 1 var. 2 var. 3 var. 4 exact figur 10: weak scalabl mn-iii total comput time exact inexact variant (see tabl 2) bddc(ce) (left), bddc(cef) (right) precondition 3d poisson problem backward face step domain (see figur 9) moder load 20k fes/core. (remarkable) observ figur 10 time scalabl exact variant start degrad higher pace 512 256 subdomain bddc(ce) bddc(cef), respec- tively, compar inexact variants. factor caus exact variant slower inexact variant point. ta- ble 8 complement figur 10 provid size (n) number nonzero (nnz) coarse-grid matrix (in column label ac), min- imum, maximum, averag local degre freedom subdomain 35 (in column label #local dofs). compar number global coars dof (n), number local dofs, aforement number subdomains, (largely) exce latter. surpris point time spent coarse-grid comput larger fine-grid on (render fine/- grid overlap longer fulli effective), domin number subdomains, scalabl curv final reflect order complex underli coarse-grid solver (i.e., o(n2) versu o(n) spars direct solver compar amg preconditioners). slope curv higher bddc(cef) bddc(ce), provid requir addit coars degre freedom (see tabl 8). solver #core ac #local dof #local coars dof n nnz min. max. avg. min. max. avg. bddc(ce) 16 39 487 3.04k 3.45k 3.32k 1.0 9.0 6.3 32 89 1437 3.18k 3.78k 3.52k 1.0 13.0 7.6 64 308 12734 3.27k 4.08k 3.63k 4.0 36.0 15.5 128 968 66268 3.22k 4.29k 3.71k 8.0 52.0 24.9 256 8213 180074 2.97k 4.74k 3.84k 7.0 66.0 28.7 512 5578 621522 3.07k 4.70k 3.85k 4.0 88.0 36.1 1024 12579 1612669 2.89k 4.86k 3.91k 6.0 102.0 40.8 2048 28272 4021436 2.91k 4.93k 4.00k 4.0 98.0 46.0 4096 61177 9299555 2.93k 4.86k 3.97k 6.0 96.0 49.8 bddc(cef) 16 66 1236 3.04k 3.45k 3.32k 2.0 13.0 9.7 32 151 3533 3.18k 3.78k 3.52k 2.0 18.0 11.4 64 511 27323 3.27k 4.08k 3.63k 6.0 46.0 21.9 128 1485 123815 3.22k 4.29k 3.71k 12.0 66.0 33.0 256 3368 326426 2.97k 4.74k 3.84k 11.0 81.0 37.6 512 8213 1059719 3.07k 4.70k 3.85k 7.0 108.0 46.4 1024 18229 2676749 2.89k 4.86k 3.91k 10.0 125.0 51.8 2048 40374 6538298 2.91k 4.93k 4.00k 7.0 118.0 57.8 4096 86592 14954200 2.93k 4.86k 3.97k 10.0 118.0 62.2 tabl 8: number coarse-grid dof (n) number nonzero element (nnz) spars coarse-grid matrix, minimum, maximum, averag number local degre freedom (among subdomains), minimum, maximum, averag number local coars degre freedom (among subdomains), bddc(ce) bddc(cef) preconditioners. 7. conclus futur work work, analyz scalabl inexact bddc precondi- tioners. inexact amg solver considered, linear complex low memori requirements. further, highli scalabl implement fine/coars duti time used, extens work [12] inexact solvers. allow fulli overlap coars problem task harm scalabl embarrassingli parallel fine task reduc check-point idling. choic motiv futur exascal scenario, larg core count reduc memori core. 36 numer test overlapped/inexact implement algorithm [13] (with slight modif coars solver approxima- tion) complement mathemat analysis. work show far implement propos scale respect number core size global problem serial amg softwar packag like hsl mi20 [38]. inexact solvers, consid fix (one two) number amg cycles. (the us pcg-amg local/coars solver considered, turn effici fix number amg cycl cases.) first, carri sensit analysis, analyz effect inexact solver iter counts/condit numbers. next, perform comprehens weak scalabl analysi 93,312 core 20 billion unknown juqueen, julich supercomput center (jsc). far know, largest scale scalabl analys simul perform far dd methods. singl core 1 gbyte memori coarse-grid problem, scalabl inexact variant repres dramat improv compar largest scale scalabl analys exact bddc method far (see [12]), justifi approach consid herein. analysis, conclud moder core counts, 35k, best option us amg cycl local coars prob- lems. however, run larger set processors, comput slightli accur coars basi neumann problem (use amg cycles) certainli pai price; result coars problem easier approxim amg, number iter reduced, fine work load fulli overlap coars tasks. respect variant bddc methods, bddc(c), bddc(ce), bddc(cef), increas number iter bddc(c) precondition dimens compens smaller coars problem scale consid herein, coars problem cpu cost (at part) overlap fine du- ties. general, bddc(ce) precondition best compromis number iter comput cost. scalabl poisson solver unstructur mesh great import deal le simul turbul flow gener geometries. pressur segreg techniqu used, lead pressur poisson problem [26, 27]. analyz effect inexact bddc set typic backward-fac step benchmark unstructur meshes, dimensions. step effort push forward balanc dd scalabl till extrem core count distribut coarse-grid problem mpi coarse-grid tasks, possibl span multipl comput nodes. ac- complish link inexact/overlap bddc implement mpi-distribut amg solver like boomeramg [18], alternatively, extend overlap bddc techniqu describ multilevel setting, recurs us implement fempar. base current ex- perienc (overlap two-level implement scale ten thousand processors) exist mathemat analyses, natur 37 expect three-level overlap implement bddc perfectli scale largest hpc system today. explor here, left excit futur line research. acknowledg work fund european research council fp7 program idea start grant no. 258443 - comfus: computa- tional method fusion technolog fp7 numexa project grant agreement 611636. a. f. martn partial fund generali- tat catalunya program ajut la incorporacio, amb caract temporal, person investigador junior le universitat publiqu del sis- tema universitari catala pdj 2013. acknowledg prace award access resourc juqueen base germani julich supercomput centr (jsc), gauss centr supercomput (gcs) provid- ing comput time john von neumann institut comput (nic) gc share juqueen. gc allianc nation supercomput centr hlr (universitat stuttgart), jsc, lrz (bayerisch akademi der wissenschaften), fund german feder min- istri educ research (bmbf) german state ministri research baden-wurttemberg (mwk), bayern (stmwfk) nordrhein- westfalen (miwf). gratefulli acknowledg jsc staff general, dirk broemmel particular, support porting/debug fempar depend to/on juqueen. finally, author thankfulli acknowledg resources, technic expertis assist provid red espanola supercomputacion. refer [1] report workshop extreme-scal solvers: transit futur architectures, tech. rep., u.s. depart energi (2012). [2] p. t. lin, j. n. shadid, m. sala, r. s. tuminaro, g. l. hennigan, r. j. hoekstra, perform parallel algebra multilevel precondi- tioner stabil finit element semiconductor devic modeling, jour- nal comput physic 228 (17) (2009) 62506267. doi: //dx.doi.org/10.1016/j.jcp.2009.05.024. [3] k. stuben, review algebra multigrid, journal com- putat appli mathemat 128 (12) (2001) 281 309. doi:10.1016/s0377-0427(00)00516-1. url s0377042700005161 [4] a. toselli, o. widlund, domain decomposit method - algorithm theory, springer-verlag, 2005. 38 [5] j. mandel, balanc domain decomposition, commun numeri- cal method engin 9 (3) (1993) 233241. url [6] c. r. dohrmann, precondition substructur base constrain energi minimization, siam journal scientif comput 25 (1) (2003) 246258. doi:10.1137/s1064827502412887. url [7] c. farhat, k. pierson, m. lesoinne, second gener feti method applic parallel solut large-scal linear geometr non-linear structur analysi problems, meth- od appli mechan engin 184 (24) (2000) 333374. doi:10.1016/s0045-7825(99)00234-0. url s0045782599002340 [8] s. badia, a. f. martn, j. principe, enhanc balanc neumann- neumann precondit comput fluid solid mechanics, in- ternat journal numer method engin 96 (4) (2013) 203230. [9] p. amestoy, i. duff, j.-y. lexcellent, multifront parallel dis- tribut symmetr unsymmetr solvers, method appli mechan engin 184 (24) (2000) 501520. doi:10.1016/s0045-7825(99)00242-x. url s004578259900242x [10] b. sousedk, j. sstek, j. mandel, adaptive-multilevel bddc parallel implementation, comput 95 (12) (2013) 10871119. doi:10.1007/s00607-013-0293-5. url s00607-013-0293-5 [11] v. hapla, d. horak, m. merta, us direct solver tfeti massiv parallel implementation, in: appli parallel scientif computing, no. 7782 lectur note science, springer berlin heidelberg, 2013, pp. 192205. url 978-3-642-36803-5_14 [12] s. badia, a. f. martn, j. principe, highli scalabl parallel implementa- tion balanc domain decomposit constraints, siam journal scientif comput (2014) c190c218doi:10.1137/130931989. url [13] c. r. dohrmann, approxim bddc preconditioner, numer linear algebra applic 14 (2) (2007) 149168. doi:10.1002/nla.514. 39 url abstract [14] x. tu, three-level bddc dimensions, siam journal scientif comput 29 (4) (2007) 17591780. doi:10.1137/050629902. url [15] j. mandel, b. sousedk, c. dohrmann, multispac multilevel bddc, comput 83 (2) (2008) 5585. doi:10.1007/s00607-008-0014-7. url abstract/ [16] j. mandel, c. r. dohrmann, r. tezaur, algebra theori primal dual substructur method constraints, appli numer mathemat- ic 54 (2) (2005) 167193. [17] j. li, o. b. widlund, us inexact subdomain solver bddc algorithms, method appli mechan engin 196 (8) (2007) 14151428. doi:10.1016/j.cma.2006.03.011. url s0045782506002611 [18] v. e. henson, u. m. yang, boomeramg: parallel algebra multigrid solver preconditioner, appli numer mathemat 41 (1) (2002) 155177. doi:10.1016/s0168-9274(01)00115-5. url s0168927401001155 [19] o. rheinbach, parallel iter substructur structur mechanics, archiv comput method engin 16 (4) (2009) 425463. doi:10.1007/s11831-009-9035-4. url s11831-009-9035-4 [20] a. klawonn, o. rheinbach, highli scalabl parallel domain decom- posit method applic biomechanics, zamm - journal appli mathemat mechan 90 (1) (2010) 532. doi:10.1002/zamm.200900329. url 200900329/abstract [21] o. schenk, k. gartner, fast factor pivot method spars symmetr indefinit systems, electron transact numer anal- ysi 23 (2006) 158179. [22] o. colomes, s. badia, r. codina, j. principe, assess variat multiscal model larg eddi simul turbul incompress- ibl flows, method appli mechan engin 285 (2015) 3263. 40 [23] o. colomes, s. badia, segreg runge-kutta method incompress- ibl navier-stok equations, press. [24] s. badia, a. f. martn, r. planas, block recurs lu precondi- tioner thermal coupl incompress inductionless mhd problem, journal comput physic 274 (2014) 562591. doi:10.1016/j.jcp.2014.06.028. url s0021999114004355 [25] s. badia, r. planas, j. v. gutirrez-santacreu, uncondition stabl op- erat split algorithm incompress magnetohydrodynam discret stabil finit element formul base projections, intern journal numer method engin 93 (3) (2013) 302328. doi:10.1002/nme.4392. url abstract [26] h. c. elman, d. j. silvester, a. j. wathen, finit element fast iter- ativ solvers: applic incompress fluid dynamics, oxford univers press, 2005. [27] s. badia, r. codina, algebra pressur segreg method in- compress navier-stok equations, archiv comput method engin 15 (3) (2008) 343369. [28] p. jolivet, v. dolean, f. hecht, f. nataf, c. prudhomme, n. spillane, high perform domain decomposit method massiv parallel architectur freefem++, j. numer. math. 20 (3-4) (2012) 287302. [29] s. badia, a. f. martn, j. principe, implement scalabl anal- ysi balanc domain decomposit methods, archiv compu- tation method engin 20 (3) (2013) 239262. doi:10.1007/ s11831-013-9086-4. [30] j. sstek, m. certkova, p. burda, j. novotny, face-bas select corner 3d substructuring, mathemat comput simul 82 (10) (2012) 17991811. doi:10.1016/j.matcom.2011.06.007. url s0378475411001820 [31] j. mandel, c. r. dohrmann, converg balanc domain decompo- sition constraint energi minimization, numer linear algebra applic 10 (7) (2003) 639659. doi:10.1002/nla.341. url [32] g. golub, q. ye, inexact precondit conjug gradient method inner-out iteration, siam journal scientif comput 21 (4) (1999) 13051320. doi:10.1137/s1064827597323415. url 41 [33] y. saad, flexibl inner-out precondit gmre algorithm, siam journal scientif comput 14 (12) (1993) 461469. [34] p. vanek, j. mandel, m. brezina, algebra multigrid smooth aggre- gation second fourth order ellipt problems, comput 56 (1996) 179196. url [35] m. sala, r. tuminaro, new petrov-galerkin smooth aggreg pre- condition nonsymmetr linear systems, siam journal scientif comput 31 (1) (2008) 143166. doi:10.1137/060659545. [36] s. balay, j. brown, k. buschelman, w. d. gropp, d. kaushik, m. g. knepley, l. c. mcinnes, b. f. smith, h. zhang, petsc web page, //www.mcs.anl.gov/petsc (2012). [37] f. hecht, freefem++ user manual. 3rd edition, version 3.22, avail (2013). url [38] j. boyle, m. mihajlovic, j. scott, hsl mi20: effici amg precondi- tioner finit element problem 3d, intern journal numeri- cal method engin 82 (1) (2010) 6498. doi:10.1002/nme.2758. url abstract [39] j. w. ruge, k. stuben, algebra multigrid (amg), in: multigrid meth- ods, s. f. mccormick edition, vol. 3 frontier appli mathematics, siam, philadelphia, pa, 1987, pp. 73130. [40] u. trottenberg, c. c. w. oosterlee, a. schuller, multigrid, academ press, 2001. [41] j. hogg, j. reid, j. scott, design multicor spars choleski factor- izat dags, siam journal scientif comput 32 (6) (2010) 36273649. doi:10.1137/090757216. url [42] g. karypis, softwar packag partit unstructur graphs, partit meshes, comput fill-reduc order spars matrices. version 5.1.0, tech. rep., univers minnesota, depart scienc engineering, minneapolis, mn, avail (2013). url pdf 42 introduct motiv balanc domain decomposit problem set bddc precondition inexact bddc highli scalabl distributed-memori implement numer experi code parallel framework inexact bddc 3d poisson structur mesh juqueen problem parallel set-up impact approxim solv intern problem scalabl overlap implement inexact solver inexact bddc complex domain unstructur mesh mn-iii 2d experi 3d experi conclus futur work