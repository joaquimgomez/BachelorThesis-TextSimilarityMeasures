upcommon portal del coneix obert de la upc http : //upcommons.upc.edu/e-print aquesta s una cpia de la versi author  final draft d'un articl publicat la revista pattern recognit letter .
url d'aquest document upcommon e-print : http : //hdl.handle.net/2117/102922 paper publicat / publish paper : gupta , parth , costa-juss , marta r. , rosso , paolo , banch , rafael e. ( 2016 ) deep source-context featur lexic select statist machin translat .
pattern recognit letter , vol .
75 , may 2016 , pg .
24-29 .
doi:10.1016/j.patrec.2016.02.014 .
http : //upcommonsdev.upc.edu/ http : //upcommonsdev.upc.edu/ http : //upcommons.upc.edu/e-print http : //hdl.handle.net/xxxx/xxxxx 1 pattern recognit letter journal homepag : www.elsevier.com deep source-context featur lexic select statist machin translat parth guptaa , marta r. costa-jussab ,  , paolo rossoa , rafael e. banchsc aprhlt research center , universitat politecnica de valencia btalp research center , universitat politecnica de catalunya , barcelona centro de investigacion en computacion , instituto politecnico nacion , mexico chuman languag technolog , institut infocomm research , singapor abstract paper present methodolog address lexic disambigu standard phrase-bas sta- tistic machin translat system .
similar among sourc context use select appropri translat unit .
inform introduc novel featur phrase-bas model use select translat unit extract train sentenc similar sentenc translat .
similar comput deep autoencod represent , allow obtain effect low-dimension embed data statist signific bleu score improve- ment two differ task ( english-to-spanish english-to-hindi ) .
c 2016 elsevi ltd. right reserv .
1 .
introduct sourc context usual relev translat text .
howev , standard phrase-bas statist machin translat ( smt ) system use sourc context limit word compos translat unit .
source-context inform becom special necessari translat differ domain .
also , source-context inform im- portant sourc languag sourc word form ( spell ) translat differ form target word .
address two differ motiv , sourc context in- format introduc phrase-bas system differ perspect : lexic semant topic adapt ( section 2 ) .
former use differ classif techniqu decid mean word multipl translat .
latter explor differ topic featur function .
paper , propos enhanc context-awar translat unit take account semant con- text provid sourc sentenc translat ( section 3 ) .
allow introduc new featur function translat unit inform similar input sentenc translat sourc sentenc correspond author e-mail : marta.ruiz @ upc.edu ( marta r. costa-jussa ) translat unit extract .
methodolog pro- pose evalu work base sourc con- text similar approach present ( banch costa-jussa , 2011 ) use latent semant analysi ( lsa ) comput sim- ilar among differ context .
differ work , introduc use auto-encod construct deep represen- tation sentenc reduc space comput simi- lariti among sentenc ( use sourc context trans- lation unit ) .
algorithm test intern evalu workshop statist machin translat 2014 ( costa-jussa et al. , 2014 ) .
evalu use fea- ture learn deep autoencod model frame- work assess semant similar among sentenc ( sec- tion 4 ) .
deep learn shown outperform compar generat model like alreadi mention lsa ( hin- ton salakhutdinov , 2006 ) lda ( salakhutdinov hinton , 2009 ; srivastava et al. , 2013 ) .
introduct unsupervis pretrain ( hinton salakhutdinov , 2006 ; erhan et al. , 2010 ) , deep autoencod , use work estim similar sentenc contextu latent space , effici train .
deep learn algorithm im- plement use gpus high scalabl .
domain adapt alreadi train model , import issu con- textual similar method , effect handl deep learn method glorot et al .
( 2011 ) ; bengio ( 2012 ) .
similar method like lsa , context matrix factor- ize scratch adapt .
methodolog , 2 research highlight ( requir ) creat highlight , pleas type highlight \item command .
 introduct source-context deep featur standard phrase-bas statist machin translat system .
 comput sentence-similar mean auto-encod .
 effect low-dimension embed data .
goal improv translat output term lexic select .
experi standard data collect english- spanish english-hindi translat task show propos method perform signific ( statisticallli ) better baselin ( section 5 ) .
also present thorough analysi scalabl aspect propos method .
rest paper organ follow .
section 2 re- port overview relat work introduc sourc context inform use deep learn standard smt system .
section 3 present phrase-bas model ex- tend sourc context inform .
section 4 explain deep represent sentenc , use better com- pute similar among sourc context .
section 5 describ experi proof relev techniqu section 6 conclud .
2 .
relat work sinc main novelti paper ad sourc context knowledg mean deep learn techniqu standard phrase-bas smt system , give overview rele- vant work ( without aim complet ) area .
2.1 .
ad sourc context smt mention previous section , address two dif- ferent motiv , sourc context inform intro- duce phrase-bas system differ perspect : lexic semant topic adapt .
lexic semant work , carpuat wu ( 2005 ) in- troduc word sens disambigu techniqu .
bonet et al .
( 2009 ) train local classifi use linguist context infor- mation translat phrase .
haqu ( 2010 ) use differ syntac- tic lexic featur propos incorpor in- format neighbour word report complet state-of-the-art introduc sourc context phrase-bas system reader refer .
topic adapt perspect , work basic fo- cus address challeng translat differ do- main .
exampl , banch costa-jussa ( 2011 ) use latent semant analysi ( lsa ) comput similar among differ- ent context .
recent , chen et al .
( 2013 ) comput phrase pair featur vector space represent captur do- main similar develop .
hasler et al .
( 2014 ) use latent dirichlet alloc ( lda ) comput topic featur function .
2.2 .
use deep learn techniqu smt last 10 year , increas studi mt use differ strategi base deep learn .
worth notic huge explos work topic last big confer acl , naacl emnlp .
approach tri modifi featur model standard smt system .
work propos novel mt architectur .
first work ad deep learn smt system use continuous-spac neural languag model , e.g .
schwenk et al .
( 2006 ) ; vaswani et al .
( 2013 ) .
one smooth bilingu languag model inspir previ- ous one , e.g .
schwenk et al .
( 2007 ) ; zamora-martnez et al .
( 2010 ) , liu et al .
( 2013 ) use deep learn algorithm improv translat target languag model mt son et al .
( 2012 ) ; kalchbrenn blunsom ( 2013 ) .
recent work use deep learn model phrase probabl , e.g .
cho et al .
( 2014 ) ; new reorder model , e.g .
li et al .
( 2013 ) ; new differ featur lu et al .
( 2014 ) .
differ neural ar- chitectur face bilingu translat present e.g .
sundermey et al .
( 2014 ) ; kalchbrenn blunsom ( 2013 ) .
2.3 .
dimesion reduct techniqu similar estima- tion field similar estim continu space also advanc recent past .
earli model base lsa ( dumai et al. , 1988 ) laid foundat dimension- aliti reduct techniqu incorpor context form correl matrix .
formul exploit advanc linear model orient principl compo- nent analysi ( opca ) ( platt et al. , 2010 ) s2net ( yih et al. , 2011 ) .
non-linear extens outperform linear counterpart includ use deep autoencod ( hinton salakhutdinov , 2006 ; srivastava et al. , 2013 ; gupta et al. , 2014 ) .
work , exploit deep autoencod base model estim sourc context similar .
3 .
extend phrase-bas model section describ standard phrase-bas smt sys- tem methodolog integr sourc context system theoret practic point view .
3.1 .
phrase-bas smt given sourc string sj1 = s1 .
.
.
j .
.
.
sj translat target string ti1 = t1 .
.
.
ti .
.
.
ti , phrase-bas smt system aim choos , among possibl target string , string highest probabl : ti1 = argmax ti1 p ( ti1| j 1 ) j number word target sourc sentenc , respect .
phrase-bas system seg- ment sourc sentenc segment , translat segment use phrase contain sourc target se- quenc word ( s1..sn|||t1..tm ) .
final , system compos target sentenc .
standard implement phrase- base system use sever featur give probabl com- bine relat frequenc togeth : target languag model , word phrase bonus source-to-target target- to-sourc lexic model reorder model koehn et al .
( 2007 ) .
3.2 .
theoret integr methodolog idea extend concept translat unit phrase ( p ) defin unit three element : phrase-sourc ( ps ) , phrase-target ( pt ) source-sent ( ss ) .
p = { ps|||pt|||ss } ( 1 ) 3 definit ident source-target phrase pair extract differ train sentenc ( sourc sentenc ) regard differ translat unit .
accord , related context consid ad- dition ( hereinaft , source-context ) featur function ( scf ) phrase input sentenc .
p = { ps|||pt|||sc f } ( 2 ) source-context featur function consist similar measur input sentenc translat sourc context compon avail translat unit illustr fig .
1 .
s1 : hotel book room t1 : el hotel reservaba mas habitacion s2 : everybodi want write book t2 : todo el mundo quier escribir un libro sobr s mismo input : read nice book book : libro book : reservar s2 s1 input fig .
1 .
illustr propos similar featur help choos trans- lation unit .
sc f includ phrase addit stan- dard featur function , i.e .
condit ( cp ) posterior ( pp ) probabl , lexic weight ( l1 , l2 ) phrase bonus ( pb ) .
therefor , extend phrase .
p = { ps|||pt|||cp , pp , l1 , l2 , pb , sc f } ( 3 ) schema similar previous work banch costa- jussa ( 2011 ) .
differ previous work , comput- ing similar input sentenc translat origin sentenc , comput cosin distanc deep represent sentenc , explain sec- tion 4 .
3.3 .
practic integr implement source-context featur function dynam de- pend input sentenc translat .
moment , featur function integr standard phrase-bas smt system describ follow procedur .
fig .
2 show procedur implement source- context featur function .
train ( ts ) valid ( vs ) ( either develop test ) sentenc , comput sim- ilar measur build similar matrix ( w ) train valid set .
, sentenc valid set ( vsn ) extract phrase list ( pn ) tm = train sentenc ( ts ) vn = n valid sentenc ( vs ) tsm 2 tm vsn 2 vn wmn = !
( tsm , vsn ) = similar ( tsm , vsn ) end end vsn 2 vn pn = phrase list 2 tm use decod p = phrase entri ||| tsm 2 pn p 2 pn p p ||| wmn end translat vsn pn end fig .
2 .
source-context featur implement algorithm .
use decod .
phrase entri ( p ) phrase list extend translat unit contain train sentenc ( tsm ) extract .
, phrase entri assign correspond source-context similar ma- trix w vsn tsm , posit wmn .
final , sentenc valid set vsn translat cor- respond extend phrase tabl ( pn ) includ source-context featur .
flow system depict fig .
3 .
4 .
deep represent sentenc repres sentenc latent space non- linear dimension reduct techniqu .
method base deep autoencod architectur allow obtain ef- fectiv low-dimension embed text data .
autoen- coder network tri learn approxim ident function output similar input .
input output dimens network ( n ) .
autoencod approxim ident function two step : ) reduct , ii ) reconstruct .
reduct step take input v 2 rn map h 2 rm < n seen function h = g ( v ) g : rn !
rm .
hand , reconstruct step take output reduct step h map v 2 rn way v  v consid v = f ( h ) function f : rm !
rn .
full autoencod seen f ( g ( v ) )  v. neural network base implement autoen- coder , visibl layer correspond input v hid- den layer correspond h. suffic small autoencod abl deriv power low-dimension repre- sentat data latent space hinton salakhutdinov ( 2006 ) .
two variant autoencod : ) sin- gle hidden layer , ii ) multipl hidden layer .
one singl hidden layer , optim solut remain pca project even ad non-linear hid- den layer bourlard kamp ( 1988 ) .
pca limit overcom stack multipl encod , constitut call deep architectur .
deep construct lead 4 sourc target autoencod parallel corpus phrase tabl lexic selector - - - .. - - - - .. - .
.
- - - .. - input sourc sentenc p = { ps|||pt|||scf } ps pt ss sourc project hs hinput scf = cosin ( hs , hinput ) one-tim index fig .
3 .
workflow system .
truli non-linear power reduc space representa- tion hinton salakhutdinov ( 2006 ) .
deep architectur constitut stack multipl restrict boltzmann machin ( rbm ) top .
let visibl unit v 2 { 0 , 1 } n binari bag-of-word repre- sentat text document hidden unit h 2 { 0 , 1 } hidden latent variabl .
energi state { v , h } follow , e ( v , h ) =  nx i=1 aivi  mx j=1 b jh j  x , j vih jwi j ( 4 ) vi , h j binari state visibl unit hidden unit j , ai , b j bias wi j weight .
, becom easi sampl data direct shown , p ( vi = 1|h ) =  ( ai + x j h jwi j ) ( 5 ) p ( h j = 1|v ) =  ( b j + x viwi j ) ( 6 )  ( x ) = 1/ ( 1 + exp ( x ) ) logist sigmoid function .
architectur autoencod shown fig .
4 .
la- tent space represent h ( 2 ) sentenc obtain shown eq .
6 .
sentenc latent space compar mean cosin similar shown : !
( s1 , s2 ) = cosin ( h ( 2 ) s1 |vs1 , h ( 2 ) s2 |vs2 ) ( 7 ) 5 .
experi section describ experiment framework use test introduct deep context featur standard phrase-bas smt system .
report detail data set use , baselin system , train deep structur similar among sentenc extract , improv techniqu term bleu score papineni et al .
( 2002 ) , final , scalabl techniqu .
5.1 .
data set baselin use english-to-spanish parallel corpus extract bibl , public avail constitut excel corpus experi test pro- pose methodolog provid rich varieti context .
corpus contain around 30,000 sentenc train around 800,000 word , 500 sentenc develop test set .
addit , larger data set , use english-to-hindi corpus avail wmt 2014 bojar et al .
( 2014 ) .
train sentenc 300,000 sentenc , 3,500,000 word , 429 sentenc develop 500 sen- tenc test .
baselin system standard state-of-the-art phrase-bas built use mose toolkit koehn et al .
( 2007 ) .
use follow option train system , includ : grow-diagonal-final-and word align symmetr , lex- ical reorder , relat frequenc ( condit pos- terior probabl ) phrase discount , lexic weight phrase bonus translat model ( phrase length 10 ) , 5-gram languag model use kneser-ney smooth word bonus model .
order com- pare techniqu built contrast system context 5 code layer output layerinput layer ( ) ( b ) v h ( 1 ) h ( 2 ) fig .
4 .
architectur autoencod .
( ) deep format stack rbms .
( b ) unrol fine-tun .
featur base lsa banch costa-jussa ( 2011 ) anoth baselin .
system comput 2intel xeon e52670 v3 2,3ghjz 12n processor server .
5.2 .
autoencod train model sentenc autoencod framework consid vocabulari remov least frequent term appear less k sentenc train partit dataset .
remov stopword appli stemmer .
bibl wmt14 dataset , consid vocabulari size ( n ) 3543 ( k=5 ) 7299 ( k=20 ) respect 1 .
autoencod first pretrain use contrast di- vergenc ( cd ) step size 1 ( hinton , 2002 ) .
minibatch size 20 100 use pretrain fine-tun re- spectiv .
architectur autoencod n-500-128- 500-n 2 shown figur 4 .
weight decay use pre- vent overfit .
addit , order encourag sparsiti hidden unit , kullback-leibl sparsiti regular use .
use gpu3 base implement autoencod train model took around 45 minut bibl dataset around 4.5 hour wmt14 dataset .
5.3 .
latent semant analysi lsa basic perform singular valu decomposit sentence-term matrix line princip compon analysi ( pca ) ( dumai et al. , 1988 ) .
lsa obtain top k prin- cipal compon consid project space sentenc compar space .
inher idea semant similar term ( dimens ) correspond similar latent compon sentenc near reduc comparison space .
method also look eigenproblem formul : cvj =  jv j , ( 8 ) 1the valu k decid consid size dataset size vocabulari 2differ architectur tri rule higher layer larger previous layer ( sparsiti data ) statist differ result observ .
also tri three layer n-500-250-128-250-500-n produc wors result , go beyond 3-layer .
3nvidia geforc gtx titan memori 6 gib 2688 cuda core ,  j jth largest eigenvalu , v j correspond eigenvector c correl matrix ( dt ) .
lsa use top k eigenvector project .
5.4 .
result tabl 1 show improv term bleu papineni et al .
( 2002 ) ad deep context baselin system english-to-spanish ( en2 ) english-to-hindi ( en2hi ) , re- spectiv develop test set .
note en2 qualiti higher en2hi former easier translat task latter higher train cor- pus .
shown tabl , propos method perform signific better baselin lsa method translat task consist .
en2 en2hi dev test dev test baselin 36.81 37.46 9.42 14.99 +lsa 37.20 37.84 9.83 15.12 +deep 37.28 38.19 10.40 15.43 tabl 1 .
bleu score en2 en2hi translat task .
  depict statist signific ( p-valu < 0.05 ) wrt baselin lsa respec- tive .
notic result en2 en2hi consist improv .
argu hindi span- ish higher vocabulari variat compar english , richer morpholog .
benefit ad source-context inform better reflect case sourc phrase various target word translat .
improv translat prove deep represent help find adequ contextu similar among train test sen- tenc .
bleu score show improv task translat direct .
analysi translat out- put present tabl 2 use asiya 4 show exampl translat improv term lexic select goal methodolog present paper .
exampl shown tabl 2 .
tabl 3 , analys method improv .
notic tabl 3 probabl sens 4http : //www.asiya.lsi.upc.edu 6 system translat sourc brake band baselin pero el rompio las tropa +deep pero el rompio las cuerda refer pero el rompio las atadura sourc soft cri depth baselin ghraiyo\ s  laym ron lgt +deep ghraiyo\ s  laym cfk refer ghraiyo\ s koml cfk tabl 2 .
manual analysi translat output .
ad deep featur allow adequ lexic select .
cp pp scf bands|||tropa 0.31 0.17 0.01 bands|||cuerda 0.06 0.07 0.23 cry|||rona 0.23 0.06 0.85 cry|||cfk 0.15 0.04 0.90 tabl 3 .
probabl valu phrase-bas system ) word band two spanish translat ; word cri nd two hindi translat .
band consid dataset tropa , liter mean  troup  .
idea propos source-context featur use contextu similar input sentenc ( ) sentenc train set addit sourc inform use decod .
therefor , given entir input sentenc : kept bound chain fet- ter ; brake band , method abl infer correct sens word band ( i.e .
case cuer- das , liter mean  rope  , synonym refer atadura , liter mean  tie rope  ) consid- ere similar train sentenc : ( s1 ) lord sent band chalde , band syr- ian ( s2 ) shall put band upon thee , shall bind thee .
case , !
( s2 , ) > !
( s1 , ) seen ta- ble 3 .
similar , hindi exampl , frequent sens word cri rona , liter mean  cri  exampl tabl 2 refer sens cri cfk , mean scream .
method could identifi context henc sc f ( cry|||cfk ) > sc f ( cry|||rona ) .
source-context featur capabl choos better translat unit given context correct trans- lation seen train data .
5.5 .
scalabl two compon method : ) incorpor source-context featur tune phase mt project train sentenc latent space ; ii ) sim- ilar estim input sentenc train sen- tenc latent space .
former step comput expens one-tim offlin , big con- cern .
similar estim onlin , effici comput use multi-cor cpu gpu es- sential matrix multipl .
howev , plan integr similar estim translat decod .
6 .
conclus work shown novel methodolog exploit deep represent techniqu effect includ deep learn- ing base contextu similar estim method han- dles sourc context incorpor end-to-end smt system .
propos method show statist signific improve- ment compar strong baselin system english-to- spanish english-to-hindi translat task .
manual analysi clear illustr advantag choos- ing appropri translat unit take account in- format input sentenc context deep relat train sentenc .
present method also scale run-tim .
interest work would includ shorter context , experi deeper auto-encod better integr dynam featur translat decod .
also , speed-up search could divid featur space chunk search hierarch , perform cluster use kd-tree .
acknowledg work support part wiq-ei ( irs grant n. 269180 ) diana-appl ( tin2012- 38603-c02-01 ) project vlc/campus microclust multimod interact inintellig system .
work second author support spanish ministerio de economa competitividad , contract tec2012-38939-c03- 02 well european region develop fund ( erdf/fed ) seventh framework program european commiss intern outgo fel- lowship mari curi action ( imtrap-2011-29951 ) .
refer banch , r.e. , costa-jussa , m.r. , 2011 .
semant featur statist ma- chine translat , : proceed fifth workshop syntax , seman- tic structur statist translat , pp .
126134 .
bengio , y. , 2012 .
deep learn represent unsupervis trans- fer learn , : icml unsupervis transfer learn , pp .
1736 .
bojar , o. , diatka , v. , rychl , p. , stranak , p. , suchomel , v. , tamchyna , a. , ze- man , d. , 2014 .
hindencorp - hindi-english hindi-on corpus ma- chine translat , : proceed ninth intern confer languag resourc evalu ( lrec  14 ) , reykjavik , iceland .
espana bonet , c. , gimenez , j. , marquez , l. , 2009 .
discrimin phrase- base model arab machin translat .
transact asian lan- guag inform process 8 , 15:115:20 .
url : http : //doi .
acm.org/10.1145/1644879.1644882 , doi:10.1145/1644879 .
1644882 .
bourlard , h. , kamp , y. , 1988 .
auto-associ multilay percep- tron singular valu decomposit .
biolog cybernet 59 , 291 294 .
url : http : //dx.doi.org/10.1007/bf00332918 , doi:10 .
1007/bf00332918 .
carpuat , m. , wu , d. , 2005 .
word sens disambigu vs. statist machin translat , : proceed 43rd annual meet associ comput linguist , associ comput linguist , stroudsburg , pa , usa .
pp .
387394 .
url : http : //dx.doi.org/10 .
3115/1219840.1219888 , doi:10.3115/1219840.1219888 .
chen , b. , kuhn , r. , foster , g. , 2013 .
vector space model adapt statist machin translat , : proceed 51st annual meet associ comput linguist ( volum 1 : long paper ) , associ comput linguist , sofia , bulgaria .
pp .
12851293 .
url : http : //www.aclweb.org/anthology/p13-1126 .
7 cho , k. , van merrienbo , b. , gulcehr , c. , bahdanau , d. , bougar , f. , schwenk , h. , bengio , y. , 2014 .
learn phrase represent use rnn encoder-decod statist machin translat , : proceed 2014 confer empir method natur languag process , emnlp 2014 , octob 25-29 , 2014 , doha , qatar , meet sigdat , special interest group acl , pp .
17241734 .
costa-jussa , m.r. , gupta , p. , rosso , p. , banch , r.e. , 2014 .
english-to-hindi system descript wmt 2014 : deep source-context featur mose , : proceed ninth workshop statist machin translat , associ comput linguist , baltimor , maryland , usa .
pp .
7983 .
url : http : //www.aclweb.org/anthology/w14-3306 .
dumai , s.t. , furna , g.w. , landauer , t.k. , deerwest , s. , harshman , r. , 1988 .
use latent semant analysi improv access textual informa- tion , : proceed sigchi confer human factor com- pute system , acm , new york , ny , usa .
pp .
281285 .
url : http : //doi.acm.org/10.1145/57167.57214 , doi:10.1145/57167 .
57214 .
erhan , d. , bengio , y. , courvill , a.c. , manzagol , p.a. , vincent , p. , bengio , s. , 2010 .
unsupervis pre-train help deep learn ?
journal machin learn research 11 , 625660 .
glorot , x. , bord , a. , bengio , y. , 2011 .
domain adapt large-scal sentiment classif : deep learn approach , : icml , pp .
513520 .
gupta , p. , bali , k. , banch , r.e. , choudhuri , m. , rosso , p. , 2014 .
queri ex- pansion mixed-script inform retriev , : 37th intern acm sigir confer research develop inform re- trieval , sigir  14 , gold coast , qld , australia - juli 06 - 11 , 2014 , pp .
677686 .
haqu , r. , 2010 .
integr source-languag context log-linear mod- el statist machin translat .
ph.d. thesi .
dublin citi univers .
hasler , e. , blunsom , p. , koehn , p. , haddaw , b. , 2014 .
dynam topic adap- tation phrase-bas mt , : proceed european as- sociat comput linguist , associ comput linguist , gothenburg , sweden .
url : http : //www.aclweb.org/ anthology/p13-1126 .
hinton , g. , salakhutdinov , r. , 2006 .
reduc dimension data neural network .
scienc 313 , 504  507 .
hinton , g.e. , 2002 .
train product expert minim contrast diverg .
neural comput 14 , 17711800 .
kalchbrenn , n. , blunsom , p. , 2013 .
recurr continu translat model , : proceed 2013 confer empir method natur languag process , associ comput linguist , seattl , washington , usa .
pp .
17001709 .
url : http : //www.aclweb.org/ anthology/d13-1176 .
koehn , p. , hoang , h. , birch , a. , callison-burch , c. , federico , m. , bertoldi , n. , cowan , b. , shen , w. , moran , c. , zen , r. , dyer , c. , bojar , o. , constantin , a. , herbst , e. , 2007 .
mose : open sourc toolkit statist machin translat , : proceed 45th annual meet acl in- teract poster demonstr session , pp .
177180 .
li , p. , liu , y. , sun , m. , 2013 .
recurs autoencod itg-bas trans- lation , : proceed 2013 confer empir method natur languag process , associ comput linguist , seattl , washington , usa .
pp .
567577 .
url : http : //www.aclweb .
org/anthology/d13-1054 .
liu , l. , watanab , t. , sumita , e. , zhao , t. , 2013 .
addit neural network statist machin translat , : proceed 51st annual meet associ comput linguist ( volum 1 : long paper ) , associ comput linguist , sofia , bulgaria .
pp .
791801 .
url : http : //www.aclweb.org/anthology/p13-1078 .
lu , s. , chen , z. , xu , b. , 2014 .
learn new semi-supervis deep auto- encod featur statist machin translat , : proceed 52nd annual meet associ comput linguist ( volum 1 : long paper ) , associ comput linguist , bal- timor , maryland .
pp .
122132 .
url : http : //www.aclweb.org/ anthology/p14-1012 .
papineni , k. , rouko , s. , ward , t. , zhu , w.j. , 2002 .
bleu : method automat evalu machin translat , : proceed 40th annual meet associ comput linguist , pp .
311 318 .
platt , j.c. , toutanova , k. , tau yih , w. , 2010 .
translingu document represen- tation discrimin project , : emnlp , pp .
251261 .
salakhutdinov , r. , hinton , g.e. , 2009 .
replic softmax : undirect topic model , : nip , pp .
16071614 .
schwenk , h. , costa-jussa , m.r. , fonollosa , j.a.r. , 2006 .
continu space languag model iwslt 2006 task , : 2006 intern workshop spoken languag translat , iwslt 2006 , keihanna scienc citi , ky- oto , japan , novemb 27-28 , 2006 , pp .
166173 .
schwenk , h. , r. costa-jussa , m. , r. fonollosa , j.a. , 2007 .
smooth bilin- gual n-gram translat , : proceed 2007 joint confer empir method natur languag process comput natur languag learn ( emnlp-conl ) , associ computa- tional linguist , pragu , czech republ .
pp .
430438 .
url : http : //www.aclweb.org/anthology/d/d07/d07-1045 .
son , l.h. , allauzen , a. , yvon , f. , 2012 .
continu space translat mod- el neural network , : proceed 2012 confer north american chapter associ comput linguist : human languag technolog , associ comput linguis- tic , stroudsburg , pa , usa .
pp .
3948 .
url : http : //dl.acm.org/ citation.cfm ? id=2382029.2382036 .
srivastava , n. , salakhutdinov , r. , hinton , g.e. , 2013 .
model document deep boltzmann machin , : uai .
sundermey , m. , alkhouli , t. , wuebker , j. , ney , h. , 2014 .
translat mod- ele bidirect recurr neural network , : proceed 2014 confer empir method natur languag process ( emnlp ) , associ comput linguist , doha , qatar .
pp .
14 25 .
url : http : //www.aclweb.org/anthology/d14-1003 .
vaswani , a. , zhao , y. , fossum , v. , chiang , d. , 2013 .
decod large- scale neural languag model improv translat , : proceed 2013 confer empir method natur languag process , emnlp 2013 , 18-21 octob 2013 , grand hyatt seattl , seattl , washing- ton , usa , meet sigdat , special interest group acl , pp .
13871392 .
yih , w. , toutanova , k. , platt , j.c. , meek , c. , 2011 .
learn discrimin project text similar measur , : proceed fifteenth confer comput natur languag learn , conll 2011 , portland , oregon , usa , june 23-24 , 2011 , pp .
247256 .
zamora-martnez , f. , bleda , m.j.c. , schwenk , h. , 2010 .
n-gram-bas ma- chine translat enhanc neural network french-english btec-iwslt  10 task , : 2010 intern workshop spoken lan- guag translat , iwslt 2010 , pari , franc , decemb 2-3 , 2010 , pp .
4552 .
url : http : //www.isca-speech.org/archive/iwslt_ 10/slta_045.html .
