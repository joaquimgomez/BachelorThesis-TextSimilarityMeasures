1 neighborhood-bas stop criterion contrast diverg enriqu romero merino ferran mazzanti castrillejo jordi delgado pin abstractrestrict boltzmann machin ( rbms ) general unsupervis learn devic ascertain generat model data distribut .
rbms often train use contrast diverg learn algorithm ( cd ) , approxim gradient data log-likelihood .
simpl reconstruct error often use stop criterion cd , although sever author rais doubt concern feasibl procedur .
mani case evolut curv recon- struction error monoton log-likelihood , thus indic former good estim optim stop point learn .
howev , mani altern reconstruct error discuss literatur .
estim log-likelihood train data base anneal import sampl feasibl comput expens .
manuscript present simpl cheap altern , base inclus inform contain neighbor state train set , stop criterion cd learn .
.
introduct learn algorithm deep multilay neural network known long time [ 1 ] , though usual could outperform simpler , shallow network .
way , deep multilay network wide use solv larg scale real-world problem last decad [ 2 ] , [ 3 ] .
2006 , deep belief network ( dbns ) [ 4 ] came real breakthrough field , sinc learn algorithm propos end feasibl practic method train deep network , interest result [ 5 ]  [ 9 ] .
dbns restrict boltzmann machin ( rbms ) [ 10 ] build block .
rbms topolog constrain boltzmann machin ( bms ) two layer , one hidden anoth visibl neuron , intralay connect .
properti make work rbms simpler regular bms , particular stochast comput log-likelihood gradient may perform effici mean gibb sampl [ 2 ] , [ 11 ] .
2002 , contrast diverg ( cd ) learn algo- rithm propos effici train method product- of-expert model , rbms special case [ 12 ] .
observ use cd train rbms work quit well practic .
fact import deep learn sinc enriqu romero merino departa de cienci de la computacio , universitat politecnica de catalunya , spain ( e-mail : eromero @ cs.upc.edu ) .
ferran mazzanti castrillejo departa de fsica en- ginyeria nuclear , universitat politecnica de catalunya , spain ( e-mail : fer- ran.mazzanti @ upc.edu ) .
jordi delgado pin departa cienci de la computacio , universitat politecnica de catalunya , spain ( e-mail : jdelgado @ cs.upc.edu ) .
author suggest multilay deep neural network better train layer pre-train separ singl rbm [ 5 ] , [ 6 ] , [ 13 ] .
thus , train rbms cd stack seem good way go design deep learn architectur .
howev , pictur nice look , sinc cd flawless train algorithm .
despit cd approxim true log-likelihood gradient [ 14 ] , bias may converg case [ 15 ]  [ 17 ] .
moreov , observ cd , variant persist cd [ 18 ] fast persist cd [ 19 ] lead steadi decreas log-likelihood learn [ 20 ]  [ 22 ] .
therefor , risk learn diverg impos requir stop criterion .
two main method use decid stop learn process .
one base monitor reconstruct error [ 23 ] .
base estim log-likelihood anneal import sampl ( ai ) [ 24 ] , [ 25 ] .
reconstruct error easi comput often use practic , though adequaci remain unclear monoton [ 21 ] .
ai seem work better reconstruct error case , though consider expens comput , may also fail [ 20 ] .
work approach problem differ perspect .
general cdn tend concentr proba- biliti small subset train data , leav littl probabl rest state .
undesir featur prevent build good model .
work propos stop criterion tri detect likelihood start degener .
sinc boltzmann distribut probabl given state proport exponenti energi , state similar energi also similar probabl .
base fact energi continu smooth function variabl , close neighborhood high-prob state expect acquir also signific amount probabl .
sens , argu inform contain neighborhood train data valuabl , incorpor learn process rbms .
use ham distanc measur close differ state .
propos stop criterion depend inform contain train set neighbor use detect chang curvatur log-likelihood .
sens criterion local need explor whole space state .
furthermor , order make comput tractabl , build stop criterion way becom independ partit function model , comput intract  2017 ieee .
person use materi permit .
permiss ieee must obtain use , current futur media , includ reprinting/republish materi advertis promot purpos , creat new collect work , resal redistribut server list , reus copyright compon work work .
doi : 10.1109/tnnls.2017.2697455 2 real-world larg space .
moreov , propos quantiti monitor learn much cheaper evalu estim log-likelihood use ai .
next section defin neighborhood-bas stop criterion cdn show perform sever data set .
ii .
learn restrict boltzmann machin a. energy-bas probabilist model energy-bas probabilist model defin probabl dis- tribut energi function , follow : p ( x , h ) = eenergi ( x , h ) z , ( 1 ) x h stand ( typic binari ) visibl hidden variabl , respect .
normal factor z call partit function read z =  x , h eenergi ( x , h ) .
( 2 ) sinc x observ , one interest margin distribut p ( x ) =  h e energi ( x , h ) z , ( 3 ) evalu partit function z computation- alli prohibit sinc involv exponenti larg number term .
way , one measur direct p ( x ) .
energi function depend sever paramet  , adjust learn stage .
done maximiz- ing likelihood data .
energy-bas model , deriv log-likelihood express   logp ( x ;  )  = e p ( h|x ) [ energi ( x , h )  ]  e p (  x ) [ e p ( h|  x ) [ energi (  x , h )  ] ] , ( 4 ) first term call posit phase second term negat phase .
express , e p (  x ) stand expect valu probabl visibl state , involv evalu partit function accord definit e p (  x ) [ f (  x ) ] =   x p (  x ) f (  x ) p (  x ) defin eq .
( 3 ) .
seen , exact comput deriv log-likelihood usual unfeas negat phase ( 4 ) , come deriv partit function .
b .
restrict boltzmann machin restrict boltzmann machin energy-bas proba- bilist model whose energi function : energi ( x , h ) = btx cth htwx , ( 5 ) w two-bodi weight connect pair hidden visibl unit , b c correspond bias term .
rbms core dbns [ 4 ] deep architectur use rbms unsupervis pre-train previous supervis step [ 5 ] , [ 6 ] , [ 13 ] .
consequ particular form energi function rbms p ( h|x ) p ( x|h ) factor .
way possibl comput p ( h|x ) p ( x|h ) one step , make possibl perform gibb sampl effici , contrast general model like boltzmann machin [ 26 ] .
c. contrast diverg common learn algorithm rbms use algorithm estim deriv log-likelihood product expert model .
algorithm call contrast diverg [ 12 ] .
contrast diverg cdn estim deriv log-likelihood given point x   logp ( x ;  )  ' e p ( h|x ) [ energi ( x , h )  ]  e p ( h|xn ) [ energi ( xn , h )  ] .
( 6 ) xn last sampl gibb chain start x obtain n step : h1  p ( h|x ) x1  p ( x|h1 ) ... hn  p ( h|xn1 ) xn  p ( x|hn ) .
usual , e p ( h|x ) [ energi ( x , h )  ] easili comput .
sever altern cdn persist cd [ 18 ] , fast per- sistent cd [ 19 ] , parallel temper [ 22 ] , dissimilar cd [ 27 ] , averag cd [ 28 ] beyond mean field correct [ 29 ] .
d. monitor learn process rbms learn rbms delic procedur involv lot data process one seek perform reason speed order abl handl larg space huge amount state .
, drastic approxim understood statist averag sens perform [ 30 ] .
one relev point consid learn stage find good way determin whether good solut found , decid learn process stop .
one wide use criteria stop base monitor reconstruct error , measur capabl network produc output consist data input .
sinc rbms probabilist model , reconstruct error data point x ( ) comput probabl x ( ) given expect valu h x ( ) : r ( x ( ) ) =  logp ( x ( ) |e p ( h|x ( ) ) [ h ] ) , ( 7 ) probabilist extens sum-of-squar re- construct error determinist network  ( x ( ) ) = ||x ( )  x ( ) n || 2 .
( 8 ) express x ( ) n stand n-th reconstruct , gibb chain mention , i-th member train set .
practic , eq .
( 7 ) comput analyt .
one 3 first evalu expect valu hidden unit given input , condit probabl visibl unit given .
author shown , case , learn induc undesir decreas likelihood goe un- detect reconstruct error [ 20 ] , [ 21 ] ( r ( x ( ) )  ( x ( ) ) usual decreas monoton ) .
sinc increas reconstruct error take place train appar way detect chang behavior log- likelihood cdn .
altern , one could evalu estim likeli- hood train data mean ai algorithm .
theoret possibl , expens comput point view system size larg , case even clear well perform [ 20 ] .
iii .
propos stop criterion propos stop criterion base monitoriza- tion ratio two quantiti : geometr averag probabl train set , sum probabl point given neighborhood train set .
formal , monitor d = [ n i=1 p ( x ( ) ) ] 1/n  yd p ( ) , ( 9 ) subset point ham distanc less equal train set .
usual , distanc given point data set taken minimum distanc given point element set .
notic use point train set improv learn also present work , [ 27 ] .
idea behind definit evolut d learn stage expect captur main trend log-likelihood certain valu d. notic two interest limit case .
one hand , span whole space ( thus equal maxim possibl ham distanc ) , d exact likelihood data sinc denomin eq .
( 9 ) add 1 .
hand , data train set involv calcul = 0 .
choic problem- depend , case one make sensibl choic , take small enough local estim , reason size order make d comput feasibl .
work , propos stop cdn learn maximum d , expect close one shown log-likelihood data .
hold suitabl valu , shown experi next section .
motiv analyt form d eq .
( 9 ) twofold .
one hand numer denomin monitor differ thing .
numer , essenti likelihood data , sensit accumul probabl mass reduc subset train data , typic featur cdn .
continu reason , denomin strong correl sum probabl train data .
problem learnt , probabl close neighborhood train set high .
valu d result delic fig .
1 .
averag logarithm probabl ( left panel ) fraction sign chang small weight chang ( right panel ) probabl state function ham distanc one thousand run rbm 12 visibl unit .
equilibrium two quantiti ( see section iv ) .
hand , notic partit function expens quantiti evalu , explicit build d z- independ quantiti .
necessari condit impos design quantiti monitor .
way , due structur d , partit function z involv numer denomin cancel .
word , comput d equival defin d = [ n i=1  h e energi ( x ( ) , h ) ] 1/n  yd  h e energi ( , h ) .
( 10 ) particular topolog rbms allow comput h e energi ( z , h ) effici [ 2 ] .
fact dramat de- creas comput cost involv calcul , would otherwis becom unfeas real-world problem rbms could success appli .
defin probabilist neighborhood train sampl general problemat clear depend valu weight bias network , comput expens .
choic ham distanc measur probabl proxim justifi statist sens : sinc energi function sum mani term involv singl bit visibl unit , one expect chang total energi smaller fewer bit chang , least small rang ham distanc .
moreov , one also expect chang probabl nearbi state follow direct .
order illustr point conduct seri synthet experi random chosen gaussian weight , small fraction whole space acquir signific amount probabl mass .
way goal reproduc usual found learn problem , train set small compar whole space .
figur 1 show result averag one thousand run rbm 12 18 visibl hidden unit , respect .
paramet adjust approxim 5 % total number state exhaust approxim 0.8 total probabl .
left panel show averag probabl neighbor state probabl one , function ham distanc .
seen plot , averag probabl smooth function ham distanc 4 show monoton behavior certain point .
right panel show fraction sign chang averag probabl small variat less 1 % weight perform ( would account small updat learn epoch ) .
seen , neighbor state follow sign chang origin state , thus reinforc idea continu probabl space .
way , idea use ham distanc measur probabilist similar support statist sens .
furthermor , one simplest cheapest metric evalu .
clear ham distanc may fail case , criterion base hypothesi domin case .
way , non-trivi metric one propos [ 31 ] , [ 32 ] could use .
numer d direct evalu data train set , problem find suitabl valu  still remain .
inde , set point given ham distanc train set independ weight bias network .
way , built begin process use requir learn .
therefor , two issu sort criterion appli .
first one decid suitabl valu d. experi differ problem show inde problem depend , illustr experiment section .
second one choic subset , strong depend size space explor .
small space one safe use complet set point distanc less equal , forbid larg real world problem .
reason explor two possibl : one includ point anoth includ random subset size train set , expens deal train set .
arbitrari decis chang , keep mind one alway need larg enough set point howev increas comput cost signific .
iv .
experi perform sever experi explor afore- mention criterion defin section iii studi be- havior d comparison log-likelihood reconstruct error data sever problem .
exact analysi , explor problem size log-likelihood exact evalu compar propos d paramet .
moreov , also includ result larg benchmark dataset , calcul exact log-likelihood unfeas approxim ai algorithm [ 25 ] .
.
small problem first small problem , denot bar stripe ( bs ) , tri identifi vertic horizont line 44 pixel imag .
train set consist whole set imag contain possibl horizont vertic line ( ) , rang line ( blank imag ) complet fill imag ( black imag ) , thus produc 2  24  2 = 30 differ imag ( avoid repetit fulli back fulli white imag ) space 216 possibl imag black white pixel .
second small problem , name label shifter ensembl ( lse ) , consist learn 19-bit state form follow : given initi 8-bit pattern , generat three new state concaten bit sequenc 001 , 010 100 .
final 8-bit pattern state origin one shift one bit left intermedi code 001 , copi unchang code 010 , shift one bit right code 100 .
one thus generat train set use possibl 28  3 = 768 state creat form , system space consist possibl 219 differ state one build 19 bit .
two problem alreadi explor [ 21 ] adequ current context sinc , still larg , dimension space allow direct monitor partit function log- likelihood learn .
sake complet , also test propos criterion random generat problem differ space dimens , number state learnt signific smaller size space .
particular , generat four differ data set ( ran10 , ran12 , ran14 ran16 ) consist nv = 10 , 12 , 14 , 16 binari input unit 2nv/2 exampl learnt , suggest [ 33 ] .
follow discuss learn process problem binari rbms , employ contrast diverg algorithm cdn n = 1 n = 10 describ section ii-c .
bs case rbm 16 visibl 8 hidden unit , lse problem number 19 10 , respect .
random data set use 10 hidden unit case .
everi simul carri total 50000 epoch , measur taken everi 50 epoch .
moreov , everi point subsequ plot averag ten differ simul start differ random valu weight bias .
weight decay use , momentum set 0.8 .
learn rate chosen order make sure log-likelihood degener , way present clear maximum detect d .
next subsect present result two seri experi .
first one ( section iv-a1 ) analyz case state includ given d. second one ( section iv-a2 ) relax comput cost evalu d select small subset state d. 1 ) complet neighborhood : present result problem hand , show analyz instanc differ plot correspond actual log-likelihood problem d differ valu , among thing .
order identifi contribut d differ neighborhood train set , defin two differ set : da contain state distanc less equal , ds account state distanc exact equal d. comput d = da = ds experi comment follow .
figur 2 show result ran10 data set .
upper left panel show log-likelihood data train .
seen , clear maximum 5 0 200 400 600 800 1000 -230 -220 -210 -200 -190 -180 0 200 400 600 800 1000 0,005 0,01 0,015 0,02 0 200 400 600 800 1000 0.003 0.0035 0.004 0.0045 0.005 0.0055 0.006 0 200 400 600 800 1000 0.001 0.0015 0.002 0.0025 0.003 0.0035 0.004 0 200 400 600 800 1000 0.001 0.0015 0.002 0.0025 0.003 0.0035 0.004 0 200 400 600 800 1000 -1 0 1 2 3 4 5 6 7 0 200 400 600 800 1000 0 0,1 0,2 0,3 0,4 0 200 400 600 800 1000 0,3 0,4 0,5 0,6 0,7 0,8 0 200 400 600 800 1000 0,86 0,88 0,9 0,92 0,94 0,96 0,98 1 0 200 400 600 800 1000 0 0,2 0,4 0,6 0,8 1 1,2 0 200 400 600 800 1000 0 0.05 0.1 0.15 0.2 0.25 0 200 400 600 800 1000 0,005 0,01 0,015 0,02 0 200 400 600 800 1000 0,003 0,004 0,005 0,006 0,007 0,008 0 200 400 600 800 1000 0 0,002 0,004 0,006 0,008 0,01 0,012 0 200 400 600 800 1000 0 0,1 0,2 0,3 0,4 fig .
2 .
result ran10 problem .
first column show log-likelihood ( top ) reconstruct error ( 7 ) ( 8 ) ( center bottom ) .
column first , second third row depict d = da ( black curv ) , sum probabl denomin d = da ( brown curv ) , d = ds ( green curv ) = 0 , 1 , 2 , 3 , respect .
x-axi number epoch along simul divid 50 plot .
data y-axi arbitrari unit .
0 200 400 600 800 1000 -1250 -1225 -1200 0 200 400 600 800 1000 0,0025 0,003 0,0035 0 200 400 600 800 1000 0,00025 0,0003 0,00035 0,0004 0 200 400 600 800 1000 0,00008 0,00009 0,00010 0,00011 0 200 400 600 800 1000 6e-05 7e-05 8e-05 0 200 400 600 800 1000 2 4 6 8 10 0 200 400 600 800 1000 0,0027 0,003 0,0033 0,0036 0 200 400 600 800 1000 0,0003 0,00035 0,0004 0,00045 0 200 400 600 800 1000 0,00012 0,00014 0,00016 0 200 400 600 800 1000 0,00015 0,0002 0,00025 0,0003 0,00035 fig .
3 .
result ran14 problem .
first column show log-likelihood reconstruct error ( 7 ) ( top bottom panel ) .
column upper lower row show d = da d = ds = 0 , 1 , 2 , 3 , respect .
identifi stop point .
panel show reconstruct error ( 7 ) ( 8 ) clear fail identifi desir extremum .
rest column show result distanc = 0 , 1 , 2 3 .
first row depict d da , state requir distanc taken account .
seen , start = 1 criterion robust consist detect maximum log- likelihood right place , thus reinforc idea neighborhood data contain valuabl inform .
second row show denomin d correspond first row , , sum probabl state includ case .
notic = 3 sum equal one d exact equal likelihood data .
interest , even sum still far away one , happen = 1 , d consist find desir point .
behavior also observ rest data set analyz .
final third row show d ds , thus show behavior criterion appli differ shell .
= 1 2 criterion detect reason well maximum log-likelihood use identifi desir stop point .
notic , though , data alon , entir contain = 0 , capabl reproduc behavior .
moreov , larger 2 criterion also fail , expect start certain distanc inform regard model lost .
pleas notic initi transitori behavior plot meaningless omit cut .
equival result ran14 case shown figur 3 .
6 0 200 400 600 800 1000 -350 -300 -250 -200 0 200 400 600 800 1000 0 0,01 0,02 0,03 0 200 400 600 800 1000 0 0,0005 0,001 0,0015 0,002 0,0025 0 200 400 600 800 1000 0 0,00025 0,0005 0,00075 0,001 0,00125 0 200 400 600 800 1000 0 0,0002 0,0004 0,0006 0,0008 0,001 0 200 400 600 800 1000 0 2,5 5 7,5 10 12,5 0 200 400 600 800 1000 0 0,005 0,01 0,015 0,02 0,025 0,03 0,035 0 200 400 600 800 1000 0,001 0,0015 0,002 0,0025 0,003 0,0035 0 200 400 600 800 1000 0 0,001 0,002 0,003 0,004 0,005 0,006 0,007 0 200 400 600 800 1000 0 0,005 0,01 0,015 fig .
4 .
figur 3 bs data set .
0 200 400 600 800 1000 -10250 -10000 -9750 -9500 -9250 -9000 -8750 0 200 400 600 800 1000 0 0,0001 0,0002 0,0003 0,0004 0,0005 0,0006 0 200 400 600 800 1000 2,5e-05 5e-05 7,5e-05 0 200 400 600 800 1000 1e-05 1,5e-05 2e-05 2,5e-05 0 200 400 600 800 1000 5e-06 1e-05 1,5e-05 0 200 400 600 800 1000 5 7,5 10 12,5 0 200 400 600 800 1000 0 0,0001 0,0002 0,0003 0,0004 0,0005 0 200 400 600 800 1000 2,5e-05 5e-05 7,5e-05 0,0001 0 200 400 600 800 1000 1e-05 2e-05 3e-05 4e-05 0 200 400 600 800 1000 1e-05 1,5e-05 2e-05 2,5e-05 3e-05 3,5e-05 4e-05 fig .
5 .
figur 3 lse data set .
log-likelihood probabilist reconstruct error ( 7 ) depict upper lower panel first column , respect .
panel show d da ds , = 0 , 1 , 2 , 3 ( top bottom row , second fifth column ) .
previous case , reconstruct error fail detect maximum likelihood , thus use present context .
contrari , stop point obtain d select near-optim model .
notic criterion robust along distanc explor , desir .
similar result found ran12 ran16 case .
plot bs lse problem found figur 4 5 .
, reconstruct error decreas monoton therefor useless present context .
lse problem , d larger 1 success task = da = ds .
howev , bs case work = da = ds > 1 .
infer result , optim valu fix beforehand problem-depend .
2 ) incomplet neighborhood : despit success criterion built = da , clear larg space unpract number state neighborhood train set larg .
reason , test criterion random select subset da  da size train set , alway comput tractabl .
sens , denot d evalu d da .
figur 6 show d compar d previous figur bs ( first row ) lse ( second row ) problem .
precis , first column show log- likelihood data along train process , rest column plot d d = 0 , 1 , 2 3 .
notic absolut scale d d may vari main due valu sum probabl denomin .
howev , sinc precis valu quantiti irrelev , decid scale proper sake comparison .
although d built much smaller set d , case captur signific featur d therefor use instead .
sens , d provid good stop criterion cd1 , although robust d due strong reduct state contribut d compar enter d .
reduct illustr tabl , show number neighbor state data set differ distanc bs lse problem .
increas number state includ d , converg d expect expens increas comput cost .
howev , present result indic , least problem hand , number exampl similar train set evalu 7 data set ham distanc 1 2 3 4 5 6 7 8 9 10 bar stripe 480 3216 11360 20744 19296 8688 1632 90 - - label shifter ensembl 8434 41160 110326 165088 132976 54160 10368 966 40 2 tabl number neighbor differ ham distanc bs lse data set .
0 200 400 600 800 1000 -350 -300 -250 -200 0 200 400 600 800 1000 0 0,01 0,02 0,03 0,04 ~ 0 200 400 600 800 1000 0 0,01 0,02 0,03 0,04 0,05 ( x 15 ) ~ 0 200 400 600 800 1000 0 0,1 0,2 0,3 0,4 0,5 ( x 400 ) ~ 0 200 400 600 800 1000 0 5 10 15 20 25 30 ( x 30000 ) ~ 0 200 400 600 800 1000 -10000 -9500 -9000 -8500 0 200 400 600 800 1000 0 0,0001 0,0002 0,0003 0,0004 0,0005 ~ 0 200 400 600 800 1000 0 0,0002 0,0004 0,0006 0,0008 0,001 ( x 10 ) ~ 0 200 400 600 800 1000 0 0,0005 0,001 0,0015 0,002 ( x 50 ) ~ 0 200 400 600 800 1000 0,001 0,002 0,003 ( x 175 ) ~ fig .
6 .
comparison d ( black curv ) d ( red curv ) bs lse data set ( upper lower row ) .
notic sinc magnitud paramet irrelev , curv scale sake clariti .
first column plot log-likelihood data along simul .
0 200 400 600 800 1000 -9500 -9000 -8500 -8000 0 200 400 600 800 1000 0 0,0002 0,0004 0,0006 ~ 0 200 400 600 800 1000 0,0005 0,001 0,0015 0,002 ( x 15 ) ~ 0 200 400 600 800 1000 0,001 0,002 0,003 0,004 da ( x 75 ) ~ 0 200 400 600 800 1000 0,002 0,004 0,006 0,008 0,01 ( x 250 ) ~ fig .
7 .
figur 6 lse problem cd10 .
d enough detect maximum log-likelihood data .
result present point show good propos stop criterion learn cd1 .
how- ever , under idea appli differ learn algorithm tri maxim log-likelihood data .
way repeat previous experi cd10 similar result one .
exampl , figur 7 show log-likelihood , d d = 0 , 1 , 2 , 3 cd10 lse data set .
clear seen , qualiti result similar cd1 case , thus stress robust criterion .
final remark , note bs problem train rbm stop use propos criterion abl qualit generat sampl similar train set .
show figur 8 complet train set ( two upper row ) number generat sampl ( two lower row ) obtain rbm train cd1 stop 5000 epoch , around maximum shown d=1 , approxim coincid optim valu log-likelihood .
import realiz , ultim , qualiti model direct measur qualiti cd1 learn , model use generat plot one largest d , quit close one largest likelihood .
b .
persist cd persist cd ( pcd ) well known cheap altern plain cd help improv learn [ 18 ] , [ 19 ] .
test stop criteria set previous section use pcd , lead similar result .
justifi fact known certain condit pcd also degener [ 20 ] , [ 21 ] much cd , due probabl concentr hand state .
therefor , measur qualit captur log- likelihood behavior cd expect work also pcd .
illustr figur 9 , d d shown lse problem learnt pcd .
previous case , evolut propos estim along simul qualit resembl ground truth , thus stop criteria detect reason good stop point .
c. mnist data set mnist data set well known benchmark problem correspond 28  28 binar imag hand-written digit huge space 2764 possibl state 1 .
1http : //yann.lecun.com/exdb/mnist 8 fig .
8 .
train data ( two upper row ) generat sampl ( two lower row ) bs problem weight bias obtain stop point detect d = 1 .
0 200 400 600 800 1000 -10000 -9500 -9000 -8500 0 200 400 600 800 1000 0 0,00025 0,0005 ~ 0 200 400 600 800 1000 0 0,0005 0,001 ( x 10 ) ~ 0 200 400 600 800 1000 0 0,0005 0,001 0,0015 0,002 ( x 50 ) ~ 0 200 400 600 800 1000 0 0,001 0,002 0,003 0,004 ( x 175 ) ~ fig .
9 .
figur 6 lse problem pcd .
0 200 400 600 800 1000 -200 -175 -150 -125 0 200 400 600 800 1000 -100 -90 -80 ~ ~ 0 200 400 600 800 1000 -95 -90 -85 -80 -75 -70 -65 ~ ~ 0 200 400 600 800 1000 -100 -90 -80 -70 -60 -50 -40 ~ ~ 0 200 400 600 800 1000 -80 -60 -40 -20 0 ~ ~ fig .
10 .
comparison d = da ( red curv ) = ds ( blue curv ) mnist problem .
first column show ais-estim log-likelihood data , rest column show d = 0 , 5 , 10 20 , respect .
case rbm 764 visibl 500 hidden unit employ .
calcul refer log-likelihood train set approxim ai techniqu , total 100 run chain 1000 k [ 25 ] .
valu chosen effici reason check provid reason estim likelihood compar result obtain larger valu .
rbm run total 1000 epoch , learn rate momentum chosen follow figur 0.0001 0.8 , respect .
weight decay use , though explor non-zero valu show littl influenc final result .
left panel figur 10 show ais-estim log- likelihood train set .
plot depict d = 0 , 5 , 10 , 20 correspond = da = ds .
notic incomplet neighborhood estim evalu total amount neighbor train set given distanc exceed larg practic use .
remark , measur work equal well case , thus show propos estim principl abl captur lead featur likelihood even larg problem .
notic case alreadi = 0 provid good estim stop point .
one could think ai estim likelihood would provid equal good stop point .
true , worth notic , standard paramet use real calcul base ai , comput cost would increas order magnitud .
exampl , paramet [ 34 ] total 5000 run chain 105 k  , comput cost would approxim 104 time larger .
addit , order compar exact result [ 25 ] , test stop criterion mnist problem 25 hidden unit .
notic case exact partit function evalu , estim use ai approxim .
best result achiev learn rate  = 103 , stop point accord exact likelihood locat epoch  100 .
contrast , criteria = ds = da give similar result suggest stop epoch  120 .
d. larg problem extend analysi large-s problem relat high dimension : adult-a5a , connect-4 , dna , 9 dataset lr optim ai ds = 0 ds = 5 ds = 10 ds = 20 epoch logl epoch logl epoch logl epoch logl epoch logl adult-a5a 0.01 976 -13.67 1000 -13.88 880 -13.90 976 -13.67 998 -14.07 caltech101 0.0005 158 -157.91 260 -188.36 354 -209.09 995 -333.74 1000 -328.05 connect-4 0.001 968 -13.77 356 -14.61 992 -13.93 992 -13.93 1000 -13.86 dna 0.01 998 -62.32 056 -80.70 987 -62.34 992 -62.48 991 -62.48 mushroom 0.001 997 -13.41 999 -13.71 1000 -13.58 1000 -13.58 1000 -13.58 nips-0-12 0.05 568 -83.95 094 -128.68 184 -98.96 325 -88.17 993 -85.87 ocr-lett 0.01 086 -41.80 051 -42.01 185 -42.73 492 -44.25 913 -46.06 rcv1 0.01 059 -52.21 050 -52.82 053 -52.77 051 -52.85 097 -54.64 web-w6a 0.001 945 -28.07 967 -28.52 971 -28.47 997 -28.32 998 -28.60 mnist 0.0001 357 -125.73 201 -127.48 266 -126.26 319 -125.85 424 -126.44 dataset lr optim ai da = 0 da = 5 da = 10 da = 20 epoch logl epoch logl epoch logl epoch logl epoch logl adult-a5a 0.01 976 -13.67 1000 -13.88 1000 -13.88 1000 -13.88 1000 -13.88 caltech101 0.0005 158 -157.91 260 -188.36 262 -189.31 256 -187.92 316 -200.47 connect-4 0.001 968 -13.77 356 -14.61 388 -14.50 356 -14.61 335 -14.70 dna 0.01 998 -62.32 056 -80.70 062 -79.83 056 -80.70 051 -81.50 mushroom 0.001 997 -13.41 999 -13.71 999 -13.71 999 -13.71 999 -13.71 nips-0-12 0.05 568 -83.95 094 -128.68 130 -112.30 111 -119.38 104 -123.05 ocr-lett 0.01 086 -41.80 051 -42.01 051 -42.01 064 -41.87 061 -42.11 rcv1 0.01 059 -52.21 050 -52.82 055 -52.60 053 -52.77 054 -52.69 web-w6a 0.001 945 -28.07 967 -28.52 970 -28.44 969 -28.48 972 -28.42 mnist 0.0001 357 -125.73 201 -127.48 201 -127.48 242 -126.60 242 -126.60 tabl ii optim ais-estim stop point , = ds = da predict function distanc , sever large-s problem also use [ 35 ]  [ 37 ] .
lr logl stand learn rate log-likelihood , respect .
epoch log-likelihood optim stop point report .
last row includ result mnist problem .
mushroom , nips-0-12 , ocr-lett , rcv1 , web-w6a ( use [ 35 ] , [ 36 ] , exampl ) caltech101 silhouett dataset ( use [ 37 ] , exampl ) .
dataset down- load http : //www.cs.toronto.edu/larocheh/code/nad .
tgz http : //www.cs.ubc.ca/bmarlin/data .
use topolog refer .
case perform ten run averag result curv , mnist problem .
tabl ii show result ( stop epoch ais-estim log-likelihood epoch ) obtain = ds = da sever distanc d. notic result report tabl repres general behavior , obtain mani run differ learn rate .
seen , criteria work well case .
likelihood achiev maximum , usual detect criteria , yield good estim optim likelihood .
still , case criterion fail detect good stop point , happen caltech101 nips-0-12 .
howev , even case valuabl inform recov , criteria detect likelihood achiev maximum point after- ward degener , suggest start learn process lower learn rate .
best likelihood achiev around last epoch train , criteria usual indic one stop near end , though case = ds perform better ( dna , connect-4 ) .
overal , criteria success detect good stop point taken end learn process .
v. conclus work introduc contribut neigh- bore point train set build stop criterion learn cd .
shown train set also neighbor state contain valuabl inform use follow evolut network along train .
base fact learn tri increas contri- bution relev state decreas contribut rest , continu smooth energi function assign probabl state close train data .
key idea behind propos stop criterion .
fact , two differ relat estim ( depend number state use comput ) propos test experiment .
first one includ state close train set , second one take fraction state small size train set .
first estim robust may requir use forbid larg amount state , second one alway tractabl captur featur first one , thus provid suitabl stop learn criterion .
second estim shown work equal well mnist larg dataset , exact comput log-likelihood possibl .
addit , main idea proxim train set explor aspect relat learn futur work .
furthermor , could tri differ metric measur proxim neighbour state .
acknowledg er : research partial fund spanish research project tin2016-79576-r. fm : work support grant .
fis2014- 56257-c2-1-p dgi ( spain ) .
10 jd : work partial support sgr2014-890 ( macda ) generalitat de catalunya mineco project apcom ( tin2014-57226-p ) refer [ 1 ] d. e. rumelhart , g. e. hinton , r. j. william ,  learn intern represent error propag ,  parallel distribut process- ing : explor microstructur cognit ( vol .
1 ) , d. e. rumelhart j. l. mcclelland , ed .
mit press , 1986 , pp .
318362 .
[ 2 ] y. bengio ,  learn deep architectur ai ,  foundat trend machin learn , vol .
2 , .
1 , pp .
1127 , 2009 .
[ 3 ] j. schmidhub ,  deep learn neural network : overview ,  neural network , vol .
61 , pp .
85  117 , 2015 .
[ 4 ] g. e. hinton , s. osindero , y. teh ,  fast learn algorithm deep belief net ,  neural comput , vol .
18 , .
7 , pp .
15271554 , 2006 .
[ 5 ] g. e. hinton r. r. salakhutdinov ,  reduc dimension data neural network ,  scienc , vol .
313 , .
5786 , pp .
504507 , 2006 .
[ 6 ] h. larochell , y. bengio , j. lourador , p. lamblin ,  explor strategi train deep neural network ,  journal machin learn research , vol .
10 , pp .
140 , 2009 .
[ 7 ] h. lee , r. gross , r. ranganath , a. y. ng ,  convolut deep belief network scalabl unsupervis learn hierarch represent ,  intern confer machin learn , 2009 , pp .
609616 .
[ 8 ] q. v. le , m. a. ranzato , r. monga , m. devin , k. chen , g. s. corrado , a. y. ng ,  build high-level featur use larg scale unsupervis learn ,  29th intern confer machin learn , 2012 .
[ 9 ] r. sarikaya , g. e. hinton , a. deora ,  applic deep belief network natur languag understand ,  ieee/acm transact audio , speech , languag process , vol .
22 , .
4 , pp .
778 784 , 2014 .
[ 10 ] p. smolenski ,  inform process dynam system : founda- tion harmoni theori ,  parallel distribut process : explo- ration microstructur cognit ( vol .
1 ) , d. e. rumelhart j. l. mcclelland , ed .
mit press , 1986 , pp .
194281 .
[ 11 ] s. geman d. geman ,  stochast relax , gibb distribut , bayesian restor imag ,  ieee transact pattern analysi machin intellig , vol .
6 , .
6 , pp .
721741 , 1984 .
[ 12 ] g. e. hinton ,  train product expert minim contrast diverg ,  neural comput , vol .
14 , pp .
17711800 , 2002 .
[ 13 ] y. bengio , p. lamblin , d. popovici , h. larochell ,  greedi layer- wise train deep network ,  advanc neural inform process ( nip  06 ) , vol .
19 .
mit press , 2007 , pp .
153160 .
[ 14 ] y. bengio o. delalleau ,  justifi general contrast diverg ,  neural comput , vol .
21 , .
6 , pp .
16011621 , 2009 .
[ 15 ] m. a. carreira-perpinan g. e. hinton ,  contrast diverg learn ,  intern workshop artifici intellig statist , 2005 , pp .
3340 .
[ 16 ] a. yuill ,  converg contrast diverg ,  advanc neural inform process system ( nip  04 ) , vol .
17 .
mit press , 2005 , pp .
15931600 .
[ 17 ] d. j. c. mackay ,  failur one-step learn algorithm ,  2001 , unpublish technic report .
[ 18 ] t. tieleman ,  train restrict boltzmann machin use approx- imat likelihood gradient ,  25th intern confer machin learn , 2008 , pp .
10641071 .
[ 19 ] t. tieleman g. e. hinton ,  use fast weight improv persist contrast diverg ,  26th intern confer machin learn , 2009 , pp .
10331040 .
[ 20 ] h. schulz , a. muller , s. behnk ,  investig converg restrict boltzmann machin learn ,  nip 2010 workshop deep learn unsupervis featur learn , 2010 .
[ 21 ] a. fischer c. igel ,  empir analysi diverg gibb sampl base learn algorithm restrict boltzmann machin ,  intern confer artifici neural network ( icann ) , vol .
3 , 2010 , pp .
208217 .
[ 22 ] g. desjardin , a. courvill , y. bengio , p. vincent , o. delalleau ,  parallel temper train restrict boltzmann machin ,  13th intern confer artifici intellig statist ( aistat ) , 2010 , pp .
145152 .
[ 23 ] g. e. hinton ,  practic guid train restrict boltzmann machin ,  neural network : trick trade .
springer , 2012 , pp .
599619 .
[ 24 ] r. m. neal ,  anneal import sampl ,  1998 , technic report 9805 , dept .
statist , univers toronto .
[ 25 ] r. salakhutdinov i. murray ,  quantit analysi deep belief network ,  intern confer machin learn , 2008 , pp .
872879 .
[ 26 ] e. aart j. korst , simul anneal boltzmann machin .
stochast approach combinatori optim neural comput .
john wiley , 1990 .
[ 27 ] a. r. sankar v. n. balasubramanian ,  similarity-bas contrast diverg method energy-bas deep learn model ,  jmlr : workshop confer proceed , vol .
45 , pp .
391406 , 2015 .
[ 28 ] x. x. wang ,  averag contrast diverg train restrict boltzmann machin ,  entropi , vol .
18 , .
1 , p. 35 , 2016 .
[ onlin ] .
avail : http : //www.mdpi.com/1099-4300/18/1/35 [ 29 ] m. gabri , e. w. tramel , f. krzakala ,  train restrict boltz- mann machin via thouless-anderson-palm free energi ,  ad- vanc neural inform process system 28 , 2015 , pp .
640 648 .
[ 30 ] a. fischer c. igel ,  train restrict boltzmann machin : introduct ,  pattern recognit , vol .
47 , pp .
2539 , 2014 .
[ 31 ] l. li , j. lv , z. yi ,  non-neg represent learn algorithm select neighbor ,  machin learn , vol .
102 , .
2 , pp .
133153 , 2016 .
[ 32 ] x. peng , z. yu , z. yi , h. tang ,  construct l2-graph robust subspac learn subspac cluster ,  ieee transact cybernet , vol .
pp , .
99 , pp .
114 , 2016 .
[ 33 ] p. buhlmann s. van de geer , statist high-dimension data : method , theori applic .
springer scienc & busi media , 2011 .
[ 34 ] m.-a .
cote h. larochell ,  infinit restrict boltzmann machin ,  neural comput , vol .
28 , pp .
12651288 , 2016 .
[ 35 ] h. larochell , y. bengio , j. turian ,  tractabl multivari binari densiti estim restrict boltzmann forest ,  neural com- putat , vol .
22 , pp .
22852307 , 2010 .
[ 36 ] h. larochell i. murray ,  neural autoregress distribut estim ,  intern workshop artifici intellig statist , 2011 , pp .
2937 .
[ 37 ] b. m. marlin , k. swerski , b. chen , n. de freita ,  induct principl restrict boltzmann machin learn ,  intern workshop artifici intellig statist , 2010 , pp .
509516 .
