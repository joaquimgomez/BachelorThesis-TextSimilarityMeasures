journal artifici intellig research 55 (2016) 1-15 submit 11/15; publish 01/16 featur paradigm: deep learn machin translat marta r. costa-jussa universitat politecnica catalunya, barcelona abstract years, deep learn algorithm highli revolution area includ speech, imag natur languag processing. specif field machin translat (mt) remain invariant. integr deep learn mt vari re-model exist featur standard statist system develop new architecture. differ neural networks, research work us feed- forward neural networks, recurr neural network encoder-decod schema. architectur abl tackl challeng have low-resourc morpholog variations. manuscript focus describ neural network integr enhanc differ aspect model statist mt, includ languag modeling, word alignment, translation, reordering, rescoring. then, report new neural mt approach descript foundat relat work recent approach subword, charact train multilingu languages, others. finally, includ analysi correspond challeng futur work deep learn mt. 1. introduct inform societi continu evolv multilinguality: e.g. differ languag english gainin import web; strong societies, like european, continu multilingual. differ languages, domains, languag style combin potenti sourc information. context, machin translat (mt), task automat translat text sourc languag target language, gain relevance. industri academi strongli investig field progress incred speed. progress directli attach introduct deep learning. basically, deep learn evolut neural network compos multiple- layer models, neural network machin learn system capabl learn task train exampl requir explicitli program task. mt applic deep learn succeed recently. neural network propos mt late nineti (forcada & neco, 1997; castano & casacuberta, 1997), integr differ part statist mt 2006, 2013 2014 competit neural mt system propos (kalchbrenn & blunsom, 2013; sutskever, vinyals, & le, 2014; cho, van merrienboer, gulcehre, bahdanau, bougares, schwenk, & bengio, 2014b), 2015, neural mt reach state-of-the-art (bahdanau, cho, & bengio, 2015). c2016 ai access foundation. right reserved. 1.1 mt approach deep learn mt approach mainli follow rule-bas corpus-bas strategy. rule- base mt system date earli 70 initi systran (philipson, 2017) eurotra (maegaard, 1989). idea rule-bas approach transforma- tion sourc target mean perform analysi sourc text, transfer (with hand-craft rules) new sourc represent target representa- tion gener final target text. corpus-bas approach learn larg amount text. popular success approach statist and, particular, phrase-bas mt (koehn, och, & marcu, 2003). statist approach benefit train larg datasets. normally, statist mt us parallel text level sentences, us co-occurr extract bilingu dictionary, finally, us monolingu text comput languag model estim fluent translat text target language. main limit statist mt reli parallel corpora. rule-bas mt, limit requir linguist resources, lot human expert time. consider research try hybrid approach (costa-jussa, 2015). type mt approaches, popular decad 80s, interlingua- based, focu find univers represent languages. however, approach fallen disus challeng expens manual univers represent languages. 1.2 mt deep learn recent appear new train optim algorithm neural networks, i.e. deep learn techniqu (hinton, osindero, & teh, 2006; bengio, 2009; goodfellow, ben- gio, & courville, 2016), avail larg quantiti data increas comput power capac benefit introduct deep learn mt. deep learn learn represent multipl level abstract complex (bengio, 2009). lot excit deep learn achiev breakthroughs, e.g. automat extract composit imag line face (lee, grosse, ranganath, & ng, 2009), imagenet classif (krizhevsky, sutskever, & hinton, 2012) reduc error rate speech recognit 10% (graves, mohamed, & hinton, 2013). lot recent activ scientif commun deep learn mt refelect in, example, explos number work relev confer 2014 date. manuscript present overview earli stage deep learn start featur function statist mt (schwenk, costa-jussa, & fonollosa, 2006) entir new paradigm, achiev state-of-the-art result (jean, cho, memisevic, & bengio, 2015) one-year development. 2 1.3 manuscript contribut organ manuscript focus collect describ research introduc deep learn mt. differ previou survei (zhang & zong, 2015), de- tail deep learn techniques, instead provid briefli descript manuscript self-contained. center attent on: overview integr deep learn mt report mt aspect improv differ type neural networks; detail new neural mt architecture, cite foundat work discuss recent advanc face challeng aspect encount neural mt architecture; depict analysi strength weak deep learn mt. rest manuscript organ follows. section 2 briefli defin type neural network enhanc mt includ feed-forward, recurr neural network encoder-decod schema. then, section 3 classifi deep learn introduc mt enhanc translat languag modelling, word alignment, contextu information, reorder rescoring. section 4 review emerg deep learn architectur mt: neural mt descript recent advanc area. section 5 underlin main challeng deep learn mt depict main strength weaknesses. finally, section bring discuss role futur deep learn mt field point work directions. 2. brief descript neural network type section briefli describ neural network type mt, detail neural network type refer complet studi (goodfellow, bengio, & courville, 2017). neural network defin type statist learn algorithm estim function larg number inputs. neural network organis layers, includ input output layer and, between, consid hidden layers. layer compos neuron elementari unit network. neuron receiv input neuron perform weight sum input pass non-linear function (activation) produc output. main advantages, algorithms: extract abstract data and, currently, provid best perform multipl domain applic learn data; requir featur engin algorithm learn data; easili adapt new problems, i.e. deep learn architectur ap- pli particular applic us applic (kaiser, gomez, shazeer, vaswani, parmar, jones, & uszkoreit, 2017). 3 main drawbacks, algorithms: train complex model requir larg data, huge number param- eter high comput cost; requir challeng task determin right architectur topolog network adequ task. section introduc main neural network type emploi mt, complement statist approach new neural mt ap- proach. mainli feed-forward recurr neural network encoder- decod schema. extend multipl hidden layer unit input output layer constitut deep neural network (dnns). network mt task train supervis learn framework, algorithm learn huge collect exampl (i.e. parallel text level sentences). consequence, main big issu deep learn architectur deal larg vocabularies. train speed goe vocabulari increases. main reason deep learn architectur mt (or relat tasks) normal requir softmax function gener final probabl output words. comput soft- max function involv take sum score word vocabulary, feasibl larg vocabularies. section 3 4, mainli direct face problem: reduc vocabulari size charact subword unit instead words; us approxim self-norm reduc comput time, model size. 2.1 feed-forward neural network feed-forward neural network (fnns), figur 1 (a), connect input hidden node output loops. basically, network classifi singl multi-lay perceptrons. consist function map input output value. consist fulli connect layer direct graph. layer nodes, node neuron non-linear activ function. convolut neural network (cnns) popular type fnn cnns, connect pattern neuron inspir overlap individu neuron anim cortex (lecun & bengio, 1998). convolut oper mathemat wai connect pattern. mention 3 basic properti cnn fnn are: local connect (onli adjac neuron connected), paramet share (replic unit share parameterization) maxpool unit form subsampling. 2.2 recurr neural network recurr neural network (rnns), figur 1 (b), class neural networks. main characterist connect unit form direct cycle, gener intern state dynam tempor behavior. fnn typic reli fixed-s context window make markov assumpt word depend n previou words. hand, rnn abl us intern memori rid 4 markov assumpt condit previou words, highli relev languag model mt. differ type rnn and, manuscript, focu neural mt systems. detail explan type neural network (goodfellow et al., 2016). long short term memori (lstm) lstm network (hochreit & schmidhuber, 1997) direct cycl structur differ structur repeat cycle. repeat cycl neural network gate (input, memory/forget output) allow discard inform solv problem rnn face vanish gradient, discov (hochreiter, 1991; bengio, simard, & frasconi, 1994). intuitevely, vanish gradient problem appear gradient-bas backpropag methods. train weight algorithms, weight updat gradient error function. point, rnns, chain rule appli entir histori sequenc appli time caus gradient tend zero (specially, activ function tanh sigmoid). lstm initi proposed, successfulli wide rang sequenc applic (graves, 2013). gate rnn altern lstm gate rnn (chung, gulcehre, cho, & bengio, 2015; cho et al., 2014b), main differ instead have gate lstms, gru gate (reset update). gru paramet train compar lstm help train faster gener better data (chung, gulcehre, cho, & bengio, 2014). bi-direct rnn bi-direct rnn us finit sequenc label sequenc element past futur context (schuster & paliwal, 1997). case, predict comput combin rnn output process sequenc left right rnn output process sequenc right left. 2.3 encoder-decod type architectur inspir autoencod try predict input (goodfellow et al., 2016). encoder-decod architectur gener idea autoencod allow have differ input output data. encoder-decod architectur aim learn represent (encoding) input data, decod represent minim error recov output data. main purpos intern represent dimension reduct capabl ex- tract relev featur dataset. schemat represent shown figur 1 (c). 3. statist mt system deep learn statist mt (lopez, 2008) focus find probabl target text given sourc text. training, approach us parallel monolingu corpu extract model (i.e. translation, languag reordering) infer combin beam-search decoding. translat model bilingu dictionari word sequenc score base co-ocurr extract parallel corpus, 5 figur 1: feed-forward neural network (a); recurr neural network (b) autoen- coder (c) schemas. automat align level words, usual ibm model (brown, pietra, pietra, & mercer, 1993). aim model provid accur translat sourc sequences. languag model monolingu dictionari word sequenc object model probabl target sequences. finally, reorder model score chang order sourc target. model assign differ weight optim procedur maximis translat quality, measur automat metric, e.g. bleu (papineni, roukos, ward, & zhu, 2002). addition, system benefit rescor n-best list deal decod search errors. statist mt enhanc neural network differ levels. section cover neural network improv languag modeling; word alignment; translat modeling; reordering; rescoring. figur 2 show proport work model (note sourc statist cite paper current manuscript). summari section, tabl 1 show main relat work introduc deep learn standard statist mt. 3.1 languag model languag model task score sequenc words. approach neural net- work languag model long histori (elman, 1990; bengio, ducharme, vincent, & janvin, 2003). subsect cover wai neural network enhanc monolingu languag model statist mt systems, bilingu languag model report follow section 3.3. (schwenk et al., 2006) us continu space languag model inspir classic approach (bengio et al., 2003) improv n-gram-bas mt (marino, banchs, crego, gispert, lambert, fonollosa, & costa-jussa, 2006). neural network multi- 6 layer perceptron, train classifier, input projection, hidden output layers. project layer input word repres distribut encod input word us linear activ function. hidden layer us hyperbol tangent activ function output layer softmax layer. case, continu space languag model rescor phrase-bas n-gram-bas mt system. following, (mikolov, 2012) us recurr neural network languag model rescor n-best lists. problem limit vocabulari (a mention section 2), (hu, auli, gao, & gao, 2014) us rnn-base minimum translat unit models. author focu address issu data sparsiti limit context size leverag continu represent unbound histori recurr network. frame problem sequenc model task minim unit and, furthermore, model minimum translat unit bag-of-words. result approach complementari strong rnn-base languag model base sole target words. (niehu & waibel, 2012) propos simplifi neural languag model base re- strict boltzmann machin integr mt decoding. fulli integr lead improv n-best rescoring. integr mt decoding, (vaswani, zhao, fossum, & chiang, 2013) us fnn architectur rectifi linear unit noise-contrast estimation, requir repeat summat vocabulari enabl train neural network larger datasets. (devlin, zbib, huang, lamar, schwartz, & makhoul, 2014) introduc neural network joint languag model augment n-gram languag model (stolcke, 2002) m-word sourc window. moreover, network self-normalized, allow increas vocabulari size. (auli & gao, 2014) rnn languag model optim bleu criterion. addition, author effici integr rnn decoding. given decod speed n-gram lm state-of-the-art, approach calcul neural probabl n-gram format (wang, utiyama, goto, sumita, zhao, & lu, 2013). framework joint translat reorder consist train sequenc encod translat word reorder inform time, (guta, alkhouli, peter, wuebker, & ney, 2015) compar perform n-gram, feedforward recurr neural network directli translation. addition, recently, studi investig effect variat neural languag model information. (luong, kayser, & manning, 2015a) investig deep neural languag model layer outper- form fewer layer term translat quality, reach signific improv jointli condit sourc target contexts. (costa-jussa, espana, madhyastha, escolano, & fonollosa, 2016) us character-awar languag model (kim, jernite, sontag, & rush, 2016) rescor n-best list outperform result enhanc phrase-bas system. 3.2 word align word align kei task statist mt system identifi word-by-word relationship given pair sentenc correspond translations. ibm model 7 (brown et al., 1993) popular probabilist formul problem successfulli giza++ implement (och & ney, 2003). recently, appear approach us deep learn perform task. (yang, liu, li, zhou, & yu, 2013) us methodolog dnn speech recognit learn extract lexic translat information. model integr multi-lay neural network hidden markov model (hmm) framework, extract context depend lexic translation. model train bilingu corpu us monolingu data pre-train word embeddings. improv qualiti classic ibm models. (tamura, watanabe, & sumita, 2014) improv previou work rnn allow unlimit align history. 3.3 translat model given work literature, distinguish studi bilingu translat models, phrase-bas model syntax-bas models. main differ reli fact bilingu translat model follow languag model structur bilingu unit (marino et al., 2006), phrase-bas model us bilingu unit context (koehn et al., 2003), and, finally, syntax-bas model incorpor explicitli represent syntax pars sourc and/or target sentenc follow type grammar e.g. (yamada & knight, 2001). bilingu languag model earli approach neural network bilingu translat model normal two-step systems, mean n-best list propos tradition way, continu space model rescor lists. (schwenk, costa-jussa, & fonollosa, 2007) propos project bilingu unit continu space. then, project allow estim translat probabl continu representation. case, author us fully-connect multi-lay perceptron. bilingu unit act neural network inputs. author face problem comput complex solv limit vocabulary. (zamora-martnez, bleda, & schwenk, 2010) appli neural languag model bilingu monolingu languag model and, relevant, decod extend neural languag model viterbi, give better result rescoring. (son, allauzen, & yvon, 2012) propos similar architecture, author us vector input layer come sourc target language. represent combin hidden layer. order deal larger vocabulary, output structur cluster tree, word belong class associ sub-classes. (wu, watanabe, & hori, 2014) tackl sparsiti problem factor bilingu tupl sourc target recurr model them. phrase-bas translat model (schwenk, 2012) us fnn estim trans- lation probabl continu represent discuss fulli integr decoding. (gao, he, yih, & deng, 2014) report novel phrase translat model score bilingu phrase distanc featur vector continu space. continu space learn multi-lay neural network weight learn bleu score. (sundermeyer, alkhouli, wuebker, & ney, 2014) present word-bas phrase-bas approach recurr translat models. 8 assum one-to-on align sourc target sentences. model phrasal translat probabl avoid sparsiti issu singl word input output units. furthermore, addit unidirect formulation, author experi- ment bidirect network sourc sentenc account predictions. particular relev challeng phrase-bas translat model fact tak- ing account larger context produc translation. standard translat provid context translation, gener limit short contexts. therefore, follow focu work aim successfulli emploi larger context phrase-bas translat model. (zou, socher, cer, & manning, 2013) learn bilingu embed larg unlabel corpus, util mt word align constrain translat equivalence. new embed ad semant featur phrase-bas significantli outperform baselin system. (cui, zhang, liu, chen, li, zhou, & yang, 2014) propos learn topic represent parallel data encoder-decod techniqu pre-train fine-tuning. pre-train fine-tun allow partial initi error function point easier train. (martnez, espana-bonet, tiedemann, & marquez, 2014) us distribut vector represent word (mikolov, le, & sutskever, 2013) handl ambigu words. author identifi content word differ translations. content words, author window previou follow word comput vector representations. comput linear combin vector obtain context vector. then, calcul score base similar vector possibl translat option. (costa-jussa, gupta, rosso, & banchs, 2014) us deep learn encoder-decod structur learn similar correspond train test sentenc integr similar measur new featur phrase-bas system. syntax-bas translat model (meng, lu, wang, li, jiang, & liu, 2015) sum- mariz relev sourc inform convolut architecture, guid target information, then, architectur integr dependency-to-str translat (xie, mi, & liu, 2011). (zhai, zhang, zhou, & zong, 2014) us rnn perform structur predict bracket transduct grammar mt (wu, 1995). 3.4 reorder word reorder respond phenomena word differ posit sourc target sentenc involv translation. challeng aspect mt larg bodi research work address issu (bisazza & federico, 2016). mainli option appli reorder model statist mt framework integr model decod formul preprocessing. perform reorder directli search target language. reorder sourc word wai better match target word order (costa-jussa & fonollosa, 2006). 9 reorder decoding. (li, liu, sun, izuha, & zhang, 2014a) propos recurs autoencoder-bas itg-bas translat (wu, 1995), type syntax-bas model. (li, liu, sun, izuha, & zhang, 2014b) propos neural reorder model condit reorder probabl word current previou phrase pairs. (kanouchi, sudoh, & komachi, 2016) us recurs autoencod architectur phrase translat word align inform test phrase-bas system. specifically, allevi data sparsiti problem, author build classifi phrase pair recurs autoencod softmax layer. phrase pair repres continu space vector recurs autoencoder. differently, (setiawan, huang, devlin, lamar, zbib, schwartz, & makhoul, 2015) develop new neural network featur model non-loc translat phenomena relat word reorder- ing improv featur tensor neural networks. author us hypothesis- enumer featur estim probabl gener target word source- enumer featur estim probabl sourc word. reorder preprocessing. (mice baron & attardi, 2015) propos class rnn model exploit sourc depend syntax. (yu & zhu, 2015) propos rnn-base rule sequenc model captur arbitrari distanc contextu inform estim probabl rule sequences. (cui, wang, & li, 2016) present lstm-base neural reorder model directli model word pair alignment. figur 2: introduct neural network statist mt. work appli main translat languag model (base paper cite manuscript). 10 tabl 1: deep learn statist mt: main relat work model main relat work statistical-bas languag model (schwenk et al., 2006; schwenk, 2010) (niehu & waibel, 2012; vaswani et al., 2013) (wang et al., 2013; luong et al., 2015a) (mikolov, 2012; devlin et al., 2014) (auli & gao, 2014; hu et al., 2014) (guta et al., 2015; costa-jussa et al., 2016) word align (yang et al., 2013) (tamura et al., 2014) translat (schwenk et al., 2007; zamora-martnez et al., 2010) (son et al., 2012; schwenk, 2012) (gao et al., 2014; meng et al., 2015) (sundermey et al., 2014) (zou et al., 2013; martnez et al., 2014) (mikolov et al., 2013; cui et al., 2014) (costa-jussa et al., 2014) (wu et al., 2014; zhai et al., 2014) reorder (setiawan et al., 2015) (mice baron & attardi, 2015) (yu & zhu, 2015) (cui et al., 2016) (li et al., 2014a, 2014b; kanouchi et al., 2016) (setiawan et al., 2015) rescor (neubig et al., 2015; stahlberg et al., 2016a) 3.5 rescor rescor task re-rank list tent translat (provid decoder) differ knowledg inform model decoder. research work area previou posterior neural mt itself. previou it, us neural lm tm rescor statistical-bas system report section languag translat model 3.1 3.3, respectively. posterior it, refer us neural mt (which detail section) purpose. direction, (neubig, morishita, & nakamura, 2015) rescor n- best list syntax-bas nmt (stahlberg, hasler, & byrne, 2016a) improv approach lattic instead. 4. neural machin translat neural mt system neural network train predict target sentenc given sourc sentence. section, inspir (cho, 2015) , describ probabilist train frame- work new approach, review main foundat work recent advanc neural mt. summari section, tabl 2 show main relat work neural mt. 4.1 probabilist train framework train framework core neural mt aim maxim probabl target sentenc given sourc sentence. particular, neural mt map sourc sentenc s = s1, ..., si (with words) target sentenc t = t1, ..., tj (with j words) parametr condit distribut p(tn|sn) train sentenc 11 corpu set n = 1...n . then, learn algorithm maxim follow object function: argmax 1 n n n=1 log p(tn|sn) (1) differ model parameters. deal variable-length input output, rnn (see section 2.2) abl maintain intern state read sequenc inputs, translat sequenc words, allow input length. 4.2 encoder-decod base rnn mention earlier, neural mt us encoder-decod schema, maxim shown equat 1. section describ explicit encod decod base rnn operate. encod follow steps: 1. build word one-hot vector, binari vector singl element set 1 (wi). 2. project one-hot vector continu representation. encod project vector matrix e column word sourc vocabulari row number dimens chosen (mi = ewi). project gener continu vector sourc word, element vector later updat maxim log-prob correct output sentence. 3. build sequenc summar rnn, hi = 0(hi1,mi), activ function rnn parameters. visual vector, seen similar sentenc close summary-vector space (sutskev et al., 2014). decoder, basic invers encoder, follow steps: 1. comput intern hidden state decod zi = (hi , ui1, zi1), hi repres summari sourc sentence, ui1 previou translat word, zi1 previou hidden state decoder. 2. comput word probability, score word k given hidden state zi e(k) = wikzi + bk, then, simplifi bias, score normal obtain probabl softmax. 3. predict word. choos ith word, step comput decod intern hidden state, score normal target word select (i+ 1)th word, repeat select end-of-sent word. simpl architectur led notabl improv achiev state-of-the-art qualiti translation, explain follow section. 12 figur 3: neural mt architecture. 4.3 foundat work earli research neural mt work like (forcada & neco, 1997; castano & casacuberta, 1997), mainli limit comput power short data. build state-spac represent input string unfold obtain correspond output string. us elman simpl rnn (elman, 1990) sourc target. propos neural mt model mainli us previou encoder-decod architectur (sutskev et al., 2014; cho et al., 2014b). explain previou section 4.2, architectur allow encod sourc text fixed-length vector decod fixed-length vector target text. encod decod train singl architectur parallel corpus. main problem type architectur compress sourc sentenc fixed-length vector. (cho, van merrienboer, bahdanau, & bengio, 2014a) analys new approach neural mt perform rel short sentenc unknown words, perform degrad rapidli increment sentenc length number unknown words. 13 address long sentenc issues, i.e. mainli caus encod input sentenc singl fixed-lentgh vector, (bahdanau et al., 2015) propos new mechan decod decid part sourc sentenc pai attent to. attent mechan reliev encod have compress sourc sentenc fixed- length vector, allow neural translat model deal better long sentences. schemat represent encoder-decod attent figur 3. case (pouget-abadie, bahdanau, van merrienboer, cho, & bengio, 2014), au- thor propos wai address challeng long sentenc automat segment input sentenc phrase easili translat neural network transla- tion model. 4.4 recent advanc neural mt young paradigm, larg room improvement. perform foundat neural mt work larg depend languag pair quantiti train resources. example, wmt 2015, neural mt beat phrase-bas system task (bojar, chatterjee, federmann, haddow, huck, hokamp, koehn, logacheva, monz, negri, post, scarton, specia, & turchi, 2015). however, year later, us subword unit (sennrich, haddow, & birch, 2016b) enlarg train data, neural mt system outperform phrase-bas system larg number task (bojar, chatterjee, federmann, graham, haddow, huck, jimeno yepes, koehn, logacheva, monz, negri, neveol, neves, popel, post, rubino, scarton, specia, turchi, verspoor, & zampieri, 2016). follow popular recent advanc appli neural mt focu solv big challeng neural mt, including: manag long trainings; cover en- tire sourc sentenc translate; correctli deal morpholog variat larg vocabularies; manag low-resourc pair languages; make train efficient. encoder-decod architectures. encod decod recent design mean differ success architectures. first, rnn (a mention section 4.2) us recurr process sequenc variabl lengths. main disadvantag network process text strict order (either left-to-right right-to-left) computation expens parallel (cho et al., 2014b). second approach (convolut neural networks) overcom limit network process element time. network allow comput vector represent sequenc word wai deal input sentenc allow learn hierarch structur (gehring, auli, grangier, yarats, & dauphin, 2017). finally, approach us self-attent mechan allow model depend limit posit (vaswani, shazeer, parmar, uszkoreit, jones, gomez, kaiser, & polosukhin, 2017). attent mechan variations. mention previou section 4.3 attention- base mechan solv limit have encod fixed-length vector entir input. propos improv origin propos (bahdanau et al., 2015). (tu, lu, liu, liu, & li, 2016) propos maintain coverag vector track attent history. help prevent over-transl (word unnecessarili 14 translat multipl times) under-transl (word mistakenli untranslated). variat proposal, (yang, hu, deng, dyer, & smola, 2016) propos model sequenc attent level word rnn check fix window previou decisions. given attent current time tend correl previou attentions, (cohn, hoang, vymolova, yao, dyer, & haffari, 2016) propos add inform attent make decision. attent sentenc case call intra self-attention. type attent translat work (vaswani et al., 2017) new structur multi-head attent allow focu attent differ part sentenc time. larg vocabularies, subwords, character-based. mention section 2, com- pute final probabl output word costli larger vocabularies. (jean et al., 2015) propos model reduc limit target vocabulari sampl re- duce complex comput normal constant output word probabil- iti neural languag model (bengio & senecal, 2008). (luong, sutskever, le, vinyals, & zaremba, 2015b) address problem rare word post-process step translat out-of-vocabulari word dictionary. differently, approaches, base intuit word class translat smaller unit words, us word segment techniqu empir subword model im- prove back-off dictionari baselin unknown word (sennrich, haddow, & birch, 2016a). furthermore, approach directli deal charact (costa-jussa & fonollosa, 2016; lee, cho, & hofmann, 2016) byte (costa-jussa, escolano, & fonollosa, 2017b) reduc vocabulari minimum. work us character-bas embed train convolut network highwai network (srivastava, greff, & schmidhuber, 2015). multilingu low-resources. have low-resourc limita- tion train competit corpus-bas mt systems. neural mt propos wai tackl problem. hand, (zoph, yuret, may, & knight, 2016) propos train high-resourc languag pair transfer learn paramet low-resourc languag pair. hand, propos train system multilingu resources. relev work direct includ system train one- to-mani languag (dong, wu, he, yu, & wang, 2015), simultan translat sentenc sourc languag multipl target languages; many-to-on languag (zoph & knight, 2016), standard neural mt model train sourc singl target language; many-to-mani (firat, cho, & bengio, 2016), neural model train sourc target languages. recent advanc includ system abl zero-shot translat (johnson, schuster, le, krikun, wu, chen, thorat, viegas, wattenberg, corrado, hughes, & dean, 2016), mean parallel corpu languag b, abl learn translat languages. but, general, phrase-bas mt handl low-resourc set better shown (koehn & knowles, 2017). ad prior/linguist knowledge. idea neural mt deep learn- ing gener ad minimum prior knowledg possibl systems, ap- proach shown ad kind linguist knowledg useful. syntact 15 tabl 2: deep learn neural mt: main relat work relat progress main relat work neural-bas foundat (forcada & neco, 1997; castano & casacuberta, 1997) (sutskev et al., 2014; cho et al., 2014a) (cho et al., 2014b; bahdanau et al., 2015) (pouget-abadi et al., 2014) enc-dec (gehr et al., 2017; vaswani et al., 2017) attent (tu et al., 2016; yang et al., 2016) (cohn et al., 2016; vaswani et al., 2017) subword (jean et al., 2015; luong et al., 2015b) (sennrich et al., 2016b; costa-jussa et al., 2017b) (costa-jussa & fonollosa, 2016; lee et al., 2016) multilingu (zoph et al., 2016; zoph & knight, 2016; firat et al., 2016) (johnson et al., 2016) linguist (eriguchi et al., 2016; stahlberg et al., 2016a) (sennrich & haddow, 2016) product (crego et al., 2016; wu et al., 2016; levin et al., 2017) knowledg ad (eriguchi, hashimoto, & tsuruoka, 2016; stahlberg, hasler, waite, & byrne, 2016b) (sennrich & haddow, 2016) train morpholog linguist featur word embed neural mt model. system production. effici neural mt challeng train week (special rnns), success neural mt system production. detail descript system (crego, kim, klein, rebollo, yang, senellart, akhanov, & et al., 2016; wu, schuster, chen, le, norouzi, & et al., 2016; levin, dhanuka, khalil, kovalev, & khalilov, 2017). 5. neural mt analysis: strength weak deep learn introduc standard statist mt system (see section 3) new mt approach (see section 4). section make analysi main strength weak neural mt approach (see summari figur 5). analysi help plan futur direct neural mt. strength main inher strength neural mt model compon jointli train allow end-to-end optimization. relev strength that, given architectur base creat interme- diat representation, neural model eventu evolv machine-learnt interlingua approach (johnson et al., 2016). interlingua represent kei outperform mt low-resourc languag pair effici deal mt highli multilingu environments. addition, neural mt shown abl learn differ basic unit granular- ities. subword-bas representations, e.g. (sennrich et al., 2016b; costa-jussa & fonollosa, 2016; lee et al., 2016), allow neural mt model open-vocabulari translat seg- ment words. differ altern build subword units, byte pair encoding, data compres technique, shown perform effici (sennrich et al., 2016b). charact allow advantag intra-word inform implement sourc (costa-jussa & fonollosa, 2016) sourc target side (lee et al., 2016). 16 figur 4: strength weak analysi neural mt. finally, new paradigm allow multimod machin translat (elliott, frank, & hasler, 2015), allow advantag imag inform translat end-to-end speech translat architectur (weiss, chorowski, jaitly, wu, & chen, 2017), reduc concaten errors. weak main inher weak neural mt difficulti trace errors, long train time high comput cost. weak high comput cost train test models. train face gpu (graphic process units) expensive. finally, ad weak relat interpret model fact model work vectors, matric tensor instead word phrases. there- fore, abil train neural model scratch requir background machin learn scienc easi users/compani abl compre- hend/interpret it. difficult adopt paradigm. small compani prefer consolid paradigm like phrase rule-based. 17 6. summary, conclus futur work deep learn integr standard statist mt system differ level (i.e. languag model, word alignment, translation, reorder rescoring) differ perspect achiev signific improv cases. field deep learn advanc quickli worth notic neural-bas techniqu work todai replac new on near future. addition, entir new paradigm proposed: neural mt. curiously, approach propos simultanea popular phrase-bas (forcada & neco, 1997; castano & casacuberta, 1997). propos name differ connectionist mt, given comput power requir prohibit time data avail train complex systems, idea abandoned. nowadays, thank gpus, comput power limit inform societi provid larg quantiti data allow train larg number paramet model have. difficult quantifi mt improv neural approach. vari languag pair task. example, result wmt 2016 evalu (bojar et al., 2016) neural mt achiev best result (in term human evaluation) languag direct german-english, english romanian, english-german, czech-english, english-czech; like romanian-english, russian-english, english-russian, english-finnish. neural mt affect larg languag differences, low resourc variat train versu test domain (aldon, 2016; costa- jussa, aldon, & fonollosa, 2017a; costa-jussa, 2017; koehn & knowles, 2017). interpret mt system difficult. evolut mt, lost rule (in transit rule statistical-bas approach) recently, lost translat unit (in transit statist neural-bas approach). nowadays, new neural-bas approach mt open new questions, e.g. machine-learnt interlingua attainable? minim unit translated? manuscript recompil systemat foundat work deep learn- ing mt progress incred fast. deep learn influenc area natur languag process expect us techniqu con- troversial. adventur envisag neural algorithm go impact mt futur stai proven recent new big compani adopt neural mt approach e.g. google1 systran(crego et al., 2016).furthermore, deep learn take field dramat shown appear end-to-end speech-to-text translat (weiss et al., 2017) multimod mt (elliott et al., 2015),interlingua-bas represent (firat, cho, sankaran, vural, & bengio, 2017) unsupervis mt (artetxe, labaka, agirre, & cho, 2017; lample, denoyer, & ranzato, 2017). 1. 18 acknoledg work support spanish ministerio economa y competitividad, eu- ropean region develop fund agencia estat investigacion, postdoctor senior grant ramon y cajal, contract tec2015-69266-p (mineco/feder,eu) contract pcin-2017-079 (aei/mineco). refer aldon, d. (2016). sistema traduccion neuron usando bitmaps. b.s. thesis, universitat politecnica catalunya. artetxe, m., labaka, g., agirre, e., & cho, k. (2017). unsupervis neural machin translation. corr, abs/1710.11041. auli, m., & gao, j. (2014). decod integr expect bleu train recurr neural network languag models. proceed 52nd annual meet associ comput linguist (volum 2: short papers), pp. 136142, baltimore, maryland. associ comput linguistics. bahdanau, d., cho, k., & bengio, y. (2015). neural machin translat jointli learn align translate. corr, abs/1409.0473. bengio, y., & senecal, j. s. (2008). adapt import sampl acceler train neural probabilist languag model. trans. neur. netw., 19 (4), 713722. bengio, y., simard, p., & frasconi, p. (1994). learn long-term depend gra- dient descent difficult. trans. neur. netw., 5 (2), 157166. bengio, y. (2009). learn deep architectur ai. found. trend mach. learn., 2 (1), 1127. bengio, y., ducharme, r., vincent, p., & janvin, c. (2003). neural probabilist languag model. j. mach. learn. res., 3, 11371155. bisazza, a., & federico, m. (2016). survei word reorder statist machin trans- lation: comput model languag phenomena. comput. linguist., 42 (2), 163205. bojar, o., chatterjee, r., federmann, c., graham, y., haddow, b., huck, m., ji- meno yepes, a., koehn, p., logacheva, v., monz, c., negri, m., neveol, a., neves, m., popel, m., post, m., rubino, r., scarton, c., specia, l., turchi, m., verspoor, k., & zampieri, m. (2016). find 2016 confer machin translation. proceed confer machin translation, pp. 131198, berlin, germany. associ comput linguistics. bojar, o., chatterjee, r., federmann, c., haddow, b., huck, m., hokamp, c., koehn, p., logacheva, v., monz, c., negri, m., post, m., scarton, c., specia, l., & turchi, m. (2015). find 2015 workshop statist machin translation. pro- ceed tenth workshop statist machin translation, pp. 146, lisbon, portugal. associ comput linguistics. 19 brown, p. f., pietra, v. j. d., pietra, s. a. d., & mercer, r. l. (1993). mathemat statist machin translation: paramet estimation. comput. linguist., 19 (2), 263311. castano, m. a., & casacuberta, f. (1997). connectionist approach mt.. proc. eurospeech conference. cho, k. (2015). natur languag understand distribut representation. corr, abs/1511.07916. cho, k., van merrienboer, b., bahdanau, d., & bengio, y. (2014a). properti neural machin translation: encoderdecod approaches. proceed ssst-8, eighth workshop syntax, semant structur statist translation, pp. 103111, doha, qatar. associ comput linguistics. cho, k., van merrienboer, b., gulcehre, c., bahdanau, d., bougares, f., schwenk, h., & bengio, y. (2014b). learn phrase represent rnn encoder-decod statist machin translation. proceed 2014 confer empir method natur languag processing, emnlp 2014, octob 25-29, 2014, doha, qatar, meet sigdat, special group acl, pp. 17241734. chung, j., gulcehre, c., cho, k., & bengio, y. (2014). empir evalu gate recurr neural network sequenc modeling. corr, abs/1412.3555. chung, j., gulcehre, c., cho, k., & bengio, y. (2015). gate feedback recurr neu- ral networks. proceed 32nd intern confer intern confer machin learn - volum 37, icml15, pp. 20672075. jmlr.org. cohn, t., hoang, c. d. v., vymolova, e., yao, k., dyer, c., & haffari, g. (2016). incor- porat structur align bias attent neural translat model. proceed 2016 confer north american chapter associa- tion comput linguistics: human languag technologies, pp. 876885, san diego, california. associ comput linguistics. costa-jussa, m. r. (2015). hybrid machin translat need?. journal associ inform scienc technology, 66 (10), 21602165. costa-jussa, m. r. (2017). catalan-spanish neural machin translation? analysis, com- parison combin standard rule phrase-bas technologies. pro- ceed eacl workshop: vardial, valencia. costa-jussa, m. r., aldon, d., & fonollosa, j. a. r. (2017a). chinese-spanish neural machin translat enhanc charact andword bitmap fonts. machin trans- lation, accept publication. costa-jussa, m. r., escolano, c., & fonollosa, j. a. (2017b). byte-bas neural machin translation. proc. 1st workshop subword charact level model nlp, pp. 154158. costa-jussa, m. r., & fonollosa, j. a. r. (2006). statist machin reordering. proceed- ing 2006 confer empir method natur languag processing, emnlp 06, pp. 7076, stroudsburg, pa, usa. associ comput lin- guistics. 20 costa-jussa, m. r., & fonollosa, j. a. r. (2016). character-bas neural machin transla- tion. proceed 54th annual meet associ comput linguist (volum 2: short papers), pp. 357361, berlin, germany. associ comput linguistics. costa-jussa, m. r., gupta, p., rosso, p., & banchs, r. e. (2014). english-to-hindi descript wmt 2014: deep source-context featur moses. proceed ninth workshop statist machin translation, pp. 7983, baltimore, maryland, usa. associ comput linguistics. costa-jussa, m., espana, c., madhyastha, p., escolano, c., & fonollosa, j. (2016). talpupc spanishenglish wmt biomed task: bilingu embed char-bas neural languag model rescor phrase-bas system. proceed wmt, berlin. crego, j. m., kim, j., klein, g., rebollo, a., yang, k., senellart, j., akhanov, e., & et al., p. b. (2016). systran pure neural machin translat systems. corr, abs/1610.05540. cui, l., zhang, d., liu, s., chen, q., li, m., zhou, m., & yang, m. (2014). learn topic represent smt neural networks. proceed 52nd annual meet associ comput linguist (volum 1: long papers), pp. 133143, baltimore, maryland. associ comput linguistics. cui, y., wang, s., & li, j. (2016). lstm neural reorder featur statist machin translation. proceed 2016 confer north american chapter associ comput linguistics: human languag technologies, pp. 977982, san diego, california. associ comput linguistics. devlin, j., zbib, r., huang, z., lamar, t., schwartz, r., & makhoul, j. (2014). fast robust neural network joint model statist machin translation. proceed 52nd annual meet associ comput linguist (volum 1: long papers), pp. 13701380, baltimore, maryland. associ comput linguistics. dong, d., wu, h., he, w., yu, d., & wang, h. (2015). multi-task learn multipl languag translation. acl. elliott, d., frank, s., & hasler, e. (2015). multi-languag imag descript neural sequenc models. corr, abs/1510.04709. elman, j. l. (1990). find structur time. cognit science, 14 (2), 179 211. eriguchi, a., hashimoto, k., & tsuruoka, y. (2016). tree-to-sequ attent neural machin translation. proceed 54th annual meet associ comput linguist (volum 1: long papers), pp. 823833, berlin, germany. firat, o., cho, k., & bengio, y. (2016). multi-way, multilingu neural machin translat share attent mechanism. proceed 2016 confer north american chapter associ comput linguistics: human languag technologies, pp. 866875, san diego, california. 21 firat, o., cho, k., sankaran, b., vural, f. t. y., & bengio, y. (2017). multi-way, mul- tilingu neural machin translation. accept public speech language, special issu deep learn machin translation. forcada, m. l., & neco, r. p. (1997). recurs hetero-associ memori translation. proceed intern work-confer artifici natur neural networks: biolog artifici computation: neurosci technology, iwann 97, pp. 453462, london, uk, uk. springer-verlag. gao, j., he, x., yih, w., & deng, l. (2014). learn continu phrase represent translat modeling. proceed 52nd annual meet associ comput linguistics, acl 2014, june 22-27, 2014, baltimore, md, usa, volum 1: long papers, pp. 699709. gehring, j., auli, m., grangier, d., yarats, d., & dauphin, y. n. (2017). convolut sequenc sequenc learning. corr, abs/1705.03122. goodfellow, i., bengio, y., & courville, a. (2016). deep learning. mit press. goodfellow, i., bengio, y., & courville, a. (2017). deep learning. mit press. graves, a. (2013). gener sequenc recurr neural networks. corr, abs/1308.0850. graves, a., mohamed, a.-r., & hinton, g. (2013). speech recognit deep recurr neural networks. 2013 ieee intern confer acoustics, speech signal processing, pp. 66456649. ieee. guta, a., alkhouli, t., peter, j.-t., wuebker, j., & ney, h. (2015). comparison be- tween count neural network model base joint translat reorder sequences. proceed 2015 confer empir method natur languag processing, pp. 14011411, lisbon, portugal. associ comput linguistics. hinton, g. e., osindero, s., & teh, y.-w. (2006). fast learn algorithm deep belief nets. neural comput., 18 (7), 15271554. hochreiter, s. (1991). untersuchungen zu dynamischen neuronalen netzen. diploma, tech- nisch universitat munchen, 91. hochreiter, s., & schmidhuber, j. (1997). long short-term memory. neural comput., 9 (8), 17351780. hu, y., auli, m., gao, q., & gao, j. (2014). minimum translat model recurr neural networks. proceed 14th confer european chapter associ comput linguistics, pp. 2029, gothenburg, sweden. associ comput linguistics. jean, s., cho, k., memisevic, r., & bengio, y. (2015). larg target vocabulari neural machin translation. proceed 53rd annual meet associ comput linguist 7th intern joint confer natur languag process (volum 1: long papers), pp. 110, beijing, china. associ comput linguistics. 22 johnson, m., schuster, m., le, q. v., krikun, m., wu, y., chen, z., thorat, n., viegas, f. b., wattenberg, m., corrado, g., hughes, m., & dean, j. (2016). googl mul- tilingu neural machin translat system: enabl zero-shot translation. corr, abs/1611.04558. kaiser, l., gomez, a. n., shazeer, n., vaswani, a., parmar, n., jones, l., & uszkoreit, j. (2017). model learn all. corr, abs/1706.05137. kalchbrenner, n., & blunsom, p. (2013). recurr continu translat models. proceed 2013 confer empir method natur languag pro- cessing, pp. 17001709, seattle, washington, usa. associ comput linguistics. kanouchi, s., sudoh, k., & komachi, m. (2016). neural reorder model consid phrase translat word align phrase-bas translation. proceed 3rd workshop asian translat (wat2016), pp. 94103, osaka, japan. cole 2016 organ committee. kim, y., jernite, y., sontag, d., & rush, a. m. (2016). character-awar neural lan- guag models. proceed 30th aaai confer artifici intellig (aaai16). koehn, p., & knowles, r. (2017). challeng neural machin translation. acl workshop neural machin translation. koehn, p., och, f. j., & marcu, d. (2003). statist phrase-bas translation. pro- ceed 2003 confer north american chapter associ comput linguist human languag technolog - volum 1, naacl 03, pp. 4854, stroudsburg, pa, usa. associ comput linguistics. krizhevsky, a., sutskever, i., & hinton, g. e. (2012). imagenet classif deep convolut neural networks. proceed 25th intern confer neural inform process systems, nips12, pp. 10971105, usa. curran associ inc. lample, g., denoyer, l., & ranzato, m. (2017). unsupervis machin translat monolingu corpora only. corr, abs/1711.00043. lecun, y., & bengio, y. (1998). handbook brain theori neural networks.. chap. convolut network images, speech, time series, pp. 255258. mit press, cambridge, ma, usa. lee, h., grosse, r., ranganath, r., & ng, a. y. (2009). convolut deep belief network scalabl unsupervis learn hierarch representations. proceed 26th annual intern confer machin learning, icml 09, pp. 609616, new york, ny, usa. acm. lee, j., cho, k., & hofmann, t. (2016). fulli character-level neural machin translat explicit segmentation. corr, abs/1610.03017. levin, p., dhanuka, n., khalil, t., kovalev, f., & khalilov, m. (2017). full- scale neural machin translat production: booking.com us case. corr, abs/1709.05820. 23 li, p., liu, y., sun, m., izuha, t., & zhang, d. (2014a). neural reorder model phrase-bas translation. proceed cole 2014, 25th intern confer comput linguistics: technic papers, pp. 18971907, dublin, ireland. dublin citi univers associ comput linguistics. li, p., liu, y., sun, m., izuha, t., & zhang, d. (2014b). neural reorder model phrase-bas translation. proceed cole 2014, 25th intern confer comput linguistics: technic papers, pp. 18971907, dublin, ireland. dublin citi univers associ comput linguistics. lopez, a. (2008). statist machin translation. acm comput. surv., 40 (3), 8:18:49. luong, m.-t., kayser, m., & manning, c. d. (2015a). deep neural languag model machin translation. proceed nineteenth confer comput natur languag learning, beijing, china. luong, t., sutskever, i., le, q., vinyals, o., & zaremba, w. (2015b). address rare word problem neural machin translation. proceed 53rd annual meet associ comput linguist 7th intern joint confer natur languag process (volum 1: long papers), pp. 11 19, beijing, china. associ comput linguistics. maegaard, b. (1989). eurotra: machin translat project european communities. perspect artifici intelligence, ii. marino, j. b., banchs, r. e., crego, j. m., gispert, a., lambert, p., fonollosa, j. a. r., & costa-jussa, m. r. (2006). n-gram-bas machin translation. comput. linguist., 32 (4), 527549. martnez, e., espana-bonet, c., tiedemann, j., & marquez, l. (2014). word vector represent meet machin translation. proc. 8th workshop syntax, semant structure, doha, qatar. meng, f., lu, z., wang, m., li, h., jiang, w., & liu, q. (2015). encod sourc languag convolut neural network machin translation. proceed 53rd annual meet associ comput linguist 7th inter- nation joint confer natur languag process (volum 1: long papers), pp. 2030, beijing, china. associ comput linguistics. mice barone, a. v., & attardi, g. (2015). non-project dependency-bas pre-reord recurr neural network machin translation. proceed 53rd annual meet associ comput linguist 7th inter- nation joint confer natur languag process (volum 1: long papers), pp. 846856, beijing, china. associ comput linguistics. mikolov, t. (2012). statist languag model base neural networks.. mikolov, t., le, q. v., & sutskever, i. (2013). exploit similar languag machin translation. corr, abs/1309.4168. neubig, g., morishita, m., & nakamura, s. (2015). neural rerank improv subject qualiti machin translation: naist wat2015. proceed 2nd work- shop asian translation, wat 2015, kyoto, japan, octob 16, 2015, pp. 3541. 24 niehues, j., & waibel, a. (2012). continu space languag model restrict boltz- mann machines.. och, f. j., & ney, h. (2003). systemat comparison statist align models. comput linguistics, 29 (1), 1951. papineni, k., roukos, s., ward, t., & zhu, w.-j. (2002). bleu: method automat evalu machin translation. proceed 40th annual meet associ comput linguistics, acl 02, pp. 311318, stroudsburg, pa, usa. associ comput linguistics. philipson, j. (2014; access septemb 2017). systran: brief histori machin transla- tion.. pouget-abadie, j., bahdanau, d., van merrienboer, b., cho, k., & bengio, y. (2014). over- come curs sentenc length neural machin translat automat segmentation. proceed ssst-8, eighth workshop syntax, semant structur statist translation, pp. 7885, doha, qatar. associ compu- tation linguistics. schuster, m., & paliwal, k. (1997). bidirect recurr neural networks. trans. sig. proc., 45 (11), 26732681. schwenk, h. (2010). continuous-spac languag model statist machin translation. pragu bull. math. linguistics, 93, 137146. schwenk, h. (2012). continu space translat model phrase-bas statist ma- chine translation. cole 2012, 24th intern confer comput linguistics, proceed conference: posters, 8-15 decemb 2012, mumbai, in- dia, pp. 10711080. schwenk, h., costa-jussa, m. r., & fonollosa, j. a. r. (2006). continu space languag model iwslt 2006 task. 2006 intern workshop spoken languag translation, iwslt 2006, keihanna scienc city, kyoto, japan, novemb 27-28, 2006, pp. 166173. schwenk, h., costa-jussa, m. r., & fonollosa, j. a. r. (2007). smooth bilingu n-gram translation. emnlp-conl 2007, proceed 2007 joint confer empir method natur languag process comput natur lan- guag learning, june 28-30, 2007, prague, czech republic, pp. 430438. sennrich, r., & haddow, b. (2016). linguist input featur improv neural machin translation. proceed confer machin translation, pp. 83 91, berlin, germany. sennrich, r., haddow, b., & birch, a. (2016a). edinburgh neural machin translat system wmt 16. proceed confer machin translation, pp. 371376, berlin, germany. associ comput linguistics. sennrich, r., haddow, b., & birch, a. (2016b). neural machin translat rare word subword units. proceed 54th annual meet associ comput linguist (volum 1: long papers), pp. 17151725, berlin, germany. associ comput linguistics. 25 setiawan, h., huang, z., devlin, j., lamar, t., zbib, r., schwartz, r., & makhoul, j. (2015). statist machin translat featur multitask tensor networks. proceed 53rd annual meet associ comput linguist 7th intern joint confer natur languag process (volum 1: long papers), pp. 3141, beijing, china. associ comput linguistics. son, l. h., allauzen, a., & yvon, f. (2012). continu space translat model neural networks. proceed 2012 confer north american chap- ter associ comput linguistics: human languag technologies, naacl hlt 12, pp. 3948, stroudsburg, pa, usa. associ comput linguistics. srivastava, r. k., greff, k., & schmidhuber, j. (2015). highwai networks. corr, abs/1505.00387. stahlberg, f., hasler, e., & byrne, b. (2016a). edit distanc transduc action: univers cambridg english-german wmt16. proceed confer machin translation, pp. 377384, berlin, germany. associ comput linguistics. stahlberg, f., hasler, e., waite, a., & byrne, b. (2016b). syntact guid neural machin translation. corr, abs/1605.04569. stolcke, a. (2002). srilm-an extens languag model toolkit. proceed interna- tional confer spoken languag processing, pp. 257286. sundermeyer, m., alkhouli, t., wuebker, j., & ney, h. (2014). translat model bidirect recurr neural networks. proceed 2014 confer em- piric method natur languag process (emnlp), pp. 1425, doha, qatar. associ comput linguistics. sutskever, i., vinyals, o., & le, q. v. (2014). sequenc sequenc learn neural networks. advanc neural inform process system 27: annual confer- enc neural inform process system 2014, decemb 8-13 2014, montreal, quebec, canada, pp. 31043112. tamura, a., watanabe, t., & sumita, e. (2014). recurr neural network word align- ment model. proceed 52nd annual meet associ com- putat linguist (volum 1: long papers), pp. 14701480, baltimore, maryland. associ comput linguistics. tu, z., lu, z., liu, y., liu, x., & li, h. (2016). model coverag neural machin translation. proceed 54th annual meet associ compu- tation linguist (volum 1: long papers), pp. 7685, berlin, germany. associ comput linguistics. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., gomez, a. n., kaiser, l., & polosukhin, i. (2017). attent need. corr, abs/1706.03762. vaswani, a., zhao, y., fossum, v., & chiang, d. (2013). decod large-scal neural languag model improv translation. proceed 2013 confer em- piric method natur languag processing, emnlp 2013, 18-21 octob 2013, 26 grand hyatt seattle, seattle, washington, usa, meet sigdat, special group acl, pp. 13871392. wang, r., utiyama, m., goto, i., sumita, e., zhao, h., & lu, b.-l. (2013). convert continuous-spac languag model n-gram languag model statist ma- chine translation. proceed 2013 confer empir method natur languag processing, pp. 845850, seattle, washington, usa. associ comput linguistics. weiss, r. j., chorowski, j., jaitly, n., wu, y., & chen, z. (2017). sequence-to-sequ model directli translat foreign speech. corr, abs/1703.08581. wu, d. (1995). trainabl coars bilingu grammar parallel text bracketing. pro- ceed workshop larg corpora (vlc). wu, y., schuster, m., chen, z., le, q. v., norouzi, m., & et al., w. m. (2016). googl neural machin translat system: bridg gap human machin translation. corr, abs/1609.08144. wu, y., watanabe, t., & hori, c. (2014). recurr neural network-bas tupl sequenc model machin translation. proceed cole 2014, 25th intern confer comput linguistics: technic papers, pp. 19081917, dublin, ireland. dublin citi univers associ comput linguistics. xie, j., mi, h., & liu, q. (2011). novel dependency-to-str model statist ma- chine translation. proceed 2011 confer empir method natur languag processing, pp. 216226, edinburgh, scotland, uk. associ comput linguistics. yamada, k., & knight, k. (2001). syntax-bas statist translat model. proceed- ing 39th annual meet associ comput linguistics, acl 01, pp. 523530, stroudsburg, pa, usa. associ comput linguistics. yang, n., liu, s., li, m., zhou, m., & yu, n. (2013). word align model context depend deep neural network. proceed 51st annual meet associ comput linguist (volum 1: long papers), pp. 166175, sofia, bulgaria. associ comput linguistics. yang, z., hu, z., deng, y., dyer, c., & smola, a. j. (2016). neural machin translat recurr attent modeling. corr, abs/1607.05108. yu, h., & zhu, x. (2015). recurr neural network base rule sequenc model statisti- cal machin translation. proceed 53rd annual meet associ comput linguist 7th intern joint confer natur languag process (volum 2: short papers), pp. 132138, beijing, china. associ- ation comput linguistics. zamora-martnez, f., bleda, m. j. c., & schwenk, h. (2010). n-gram-bas machin trans- lation enhanc neural network french-english btec-iwslt10 task. 2010 intern workshop spoken languag translation, iwslt 2010, paris, france, decemb 2-3, 2010, pp. 4552. 27 zhai, f., zhang, j., zhou, y., & zong, c. (2014). rnn-base deriv structur predict smt. proceed 52nd annual meet associ compu- tation linguist (volum 2: short papers), pp. 779784, baltimore, maryland. associ comput linguistics. zhang, j., & zong, c. (2015). deep neural network machin translation: overview. ieee intellig systems, 15411672. zoph, b., & knight, k. (2016). multi-sourc neural translation. corr, abs/1601.00710. zoph, b., yuret, d., may, j., & knight, k. (2016). transfer learn low-resourc neural machin translation. corr, abs/1604.02201. zou, w. y., socher, r., cer, d., & manning, c. d. (2013). bilingu word embed phrase-bas machin translation. proceed 2013 confer empir method natur languag processing, pp. 13931398, seattle, washington, usa. associ comput linguistics. 28