comput linguist netherland journal 3 ( 2013 ) 217-233 submit 05/2013 ; publish 12/2013 neural network languag model select best translat maxim khalilov maxim @ tauslabs.com taus lab , 1011kw , amsterdam , netherland jose a.r .
fonollosa adrian @ gps.tsc.upc.edu centr de recerca talp , universitat politecnica de catalunya 08034 , barcelona , spain francisco zamora-martnez francisco.zamora @ uch.ceu. dep .
de ciencia fsica , matematica de la computacion universidad ceu-carden herrera 46115 alfara del patriarca ( valencia ) , spain maria jose castro-bleda mcastro @ dsic.upv. salvador espana-boquera sespana @ dsic.upv. dep .
de sistema informatico computacion universitat politecnica de valencia 46022 valencia , spain abstract qualiti translat produc statist machin translat ( smt ) system crucial depend general abil provid statist model involv process .
modern smt system use n-gram model predict next element sequenc token , system use continu space languag model ( lm ) base neural network ( nn ) .
contrast work nn lm use estim probabl shortlist word ( schwenk 2010 ) , calcul posterior probabl out-of-shortlist word use addit neuron unigram probabl .
experiment result small italian- to-english larg arabic-to-english translat task , take account differ word histori length ( n-gram order ) , show nn lms scalabl small larg data improv n-gram-bas smt system .
part , approach aim improv translat qualiti task lack translat data , also demonstr scalabl large-vocabulari task .
1 .
introduct translat one natur languag anoth one complex higher-ord activ human brain .
machin translat ( mt ) technolog field comput linguist investig model translat text , statist machin translat ( smt ) , contrast mani automat rule-bas translat system , translat paradigm base statist learn techniqu .
languag model import part mt system , receiv much spe- cializ attent within smt communiti .
instead , research focus 2013 maxim khalilov , jose a.r .
fonollosa , francisco zamora-martnez , maria jose castro-bleda salvador espana-boquera .
khalilov et al .
special translat model , decod algorithm , train techniqu .
contrast , field natur languag process , particular automat speech recognit understand , one find larg bodi research address specif problem languag model .
great extent , discrep consequ noisi experiment result inconsist languag model ( lm ) configur translat system perform .
howev , increas avail train data made applic monolingu techniqu quit promis sinc , typic , greater amount data use estim paramet lm , better lm perform .
regardless intern configur , smt system typic take basi log-linear combin approach target languag sentenc defin combin featur function .
set normal includ target-sid lm , inform translat decod correct given sentenc fluenci translat hypothesi .
paper follow continu space lm approach , coher natur evolut probabilist lms .
show deal better smooth challeng therebi provid better general unknown n-gram concentr scalabl problem crucial type lms .
use continu space represent languag success appli recent neural network ( nn ) approach languag model ( xu jelinek 2004 , bengio et al .
2003 , castro prat 2003 , arisoy et al .
2012 ) , domain adapt ( lavergn et al .
2011 , park et al .
2010 ) speech recognit ( schwenk 2007 ) , neural network languag model ( nn lm ) applic state-of-the-art smt system popular .
work trace back studi done schwenk et al. , nn lm appli train target-sid lm ( schwenk et al .
2006 , schwenk 2010 ) form fully-connect multilay perceptron smooth probabl involv bilingu tupl translat model ( schwenk et al .
2007 ) .
nn lm describ paper follow similar approach , differ way probabl out-of-shortlist word estim .
detail found section 3.3 .
inabl modern smt system accommod increas workload caus nn lms , open way new less comput expens mechan support translat process .
contrast previous describ techniqu improv perform smt system incorpor nn lm small amount train materi avail , describ two altern scenario .
first , experi small-vocabulari italian-to-english translat task demonstr nn lm potenti mt .
howev , interest altern field appli approach address scalabl problem especi crucial nn lm use task larg amount train data .
thus , second stage , provid translat result real-world large-vocabulari arabic-to-english task demonstr improv term final translat qualiti achiev circumvent difficulti impos complex structur natur languag .
rest paper structur follow : section 2 briefli outlin n-gram-bas smt system .
section 3 novel featur present paper describ , i.e .
nn lms train algorithm .
section 4 present experiment setup obtain result , section 5 conclud articl lead discuss .
2 .
upc n-gram-bas smt system modern smt system follow phrase-bas ( koehn et al .
2007 ) hierarch ( chiang 2007 ) translat approach .
studi , follow altern algorithm , n-gram-bas tuple-bas smt ( marino et al .
2006 ) prove competit state-of-the-art system recent evalu campaign ( khalilov et al .
2008 , lambert et al .
2007 ) .
218 neural network languag model select best translat n-gram-bas smt deal bilingu n-gram , so-cal tupl .
tupl extract word-to-word align ( perform giza++1 ( och ney 2000 ) ) accord certain constraint ( marino et al .
2006 ) compos one word sourc languag zero word target one .
henc , tupl induc uniqu segment pair sentenc .
contrast , phrase-bas system produc possibl pair phrase consist , lead sever number segment possibl given pair sentenc .
regular phrase-bas smt consid context phrase reorder transla- tion , n-gram-bas approach condit translat decis previous translat decis .
context use way bilingu translat model seen lm , languag compos tupl .
uniqu segment sentenc pair , case n-gram-bas smt , trans- lation procedur regard stochast process maxim joint probabl p ( f , e ) , approxim sentenc level .
besid n-gram translat model , featur model taken consider : ( 1 ) target lm word ( train sri languag model toolkit2 ( stolck 2002 ) ) ; ( 2 ) word bonus model ( penal target sentenc length ) ( 3 ) two-direct lexicon model .
2.1 decod optim mari decoder3 ( crego et al .
2005 ) extend monoton distort use search engin translat system .
decod implement beam-search algorithm prune capabl .
featur function describ taken account decod process .
given develop set set refer translat , log-linear combin weight adjust use simplex optim method ( nelder mead 1965 ) maxim score function accord combin automat evalu metric .
2.2 extend word reorder n-gram-bas translat system high sensit differ word order sourc target languag .
extend monoton distort model base automat ex- tract reorder pattern use experi .
reorder pattern extract train stage cross link found word align ; next step , monoton search graph extend re-ord follow pattern found train set ( crego marino 2007 ) .
search lattic built , decod travers graph look best translat .
2.3 rescor nn lm model integr n-gram-bas smt system within discrimin rescor- ing/rerank framework ( compos two step ) , incorpor complex featur function use entir translat hypothesi generat score .
first step , mari decod produc list candid translat base vector weight train basic featur ( exclud orthodox n-gram lm order diminish nn lm effect ) .
, statist score generat translat candid rescor use inform provid nn lm .
modul presum add knowledg includ decod better distinguish higher lower qualiti translat .
step , rescor vector train + 1 featur provid differ , better motiv choic single-best translat hypothesi .
1. code.google.com/p/giza-pp/ 2. www.speech.sri.com/projects/srilm/ 3. talp.upc.edu/talp/index.php/en/resources/tools/mari 219 code.google.com/p/giza-pp/ www.speech.sri.com/projects/srilm/ talp.upc.edu/talp/index.php/en/resources/tools/mari khalilov et al .
altern way incorpor nn lm smt system use continu space lm direct decod .
decid pursu strategi sinc would result dramat increas decod time .
2.4 translat score adopt three widely-us metric automat evalu mt qualiti :  bleu score account translat qualiti evalu , measur dis- tanc given translat set refer translat use n-gram lm ( 4-gram framework studi ) ( papineni et al .
2002 ) .
 nist score sensit metric translat qualiti , base bleu score , weight n-gram order provid less inform n-gram higher weight ( doddington 2002 ) .
 meteor score metric evalu mt output , calcul averag mean precis benefit recal , consid stem synonym match ( detail found banerje lavi ( 2005 ) ) .
3 .
neural network languag model like encount new n-gram never wit train due heavi tail structur natur languag .
n -gram lms often critic lack explicit represent depend longer n  1 preced token , effect rang depend signific longer .
address problem lm smooth continu domain use connectionist lm .
major differ classic n-gram lm nn lm approach lie distinct mechan use implement smooth process .
regular n-gram model ,  de facto  standard smooth algorithm modifi kneser-ney discount ( chen goodman 1999 ) , consid extens absolut discount .
method take account lower-ord model signific higher order count small zero ( jame 2000 , chen goodman 1999 ) .
contrast kneser-ney backing-off , interpol smooth model ( instanc , jelinek-merc interpol chen-goodman model ( chen goodman 1999 ) ) use inform lower-ord model determin probabl n-gram non-zero count .
within nn lm , posterior probabl interpol possibl context length n1 rather backing-off shorter context .
unfortun , general involv greater compu- tation cost evalu train nn lm linear depend number weight .
number domin size last hidden layer multipli vocabulari size .
grow number requir calcul quick overwhelm modern comput resourc make implement comput intract even average-s vocabulari task .
howev , zipf  law ( zipf 1949 ) state given corpus natur languag utter , frequenc word invers proport rank frequenc tabl .
consequ , frequent word occur approxim twice often second frequent word , occur twice often fourth frequent word , .
observ explain input output nn , practic , limit shortlist k frequent word vocabulari .
choic shortlist n-gram order trade-off nn lm train time smt system perform .
220 neural network languag model select best translat 3.1 model architectur nn lm statist lm follow n-gram assumpt estim lm probabl sequenc word length |w | : p ( w1 .
.
.
w|w | )  |w | i=1 p ( wi|win+1 .
.
.
wi1 ) ( 1 ) probabl appear former express estim within nn .
model natur fit probabilist interpret output nns : nn train classifi , output associ class estim posterior probabl defin class ( bishop 1995 , bengio et al .
2003 ) .
train set lm sequenc w1w2 .
.
.
w|w | word vocabulari  .
order train nn predict next word given histori length n  1 , input word must encod .
natur represent local encod follow  1-of-||  scheme4 .
problem relat encod task larg vocabulari ( often case ) huge size result nn .
address problem follow bengio et al .
( 2003 ) , develop distribut represent word.5 nn lm abl learn joint distribut represent word continu space condit probabl estim equat ( 1 ) .
procedur allow nn lm smooth unseen sequenc word .
figur 1 illustr architectur feed-forward nn use estim nn lm :  input compos word win+1 , .
.
.
, wi1 equat ( 1 ) .
exampl , input word wi3 , wi2 , wi1 4-gram .
word repres use local encod lj set neuron .
 p project layer input word , form p1 , .
.
.
, pn1 subset project unit .
subset project unit pj repres distribut encod input neuron lj ( correspond word input posit j ) .
weight project layer share , , weight local encod input lj correspond subset project unit pj input word .
train , codif layer remov network pre-comput tabl size || serv distribut encod .
codif word comput follow : pj = l j wl , p + b ( 2 ) ltj transpos vector lj , repres local codif cor- respond word win+1+j , wl , p matrix nn weight input word correspond project unit subset ( share input word ) , b vector bias project unit subset ( share subset ) , pj vector repres distribut encod correspond word .
 h denot hidden layer , comput : h = tanh ( pt wp , h + c ) ( 3 ) pt transpos project layer vector ( concaten p1 , p2 , .
.
.
, pn1 ) , wp , h matrix nn weight project layer hidden layer , c vector hidden layer bias , tanh ( . )
component-wis hyperbol tangent activ function .
4 .
word local encod need vector || neuron , exist one-to-on map neuron word , neuron repres word activ 1 , rest neuron 0s .
5 .
distribut represent , word map ( project ) continu space , use number neuron much smaller || .
221 khalilov et al .
 output layer || unit , one word vocabulari .
calcul follow : = ht wh , + ( 4 ) = exp ( ) || j=1 exp ( aj ) ( 5 ) wh , matrix weight hidden layer output layer , vector output layer bias , vector activ valu comput appli softmax normal , aj compon j .
cross-entropi error function use train experi , ad l2 regular term weight , except bias : e =  log ( ) +   wiw w2i 2 ( 6 ) vector desir output , w union weight matrix wl , p , wp , h , wh , , e comput error .
nn estim posterior probabl word wi vocabulari given histori , i.e. , p ( wi|win+1 .
.
.
wi1 ) .
.
.
.
.
.
.
.
.
.
0 0 1 0 .
.
.
0 0 1 0 .
.
.
0 1 0 0 0 .
.
.
.
.
.
.
.
.
.
.
.
figur 1 : architectur continu space nn lm 4-gram , hi = win+1 .
.
.
wi1 .
illustr huge size nns , comput number weight largest nn lm , contain 224 neuron project layer 200 neuron hidden layer .
case italian-to-english translat , nn includ vocabulari word appear twice 914,905 weight .
arabic-to-english task , nn 56 neuron project layer oper 2,266,803 weight .
nevertheless , standard 4-gram train corpora 20,817,600 paramet .
3.2 continu space lm experi strategi nn lm experi use italian-to-english btec corpus prelimi- nari experi , larger amount effort dedic arabic-to-english nist 222 neural network languag model select best translat task .
under idea prove nn lms use large-scal mt trial focus issu practic use continu space lms .
consid two key paramet continu space nn lms :  shortlist size defin k frequent word , word frequenc threshold  impli everi word occur less  time train corpus discard .
 n-gram order limit word histori n preced word .
n -gram order 3 4 consid italian-to-english experi ; 4 , 5 6-gram configur , well interpol high- low-ord n-gram , test translat arab .
 valu set 2 , 3 4 italian-to-english translat , correspond 4105 , 3093 , 2498 word nn lm vocabulari , respect .
translat arab english took differ approach : use 10 k frequent word full train vocabulari .
refer tabl 1 actual vocabulari size valu .
italian-to-english  # word english train set 4 2,498 3 3,093 2 4,105 arabic-to-english  # word english train set n/a 10,000 tabl 1 : number word reduc train corpora .
note train set use italian-to-english translat contain 10.2k uniqu word , vocabulari arabic- to-english 157.6k word .
addit neuron ad input output layer take account out- of-shortlist word , call osl neuron .
train out-of-shortlist word replac osl identifi .
test step , comput probabl out-of-shortlist word need , activ osl neuron combin simpl standard unigram model comput out-of-shortlist word .
probabl comput follow : p ( wi|win+1 .
.
.
wi1 ) = { owi iff wi shortlist ; oosl  p ( wi|osl ) iff otherwis , ( 7 ) owi output neuron activ relat wi , oosl osl neuron activ , p ( wi|osl ) standard unigram probabl comput out-of-shortlist word .
final , nn lm combin four nns , one size project layer ( see section 3.1 ) .
defin p ( wi|osl ) valu follow trade-off two altern strategi pre- sent compar emami mangu ( 2007 ) .
first approach impli standard n-gram calcul , substitut p ( wi|osl ) p ( wi|osl , win+1 .
.
.
wi1 ) , way describ schwenk ( 2007 ) .
accord second strategi , p ( wi|osl ) valu set 0 .
sinc ob- tain result indistinguish , follow compromis strategi , use unigram probabl model p ( wi|osl ) .
reestim weight coeffici new log-linear model nn lm , differ start point tri , best set weight result 100 bleu + 4 nist criteria .
223 khalilov et al .
3.3 differ nn lm schwenk  system best knowledg , studi found paper first attempt present nn lm differ work schwenk .
arm zipf  law , estim posterior probabl k frequent word vocabulari without signific loss general .
posterior probabl rare word estim introduc extra neuron use standard unigram model .
contrast , model previous present schwenk et al .
( 2006 ) comput posterior probabl out-of-shortlist word use standard n-gram model .
requir estim contribut shortlist word standard n-gram model .
besid , vocabulari use nn lm input , codif less frequent word well learn ( hai-son et al .
2010 ) .
4 .
experi 4.1 data experi result obtain use two corpora , differ size train- ing materi ( see brief statist tabl 2 ) .
first one italian-to-english btec cor- pus ( takezawa et al .
2002 ) , collect spoken dialogu data .
second corpus consider 37m-word extract arabic-to-english nist6 corpus ( news domain ) .
experi , develop test set includ one ground truth refer ( refer differ translat ) , normal help automat evalu process error measur best refer .
note statist shown sourc ( italian arab ) portion develop test bilingu corpora .
set languag # sentenc # word vocab .
size averag sent .
length refer italian-to-english btec corpus train italian 24.5k 166.3k 10.2k 6.5 - train english 24.5k 155.4k 7.3k 6.1 - dev italian 489 5.2k 1.2k 6.5 7 test italian 500 6.0k 1.3k 6.9 7 arabic-to-english nist corpus ( 1.2m-line extract ) train arab 1.2m 37.4m 186.9k 31.2 - train english 1.2m 37.4m 157.6k 31.2 - dev arab 2,075 62.7k 10.1k 30.2 4 test arab 2,040 61.6k 9.9k 30.2 4 tabl 2 : basic statist train , develop test data .
4.2 data preprocess italian-to-english system , preprocess step consist token , tag , lemma- tizat , separ contract italian part , describ crego et al .
( 2006 ) .
regard arabic-to-english translat , use similar approach shown habash sadat ( 2006 ) .
mada+tokan7 ( roth et al .
2008 ) system util disambigu token .
disambigu diacrit unigram statist employ .
token 6 .
nation institut standard technolog 7. www1.ccls.columbia.edu/~cadim/mada.html 224 www1.ccls.columbia.edu/~cadim/mada.html neural network languag model select best translat use d3 scheme -tagbi option split follow set clitic : w+ , f+ , b+ , k+ , l+ , al+ pronomin clitic .
-tagbi option produc bie pos tag taggabl token .
first step n -best list extract , paramet n set 1000 , limit size list possibl translat generat mari decod output lattic .
4.3 system configur provid reason comparison nn lm experi , consid regular n-gram lms rescor way nn lms ( integr n-gram lm rescor step ) .
call system configur baselin 1 .
altern configur , consid inclus regular lm featur set function combin log-linear way decod ( call system dec ) .
result shown dec system correspond perform standard n-gram-bas smt .
secondari baselin ( baselin 2 ) use continu space system default param- eter , describ schwenk ( 2010 ) .
packag use avail onlin : http : //www-lium .
univ-lemans.fr/fr/content/cslm .
everi nn lm experi , n -best list size set 1000 .
modifi kneser-ney discount chosen comput smooth n-gram lms sinc demonstr best result term perplex final translat score ( bleu ) measur concaten refer translat ( develop dataset ) .
compar origin kneser-ney discount good-tur chen-goodman ( uninterpol interpol version ) discount algorithm ( chen goodman 1999 ) .
applic modifi kneser- ney techniqu demonstr signific improv perplex (  12 % ) translat qualiti accord bleu score (  3.9 % ) comparison altern smooth algorithm .
order achiev good general perform , train four nns linear combin build final nn lm .
nn differ project layer size ( 128 , 160 , 192 , 224 italian-to-english task , 160 , 192 , 224 , 256 arabic-to-english task ) hidden layer size ( set 200 neuron ) .
number paramet nn lm depend size layer .
number taken preliminari work done two task , base literatur .
combina- tion differ project layer seem better optim size layer ( time consum ) .
hidden layer import effect perform model , big comput impact ( schwenk 2007 ) .
automat evalu condit case-sensit includ punctuat mark .
automat score calcul 7 ( btec experi ) 4 ( nist experi ) refer translat .
4.3.1 italian-to-english experi tabl 3 show bleu , nist , meteor score system 3- 4-gram nn lms integr compon combin smt system reduc amount train materi .
baselin 1 dec system train test use 4-gram target-sid lms .
best score place cell fill grey .
4.3.2 arabic-to-english experi arabic-to-english experi could use higher-ord nn lms italian-to- english task due larger amount train data .
differ baselin system con- sider :  baselin 1 ( 4 ) , ( 5 ) , ( 6 ) system employ regular lms correspond order rescor step , 225 http : //www-lium.univ-lemans.fr/fr/content/cslm http : //www-lium.univ-lemans.fr/fr/content/cslm khalilov et al .
system dev test bleu nist meteor bleu nist meteor baselin 1 39.2 8.5 73.0 33.5 7.7 70.3 dec 39.4 8.6 73.2 33.6 7.7 70.9 nn lms baselin 2 42.2 8.9 74.0 34.9 7.9 71.2 nn lm 3-gram 41.7 8.8 73.4 34.3 7.8 70.2  > 4 4-gram 40.9 8.6 73.0 34.4 7.8 70.2 nn lm 3-gram 41.6 8.7 73.1 34.4 7.8 70.7  > 3 4-gram 42.0 8.8 73.8 34.4 7.8 70.4 nn lm 3-gram 41.8 8.8 73.5 34.3 7.8 70.4  > 2 4-gram 42.3 8.8 74.0 34.9 7.8 71.2 tabl 3 : evalu score develop test dataset italian-to-english btec translat .
system dev test bleu nist meteor bleu nist meteor baselin 1 ( 4 ) 46.1 10.1 64.5 38.1 9.6 60.3 baselin 1 ( 5 ) 45.7 10.1 64.3 38.1 9.6 60.4 baselin 1 ( 6 ) 45.3 10.0 64.7 37.9 9.6 60.5 baselin 1 ( 4+5+6 ) 45.0 9.9 64.1 37.9 9.6 60.6 dec ( 6 ) 45.1 9.9 64.1 37.9 9.6 60.5 nn lms baselin 2 46.6 10.2 64.5 38.6 9.8 60.6 4-gram 46.5 10.2 64.6 38.4 9.7 60.6 5-gram 46.5 10.2 64.4 38.6 9.7 60.3 6-gram 46.6 10.2 64.4 38.5 9.7 60.3 4 + 5 + 6-gram 46.6 10.2 64.3 38.5 9.7 60.5 nn lms exclud sri lm model 4-gram 46.3 10.1 64.2 38.0 9.6 60.5 5-gram 46.5 10.3 64.2 38.3 9.6 60.4 6-gram 46.4 10.2 64.5 38.4 9.6 60.4 4 + 5 + 6-gram 46.5 10.3 64.5 38.2 9.6 60.6 tabl 4 : evalu score develop test dataset arabic-to-english nist translat .
 baselin 1 ( 4+5+6 ) combin 4- , 5- , 6-gram one n -best list ,  dec ( 6 ) provid decod access 6-gram standard lm without addit rescor .
along independ nn lms (  4- , 5- , 6-gram  ) , train network interpol high- low-ord n-gram (  4+5+6-gram  ) .
isol impact nn lms translat 226 neural network languag model select best translat qualiti rescor n -best list exclud score generat regular n-gram lms (  nn lms exclud sri lm model  ) .
best system configur highlight aforement tabl .
4.4 perplex analysi output sentenc smt system built aggreg word sequenc high-scor combin probabl provid bilingu tupl translat model set featur model , includ lm .
therefor , clear correl impact lm perplex assembl translat .
howev , perplex measur predict power lm , use compar well lm predict next word previous unseen piec text .
tabl 5 show perplex valu stand-alon lms measur merg set translat refer test corpora italian-to-english btec arabic-to-english nist task .
languag model perplex italian-to-english btec task convent 3-gram 156 convent 4-gram 155 nn lm 3-gram ,  > 4 131 nn lm 4-gram ,  > 4 130 nn lm 3-gram ,  > 3 132 nn lm 4-gram ,  > 3 128 nn lm 3-gram ,  > 2 130 nn lm 4-gram ,  > 2 130 arabic-to-english nist task convent 4-gram 132 convent 5-gram 151 convent 6-gram 176 nn lm 4-gram 185 nn lm 5-gram 175 nn lm 6-gram 173 nn lm interpol 4 , 5 , 6-gram 167 tabl 5 : perplex result differ languag model .
architectur differ target-sid ( english ) nn lms italian-to-english arabic-to-english translat task lie distinct algorithm not-in-the-vocabulari ( unk ) word process .
italian-to-english translat , perplex valu calcul basi sri lm model higher one nn lms .
note neural network comput n-gram probabl subset task vocabulari , comput out-of-shortlist word probabl combin osl neuron multipli unigram model , affect perplex model .
arabic-to-english task , output neural network cover 10 k frequent word , task vocabulari 157.6 k size .
perplex loss due out- of-shortlist word import task .
impli perplex nn lms calcul use new neuron higher perplex sri lm model .
227 khalilov et al .
4.5 analysi italian-to-english result sentence-bas bleu scores8 show share sentenc improv nn lms integr smt pipelin best perform integr system ( 4-gram  > 2 ) 46 % .
time bleu score 12 % sentenc becam wors remain segment remain unchang .
observ , consider improv obtain use nn lm italian-to-english translat .
develop dataset , bleu score nn lm experi higher one baselin system nn lm system .
best- perform 4-gram  > 2 nn lm system allow gain 1.3 bleu point test set system includ convent n-gram lm featur decod ( dec ) ; gain 1.4 bleu point test dataset system use regular lm rescor ( baselin ) .
aforement differ statist signific 95 % confid interv 1,000 resampl use bootstrap resampl method ( koehn 2004 ) .
upper-bound statist signific threshold ( bleu score calcul test dataset translat baselin system ) lie 34.0 bleu point .
analysi nist score italian-to-english system show baselin result test set exceed nn lm system .
concern meteor score , 4-gram ,  > 4 system provid better lm general .
perform shown best system statist indistinguish result shown schwenk  system bleu meteor score .
correl automat subject human evalu metric ( fluenci adequaci ) one main topic area mt evalu .
report paul ( 2006 ) small btec translat task , fluenci correl best bleu , adequaci correl best meteor .
nist metric moder correl subject human evalu metric .
take aforement observ consider , work demonstr potenti applic nn lms smt system improv translat fluenci , adequaci remain .
posit impact higher-ord n-gram clear , possibl due relat short sentenc provid within btec corpus .
anoth possibl issu higher-ord n-gram order slight decreas translat qualiti , yet time , introduc noisier translat .
exampl typic sentenc italian-to-english btec corpus shown figur 2 .
4.6 analysi arabic-to-english result best-perform nn lm system ( 5-gram nn lms ) term bleu , 34 % sentenc improv comparison baselin 2 system , 10 % decreas perform rest dataset chang observ .
arabic-to-english translat , bleu nist score calcul develop- ment dataset improv nn lm appli comparison perform shown baselin engin , meteor valu generat nn lm configur vari around score produc system integr convent n-gram lm .
consid test data translat score , differ bleu score shown best nn lm system best system baselin popul 0.5 bleu point , statist signific threshold task ( 0.5 ) .
time , result achiev rescor n -best list includ nn lms , exclud standard n-gram lms , statist distinguish neither baselin , nn lm system .
system configur provid better bleu score correspond 5-gram lms .
incorpor nn lms n-gram-bas smt system allow gain 0.7 bleu point 8 .
general set , sentence-bas bleu score make much sens due accumul natur bleu .
228 neural network languag model select best translat sourc oggi abbiamo scelta insalata ai frutti di mare insalata di patat e insalata mista .
ref today choic seafood salad potato salad wild veget salad .
serv seafood salad potato salad wild veget salad today .
today  salad enjoy seafood potato wild veget .
salad seafood potato wild veget today .
today  select seafood salad potato salad wild veget salad .
today seafood salad potato salad wild veget salad .
today choos seafood salad potato salad wild veget salad .
baselin today select seafood salad potato salad mix salad .
3-gram  = 5 today choos seafood salad potato salad mix salad .
4-gram  = 5 today select seafood salad potato salad mix salad .
3-gram  = 3 today choos seafood salad potato salad mix salad .
4-gram  = 3 today choos seafood salad potato salad mix salad .
figur 2 : exampl italian-to-english translat .
italian express  oggi abbiamo scelta  translat baselin system  today select  , wherea three four nn lm system provid fluent translat  today choos  .
sourc w aelnt wkalp alanba  alamaratyp en wswl aleahl alardni mn dwn twdyh brnamj aw mdp zyarp h .
ref emir news agenc announc arriv jordanian monarch without specifi either programm durat visit .
emir news agenc announc arriv jordanian king , without give detail program , durat visit .
emir news agenc announc jordanian monarch  arriv withoutnot schedul durat stay .
emir news agenc announc jordanian monarch  visit without give detail purpos durat .
baselin emir agenc announc king access without clarifi programm durat visit .6-gram dec emir news agenc said jordanian monarch access without clarifi programm durat visit .
4-gram news agenc announc jordanian monarch access without clarif programm durat visit .
5-gram news agenc announc arriv jordanian monarch without clarif programm durat visit .
6-gram news agenc announc jordanian monarch access without clarifi programm durat visit .
4 + 5 + 6 news agenc announc arriv jordanian monarch without clarif programm durat visit .
figur 3 : exampl arabic-to-english translat .
nn lms ( along dec system ) manag generat correct translat  jordanian monarch  .
5- 4+5+6- gram model produc correct translat arab word  wswl  ,  arriv  .
system translat incorrect  access  .
229 khalilov et al .
test set dec around 0.5 bleu point best baselin configur .
increas n-gram order 6 lead perform improv , neither interpol 4 , 5 , 6-gram .
obtain result show ad nn lms regular nn lms within discrimin rescor framework provid deliver slight improv consist translat qualiti comparison system consid standard lms .
task , schwenk system perform slight better nn lms term bleu nist score good latter consid meteor .
figur 3 illustr example9 one sentenc arabic-to-english nist .
5 .
discuss conclus architectur smt system impli , smaller amount avail train data , wors perform translat system .
paper , shown follow : 1 .
robust nn lms , even high limit train corpus .
in-domain nn lm provid signific better general target languag , better smooth smt output , enhanc improv automat evalu translat score .
nn lm turn benefici even train excerpt frequent word vocabulari .
also claim small translat task , integr nn lm improv translat fluenci , adequaci remain .
empir proof claim plan done near futur .
2 .
proof claim nn lm approach scalabl even modern level technolog develop .
demonstr techniqu use nn lm set k frequent word , probabl less frequent word estim use extra neuron unigram probabl , lead minor improv translat qualiti large-vocabulari task .
comparison exist system base continu space languag model show approach perform practic good known schwenk  system term bleu ( corre- late fluenci , least small translat task ) slight better consid meteor ( correl adequaci ) ( paul 2006 ) .
main disadvantag continu space lm high comput cost train .
tradit n-gram lms train minut use sri lm toolkit , take day estim continu space lm large-vocabulari task .
possibl solut problem either applic fast-train techniqu ( lattic regroup util special nn librari abil parallel calcul ) involv power ( expens ) comput resourc .
high comput cost caus re- search effort done decoupl system , base n -best rescor .
recent research yield promis result effici integr nn lm decod ( zamora-martnez et al .
2009 , zamora-martnez et al .
2010 ) .
three urgent task plan undertak increas credibl nn lm integr smt framework : 1 .
experi higher amount train data , probabl focus distant languag pair .
2 .
run human evalu campaign , base adequacy/flu score , confirm result automat evalu .
9 .
arab exampl provid buckwalt transliter ( buckwalt 1994 ) .
230 neural network languag model select best translat 3 .
check applic describ mechan hierarch smt system , like one describ chiang ( 2005 ) chiang ( 2007 ) .
refer arisoy , e. , t.n .
sainath , b. kingsburi , b. ramabhadran ( 2012 ) , deep neural network languag model , proceed naacl-hlt 2012 workshop : ever realli replac n-gram model ?
futur languag model hlt , montreal , canada , pp .
2028 .
banerje , s. a. lavi ( 2005 ) , meteor : automat metric mt evalu improv correl human judgment , proceed acl workshop intrins extrins evalu measur machin translat and/or summar , pp .
6572 .
bengio , y. , r.e .
ducharm , p. vincent ( 2003 ) , neural probabilist languag model , journal machin learn research 3 , pp .
11371155 .
bishop , c.m .
( 1995 ) , neural network pattern recognit , oxford univers press .
buckwalt , t. ( 1994 ) , issu arab orthographi morpholog analysi , proceed col- ing 2004 , geneva , switzerland , pp .
3134 .
castro , m.j. f. prat ( 2003 ) , new direct connectionist languag model , comput method neural model , springer-verlag .
chen , s.f .
j. goodman ( 1999 ) , empir studi smooth techniqu languag mod- ele , comput speech languag 4 ( 13 ) , pp .
359394 .
chiang , d. ( 2005 ) , hierarch phrase-bas model statist machin translat , proceed associ comput linguist ( acl ) 2005 , pp .
263270 .
chiang , d. ( 2007 ) , hierarch phrase-bas translat , comput linguist 2 ( 33 ) , pp .
201 228 .
crego , j.m. , a. de gispert , p. lambert , m. khalilov , m. costa-jussa , j.b. marino , r. banch , j.a.r .
fonollosa ( 2006 ) , talp ngram-bas smt system iwslt 2006 , proceed iwslt 2006 , pp .
116122 .
crego , j.m .
j.b. marino ( 2007 ) , improv statist mt coupl reorder decod , machin translat 20 ( 3 ) , pp .
199215 .
crego , j.m. , j.b. marino , a. de gispert ( 2005 ) , ngram-bas statist machin translat decod , proceed interspeech05 .
doddington , g. ( 2002 ) , automat evalu machin translat qualiti use n-gram co- occurr statist , hlt 2002 ( second confer human languag technolog ) , pp .
128132 .
emami , a. l. mangu ( 2007 ) , empir studi neural network languag model ara- bic speech recognit , proceed ieee automat speech recognit understand workhsop ( asru 2007 ) , pp .
147152 .
habash , n. f. sadat ( 2006 ) , arab preprocess scheme statist machin translat , proceed human languag technolog confer naacl , pp .
4952 .
hai-son , l. , a. alluzen , g. wisniewski , f. yvon ( 2010 ) , train continu space languag model : practic issu , proceed emnlp , pp .
778788 .
231 khalilov et al .
jame , f. ( 2000 ) , modifi kneser-ney smooth n-gram model , technic report , research institut advanc comput scienc ( riac ) .
khalilov , m. , a.h. hernandez , m.r .
costa-jussa , j.m .
crego , c.a .
henrquez , p. lambert , j.a.r .
fonollosa , j.b. marino , r. banch ( 2008 ) , talp-upc ngram-bas statist ma- chine translat system acl-wmt 2008 , proceed acl 2008 third workshop statist machin translat ( wmt  08 ) , pp .
127131 .
koehn , p. ( 2004 ) , statist signific test machin translat evalu , proceed empir method natur languag process ( emnlp ) 2004 , pp .
388395 .
koehn , p. , h. hoang , a. birch , c. callison-burch , m. federico , n. bertoldi , b. cowan , w. shen , c. moran , r. zen , c. dyer , o. bojar , a. constantin , e. herbst ( 2007 ) , mose : open- sourc toolkit statist machin translat , proceed associ computa- tional linguist ( acl ) 2007 , pp .
177180 .
lambert , p. , m.r .
costa-jussa , j.m .
crego , m. khalilov , j. marino , r.e .
banch , j.a.r .
fonollosa , h. schwenk ( 2007 ) , talp ngram-bas smt system iwslt 2007 , proceed intern workshop spoken languag translat ( iwslt07 ) , pp .
169174 .
lavergn , t. , a. allauzen , h-s .
le , f. yvon ( 2011 ) , limsi  experi domain adapt iwslt11 , proceed iwslt 2011 , san francisco , ca , usa , pp .
6267 .
marino , j.b. , r.e .
banch , j.m .
crego , a. de gispert , p. lambert , j.a.r .
fonollosa , m.r .
costa-jussa ( 2006 ) , n-gram base machin translat , comput linguist 32 ( 4 ) , pp .
527549 , acl .
nelder , j.a .
r. mead ( 1965 ) , simplex method function minim , comput organ 7 , pp .
308313 .
och , f. h. ney ( 2000 ) , improv statist align model , proc .
38th annual meet associ comput linguist , pp .
440447 .
papineni , k. , s. rouko , t. ward , w. zhu ( 2002 ) , bleu : method automat evalua- tion machin translat , proceed 40th annual meet associ comput linguist ( acl ) 2002 , pp .
311318 .
park , j. , x. liu , m.j.f .
gale , p.c .
woodland ( 2010 ) , improv neural network base languag model adapt , proceed interspeech 2010 .
paul , m. ( 2006 ) , overview iwslt 2006 evalu campaign , proceed iwslt06 , pp .
115 .
roth , r. , o. rambow , n. habash , m. diab , c. rudin ( 2008 ) , arab morpholog tag , diacrit , lemmat use lexem model featur rank , proceed associ comput linguist ( acl ) , columbus , ohio .
schwenk , h. ( 2007 ) , continu space languag model , comput speech languag 21 ( 3 ) , pp .
492518 .
schwenk , h. ( 2010 ) , continuous-spac languag model statist machin translat , pragu bulletin mathemat linguist ( 93 ) , pp .
137146 .
schwenk , h. , m.r .
costa-jussa , j.a.r .
fonollosa ( 2006 ) , continu space languag model iwslt 2006 task , proceed iwslt 2006 , pp .
166173 .
232 neural network languag model select best translat schwenk , h. , m.r .
costa-jussa , j.a.r .
fonollosa ( 2007 ) , smooth bilingu translat , pro- ceed empir method natur languag process ( emnlp ) , pragu , czech republ , pp .
430438 .
stolck , .
( 2002 ) , srilm : extens languag model toolkit , proceed int .
conf .
spoken languag process , pp .
901904 .
takezawa , t. , e. sumita , f. sugaya , h. yamamoto , s. yamamoto ( 2002 ) , toward broad- coverag bilingu corpus speech translat travel convers real world , pro- ceed lrec 2002 , pp .
147152 .
xu , p. f. jelinek ( 2004 ) , random forest languag model , proceed emnlp 2004 , pp .
325332 .
zamora-martnez , f. , m.j. castro-bleda , h. schwenk ( 2010 ) , n-gram-bas machin translat enhanc neural network french-english btec-iwslt  10 task , proceed seventh intern workshop spoken languag translat ( iwslt ) , pp .
4552 .
zamora-martnez , f. , m.j. castro-bleda , s. espana-boquera ( 2009 ) , fast evalu connec- tionist languag model , cabestani , joan , francisco sandov , alberto prieto , juan m. corchado , editor , proceed 10th intern work-confer artifici neural network , iwann 2009 , vol .
5517 lncs , springer , pp .
3340 .
zipf , g.k. ( 1949 ) , human behavior principl least-effort , addison-wesley .
233 introduct upc n-gram-bas smt system decod optim extend word reorder rescor translat score neural network languag model model architectur continu space lm experi differ nn lm schwenk 's system experi data data preprocess system configur italian-to-english experi arabic-to-english experi perplex analysi analysi italian-to-english result analysi arabic-to-english result discuss conclus
