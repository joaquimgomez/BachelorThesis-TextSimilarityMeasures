restrict boltzmann machin vector represent speech speaker recognit avail onlin www.sciencedirect.com comput speech & languag 47 ( 2017 ) 1629 www.elsevier.com/locate/csl restrict boltzmann machin vector represent speech speaker recognitioni taggedpd1x xomid ghahabi d2x x* , d3x xjavier hernando d4x x taggedptalp research center , depart signal theori communic , universitat politecnica de catalunya , barcelona 08034 , spain receiv 26 octob 2016 ; receiv revis form 2 may 2017 ; accept 19 june 2017 taggedpabstract last year , i-vector state-of-the-art techniqu speaker recognit .
recent advanc deep learn ( dl ) technolog improv qualiti i-vector dl techniqu use comput expens need phonet label background data .
aim work develop effici altern vector represent speech keep comput cost low possibl avoid phonet label , alway access .
propos vector base gaussian mixtur model ( gmm ) restrict boltzmann machin ( rbm ) refer gmmrbm vector .
role rbm learn total speaker session variabl among background gmm supervector .
rbm , refer univers rbm ( urbm ) , use transform unseen supervector propos low dimension vector .
use differ activ function train urbm differ- ent transform function extract propos vector investig .
end , variant rectifi linear unit ( relu ) refer variabl relu ( vrelu ) propos .
experi core test condit 5 nist sre 2010 show compar result convent i-vector achiev clear lower comput load vector extract process .
 2017 author .
publish elsevi ltd .
open access articl cc by-nc-nd licens .
( http : //creativecommons.org/licenses/by-nc-nd/4.0/ ) .
taggedpkeyword : restrict boltzmann machin ; deep learn ; variabl rectifi linear unit ; speaker recognit ; gmmrbm vector ; i-vector 1 .
introduct taggedpth low dimension represent speech utter base factor analysi techniqu well-known i-vector ( dehak et al. , 2011a ) .
past year , i-vector shown great perform speaker recognit also applic ( e.g. , dehak et al. , 2011b ; bahari et al. , 2012 ; xia liu , 2012 ) .
two common use score techniqu i-vector cosin distanc ( dehak et al. , 2010 ; 2011a ) probabilist linear discrimin analysi ( plda ) ( princ elder , 2007 ; kenni , 2010 ) .
plda score lead superior per- formanc need speaker-label background data cost access easili .
paper recommend accept roger moor .
* correspond author .
e-mail address : omid.ghahabi @ upc.edu ( o. ghahabi ) .
http : //dx.doi.org/10.1016/j.csl.2017.06.007 0885-2308/ 2017 author .
publish elsevi ltd .
open access articl cc by-nc-nd licens .
( http : //creativecommons.org/licenses/by-nc-nd/4.0/ ) http : //creativecommons.org/licenses/by-nc-nd/4.0/ mailto : omid.ghahabi @ upc.edu http : //dx.doi.org/10.1016/j.csl.2017.06.007 http : //creativecommons.org/licenses/by-nc-nd/4.0/ http : //www.sciencedirect.com http : //dx.doi.org/ http : //www.elsevier.com/locate/csl http : //crossmark.crossref.org/dialog/ ? doi=10.1016/j.csl.2017.06.007 & domain=pdf http : //crossmark.crossref.org/dialog/ ? doi=10.1016/j.csl.2017.06.007 & domain=pdf o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 17 taggedpmotiv success use deep learn ( dl ) speech process applic ( e.g. , moham et al. , 2010 ; dahl et al. , 2012 ; moham et al. , 2012 ; hinton et al. , 2012 ; senior et al. , 2015 ) , dl techniqu also use speaker recognit differ purpos .
exampl , dl techniqu appli backend i-vector ( stafylaki et al. , 2012b ; senoussaoui et al. , 2012 ; stafylaki et al. , 2012a ; novoselov et al. , 2014 ; ghahabi hernando , 2014a ; 2014b ; 2017 ) , use i-vector extract algorithm ( lei et al. , 2014 ; kenni et al. , 2014 ; mclaren et al. , 2015 ; richardson et al. , 2015 ; liu et al. , 2015 ; campbel , 2014 ; garcia-romero et al. , 2014 ) , also employ compact represent speech signal ( vasilakaki et al. , 2013 ; variani et al. , 2014 ; liu et al. , 2015 ; ghahabi hernando , 2015 ; safari et al. , 2016 ) discrimin featur classif ( safari et al. , 2015 ) .
taggedpdl technolog use i-vector extract algorithm two way .
first , deep neural network ( dnn ) use acoust model rather typic gaussian mixtur model ( gmm ) ( lei et al. , 2014 ; kenni et al. , 2014 ; campbel , 2014 ; richardson et al. , 2015 ; garcia-romero et al. , 2014 ; liu et al. , 2015 ) .
second , convent spectral featur replac append so-cal dnn bottleneck featur dnn gmm use acoust model ( mclaren et al. , 2015 ; richardson et al. , 2015 ; liu et al. , 2015 ) .
shown best result obtain spectral featur append bottleneck featur gmm use acoust model ( mclaren et al. , 2015 ; richardson et al. , 2015 ; lozano-diez et al. , 2016 ) .
howev , main problem use dnn either acoust model bottleneck featur extractor increas high comput cost i-vector extract process .
moreov , case phonet label requir dnn train , alway access .
taggedpon hand , work tri make use dl techniqu build compact representa- tion speech signal without use convent i-vector algorithm .
vasilakaki et al .
( 2013 ) , variani et al .
( 2014 ) , liu et al .
( 2015 ) , deep architectur train use background featur vector .
featur vector given utter forward-propag mean posterior probabl particular hidden layer ( variani et al. , 2014 ) pca dimens reduc version ( liu et al. , 2015 ) , pca dimens reduc version mean vector ( vasilakaki et al. , 2013 ) consid new compact represent .
safari et al .
( 2016 ) , paramet adapt network stack build supervec- tor .
dimens new supervector reduc pca .
ghahabi hernando ( 2015 ) , author use gmm supervector , rather featur vector , input restrict boltzmann machin ( rbm ) .
rbm use dimens reduct stage scenario .
although liu et al .
( 2015 ) variani et al .
( 2014 ) shown success text-depend speaker recognit , still signif- icant improv report text-independ task .
moreov , work dl techniqu featur vector domain cost .
taggedpth aim work develop effici framework vector represent speech keep comput cost low possibl avoid phonet label .
order achiev goal , global rbm refer univers rbm ( urbm ) train given background gmm supervector .
urbm tri learn total session speaker variabl among background supervector .
use transform unseen supervector lower dimension vector refer gmmrbm vector .
taggedpcompar preliminari work present ghahabi hernando ( 2015 ) , whiten supervector domain , comput cost , replac warp featur vector domain .
chang make possibl obtain higher speaker recognit accuraci , special lower dimension vector .
moreov , effect type activ function train urbm type transform function gmmrbm vector extract investig .
end , variat linear rectifi unit ( relu ) , refer variabl relu ( vrelu ) , propos train urbm , linear function use transform vector extract stage .
taggedpth core condit nist sre 2006 ( nist , 2006 ) use develop core condit 5 nist sre 2010 ( nist , 2010 ) much bigger background data use test evalu .
experi evalu set show propos gmmrbm vector achiev compar perform convent i-vector lower comput cost requir vector extract .
conclus valid cosin plda score .
moreov , combin gmmrbm vector i-vector score level improv perform .
taggedpth rest paper organ follow .
section 2 give brief background overview convent i-vector plda .
section 3 describ propos gmmrbm vector .
section 4 investig effect 18 o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 taggedpactiv transform function use , respect , urbm train gmmrbm vector extract discuss databas , baselin system , experiment result .
section 5 conclud paper .
2 .
convent i-vector taggedpan i-vector ( dehak et al. , 2011a ) low rank vector represent speech utter .
featur vector speech signal repres gaussian mixtur model ( gmm ) adapt univers background model ( ubm ) .
mean vector adapt gmm stack build supervector s. supervector model follow ,  subm  tn 1 subm speaker- session-independ mean supervector , typic ubm , total variabl matrix , n vector latent variabl .
posterior distribut n condit baumwelch statis- tic given speech utter .
mean posterior distribut refer i-vector v comput follow , v   ts1n ut  1 ts 1 efu 2 n u diagon matrix contain zeroth order baumwelch statist , efu supervector central first order statist , diagon covari matrix initi subm updat factor analysi train , denot transpos oper .
tmatrix train use expectationmaxim ( em ) algorithm given baumwelch statist background speech utter .
detail found dehak et al .
( 2011a ) .
taggedptwo main score techniqu i-vector cosin ( dehak et al. , 2010 ; 2011a ) probabilist linear dis- crimin analysi ( plda ) ( princ elder , 2007 ) .
plda effect techniqu perform score along session variabl compens .
assum i-vector decompos , v  mfz   3 global offset , column f eigenvoic , z latent vector standard normal prior , residu vector e normal distribut zero mean full covari matrix .
model paramet estim larg collect speaker-label background data use em algorithm princ elder ( 2007 ) .
within class i-vector covari matric depend model paramet store use score .
3 .
propos gmmrbm vector taggedprec , advanc deep learn ( dl ) improv qualiti i-vector , dl techniqu use comput expens need phonet label background data .
propos work altern vector-bas represent speaker less comput expens manner use phonet speaker label .
taggedprbm good potenti purpos good represent power unsuper- vise comput low cost .
fact , rbms generat network two fulli connect layer visi- ble hidden stochast unit .
work , assum input visibl unit gmm supervector output hidden unit low dimension vector look .
rbm train given back- ground gmm supervector refer univers rbm ( urbm ) .
role urbm learn total session speaker variabl among background supervector .
differ type unit activ function use train urbm mention section 3.2 evalu section 4 applic .
train urbm , visiblehidden connect weight matrix use transform unseen gmm supervector lower dimension vector refer gmmrbm vector work .
taggedpfig .
1 show block-diagram propos framework .
whole process divid three main stage detail follow section , correspond block fig .
1 .
first , gmm supervector built warp spectral featur given ubm , normal use ubm paramet .
second , fig .
1 .
block-diagram propos gmmrbm vector framework .
w b paramet univers rbm ( urbm ) , global mean , andh whiten matrix obtain background gmmrbm vector .
o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 19 taggedpbackground gmm supervector use train urbm option normal provid appropri transform matrix supervector propos low dimension vector .
third , given unseen gmm supervector paramet urbm , gmmrbm vector extract .
3.1 .
featur warp gmm supervector taggedpa shown block fig .
1 , input speech signal first character spectral featur vector .
after- ward , featur warp appli map distribut individu featur gaussian distribut time interv base cumul distribut function ( cdf ) .
method assum compon featur vector independ process individu separ stream .
cdf match perform slide window size n central frame window warp .
featur given window sort ascend order .
given compon valu x central frame rank r ( 1  r  n ) , warp valu bx satisfi ( pelecano sridharan , 2001 ; xiang et al. , 2002 ) , r  1=2=n  z bx 1 f zdz 4 left side approxim cdf valu x , right side cdf valu bx ; f ( z ) probabil- iti densiti function ( pdf ) standard normal distribut ( pelecano sridharan , 2001 ; xiang et al. , 2002 ) .
taggedpit shown featur warp help compens undesir session variabl speech signal ( pelecano sridharan , 2001 ) well gaussian featur distribut .
shown experiment result section featur warp high impact perform propos gmmrbm vector .
taggedpwarp featur model gmm adapt background model ( ubm ) .
mean vector adapt gmm stack build supervector .
order increas discrimin power , supervector model-norm use mean supervector diagon covari matrix ubm ( subm subm ) , 0  s1=2ubm s subm : 5 taggedpmodel normal help also zero mean unit varianc supervector prior assumpt train rbm real-valu input describ next section .
3.2 .
univers rbm train taggedpnorm supervector obtain background data use train urbm ( block b fig .
1 ) .
role urbm learn session speaker variabl among background supervector .
urbm 20 o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 taggedpparamet use transform unseen supervector lower dimension gmmrbm vector .
differ visibl hidden unit , activ function use train rbm ( hinton , 2012 ) .
sinc input applic real-valu supervector , visibl unit gaussian .
howev , sigmoid rectifi linear unit ( relu ) use hidden layer train urbm .
men- tion hinton ( 2012 ) prove experi , train rbm linear hidden visibl unit high unstabl .
therefor , pure linear hidden unit discard work .
given urbm paramet , reason transform function could use transform unseen supervector .
section , prob- lem use tradit sigmoid function activ transform first address potenti solut propos .
variant relu , refer variabl relu ( vrelu ) , propos applic .
shown section 4 propos vrelu suffer problem sigmoid relu .
taggedpfig .
2 show histogram posterior probabl first hidden unit urbm nonlinear transform .
urbm train tradit sigmoid activ function .
typic sigmoid function log sigmoid function , use ghahabi hernando ( 2015 ) , employ trans- format .
hidden unit show also similar behavior .
seen figur , posterior probabl distribut hidden unit sigmoid transform compress around zero far gaussian distribut ideal propos gmmrbm vector .
fact degrad perform signific .
fig .
2 .
histogram first hidden unit valu ( bottom ) ( left ) transform sigmoid log sigmoid function .
urbm train sigmoid hidden unit .
o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 21 taggedpth behavior better case log sigmoid function sinc part distribut transform lin- ear part function , still problem valu around zero transform .
although whiten transform posterior probabl afterward correct distribut extent , still per- formanc low specif sigmoid transform .
taggedpa potenti solut chang mean varianc posterior probabl distribut , transform , somehow fall activ nonlinear part transform function .
easili perform urbm paramet normal propos follow , cw  w max ; j jwijj 6 bbi  b bi  b  7 w visiblehidden connect weight , b vector hidden bias term , b two paramet control , respect , varianc mean posterior probabl distribut hidden unit nonlinear transform , wij ( , j ) element w , bi b ith element mean valu b , respect .
taggedpfig .
3 show chang b move distribut posterior probabl hidden unit desir interv .
show section 4 movement improv qualiti gmmrbm vector urbm train sigmoid hidden unit .
taggedpanoth altern unit relu .
relu kind linear unit negat valu zero .
urbm train relu input transform linear function train , none problem occur .
howev , show section 4 , problem distribut posterior probabl hidden unit asymmetr around mean valu , appropri plda score .
therefor , propos work variant relu , refer variabl relu ( vrelu ) .
vrelu , unit valu less threshold zero , rather fix threshold zero relu .
threshold random select normal distribut n ( 0 , 1 ) hidden unit input sampl train iter .
fact , vrelu defin follow , f x  x x > 0 x ; 2n0 ; 1  8 taggedpfig .
4 compar relu vrelu posit negat valu t. shown section 4 vrelu solv asymmetr problem posterior probabl distribut great extent , therefor , work better relu plda score use .
taggedpth full train algorithm rbm sigmoid hidden unit found ghahabi hernando ( 2015 ) .
follow , explain rbm train algorithm propos vrelu .
fig .
5 show train step .
connect weight w first random initi n ( 0 , 0.01 ) visibl hidden bias term ( b , respect ) set zero .
given normal supervector s0 , posterior probabl lower fig .
3 .
histogram posterior probabl first hidden unit urbm normal urbm ( two differ pair b ) nonlinear transform .
histogram obtain background dataset use develop .
fig .
4 .
comparison relu propos vrelu .
epoch , per hidden unit per input sampl , random select normal distribut zero mean unit varianc .
( b ) ( c ) show two exampl vrelu posit negat , respect .
22 o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 taggedpdimension hidden vector h calcul use eq .
( 8 ) .
afterward , supervector reconstruct given hidden unit valu .
reconstruct supervector s0r use recalcul posterior probabl hidden unit .
three step , mark fig .
5 , provid enough inform updat paramet network .
actual , train process base maximum likelihood criterion use stochast gradient descent algorithm ( hinton salakhutdinov , 2006 ; hinton et al. , 2006 ) , gradient estim approxim version contrast diverg ( cd ) algorithm call cd1 ( hinton et al. , 2006 ; hinton , 2012 ) .
taggedpth train process summar follow , taggedp initi network paramet ( w , b , ) taggedp cd1 step taggedp1 .
h  f bws0  9 taggedp2 .
0 r  aw th 10 taggedp3 .
hr  f bws0r   11 taggedp updat network paramet taggedp1 .
dw  h s0ht  s0rhtr  t 12 taggedp2 .
da  h s0  s0r   13 taggedp3 .
db  h h hr  14 taggedpwher h learn rate f ( . )
vrelu function calcul eq .
( 8 ) .
taggedpaddit , momentum factor use smooth updat , weight decay regular use penal larg weight .
paramet updat process minibatch updat procedur repeat minibatch process .
fig .
5 .
train univers rbm ( urbm ) given background gmm supervector .
o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 23 3.3 .
gmmrbm vector extract taggedpgiven gmm supervector block urbm paramet block b fig .
1 , gmmrbm vector extract block c follow , vr  ws1=2ubm s subm 15 taggedpa linear transform function use , hidden unit bias term b easili discard visiblehidden connect weight w use transform .
reformul eq .
( 15 ) base zeroth first order baumwelch statist , , vr  ws1=2ubm n 1uefu 16 relev factor map adapt also ad ton u .
taggedplik case i-vector , result gmmrbm vector mean normal whiten use mean vector whiten matrix obtain background data , h  v d  1=2v 17 h whiten matrix , v matrix eigenvector , diagon matrix correspond eigenvalu ,  small constant regular factor ad avoid larg valu practic .
3.4 .
comput load compar i-vector taggedpth comparison eq .
( 2 ) ( 16 ) impli clear gmmrbm vector extract need much less comput load .
compar comput load term number product oper requir extract i-vector gmmrbm vector size base eq .
( 2 ) ( 16 ) .
consid comput cost multipl two matric n   k order onmk matrix invers size n  n order on3 ; fact n u diagon ws1=2ubm eq .
( 16 ) ts 1 eq .
( 2 ) comput offlin , minimum comput load i-vector gmmrbm vector extract on3  2n2  2nm on 1m ; respect , n dimens i-vector/ gmmrbm vector size supervector .
taggedpfig .
6 compar minimum comput load extract i-vector gmmrbm vector differ valu n m. figur impli number oper requir extract gmmrbm vector 106  108 compar i-vector requir 108  1011 oper .
comput load higher import onlin applic frequenc vector extract high .
fig .
6 .
comparison number product oper requir extract i-vector gmmrbm vector term ( ) size i-vector/gmmrbm vector n ( b ) size supervector m. 24 o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 4 .
experiment result taggedpth detail databas , setup baselin propos approach , experiment result given section .
baselin system base convent i-vector score use either cosin plda techniqu .
propos gmmrbm vector build accord block-diagram fig .
1 .
effect featur warp , urbm normal , type activ transform function , well score combin cosin plda techniqu shown section .
4.1 .
setup , baselin , databas taggedptwo set databas use experi .
develop , core test condit nist 2006 sre evalu ( nist , 2006 ) use .
includ 816 target model 51,068 trial .
train test phase , durat speech signal approxim two minut .
background data includ 6063 speech file collect nist 2004 2005 sre corpora .
background data use train ubm , urbm , plda , whiten matric .
taggedpfor evalu , nist 2010 sre ( nist , 2010 ) , core test-common condit 5 , includ differ num- ber trial involv normal vocal effort convers telephon speech train test , use .
back- ground data collect form nist sre 20042008 includ 37,600 speech utter 18,140 signal label use plda train .
taggedpfrequ filter ( ff ) featur ( nadeu et al. , 2001 ) use experi .
like mel-frequ cepstral coeffici ( mfcc ) , ffs decorrel version log filter bank energi ( fbe ) ( nadeu et al. , 2001 ) .
shown ff featur achiev perform equal better mfccs ( nadeu et al. , 2001 ) .
featur extract everi 10 ms use 30 ms ham window .
number static ff featur 16 along delta ff delta log energi , 33-dimension featur vector built .
featur extract , speech signal sub- ject energy-bas silenc remov process .
featur extract , 3-second slide window use fea- ture warp .
taggedp open sourc softwar ( larcher et al. , 2013 ) use build i-vector baselin system cosin plda score techniqu employ .
dimens i-vector 400 plda size develop data 250 evalu 400 .
gender-independ ubm repres diagonal-covari 512-compon gmm .
taggedpin propos gmmrbm vector framework , gmms adapt ubm relev factor 16 .
mean vector adapt .
dimens supervector , therefor , 512  33 = 16,896 .
two urbm hidden layer size 400 8000 train creat gmmrbm vector .
bigger one train sigmoid activ function use compar result report ghahabi her- nando ( 2015 ) .
urbm hidden layer size 400 train sigmoid , relu , propos vrelu .
learn rate , number epoch , minibatch size , weight decay , momentum urbm , train vrelu , set 0.0014 , 40 , 50 , 2 103 ; 0.9 , respect .
taggedpperform evalu use equal error rate ( eer ) , minimum decis cost function ( mindcf ) calcul use cm  10 ; cfa  1 ; pt  0:01 develop experi ( nist , 2006 ) cm  1 ; cfa  1 ; pt  0:001 evalu experi ( nist , 2010 ) .
4.2 .
result taggedpa mention section 1 , one main differ work prior work ghahabi hernando ( 2015 ) discard whiten supervector level make use featur warp instead .
result report tabl 1 impli featur warp use , whiten supervector level help .
exact ghahabi hernando ( 2015 ) .
howev , featur vector warp , whiten supervector effect anymor .
moreov , time memori consum .
best result obtain featur warp use .
taggedpfig .
7 show histogram first compon gmmrbm vector obtain urbm , train sigmoid activ function .
howev , sigmoid , log sigmoid , linear transform function use vector extract .
histogram compon show similar behavior .
sigmoid log sigmoid tabl 1 effect featur warp whiten input gmm supervector propos gmmrbm framework .
number parenthes indic dimens gmmrbm vector .
result obtain develop databas cosin score .
input rbm output rbm raw featur warp featur eer mindcf eer mindcf whiten supervector gmmrbm vector ( 8000 ) 7.58 0.0346 6.90 0.0331 raw supervector gmmrbm vector ( 8000 ) 7.92 0.0379 6.89 0.0323 raw supervector gmmrbm vector ( 400 ) 10.45 0.0475 8.08 0.0383 o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 25 taggedptransform , histogram present urbm normal urbm paramet .
normaliza- tion paramet b eq .
( 6 ) ( 7 ) set 0.05 0.5 , respect .
move approxim posterior distribut interv 2 2 ( fig .
3 ) correspond activ nonlinear part sigmoid log sigmoid transform function .
sigmoid log sigmoid , urbm normal help gaussian-lik histogram .
shown later , increas perform gmmrbm vector urbm train sigmoid hidden unit .
taggedpfig .
8 show histogram gmmrbm vector urbm train relu vrelu linear transform use case .
figur impli histogram asymmetr case fig .
7 .
comparison histogram first compon background gmmrbm vector obtain sigmoid activ function transform function ( ) sigmoid , ( b ) log sigmoid , ( c ) linear .
fig .
8 .
comparison histogram first compon background gmmrbm vector obtain ( ) relu ( b ) propos vrelu activ function linear transform function .
26 o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 taggedprelu due train process hidden unit encourag posit valu .
hand , random threshold propos vrelu make possibl posit negat hidden valu train process .
improv histogram shown fig .
8 .
taggedpt 2 compar perform gmmrbm vector extract differ urbm transform function .
comparison base cosin plda score .
expect , wors result sigmoid hidden unit transform function .
urbm normal improv signific perfor- manc vector .
use log sigmoid perform better sigmoid discuss fig .
2 7 .
urbm normal improv also perform case amount improv much sigmoid transform .
urbm train sigmoid hidden unit paramet use linear transform input supervector , perform wors log sigmoid cosin scor- ing better plda score .
use relu train urbm linear function transform , keep perform good log sigmoid cosin score improv plda result obtain sigmoid urbm linear transform .
urbm train vrelu improv plda result slight .
show later vrelu work better relu unseen evalu set cosin plda score .
taggedpt 3 compar perform gmmrbm vector , obtain urbm train relu vrelu , tradit i-vector evalu set .
use propos vrelu show better perform use relu cosin plda score .
fact impli variabl threshold vrelu increas general power urbm addit correct histogram .
tabl , per- formanc best gmmrbm vector compar i-vector cosin plda score .
signific achiev sinc comput load gmmrbm vector extract much less tra- dition i-vector extract discuss section 3.4 .
end , best result achiev score fusion i-vector gmmrbm vector show 77.5 % 46.5 % relat improv term eer mindcf , respect , compar i-vector .
score fusion , bosari toolkit ( brummer villier , 2011 ) use .
fusion weight train develop set .
tabl 2 effect type hidden unit train urbm transform function extract gmmrbm vector .
result obtain develop databas vector dimens 400 .
vrelu refer propos variabl relu .
hidden unit transform cosin plda eer ( % ) mindcf eer ( % ) mindcf sigmoid sigmoid 13.55 0.0570 11.05 0.0517 sigmoid ( normal urbm ) 8.67 0.0407 6.08 0.0338 log sigmoid 8.08 0.0383 6.51 0.0316 log sigmoid ( normal urbm ) 7.85 0.0366 6.28 0.0317 linear 8.24 0.0382 5.86 0.0317 relu linear 7.82 0.0372 5.58 0.0305 vrelu linear 7.82 0.0373 5.52 0.0297 tabl 3 perform comparison propos gmmrbm vector convent i-vector evalu set core test condition-common 5 nist 2010 sre .
gmmrbm vector i-vector size 400 .
cosin plda eer ( % ) mindcf eer ( % ) mindcf [ 1 ] i-vector 6.270 0.05450 4.096 0.04993 [ 2 ] gmmrbm vector ( train relu ) 6.638 0.06228 4.517 0.05085 [ 3 ] gmmrbm vector ( train vrelu ) 6.497 0.06099 3.907 0.05184 fusion [ 1 ] [ 3 ] 5.791 0.05238 3.814 0.04673 o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 27 5 .
conclus taggedpw present work new vector represent speech text-independ speaker recognit .
gmm supervector transform univers rbm ( urbm ) lower dimension vector , refer gmmrbm vector .
role urbm learn total speaker session variabl among back- ground gmm supervector .
use differ hidden unit train urbm differ transform function vector extract investig .
variant linear rectifi unit ( relu ) , refer var- iabl relu ( vrelu ) , propos .
variabl threshold defin unit correct histogram gmmrbm vector lead higher general power urbm .
experiment result core test- common condit 5 nist 2010 sre show perform gmmrbm vector compar tradit i-vector cosin plda score much less comput load .
moreov , best result obtain score fusion gmmrbm vector i-vector .
acknowledg taggedpthi work fund spanish project deepvoic ( tec2015-69266-p ) european project camomil ( pcin-2013-067 ) .
would like thank miquel india help extract tradit i-vector nist sre 2010 data .
refer taggedpbahari , m.h. , mclaren , m.l. , hamm , h.v. , leeuwen , d.a.v. , 2012 .
age estim telephon speech use i-vector .
: proceed 2012 annual confer intern speech communic associ ( interspeech ) .
taggedpbrumm , n. , villier , e. , 2011 .
bosari toolkit user guid : theori , algorithm code binari classifi score process .
[ onlin ] .
avail- abl : https : //sites.google.com/site/bosaristoolkit/ .
taggedpcampbel , w.m. , 2014 .
use deep belief network vector-bas speaker recognit .
: proceed 2014 annual confer intern speech communic associ ( interspeech ) , pp .
676680 .
taggedpdahl , g. , yu , d. , deng , l. , acero , a. , 2012 .
context-depend pre-train deep neural network large-vocabulari speech recognit .
ieee tran .
audio speech lang .
process .
20 ( 1 ) , 3042 .
doi : 10.1109/tasl.2011.2134090 .
taggedpdehak , n. , dehak , r. , glass , j. , reynold , d. , kenni , p. , 2010 .
cosin similar score without score normal techniqu .
: proceed 2010 speaker languag recognit workshop ( odyssey ) .
taggedpdehak , n. , kenni , p. , dehak , r. , dumouchel , p. , ouellet , p. , 2011a .
front-end factor analysi speaker verif .
ieee tran .
audio speech lang .
process .
19 ( 4 ) , 788798 .
doi : 10.1109/tasl.2010.2064307 .
taggedpdehak , n. , torres-carrasquillo , p. , reynold , d. , dehak , r. , 2011b .
languag recognit via i-vector dimension reduct .
: proceed- ing 2011 annual confer intern speech communic associ ( interspeech ) .
cites , pp .
857860 .
taggedpgarcia-romero , d. , zhang , x. , mccree , a. , povey , d. , 2014 .
improv speaker recognit perform domain adapt challeng use deep neural network .
: proceed 2014 ieee spoken languag technolog workshop ( slt ) , pp .
378383 .
taggedpghahabi , o. , hernando , j. , 2014a .
deep belief network i-vector base speaker recognit .
: proceed 2014 ieee intern confer acoust , speech signal process ( icassp ) , pp .
17001704 .
taggedpghahabi , o. , hernando , j. , 2014b .
i-vector model deep belief network multi-sess speaker recognit .
: proceed 2014 speaker languag recognit workshop ( odyssey ) , pp .
305310 .
taggedpghahabi , o. , hernando , j. , 2015 .
restrict boltzmann machin supervector speaker recognit .
: proceed 2015 ieee interna- tional confer acoust , speech signal process ( icassp ) , pp .
48044808 .
taggedpghahabi , o. , hernando , j. , 2017 .
deep learn backend singl multisess i-vector speaker recognit .
ieee/acm tran .
audio speech lang .
process .
25 ( 4 ) , 807817 .
doi : 10.1109/taslp.2017.2661705 .
http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0001 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0001 https : //sites.google.com/site/bosaristoolkit/ http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0003 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0003 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0005 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0005 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0005 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0007 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0007 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0007 http : //dx.doi.org/10.1109/taslp.2017.2661705 http : //dx.doi.org/10.1109/taslp.2017.2661705 http : //dx.doi.org/10.1162/neco.2006.18.7.1527 http : //dx.doi.org/10.1162/neco.2006.18.7.1527 http : //dx.doi.org/10.1126/science.1127647 http : //dx.doi.org/10.1126/science.1127647 http : //dx.doi.org/10.1109/tasl.2011.2109382 http : //dx.doi.org/10.1109/tasl.2011.2109382 http : //dx.doi.org/10.1109/taslp.2017.2661705 28 o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 taggedphinton , g. , 2012 .
practic guid train restrict boltzmann machin .
neural network : trick trade .
lectur note comput scienc .
springer berlin heidelberg , pp .
599619 .
taggedphinton , g. , deng , l. , yu , d. , dahl , g. , moham , a. , jait , n. , senior , a. , vanhouck , v. , nguyen , p. , sainath , t. , 2012 .
deep neural network acoust model speech recognit : share view four research group .
ieee signal process .
mag .
29 ( 6 ) , 8297 .
taggedphinton , g. , osindero , s. , teh , y.-w. , 2006 .
fast learn algorithm deep belief net .
neural comput .
18 ( 7 ) , 15271554 .
doi : 10.1162/ neco.2006.18.7.1527 .
taggedphinton , g. , salakhutdinov , r. , 2006 .
reduc dimension data neural network .
scienc 313 ( 5786 ) , 504507 .
doi : 10.1126/sci- ence.1127647 .
taggedpkenni , p. , 2010 .
bayesian speaker verif heavi tail prior .
: proceed 2010 speaker languag recognit workshop ( odyssey ) .
taggedpkenni , p. , gupta , v. , stafylaki , t. , ouellet , p. , alam , j. , 2014 .
deep neural network extract baumwelch statist speaker recogni- tion .
: proceed 2014 speaker languag recognit workshop ( odyssey ) , pp .
293298 .
taggedplarch , a. , bonastr , j.-f. , fauv , b. , lee , k. , lvi , c. , li , h. , mason , j. , parfait , j.-y. , 2013 .
aliz 3.0 open sourc toolkit state-of-the-art speaker recognit .
: proceed 2013 annual confer intern speech communic associ ( interspeech ) , pp .
27682771 .
taggedplei , y. , scheffer , n. , ferr , l. , mclaren , m. , 2014 .
novel scheme speaker recognit use phonetically-awar deep neural network .
: proceed 2014 ieee intern confer acoust , speech signal process ( icassp ) .
taggedpliu , y. , qian , y. , chen , n. , fu , t. , zhang , y. , yu , k. , 2015 .
deep featur text-depend speaker verif .
speech commun .
73 , 113 .
taggedplozano-diez , a. , silnova , a. , matejka , p. , glembek , o. , plchot , o. , pesan , j. , burget , l. , gonzalez-rodriguez , j. , 2016 .
analysi optim bottleneck featur speaker recognit .
: proceed 2016 speaker languag recognit workshop ( odyssey ) , pp .
352 357 .
taggedpmclaren , m. , lei , y. , ferr , l. , 2015 .
advanc deep neural network approach speaker recognit .
: proceed 2015 ieee inter- nation confer acoust , speech signal process ( icassp ) .
taggedpmoham , a. , dahl , g. , hinton , g. , 2012 .
acoust model use deep belief network .
ieee tran .
audio speech lang .
process .
20 ( 1 ) , 14 22. doi : 10.1109/tasl.2011.2109382 .
taggedpmoham , a. , yu , d. , deng , l. , 2010 .
investig full-sequ train deep belief network speech recognit .
: proceed 2010 annual confer intern speech communic associ ( interspeech ) , pp .
28462849 .
taggedpnadeu , c. , macho , d. , hernando , j. , 2001 .
time frequenc filter filter-bank energi robust hmm speech recognit .
speech com- mun .
34 ( 12 ) , 93114 .
doi : 10.1016/s0167-6393 ( 00 ) 00048-0 .
taggedpnist , 2006 .
nist year 2006 speaker recognit evalu plan .
[ onlin ] .
avail : http : //www.nist.gov/speech/tests/spk/2006/index.htm .
taggedpnist , 2010 .
nist year 2010 speaker recognit evalu plan .
[ onlin ] .
avail : https : //www.nist.gov/itl/iad/mig/speaker_recognition_ evaluation_2010 .
taggedpnovoselov , s. , pekhovski , t. , simonchik , k. , shulipa , a. , 2014 .
rbm-plda subsystem nist i-vector challeng .
: proceed 2014 annual confer intern speech communic associ ( interspeech ) , pp .
378382 .
taggedppelecano , j. , sridharan , s. , 2001 .
featur warp robust speaker verif .
: proceed 2001 speaker languag recognit workshop ( odyssey ) .
crete , greec , pp .
213218 .
taggedpprinc , s. , elder , j. , 2007 .
probabilist linear discrimin analysi infer ident .
: proceed eleventh ieee interna- tional confer comput vision , ( iccv 2007 ) .
taggedprichardson , f. , reynold , d. , dehak , n. , 2015 .
deep neural network approach speaker languag recognit .
ieee signal process .
lett .
22 ( 10 ) , 16711675 .
taggedpsafari , p. , ghahabi , o. , hernando , j. , 2015 .
featur classif mean deep belief network speaker recognit .
: proceed 2015 european signal process confer ( eusipco ) , pp .
21622166 .
taggedpsafari , p. , ghahabi , o. , hernando , j. , 2016 .
featur speaker vector mean restrict boltzmann machin adapt .
: proceed- ing 2016 speaker languag recognit workshop ( odyssey ) , pp .
366371 .
taggedpsenior , a. , sak , h. , shafran , i. , 2015 .
context depend phone model lstm rnn acoust model .
: proceed 2015 ieee intern confer acoust , speech signal process ( icassp ) , pp .
45854589 .
taggedpsenoussaoui , m. , dehak , n. , kenni , p. , dehak , r. , dumouchel , p. , 2012 .
first attempt boltzmann machin speaker verif .
: pro- ceed 2012 speaker languag recognit workshop ( odyssey ) .
taggedpstafylaki , t. , kenni , p. , senoussaoui , m. , dumouchel , p. , 2012a .
plda use gaussian restrict boltzmann machin applic speaker verif .
: proceed 2012 annual confer intern speech communic associ ( interspeech ) .
taggedpstafylaki , t. , kenni , p. , senoussaoui , m. , dumouchel , p. , 2012b .
preliminari investig boltzmann machin classifi speaker recogni- tion .
: proceed 2012 speaker languag recognit workshop ( odyssey ) .
taggedpvariani , e. , lei , x. , mcdermott , e. , lopez moreno , i. , gonzalez-dominguez , j. , 2014 .
deep neural network small footprint text-depend speaker verif .
: proceed 2014 ieee intern confer acoust , speech signal process ( icassp ) , pp .
40524056 .
taggedpvasilakaki , v. , cumani , s. , lafac , p. , 2013 .
speaker recognit mean deep belief network .
: proceed 2013 biometr tech- nolog forens scienc , pp .
5257 .
taggedpxia , r. , liu , y. , 2012 .
use i-vector space model emot recognit .
: proceed 2012 annual confer intern speech communic associ ( interspeech ) , pp .
22272230 .
taggedpxiang , b. , chaudhari , u.v. , navrtil , j. , ramaswami , g.n. , gopinath , r.a. , 2002 .
short-tim gaussian robust speaker verif .
: proceed 2002 ieee intern confer acoust , speech signal process ( icassp ) , pp .
681684 .
http : //dx.doi.org/10.1016/s0167-6393 ( 00 ) 00048-0 http : //dx.doi.org/10.1016/s0167-6393 ( 00 ) 00048-0 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0014 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0014 http : //dx.doi.org/10.1162/neco.2006.18.7.1527 http : //dx.doi.org/10.1162/neco.2006.18.7.1527 http : //dx.doi.org/10.1126/science.1127647 http : //dx.doi.org/10.1126/science.1127647 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0017 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0017 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0018 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0018 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0019 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0019 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0019 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0020 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0020 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0021 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0022 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0022 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0022 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0023 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0023 http : //dx.doi.org/10.1109/tasl.2011.2109382 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0025 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0025 http : //dx.doi.org/10.1016/s0167-6393 ( 00 ) 00048-0 http : //www.nist.gov/speech/tests/spk/2006/index.htm https : //www.nist.gov/itl/iad/mig/speaker_recognition_evaluation_2010 https : //www.nist.gov/itl/iad/mig/speaker_recognition_evaluation_2010 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0029 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0029 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0030 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0030 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0031 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0031 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0032 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0032 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0033 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0033 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0034 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0034 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0035 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0035 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0036 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0036 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0037 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0037 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0038 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0038 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0039 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0039 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0039 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0040 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0040 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0041 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0041 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0042 http : //refhub.elsevier.com/s0885-2308 ( 16 ) 30292-3/sbref0042 omid ghahabi receiv m.sc .
degre electr engin shahid beheshti univers , tehran , iran , 2009 .
current work toward ph.d. degre technic univers catalonia ( upc ) , barcelona , spain .
2009 2011 , speech process group , research center intellig signal process , tehran , iran .
2011 2016 , research speech process group , signal theori communic depart , upc .
sinc late 2016 , eml european media laboratori gmbh , heidelberg , germani , o. ghahabi j. hernando / comput speech & languag 47 ( 2017 ) 1629 29 speech technologist .
research interest includ speaker languag recognit , speaker diarize , speech signal process , deep learn .
author coauthor sever journal confer paper topic .
member research center languag speech technolog applic , barcelona , spain .
javier hernando receiv m.s .
ph.d. degre telecommun engin technic univers catalonia ( upc ) , barcelona , spain , 1988 1993 , respect .
sinc 1988 , depart sig- nal theori communic , upc , current full professor director research center languag speech .
academ year 20022003 , visit research panason speech technolog laboratori , santa barbara , ca , usa .
led upc team sever european , spanish , catalan project .
research interest includ robust speech analysi , speech recognit , speaker verif local , oral dialogu , multimod interfac .
author coauthor 200 public book chapter , review articl , confer paper topic .
receiv 1993 extraordinari ph.d. award upc .
restrict boltzmann machin vector represent speech speaker recognit 1 .
introduct 2 .
convent i-vector 3 .
propos gmm-rbm vector 3.1 .
featur warp gmm supervector 3.2 .
univers rbm train 3.3 .
gmm-rbm vector extract 3.4 .
comput load compar i-vector 4 .
experiment result 4.1 .
setup , baselin , databas 4.2 .
result 5 .
conclus acknowledg refer
