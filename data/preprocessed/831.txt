distribut train strategi comput vision deep learn algorithm distribut gpu cluster sciencedirect avail onlin www.sciencedirect.com procedia comput scienc 108c ( 2017 ) 315324 1877-0509  2017 author .
publish elsevi b.v. peer-review respons scientif committe intern confer comput scienc 10.1016/j.procs.2017.05.074 intern confer comput scienc , icc 2017 , 12-14 june 2017 , zurich , switzerland 10.1016/j.procs.2017.05.074 1877-0509  2017 author .
publish elsevi b.v. peer-review respons scientif committe intern confer comput scienc space reserv procedia header , use distribut train strategi comput vision deep learn algorithm distribut gpu cluster vctor campo  , francesc sastr  , maurici yagu  , mriam bellver  , xavier giro-i-nieto  , jordi torr   barcelona supercomput center ( bsc )  universitat politecnica de catalunya { victor.campo , francesc.sastr , maurici.yagu , miriam.bellv , jordi.torr } @ bsc.es xavier.giro @ upc.edu abstract deep learn algorithm base success build high learn capac model million paramet tune data-driven fashion .
model train process million exampl , develop accur algorithm usual limit throughput comput devic train .
work , explor train state-of-the-art neural network comput vision parallel distribut gpu cluster .
effect distribut train process address two differ point view .
first , scalabl task perform distribut set analyz .
second , impact distribut train method final accuraci model studi .
keyword : distribut comput ; parallel system ; deep learn ; convolut neural network 1 introduct method base deep neural network establish state-of-the-art comput vision task [ 15 , 27 , 9 ] , machin translat [ 29 ] , speech generat [ 21 ] even defeat world champion game go [ 26 ] .
although develop algorithm span mani decad [ 16 ] , potenti unlock increas comput power specif acceler , e.g .
graphic process unit ( gpus ) , creation large-scal dataset [ 8 , 13 ] .
even use specif hardwar devic , train algorithm comput intens take day , even week , converg singl machin .
scale problem distribut set shorten train time becom crucial challeng research industri applic .
1 space reserv procedia header , use distribut train strategi comput vision deep learn algorithm distribut gpu cluster vctor campo  , francesc sastr  , maurici yagu  , mriam bellver  , xavier giro-i-nieto  , jordi torr   barcelona supercomput center ( bsc )  universitat politecnica de catalunya { victor.campo , francesc.sastr , maurici.yagu , miriam.bellv , jordi.torr } @ bsc.es xavier.giro @ upc.edu abstract deep learn algorithm base success build high learn capac model million paramet tune data-driven fashion .
model train process million exampl , develop accur algorithm usual limit throughput comput devic train .
work , explor train state-of-the-art neural network comput vision parallel distribut gpu cluster .
effect distribut train process address two differ point view .
first , scalabl task perform distribut set analyz .
second , impact distribut train method final accuraci model studi .
keyword : distribut comput ; parallel system ; deep learn ; convolut neural network 1 introduct method base deep neural network establish state-of-the-art comput vision task [ 15 , 27 , 9 ] , machin translat [ 29 ] , speech generat [ 21 ] even defeat world champion game go [ 26 ] .
although develop algorithm span mani decad [ 16 ] , potenti unlock increas comput power specif acceler , e.g .
graphic process unit ( gpus ) , creation large-scal dataset [ 8 , 13 ] .
even use specif hardwar devic , train algorithm comput intens take day , even week , converg singl machin .
scale problem distribut set shorten train time becom crucial challeng research industri applic .
1 space reserv procedia header , use distribut train strategi comput vision deep learn algorithm distribut gpu cluster vctor campo  , francesc sastr  , maurici yagu  , mriam bellver  , xavier giro-i-nieto  , jordi torr   barcelona supercomput center ( bsc )  universitat politecnica de catalunya { victor.campo , francesc.sastr , maurici.yagu , miriam.bellv , jordi.torr } @ bsc.es xavier.giro @ upc.edu abstract deep learn algorithm base success build high learn capac model million paramet tune data-driven fashion .
model train process million exampl , develop accur algorithm usual limit throughput comput devic train .
work , explor train state-of-the-art neural network comput vision parallel distribut gpu cluster .
effect distribut train process address two differ point view .
first , scalabl task perform distribut set analyz .
second , impact distribut train method final accuraci model studi .
keyword : distribut comput ; parallel system ; deep learn ; convolut neural network 1 introduct method base deep neural network establish state-of-the-art comput vision task [ 15 , 27 , 9 ] , machin translat [ 29 ] , speech generat [ 21 ] even defeat world champion game go [ 26 ] .
although develop algorithm span mani decad [ 16 ] , potenti unlock increas comput power specif acceler , e.g .
graphic process unit ( gpus ) , creation large-scal dataset [ 8 , 13 ] .
even use specif hardwar devic , train algorithm comput intens take day , even week , converg singl machin .
scale problem distribut set shorten train time becom crucial challeng research industri applic .
1 http : //crossmark.crossref.org/dialog/ ? doi=10.1016/j.procs.2017.05.074 & domain=pdf 316 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324distribut train strategi deep learn gpu cluster .
vctor campo et al .
affect comput [ 22 ] recent garner much research attent .
machin abl understand convey subject affect would lead better human-comput interact key field robot medicin .
despit success constrain environ emot understand facial express [ 18 ] , autom affect understand unconstrain domain remain open challeng still far task machin approach even surpass human perform .
work , focus detect adject noun pair ( anp ) use large-scal imag dataset collect flickr [ 12 ] .
mid-level represent , rise approach overcom affect gap low level imag featur high level affect semant , use train accur model visual sentiment analysi visual emot predict .
work , explor train convolut neural network ( cnns ) anp classif acceler distribut gpu cluster .
contribut three-fold : ( 1 ) studi trade-off final classif accuraci speedup analyz result learn hpc standpoint , ( 2 ) distribut train way make cluster resourc , leverag intra-nod inter-nod parallel , ( 3 ) propos modif distribut configur , order reduc resourc use train .
2 relat work massiv number convolut matrix multipl neural network led gpu implement cuda [ 20 ] effici , task specif primit use cudnn [ 7 ] .
earli deep learn framework caff [ 10 ] provid fast easi access primi- tive , initi design singl machin oper , without support distribut environ .
effort toward distribut former framework tradit hpc tool spark mpi result project sparknet [ 19 ] theano-mpi [ 17 ] .
nativ support distribut set includ recent framework tensorflow [ 2 ] mxnet [ 6 ] .
howev , scale train algorithm singl machin environ distribut set pose two main challeng .
comput perform stand- point , optim use resourc main goal , wherea learn side , final accuraci suffer drop compar singl machin counterpart .
consid task distribut convolut neural network ( cnns ) , specif type feed-forward neural network .
cnns compos seri layer appli specif oper input , e.g .
convolut , dot product pool , train minim cost object mean gradient descent backpropag batch data .
two main approach propos literatur [ 14 ] train cnns multi-gpu environ , either singl machin distribut set : model parallel data parallel .
model parallel split layer cnn among differ gpus , i.e .
gpu oper batch input data , appli differ oper , most use oper larg number paramet may fit gpu  memori .
hand , data parallel consist place replica model gpu , oper differ batch data .
model replica share paramet , method equival larger batch size .
modern cnn architectur aim reduc number paramet increas number layer , find bottleneck store intermedi activ memori .
unlik model parallel , data parallel introduc one synchron point regardless number gpus , thus reduc communic overhead make suitabl current cnn architectur .
besid , 2 distribut train strategi deep learn gpu cluster .
vctor campo et al .
balanc load gpus straightforward paradigm , would requir care tune specif cnn architectur number gpus model paral- lelism approach .
reason , consid multi-gpu data parallel singl machin distribut set .
3 dataset adject noun pair ( anp ) power mid-level represent [ 3 ] use affect relat task visual sentiment analysi emot recognit .
large-scal anp ontolog 12 differ languag , name multilingu visual sentiment ontolog ( mvso ) , collect jou et al .
[ 12 ] follow model deriv psycholog studi , plutchick  wheel emot [ 23 ] .
noun compon anp understood ground visual appear entiti , wherea adject polar content toward posit negat sentiment , emot [ 12 ] .
properti tri bridg affect gap low level imag featur high level affect semant , goe far beyond recogn main object imag .
wherea tradit object classif algorithm may rec- ogniz babi imag , finer-grain classif happi babi cri babi usual need fulli understand affect content convey imag .
captur sophist differ anp pose challeng task benefit leverag large-scal annot dataset mean high learn capac model [ 5 ] .
experi consid subset english partit mvso , tag-restrict subset , contain 1.2m sampl cover 1,200 differ anp .
sinc imag mvso download flickr automat annot use metadata , annot consid weak label , i.e .
label may match real content imag .
tag-pool subset contain sampl annot obtain tag flickr instead metadata , annot like match real ground truth .
4 cnn architectur sinc first success applic cnns large-scal visual recognit , design improv architectur improv classif perform focus increas depth , i.e .
number layer , keep even reduc number trainabl param- eter .
trend seen compar 8 layer alexnet [ 15 ] , first cnn-base method win imag larg scale visual recognit challeng ( ilsvrc ) , dozen , even hundr , layer residu net ( resnet ) [ 9 ] .
despit huge increas overal depth , resnet 50 layer rough half paramet alexnet .
howev , impact increas depth notori memori footprint deeper architec- ture , store intermedi result come output singl layer , thus benefit multi-gpu setup allow use larger batch size .
adopt resnet50 cnn [ 9 ] experi , architectur 50 layer map 224  224  3 input imag 1,200-dimension vector repres probabl distribut anp class dataset .
overal , model contain 25  106 single-precis floating-point paramet involv 4109 floating-point oper tune train .
import notic comput demand cnn , larger gain distribut train due amount time spent parallel comput respect ad communic overhead .
3 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324 317distribut train strategi deep learn gpu cluster .
vctor campo et al .
affect comput [ 22 ] recent garner much research attent .
machin abl understand convey subject affect would lead better human-comput interact key field robot medicin .
despit success constrain environ emot understand facial express [ 18 ] , autom affect understand unconstrain domain remain open challeng still far task machin approach even surpass human perform .
work , focus detect adject noun pair ( anp ) use large-scal imag dataset collect flickr [ 12 ] .
mid-level represent , rise approach overcom affect gap low level imag featur high level affect semant , use train accur model visual sentiment analysi visual emot predict .
work , explor train convolut neural network ( cnns ) anp classif acceler distribut gpu cluster .
contribut three-fold : ( 1 ) studi trade-off final classif accuraci speedup analyz result learn hpc standpoint , ( 2 ) distribut train way make cluster resourc , leverag intra-nod inter-nod parallel , ( 3 ) propos modif distribut configur , order reduc resourc use train .
2 relat work massiv number convolut matrix multipl neural network led gpu implement cuda [ 20 ] effici , task specif primit use cudnn [ 7 ] .
earli deep learn framework caff [ 10 ] provid fast easi access primi- tive , initi design singl machin oper , without support distribut environ .
effort toward distribut former framework tradit hpc tool spark mpi result project sparknet [ 19 ] theano-mpi [ 17 ] .
nativ support distribut set includ recent framework tensorflow [ 2 ] mxnet [ 6 ] .
howev , scale train algorithm singl machin environ distribut set pose two main challeng .
comput perform stand- point , optim use resourc main goal , wherea learn side , final accuraci suffer drop compar singl machin counterpart .
consid task distribut convolut neural network ( cnns ) , specif type feed-forward neural network .
cnns compos seri layer appli specif oper input , e.g .
convolut , dot product pool , train minim cost object mean gradient descent backpropag batch data .
two main approach propos literatur [ 14 ] train cnns multi-gpu environ , either singl machin distribut set : model parallel data parallel .
model parallel split layer cnn among differ gpus , i.e .
gpu oper batch input data , appli differ oper , most use oper larg number paramet may fit gpu  memori .
hand , data parallel consist place replica model gpu , oper differ batch data .
model replica share paramet , method equival larger batch size .
modern cnn architectur aim reduc number paramet increas number layer , find bottleneck store intermedi activ memori .
unlik model parallel , data parallel introduc one synchron point regardless number gpus , thus reduc communic overhead make suitabl current cnn architectur .
besid , 2 distribut train strategi deep learn gpu cluster .
vctor campo et al .
balanc load gpus straightforward paradigm , would requir care tune specif cnn architectur number gpus model paral- lelism approach .
reason , consid multi-gpu data parallel singl machin distribut set .
3 dataset adject noun pair ( anp ) power mid-level represent [ 3 ] use affect relat task visual sentiment analysi emot recognit .
large-scal anp ontolog 12 differ languag , name multilingu visual sentiment ontolog ( mvso ) , collect jou et al .
[ 12 ] follow model deriv psycholog studi , plutchick  wheel emot [ 23 ] .
noun compon anp understood ground visual appear entiti , wherea adject polar content toward posit negat sentiment , emot [ 12 ] .
properti tri bridg affect gap low level imag featur high level affect semant , goe far beyond recogn main object imag .
wherea tradit object classif algorithm may rec- ogniz babi imag , finer-grain classif happi babi cri babi usual need fulli understand affect content convey imag .
captur sophist differ anp pose challeng task benefit leverag large-scal annot dataset mean high learn capac model [ 5 ] .
experi consid subset english partit mvso , tag-restrict subset , contain 1.2m sampl cover 1,200 differ anp .
sinc imag mvso download flickr automat annot use metadata , annot consid weak label , i.e .
label may match real content imag .
tag-pool subset contain sampl annot obtain tag flickr instead metadata , annot like match real ground truth .
4 cnn architectur sinc first success applic cnns large-scal visual recognit , design improv architectur improv classif perform focus increas depth , i.e .
number layer , keep even reduc number trainabl param- eter .
trend seen compar 8 layer alexnet [ 15 ] , first cnn-base method win imag larg scale visual recognit challeng ( ilsvrc ) , dozen , even hundr , layer residu net ( resnet ) [ 9 ] .
despit huge increas overal depth , resnet 50 layer rough half paramet alexnet .
howev , impact increas depth notori memori footprint deeper architec- ture , store intermedi result come output singl layer , thus benefit multi-gpu setup allow use larger batch size .
adopt resnet50 cnn [ 9 ] experi , architectur 50 layer map 224  224  3 input imag 1,200-dimension vector repres probabl distribut anp class dataset .
overal , model contain 25  106 single-precis floating-point paramet involv 4109 floating-point oper tune train .
import notic comput demand cnn , larger gain distribut train due amount time spent parallel comput respect ad communic overhead .
3 318 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324distribut train strategi deep learn gpu cluster .
vctor campo et al .
cross-entropi output cnn ground truth , i.e .
real class distribut , use loss function togeth l2 regular term weight decay rate 104 .
paramet model tune minim former cost object use batch gradient descent .
5 distribut cnn train process train cnns batch gradient descent decompos two main step : forward backward pass net .
forward pass comput output batch data , error respect desir result calcul .
error , cost , differenti respect everi paramet cnn call backward pass .
final , result gradient use updat weight net .
step iter repeat converg , i.e .
local minima error function reach .
implement use data parallel paradigm , defin two kind node .
first , worker node replica model , oper separ batch data .
second , paramet server ( ps ) node store updat model paramet [ 1 ] .
essenc , worker receiv model paramet , comput batch data , send back gradient ps model updat order improv [ 4 ] .
howev , differ model updat polici chosen ps perform train synchron mode : case , ps wait worker node comput gradient respect data batch .
gradient receiv ps , appli current weight updat model sent back worker node .
method fast slowest node , updat perform worker node finish comput , may suffer unbalanc network speed cluster share user .
howev , faster converg achiev accur gradient estim obtain .
author [ 4 ] present altern strategi allevi slowest worker updat problem use backup worker .
asynchron mode : everi time ps receiv gradient worker , model paramet updat .
despit deliv enhanc throughput compar synchron counterpart , everi worker may oper slight differ version model , thus provid poorer gradient estim .
result , iter requir converg due stale gradient updat .
increas number worker may result throughput bottleneck communic ps , case ps need ad .
mix mode : mix mode appear trade-off adequ batch size through- put perform asynchron updat model paramet , use synchron averag gradient come subgroup worker .
larger learn rate use thank increas batch size , lead faster converg , reach throughput rate close asynchron mode .
strategi also reduc communic compar pure asynchron mode .
other : improv tradit gradient descent algorithm appli distribut set propos literatur [ 31 , 24 , 25 ] .
work , howev , focus scale problem singl node distribut set minim modif train algorithm .
4 distribut train strategi deep learn gpu cluster .
vctor campo et al .
6 experiment setup evalu experi gpu cluster , node equip 2 nvidia kepler k80 dual gpu card , 2 intel xeon e5-2630 8-core processor 128gb ram .
inter-nod communic perform trough 56gb/s infiniband network .
cnn ar- chitectur train implement tensorflow1 , run cuda 7.5 use cudnn 5.1.3 primit improv perform .
sinc train process need submit slurm workload manag , task distribut communic node achiev greasy2 .
unlik work worker defin singl gpu [ 4 , 11 ] , use avail gpus node defin singl worker .
given dual natur nvidia k80 card , four model replica place node .
follow mix approach synchron averag gradient model replica node communic ps , perform asynchron model updat .
setup offer two main advantag : ( 1 ) communic overhead reduc , singl collect gradient need exchang network set four model replica , ( 2 ) worker larger effect batch size , provid better gradient estim allow use larger learn rate faster converg .
loss function minim use rmsprop [ 28 ] per-paramet adapt learn rate optim method learn rate 0.1 , decay 0.9  = 1.0 .
worker effect batch size 128 sampl , i.e .
32 imag process time gpu .
prevent overfit , data augment consist random crop and/or horizont flip asynchron perform cpu previous batch process gpus .
cnn weight initi use model pre-train ilsvrc [ 8 ] , practic proven benefici even train large-scal dataset [ 30 ] .
previous public distribut train tensorflow [ 1 , 4 ] tend use differ server configur worker ps task .
given ps store updat model need gpu comput , cpu-on server use task .
hand , worker job involv matrix comput , server equip gpus use .
node cluster use experi gpu equip node , mean place ps worker differ node would result under-util gpu resourc .
studi impact share resourc ps worker compar former configur whether setup suitabl homogen cluster node equip gpu card .
7 result discuss 7.1 intra-nod gpu parallel first studi scalabl deploy multipl replica synchron model updat singl node gpu cluster .
due dual natur nvidia k80 card , four model replica deploy everi node .
ensur proper weight share model replica , variabl comput graph store ram , wherea gpu perform oper cnn differ batch data .
gradient comput replica averag perform weight updat , step becom synchron point graph .
1https : //www.tensorflow.org/ 2https : //github.com/jonarbo/greasi 5 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324 319distribut train strategi deep learn gpu cluster .
vctor campo et al .
cross-entropi output cnn ground truth , i.e .
real class distribut , use loss function togeth l2 regular term weight decay rate 104 .
paramet model tune minim former cost object use batch gradient descent .
5 distribut cnn train process train cnns batch gradient descent decompos two main step : forward backward pass net .
forward pass comput output batch data , error respect desir result calcul .
error , cost , differenti respect everi paramet cnn call backward pass .
final , result gradient use updat weight net .
step iter repeat converg , i.e .
local minima error function reach .
implement use data parallel paradigm , defin two kind node .
first , worker node replica model , oper separ batch data .
second , paramet server ( ps ) node store updat model paramet [ 1 ] .
essenc , worker receiv model paramet , comput batch data , send back gradient ps model updat order improv [ 4 ] .
howev , differ model updat polici chosen ps perform train synchron mode : case , ps wait worker node comput gradient respect data batch .
gradient receiv ps , appli current weight updat model sent back worker node .
method fast slowest node , updat perform worker node finish comput , may suffer unbalanc network speed cluster share user .
howev , faster converg achiev accur gradient estim obtain .
author [ 4 ] present altern strategi allevi slowest worker updat problem use backup worker .
asynchron mode : everi time ps receiv gradient worker , model paramet updat .
despit deliv enhanc throughput compar synchron counterpart , everi worker may oper slight differ version model , thus provid poorer gradient estim .
result , iter requir converg due stale gradient updat .
increas number worker may result throughput bottleneck communic ps , case ps need ad .
mix mode : mix mode appear trade-off adequ batch size through- put perform asynchron updat model paramet , use synchron averag gradient come subgroup worker .
larger learn rate use thank increas batch size , lead faster converg , reach throughput rate close asynchron mode .
strategi also reduc communic compar pure asynchron mode .
other : improv tradit gradient descent algorithm appli distribut set propos literatur [ 31 , 24 , 25 ] .
work , howev , focus scale problem singl node distribut set minim modif train algorithm .
4 distribut train strategi deep learn gpu cluster .
vctor campo et al .
6 experiment setup evalu experi gpu cluster , node equip 2 nvidia kepler k80 dual gpu card , 2 intel xeon e5-2630 8-core processor 128gb ram .
inter-nod communic perform trough 56gb/s infiniband network .
cnn ar- chitectur train implement tensorflow1 , run cuda 7.5 use cudnn 5.1.3 primit improv perform .
sinc train process need submit slurm workload manag , task distribut communic node achiev greasy2 .
unlik work worker defin singl gpu [ 4 , 11 ] , use avail gpus node defin singl worker .
given dual natur nvidia k80 card , four model replica place node .
follow mix approach synchron averag gradient model replica node communic ps , perform asynchron model updat .
setup offer two main advantag : ( 1 ) communic overhead reduc , singl collect gradient need exchang network set four model replica , ( 2 ) worker larger effect batch size , provid better gradient estim allow use larger learn rate faster converg .
loss function minim use rmsprop [ 28 ] per-paramet adapt learn rate optim method learn rate 0.1 , decay 0.9  = 1.0 .
worker effect batch size 128 sampl , i.e .
32 imag process time gpu .
prevent overfit , data augment consist random crop and/or horizont flip asynchron perform cpu previous batch process gpus .
cnn weight initi use model pre-train ilsvrc [ 8 ] , practic proven benefici even train large-scal dataset [ 30 ] .
previous public distribut train tensorflow [ 1 , 4 ] tend use differ server configur worker ps task .
given ps store updat model need gpu comput , cpu-on server use task .
hand , worker job involv matrix comput , server equip gpus use .
node cluster use experi gpu equip node , mean place ps worker differ node would result under-util gpu resourc .
studi impact share resourc ps worker compar former configur whether setup suitabl homogen cluster node equip gpu card .
7 result discuss 7.1 intra-nod gpu parallel first studi scalabl deploy multipl replica synchron model updat singl node gpu cluster .
due dual natur nvidia k80 card , four model replica deploy everi node .
ensur proper weight share model replica , variabl comput graph store ram , wherea gpu perform oper cnn differ batch data .
gradient comput replica averag perform weight updat , step becom synchron point graph .
1https : //www.tensorflow.org/ 2https : //github.com/jonarbo/greasi 5 320 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324distribut train strategi deep learn gpu cluster .
vctor campo et al .
figur 2a show throughput function number gpus .
observ speed train almost linear respect number gpus perform synchron updat , confirm optim polici intra-nod paralel .
7.2 throughput increas distribut train paramet server perform task requir gpus .
howev , given ho- mogeneus configur cluster experi perform , use node exclus ps would impli under-util resourc .
section , studi impact share resourc ps worker task .
tabl 1 show trade-off speedup resourc requir use 4 worker node 4 gpus , follow mix approach describ section 5 .
despit provid largest speedup , configur 3 dedic ps use requir 7 node instead 4 .
addit gpu-equip node avail , result figur 1 show fix number node , solut use worker provid largest speedup .
given figur , share resourc worker ps task emerg effici solut homogeneus gpu cluster one consid experi .
figur 1 : throughput comparison differ distribut setup .
set proper number paramet server ( ps ) key maxim throughput .
result tabl 2 figur 2b show throughput speedup close linear configur resourc share ps worker task .
figur demonstr scalabl propos approach constrain number node requir achiev .
7.3 converg speedup distribut train studi impact distribut train process , two main factor take account .
first , time requir model reach target loss valu , target function optim , determin speedup train process .
6 distribut train strategi deep learn gpu cluster .
vctor campo et al .
configur throughput speedup effici 1 node 124.18 img/sec - - 7 node ( 4 worker + 3 ps ) 396.62 img/sec 3.19 0.46 4 node ( 4 worker + 3 ps ) 374.73 img/sec 3.02 0.76 4 node ( 4 worker + 1 ps ) 292.22 img/sec 2.35 0.58 tabl 1 : comparison differ configur use 4 worker .
use dedic node paramet server slight improv throughput , involv much larger resourc util .
effici relat speedup number use node , show clear grade resourc exploit .
( ) ( b ) figur 2 : throughput speedup use differ number resourc .
( ) parallel speedup insid node use differ number gpus .
( b ) distribut speedup use node 4 gpus , use best configur figur 1 .
second , final accuraci determin asynchron updat affect optim cost function .
despit throughput increas close linear respect number node , figur 3 show time requir setup reach target loss valu benefit linear addit node .
result expect , sinc asynchron gradient descent method n worker model updat perform respect weight wich n1 configur throughput speedup effici 1 node 124.18 img/sec - - 2 node ( 2 worker + 1 ps ) 195.60 img/sec 1.58 0.79 4 node ( 4 worker + 3 ps ) 383.09 img/sec 3.09 0.77 8 node ( 8 worker + 7 ps ) 809.10 img/sec 6.52 0.82 tabl 2 : throughput achiev distribut configur .
speedup effici figur comput respect 1 node scenario .
7 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324 321distribut train strategi deep learn gpu cluster .
vctor campo et al .
figur 2a show throughput function number gpus .
observ speed train almost linear respect number gpus perform synchron updat , confirm optim polici intra-nod paralel .
7.2 throughput increas distribut train paramet server perform task requir gpus .
howev , given ho- mogeneus configur cluster experi perform , use node exclus ps would impli under-util resourc .
section , studi impact share resourc ps worker task .
tabl 1 show trade-off speedup resourc requir use 4 worker node 4 gpus , follow mix approach describ section 5 .
despit provid largest speedup , configur 3 dedic ps use requir 7 node instead 4 .
addit gpu-equip node avail , result figur 1 show fix number node , solut use worker provid largest speedup .
given figur , share resourc worker ps task emerg effici solut homogeneus gpu cluster one consid experi .
figur 1 : throughput comparison differ distribut setup .
set proper number paramet server ( ps ) key maxim throughput .
result tabl 2 figur 2b show throughput speedup close linear configur resourc share ps worker task .
figur demonstr scalabl propos approach constrain number node requir achiev .
7.3 converg speedup distribut train studi impact distribut train process , two main factor take account .
first , time requir model reach target loss valu , target function optim , determin speedup train process .
6 distribut train strategi deep learn gpu cluster .
vctor campo et al .
configur throughput speedup effici 1 node 124.18 img/sec - - 7 node ( 4 worker + 3 ps ) 396.62 img/sec 3.19 0.46 4 node ( 4 worker + 3 ps ) 374.73 img/sec 3.02 0.76 4 node ( 4 worker + 1 ps ) 292.22 img/sec 2.35 0.58 tabl 1 : comparison differ configur use 4 worker .
use dedic node paramet server slight improv throughput , involv much larger resourc util .
effici relat speedup number use node , show clear grade resourc exploit .
( ) ( b ) figur 2 : throughput speedup use differ number resourc .
( ) parallel speedup insid node use differ number gpus .
( b ) distribut speedup use node 4 gpus , use best configur figur 1 .
second , final accuraci determin asynchron updat affect optim cost function .
despit throughput increas close linear respect number node , figur 3 show time requir setup reach target loss valu benefit linear addit node .
result expect , sinc asynchron gradient descent method n worker model updat perform respect weight wich n1 configur throughput speedup effici 1 node 124.18 img/sec - - 2 node ( 2 worker + 1 ps ) 195.60 img/sec 1.58 0.79 4 node ( 4 worker + 3 ps ) 383.09 img/sec 3.09 0.77 8 node ( 8 worker + 7 ps ) 809.10 img/sec 6.52 0.82 tabl 2 : throughput achiev distribut configur .
speedup effici figur comput respect 1 node scenario .
7 322 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324distribut train strategi deep learn gpu cluster .
vctor campo et al .
step old averag .
final accuraci test set reach configur detail tabl 3 .
none distribut setup abl reach final accuraci singl node model , confirm stale gradient negat impact final minima reach model converg .
moreov , found keep similar throughput worker node critic factor success learn process , sinc worker constant behind rest node noth aggrav stale gradient problem .
worker ( gpus ) test accuraci time ( h ) speedup 1 node ( 4 ) 0.228 106.43 1.00 2 node ( 8 ) 0.217 62.78 1.69 4 node ( 16 ) 0.202 37.99 2.80 8 node ( 32 ) 0.217 22.50 4.73 tabl 3 : result test set differ distribut configur .
despit benefit larger throughput , setup node requir iter converg .
figur 3 : train loss evolut differ distribut configur .
node , faster target loss valu reach .
8 conclus futur work distribut train strategi deep learn architectur becom import size dataset increas .
allow research receiv earlier feedback idea increas pace algorithm develop , thus understand best practic distribut train model key research area .
work , studi adapt train algorithm avail hardwar resourc order acceler train cnn homogeneus gpu cluster .
first , show close linear speedup achiev intra-nod parallel .
base result , develop mix approach effici leverag amount inter-nod communic reduc compar pure asynchron polici .
proper tune number paramet server configur , method yield import speedup number sampl per second process system even setup minimum hardwar overhead .
spite good scalabl demonstr term throughput , configur node requir addit train step reach target loss valu , although increas throughput compens issu still reduc train time consider .
8 distribut train strategi deep learn gpu cluster .
vctor campo et al .
drawback becom import increas number node , result suggest differ strategi employ high distribut set dozen node .
futur work compris two main research line .
first , develop tool gain insight perform individu compon help detect bottleneck push even scalabl system .
hand , plan implement evalu pure synchron gradient descent strategi .
despit solv stale gradient problem , overal throughput method determin slowest worker , thus less effici mix approach propos work use backup worker might option achiev great accuraci maintain perform .
besid , increas effect batch size may negat impact general capabl model , effect requir evalu experiment .
acknowledg work partial support spanish ministri economi competit contract tin2012-34557 , bsc-cns severo ochoa program ( sev-2011-00067 ) , sgr programm ( 2014-sgr-1051 2014-sgr-1421 ) catalan govern framework project biggraph tec2013-43935-r , fund spanish ministerio de economia competitividad european region develop fund ( erdf ) .
also would like thank technic support team barcelona supercomput center ( bsc ) especi carlo tripiana .
refer [ 1 ] m. abadi , p. barham , j. chen , z. chen , a. davi , j .
dean , m. devin , s. ghemawat , g. irv , m. isard , m. kudlur , j. levenberg , r. monga , s. moor , d. g. murray , b. steiner , p. tucker , v. vasudevan , p. warden , m. wick , y. yu , x. zheng .
tensorflow : system large-scal machin learn .
arxiv e-print , may 2016 .
[ 2 ] martn abadi , ashish agarw , paul barham , eugen brevdo , zhifeng chen , craig citro , greg corrado , andi davi , jeffrey dean , matthieu devin , et al .
tensorflow : large-scal machin learn heterogen system .
2015 .
[ 3 ] damian borth , rongrong ji , tao chen , thoma breuel , shih-fu chang .
large-scal visual sentiment ontolog detector use adject noun pair .
acm mm , 2013 .
[ 4 ] jianmin chen , xinghao pan , rajat monga , sami bengio , rafal jozefowicz .
revisit dis- tribut synchron sgd .
iclr 2017 confer submiss , 2016 .
[ 5 ] tao chen , damian borth , trevor darrel , shih-fu chang .
deepsentibank : visual sentiment concept classif deep convolut neural network .
arxiv:1410.8586 , 2014 .
[ 6 ] tianqi chen , mu li , yutian li , min lin , naiyan wang , minji wang , tianjun xiao , bing xu , chiyuan zhang , zheng zhang .
mxnet : flexibl effici machin learn librari heterogen distribut system .
arxiv preprint arxiv:1512.01274 , 2015 .
[ 7 ] sharan chetlur , cliff woolley , philipp vandermersch , jonathan cohen , john tran , bryan catanzaro , evan shelham .
cudnn : effici primit deep learn .
arxiv preprint arxiv:1410.0759 , 2014 .
[ 8 ] jia deng , wei dong , richard socher , li-jia li , kai li , li fei-fei .
imagenet : large-scal hierarch imag databas .
cvpr , 2009 .
[ 9 ] kaim , xiangyu zhang , shaoq ren , jian sun .
deep residu learn imag recognit .
cvpr , 2016 .
9 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324 323distribut train strategi deep learn gpu cluster .
vctor campo et al .
step old averag .
final accuraci test set reach configur detail tabl 3 .
none distribut setup abl reach final accuraci singl node model , confirm stale gradient negat impact final minima reach model converg .
moreov , found keep similar throughput worker node critic factor success learn process , sinc worker constant behind rest node noth aggrav stale gradient problem .
worker ( gpus ) test accuraci time ( h ) speedup 1 node ( 4 ) 0.228 106.43 1.00 2 node ( 8 ) 0.217 62.78 1.69 4 node ( 16 ) 0.202 37.99 2.80 8 node ( 32 ) 0.217 22.50 4.73 tabl 3 : result test set differ distribut configur .
despit benefit larger throughput , setup node requir iter converg .
figur 3 : train loss evolut differ distribut configur .
node , faster target loss valu reach .
8 conclus futur work distribut train strategi deep learn architectur becom import size dataset increas .
allow research receiv earlier feedback idea increas pace algorithm develop , thus understand best practic distribut train model key research area .
work , studi adapt train algorithm avail hardwar resourc order acceler train cnn homogeneus gpu cluster .
first , show close linear speedup achiev intra-nod parallel .
base result , develop mix approach effici leverag amount inter-nod communic reduc compar pure asynchron polici .
proper tune number paramet server configur , method yield import speedup number sampl per second process system even setup minimum hardwar overhead .
spite good scalabl demonstr term throughput , configur node requir addit train step reach target loss valu , although increas throughput compens issu still reduc train time consider .
8 distribut train strategi deep learn gpu cluster .
vctor campo et al .
drawback becom import increas number node , result suggest differ strategi employ high distribut set dozen node .
futur work compris two main research line .
first , develop tool gain insight perform individu compon help detect bottleneck push even scalabl system .
hand , plan implement evalu pure synchron gradient descent strategi .
despit solv stale gradient problem , overal throughput method determin slowest worker , thus less effici mix approach propos work use backup worker might option achiev great accuraci maintain perform .
besid , increas effect batch size may negat impact general capabl model , effect requir evalu experiment .
acknowledg work partial support spanish ministri economi competit contract tin2012-34557 , bsc-cns severo ochoa program ( sev-2011-00067 ) , sgr programm ( 2014-sgr-1051 2014-sgr-1421 ) catalan govern framework project biggraph tec2013-43935-r , fund spanish ministerio de economia competitividad european region develop fund ( erdf ) .
also would like thank technic support team barcelona supercomput center ( bsc ) especi carlo tripiana .
refer [ 1 ] m. abadi , p. barham , j. chen , z. chen , a. davi , j .
dean , m. devin , s. ghemawat , g. irv , m. isard , m. kudlur , j. levenberg , r. monga , s. moor , d. g. murray , b. steiner , p. tucker , v. vasudevan , p. warden , m. wick , y. yu , x. zheng .
tensorflow : system large-scal machin learn .
arxiv e-print , may 2016 .
[ 2 ] martn abadi , ashish agarw , paul barham , eugen brevdo , zhifeng chen , craig citro , greg corrado , andi davi , jeffrey dean , matthieu devin , et al .
tensorflow : large-scal machin learn heterogen system .
2015 .
[ 3 ] damian borth , rongrong ji , tao chen , thoma breuel , shih-fu chang .
large-scal visual sentiment ontolog detector use adject noun pair .
acm mm , 2013 .
[ 4 ] jianmin chen , xinghao pan , rajat monga , sami bengio , rafal jozefowicz .
revisit dis- tribut synchron sgd .
iclr 2017 confer submiss , 2016 .
[ 5 ] tao chen , damian borth , trevor darrel , shih-fu chang .
deepsentibank : visual sentiment concept classif deep convolut neural network .
arxiv:1410.8586 , 2014 .
[ 6 ] tianqi chen , mu li , yutian li , min lin , naiyan wang , minji wang , tianjun xiao , bing xu , chiyuan zhang , zheng zhang .
mxnet : flexibl effici machin learn librari heterogen distribut system .
arxiv preprint arxiv:1512.01274 , 2015 .
[ 7 ] sharan chetlur , cliff woolley , philipp vandermersch , jonathan cohen , john tran , bryan catanzaro , evan shelham .
cudnn : effici primit deep learn .
arxiv preprint arxiv:1410.0759 , 2014 .
[ 8 ] jia deng , wei dong , richard socher , li-jia li , kai li , li fei-fei .
imagenet : large-scal hierarch imag databas .
cvpr , 2009 .
[ 9 ] kaim , xiangyu zhang , shaoq ren , jian sun .
deep residu learn imag recognit .
cvpr , 2016 .
9 324 vctor campo et al .
/ procedia comput scienc 108c ( 2017 ) 315324distribut train strategi deep learn gpu cluster .
vctor campo et al .
[ 10 ] yangq jia , evan shelham , jeff donahu , sergey karayev , jonathan long , ross girshick , ser- gio guadarrama , trevor darrel .
caff : convolut architectur fast featur embed .
acm mm , 2014 .
[ 11 ] peter h jin , qiaochu yuan , forrest iandola , kurt keutzer .
scale distribut deep learn ?
arxiv preprint arxiv:1611.04581 , 2016 .
[ 12 ] brendan jou , tao chen , nikolao pappa , miriam redi , mercan topkara , shih-fu chang .
visual affect around world : large-scal multilingu visual sentiment ontolog .
acm mm .
[ 13 ] andrej karpathi , georg toderici , sanketh shetti , thoma leung , rahul sukthankar , li fei- fei .
large-scal video classif convolut neural network .
cvpr , 2014 .
[ 14 ] alex krizhevski .
one weird trick parallel convolut neural network .
arxiv preprint arxiv:1404.5997 , 2014 .
[ 15 ] alex krizhevski , ilya sutskev , geoffrey e. hinton .
imagenet classif deep con- volut neural network .
nip , 2012 .
[ 16 ] yann lecun , leon bottou , yoshua bengio , patrick haffner .
gradient-bas learn appli document recognit .
proceed ieee , 86 ( 11 ) , 1998 .
[ 17 ] , fei mao , graham w taylor .
theano-mpi : theano-bas distribut train frame- work .
arxiv preprint arxiv:1605.08325 , 2016 .
[ 18 ] daniel mcduff , rana el kalioubi , jeffrey f cohn , rosalind w picard .
predict ad like purchas intent : large-scal analysi facial respons ad .
ieee transact affect comput , 6 ( 3 ) , 2015 .
[ 19 ] philipp moritz , robert nishihara , ion stoica , michael jordan .
sparknet : train deep network spark .
arxiv preprint arxiv:1511.06051 , 2015 .
[ 20 ] cuda nvidia .
comput unifi devic architectur program guid .
2007 .
[ 21 ] aaron van den oord , sander dieleman , heiga zen , karen simonyan , oriol vinyal , alex grave , nal kalchbrenn , andrew senior , koray kavukcuoglu .
wavenet : generat model raw audio .
arxiv preprint arxiv:1609.03499 , 2016 .
[ 22 ] rosalind w. picard .
affect comput , volum 252 .
mit press cambridg , 1997 .
[ 23 ] robert plutchik .
emot : psychoevolutionari synthesi .
harper & row , 1980 .
[ 24 ] sundhar ram , angelia nedic , venugop v veerav .
asynchron gossip algorithm stochast optim .
gamenet , 2009 .
[ 25 ] frank seid , hao fu , jasha droppo , gang li , dong yu .
1-bit stochast gradient descent applic data-parallel distribut train speech dnns .
interspeech 2014 .
[ 26 ] david silver , aja huang , chris j maddison , arthur guez , laurent sifr , georg van den driess- che , julian schrittwies , ioanni antonoglou , veda panneershelvam , marc lanctot , et al .
mas- tere game go deep neural network tree search .
natur , 529 ( 7587 ) , 2016 .
[ 27 ] christian szegedi , wei liu , yangq jia , pierr sermanet , scott reed , dragomir anguelov , dumitru erhan , vincent vanhouck , andrew rabinovich .
go deeper convolut .
cvpr , 2015 .
[ 28 ] tijmen tieleman geoffrey hinton .
lectur 6.5-rmsprop : divid gradient run averag recent magnitud .
coursera : neural network machin learn , 4 , 2012 .
[ 29 ] yonghui wu , mike schuster , zhifeng chen , quoc v le , mohammad norouzi , wolfgang macherey , maxim krikun , yuan cao , qin gao , klaus macherey , et al .
googl  neural machin trans- lation system : bridg gap human machin translat .
arxiv preprint arxiv:1609.08144 , 2016 .
[ 30 ] jason yosinski , jeff clune , yoshua bengio , hod lipson .
transfer featur deep neural network ?
nip , 2014 .
[ 31 ] sixin zhang , anna e choromanska , yann lecun .
deep learn elast averag sgd .
nip , 2015 .
10
