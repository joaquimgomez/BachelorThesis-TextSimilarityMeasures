januari 28, 2015 journal experiment & theoret artifici intellig main appear journal experiment & theoret artifici intellig vol. 00, no. 00, month 20xx, 116 approxim polici iter regular bellman residu minim g. espositoa m. martina universitat politecnica catalunya, barcelona, spain; (v1.0 releas octob 2014) paper present approxim polici iter (api) method call apibrm effect implement increment support vector regress (svr) approxim valu function abl gener continu (or large) space reinforc learn (rl) problems. rl ia methodolog abl solv complex uncertain decis problem usual model markov decis problem (mdp). api brm formal non-parametr regular problem base outcom bellman residu minim (brm) abl minim varianc problem. apibrm increment appli rl on-lin agent interact framework. base non-parametr svr api brm abl global solut problem converg guarante optim solution. valu function defin optim polici specifi total reward agent expect current state take action. agent us valu function choos action take. experiment evid perform known rl benchmark presented. keywords: reinforc learning, support vector machine, approxim polici iteration, regularization, regress 1. introduct rl autonom agent interact environ learn correct action situat order reach goal. rl give method abl solv difficult uncertain sequenti decis problems, challeng real-world applications. rl problem model mdp, deepli studi literature. main particular rl algorithm respect approach mdp rl agent learn optim polici experi know paramet mdp. usual optim polici valu function defin specifi total reward agent expect current state take action. agent choos action valu function. want handl problem continu larg state spaces, gener used. gener properti rl algorithm import factor determin predict perform cases. main problem gener approxim rl converg guarante qualiti solution. parametr method consid literature, show advantag fast easi learn mechanisms. however, present inher problem case solut problem express given architectur number chosen parameters. candid non correspond author. email: 1 januari 28, 2015 journal experiment & theoret artifici intellig main parametr valu function approxim svr known good properti gener ability. svr convex optim problem, suffer sub-optimality, find global optim solut approxim problem. non parametr learn method, abl automat adapt complex problem. non parametr method approxim rl allow choos capac approxim function space mean fix kernel function number support determin need order requir solution. algorithm present paper instanc api. like polici iteration, algorithm repeatedli comput evalu function polici previou step us evalu function comput improv policy. order avoid need learn model, action valu function computed, make polici improv step simple, like least-squar polici iter (lspi) algorithm (lagoudaki parr, 2003). lspi reli squar tempor differ learn (lstd) build algorithm brm. major novelti algorithm base exact increment svr context brm approxim polici iteration. idea bellman residu polici iter goe (baird, 1995) propos comput approxim state valu function given model finite-st action mdp quadrat loss function. small bellman error yield good approxim polici evalu function, turn impli good final performance. major obstacl brm learn model sampl base approxim squar loss function base data singl trajectori behavior policy, unbias estim bellman residual. algorithm insensit loss function solv problem. main condit converg method ask trajectori suffici repres rapidli mixing. requir state trajectori follow stationari distribution. mix condit essenti effici learn particular us exponenti mix condition. 2. background notat space -algebra defin m() set probabl measur b() space bound measur function w.r.t. b(, l) space bound measur function bound 0 < l <. introduc definit 2.1: (continu state mdp) continu state finit action dis- count mdp defin tupl (s,a, p, ), follow definit contents: s measur state space st s denot state agent time t. finit set avail action denot action agent perform time t. p : s am(r s) map evalu (s, a) s give distribut r s denot p (r, s |s, a). margin p defin p(|s, a) = r p (dr, |s, a) denot probabl end state s perform action state s (transit probability) r(|s, a) = s p (, ds |s, a) reward function denot expect reward agent transit state s state s perform action a. immedi expect reward defin r(s, a) = r rr(dr|s, a) = e[r|s, a] [0, 1] discount factor. 2 januari 28, 2015 journal experiment & theoret artifici intellig main stage t action select agent control process respons pair (rt, s t) drawn distribut p (r, s |st, at) i. (rt, s t) p (r, s |st, at) rt reward agent receiv s t mdp state. procedur continu lead random trajectori t = {s1, a1, r1, s2, a2, r2, ...} denot space possibl trajectories. mdp agent consist mainli action select polici = (st). stationari stochast polici map state distribut action space t : s m(a) t(a|s) denot probabl agent select action perform state s time t. us (s) refer probabl distribut probabl mass function action state s. stochast polici call soft commit singl action state. (a|s) stand probabl soft polici choos action state s. greedi polici soft polici 0 1 pick determinist particular action probabl 1 uniformli random action probabl . us (|s) indic action chosen accord probabl function state s. definit 2.2: (valu function) agent follow polici consid sequenc reward {rt : t 1} mdp start state ac- tion (s1, a1) (s, a) m( a) action valu function q defin q(s, a) = e{ t=1 t1rt| s1 = s, a1 = a, } optim action valu q(s, a) = sup q (s, a) (s, a) s a. polici optim achiev best valu state polici = (, q) greedi w.r.t. action valu function q s s choos (s) = arg maxaa q(s, a). definit 2.3: (bellman operator) bellman oper defin t : b( a) b( a) action valu function defin (t q)(s, a) = r(s, a) + p(d |s, a)q( , (s )) = e[r + (a |s )q( , )] b() repres space bound measur functions. given polici fix point bellman oper- ator action valu function q = t q. bellman optim oper defin action valu function (t q)(s, a) = r(s, a) + p(d |s, a) maxaaq( , a) definit 2.4: (reproduc kernel hilbert spaces) consid subset measur function f : r subset vector valu measur function f |a| : sa r|a| f |a| = {(q1, ..., q|a|) : qi f , = 1, ..., |a|} call hypothesi space h chosen reproduc kernel hilbert space (rkhs) h : s r defin s inner product , character symmetr posit definit function : (s a) (s a) r call reproduc kernel, continu sa (s, a) sa follow reproduc properti holds: q(s, a) = q(), (, s, a)h (, s, a) h assum measur function space f |a| = h. h closur linear span set function span = {(s, a) = (, s, a) (s, a) s a} consid map : s csa denot function assign valu (st, at, s, a) (st, at) s a. kernel function (, ) plai import role kernel-bas learn method map element space input pattern real number taken state space mdp, thought similar measur input space. deriv kernel methods, kernel function aris natur inner product high-dimension featur space. kernel satisfi import properti inner product: symmetr posit semidefinite, mean associ gram matrix kij = (si, ai, sj , aj) posit semidefinite. kernel satisfi properti said admissible. import aspect method solv rl problem wai data collect processed. data 3 januari 28, 2015 journal experiment & theoret artifici intellig main collect set categor onlin offlin data process method categor batch incremental. onlin sampl set agent choos action sequenc t(|st) directli influenc data stream gener dn = {(s1, a1, s 1, r1), ..., (sn, an, s n, rn)} assum stationari gener non-i.i.d. process. stochast function t(|s), evalu state s dn defin stochast process n = {1(| 1), ..., n(| n)}. assum control mix condit non-i.i.d. process, allow gener strictli i.i.d. scenarios. defin order multiset {(s1, a1), ..., (sn, an)} {(r1, s 1), ..., (r1, s 1)} = bt(st) st s(s) s(s) m(s) (rt, s t) p (r, s |st, at). b behavior stationari polici produc (st, at) (s, a) (s, a) m( a) result state action distribution. collect data dn defin empir oper thought empir approxim true operators. definit 2.5: (empir bellman operators) given polici consid dn data set empir bellman oper defin (t q)(st, at) = rt + (a |s t)q( t, ) empir bellman optim oper t : zn rn defin (t q)(st, at) = rt + maxaaq( t, ). provid unbias estim bellman oper given polici fix bound measur de- terminist function q : s r hold e[t q(st, at)|st, at] = t q(st, at) e[t q(st, at)|st, at] = t q(st, at) 1 t n. us symbol en[f ] = 1 n n t=1 f(zt) shorthand empir version expect oper appli function f(z) data set zn = {z1, ..., zn}. 3. approxim polici iter bellman residu minim polici iter method discov optim polici given mdp, pro- vide iter procedur polici space. pi discov optim polici gener sequenc monoton improv policies. iter consist phases: polici evalu comput action valu function qk current polici k solv linear bellman equat polici improv defin improv greedi polici k+1 q k k+1 = arg maxaaqk(s, a). valu function qk typic chosen qk t kqk, i.e., approxim fix point t k . polici k+1 good k bet- ter. step repeat chang polici case iter converg optim policy, surprisingli small number iterations. guarante converg polici iter optim polici reli heavili tabular represent valu function, exact solut bellman equations, tabular represent policy. exact represent method impract larg state action spaces. cases, approxim method used. approxim polici iter framework introduc represent valu function policy. crucial factor success approxim algorithm choic approxim architectur form polici iter known approxim polici iter (api). wai implement api brm case api proce iter k evalu k choos qk bellman residu br k = |qk t kqk| small (i.e. approxim fix point t k). api calcul k+1 = (, qk) produc sequenc q0 1 q1 .... sequenc {qk}k1k=0 bellman residu (br) polici approxim error (ae) defin iter 4 januari 28, 2015 journal experiment & theoret artifici intellig main brk = qk t kqk ae k = qk q k . brm minim bellman error (be) bellman residu q given distribut input data given loss function ` defin lbrm`(q, ) = `(q(s, a) t q(s, a))d(s, a) sampl data setdn polici process n tri evalu empir estim lbrm`(q,n, dn) = en[`(q(st, at)t q(st, at))]. brm problem solv contest api method reproduc kernel hilbert space hypothesi space take linear combin form q(s, a) = n t=1 t(st, at, s, a), t r. accordingli regular brm problem written q = arg min qh {lbrm`(q,n, dn) + q 2 h} standard brm algorithm (baird, 1995) us squar loss function approxim action valu function data. unfortun approach bring bias estim overcom problem common suggest us uncorrelated, doubl sampl data sets. 4. bellman residu minim svr introduc method aim solv brm problem svr. con- sider bellman residu br(s, a) = q t q = qr (s, a) r(s, a) approx- imat function qr (s, a) = q(s, a) p(d |s, a) (a |s )q( , ) = e[q(s, a) (a |s )q( , )|s, a] = e[ qr (s, a, s )|s, a] r(s, a) = e[r|s, a] brm insensit loss function `(q r r) = max(0, |qr r| ) written lbrm(q, ) = e[`(qr r)] = e[`(q t q)]. data set n, dn empir estim be- comes: lbrm(q,n, dn) = en[`(q r r)] = en[`(q t q)] qr (st, at, s t) = q(st, at) ta t(a t| t)q( t, t). brm optim problem q = arg minqh{ lbrm(q,n, dn) + q2h } regular term us norm hilbert space h. brm show remark sparsiti properti solut essenti reli train support vectors. lbrm(q,n, dn) unbias estim lbrm(q, ) result evalu expect empir loss e[lbrm(q,n, dn)|n, dn] = e[en[`(qrr)]|n, dn] = e[en[`(qt q)]|n, dn] e[ `(q r r)] = lbrm(q, ) jensen inequ `(e[ x]) e[ `(x)] hold convex function ` qr (s, a) = e[q r (s, a, s )|s, a] r(s, a) = e[r|s, a] t q = e[t q]. practic empir estim bias slack present i.e. error regress function fix thresh- old . unbias error contain resolut tube svr. choic svr paramet c give wai control effect. 5. api brm dual batch solut consid subset observ sampl dn express approxim valu function linear architectur q(s, a) = (s, a),w+ b w = (w1, ..., wd)t 5 januari 28, 2015 journal experiment & theoret artifici intellig main weight vector (s, a) = (1(s, a), ..., d(s, a)) t featur vector point (s, a) build kernel function (st, at, s, a) = (st, at),(s, a). action valu function belong hilbert space q h weight vector w h. represent theorem know function h express linear combin element span span = {(s, a) = (, s, a) (s, a) s a} express q(s, a) = t t(s, a, st, at). definit bellman oper t q bellman residu train point fix polici write: br(st, at) = q(st, at) t q(st, at) = qr (st, at) r(st, at) = q(st, at) p(d |st, at) (a |s ) q( , ) r(st, at) substitut function form q yield br(st, at) = (st, at),w p(d |st, at) (a |s ) (s , ),w + (1 )b r(st, at) express bellman residu weight w featur map (). polici mdp dynam includ hilbert space h. altern wai express bell- man residu trough combin term bellman featur map- ping (st, at) = (st, at) p(d |st, at) (a |s )( , ) take ac- count structur mdp dynamics. bellman residu express br(st, at) = (st, at),w + (1 )b r(st, at) bellman featur vector (st, at) build bellman kernel (st, at, s, a) = (st, at),(s, a). function qr weight vector w belong bellman hilbert space h . represent theorem function h express linear combin element span span = {(s, a) = (, s, a) (s, a) s a} express qr (s, a) = t t (s, a, st, at). choic polici mdp dynam directli incorpor hilbert space h . consid empir bellman oper t q bellman residu writ- br(st, at, s t) = q(st, at) t q(st, at) = qr (st, at, s t) rt = q(st, at) ta (a t| t) q( t, t) rt substitut function form q yield hatbr(st, at, s t) = (st, at),w ta (a t| t)( t, t),w+(1)brt ex- press bellman residu weight w featur map (). em- piric bellman featur map (st, at, s t) = (st, at) ta (a t| t)( t, t) empir bellman residu br(st, at, s t) = (st, at, s t),w + (1 )b rt empir bellman featur vector (st, at, s t) build bellman kernel (st, at, s t, s, a, s ) = (st, at, s t), (s, a, s ). bellman kernel weight vector w featur map (s, a) search solut regress function qr (s, a) = (s, a),w+ b qr h solv svr problem min w,b,, 1 2 w2h + c n t=1(t + t ) (1) s.t. r(st, at) (st, at),w (1 )b + t r(st, at) + (st, at),w+ (1 )b + t t, t 0 bellman kernel (st, at, s, a) reward r(st, at) provid solv principl standard svm package. 6. api brm comput complex svr power tools, comput storag requir increas rapidli number train vectors. solv svr reli quadrat program (qp) optimization, abl separ support vector rest train- 6 januari 28, 2015 journal experiment & theoret artifici intellig main ing data. svr implement test have optim solut involv o(n2) dot products, solv qp problem directli involv invert kernel matrix, complex o(n3) n size train set. computa- tional complex k-iter apibrm algorithm domin factors: comput gram matrix bellman kernel (zi, zj) pair zi = (si, ai), zj = (sj , aj), second solv regular regress problem number polici iter step algorithm. comput pair involv enumer successor state si sj evalu base kernel (, ) pair successor states. empir defin averag branch factor finit state mdp averag number possibl successor state state s s limit continu state space assum = 1. equival given dataset dn estim transit kernel p s s,a, branch factor averag number term p s s,a non zero given (s, a). assum n sampl data set mdp empir present averag branch factor state, comput singl element gram matrix requir o(2) operations. gram matrix dimens n n symmetr n(n + 1)/2 uniqu element computed. therefore, total number oper comput gram matrix o(kernel) = o(2n(n+ 1)/2). gram matrix construct principl invers evaluated. gram matrix posi- tive definit symmetric, choleski decomposit used, result total complex o(n3). result, total complex brm algorithm o(n 3 + 2n(n + 1)/2) k polici iter step assum complex o(k(n3 + 2n(n + 1)/2)). clearli pessimist result thank sparsiti properti svr, assum averag number support vector nsv n obtain posterior complex o(k(n3sv+ 2nsv(nsv+1)/2)). increment implement apibrm consid worst case, add new sampl need o(n3)o(kernel) train- ing sampl support vector (martin, 2002). averag algorithm complex o(n2) increment api brm assum o(k(n2 + 2n(n+ 1)/2))) complex bound optimist posterior bound o(k(n2sv + 2nsv(nsv + 1)/2))) nsv averag number support vector k polici iteration. 7. api brm algorithm implement speed learn depend number support vectors, influenc significa- tive performance. reason implement increment version apibrm algorithm. reason come fact version algorithm easili implement onlin set agent interact environment. increment step (or them) allow implement polici evalu updat approxim valu func- tion, perform polici improv updat policy. fact, differ offlin case final perform matters, onlin learn perform improv transit samples. polici iter requir account perform polici improv transi- tion samples, accur evalu current polici completed. practic implement onlin api brm, current behavior polici algorithm collect sampl interact system. consequ explor ad polici soft. mention greedi polici soft polici 0 1 pick determinist particular action probabl 1 uniformli random action probabl 7 januari 28, 2015 journal experiment & theoret artifici intellig main algorithm 1 onlin api brm greedi explor require: (, , ,kp , k function) l 0 initi q0(s, a) (0) solv initi svr: q1 api brm(0, d0, , ) (polici evaluation) store initi state polici 0 measur initi state s0 time step k > 0 updat explor factor k choos action: ak = {k() w.p. 1 k random action w.p. k} appli ak measur state sk+1 reward rk+1 updat train sampl set: dk dk1 (sk, ak, rk+1, sk+1) updat state polici k k1 k(, sk+1) solv increment svr: qk incrementalapi brm(k, dk, , ) k = (l + 1)kp updat polici l() (, qk1) end l l + 1 end return . polici improv implement wait action valu function estim close asymptot valu current policy. henceforth estim updat continu reset polici chang practic correspond have multipl policies. principl assum valu function estim remain similar subsequ polici al chang much. possibl rebuild action valu function estim scratch updat (some sort purg svr). unfortun altern computation costli necessari practice. number transit consecut polici improv crucial paramet algorithm large, avoid potenti bad polici long. assumpt polici induc stationari mix process mdp stationari distribut . mix process start arbitrari initi distribut follow current policy, state probabl distribut rapidli tend stationari distribut markov chain. guarante converg onlin version api brm addition requir sampl follow stationari distribut state- action pair induc polici consid . intuit mean weight state-act pair (s, a) equal steady-st probabl pair infinitely-long trajectori gener polici . assum distribut stationari result bellman equat uniqu solut bellman oper contract admit uniqu fix point. consid increment onlin api brm algorithm, perform guarante reli small polici evalu error assum polici improv accur valu function available. practic mean polici evalu error larg affect performance. nevertheless, algorithm work fine practic standard rl benchmarks. 8 januari 28, 2015 journal experiment & theoret artifici intellig main 8. api brm experi apibrm algorithm implement combin matlab c routin test follow standard rl benchmarks: invert pendulum, bicycl balanc riding. simul implement gener model capabl simul environ learn agent repres api brm algorithm. domain standard benchmark rl literatur featur continu state space non-linear dynamics. experiments, compar perfor- manc api brm algorithm learn method q-learn parametr linear approxim architectur lspi algorithm implement offlin (lagoudaki parr, 2003) onlin (busoniu, lazaric, ghavamzadeh, munos, babuka, schutter, 2012). rank perform necessari introduc met- ric measur qualiti solutions. benchmark specif goal foreseen perform measur repres fulfil given task. exampl invert pendulum, bicycl domain assess qualiti polici abil avoid crash certain period time. measur qualiti solut us stationari polici produces, comput expect return higher expect return is, better rl algorithm per- forms. defin set initi state s0 comput averag expect return stationari polici chosen independ set tupl dn. kind metric call score polici (see (daniel, pierre, luis, 2005) details). given learn polici score defin score = s0s0 r(s0) |s0| r(s0) empir estim r (s) = e[ n1 t=0 tr(st, (st))|s0 = s] av- erag return. order evalu score estim averag empir return initi state s0 s0 monte-carlo simulations. consid benchmark non determinist averag score 10 differ simu- lations. import aspect mind larg flexibl method base gener abil svr us incremental- ity. gener reli statist properti structur risk minim svr us suitabl kernel function. experi pair zi = (si, ai) zj = (sj , aj) us rbf kernel (zi, zj) = e 1 2 (zizj)t2(zizj) diagon matrix specifi weight state-act vector compo- nent. kernel allow manag possibl variant problem action space consid continu eventu noisy. studi statist properti finit action spaces, practic algorithm work fine continu action space. matrix defin svr paramet (c, ). perform grid search appropri set paramet (, c, ) look result perform learn system. fact, differ set paramet help find near-optim polici perform measur score abil reach goal. finally, import aspect affect perform algorithm repre- sent wai collect data manag compromis need explor exploit learn policy. thank flexibl method, run experi differ methods: method-1 (onlin api brm): data gener offlin random behavior polici produc set d0 tupl i.i.d. eventu set differ initi state s0 fix s0. (thi step avoid set directli q = 0 state). data set initi algorithm solv brm provid initi approxim valu function. api brm algorithm proce increment ad new experi improv polici kp step (with kp tunabl parameter) greedi policy. explor partli reli 9 januari 28, 2015 journal experiment & theoret artifici intellig main paramet descript valu um g graviti constant 9.8 m/s2 m pole mass 2.0 m cart mass 8.0 l pole length 0.5 m 1/(m+m) 0.1 kg1 dt simul step 0.1 s r reward 0/-1 discount factor 0.95 tabl 1. paramet simul invert pendulum control problem initi data set d0 partli depend wai manag explor . gener forese exponenti decai factor start valu 0 1 explor requir fix minimum valu = 0.1. assum process underli collect data dn follow unknown mix distribution. method-2 (online-growth api brm): possibl consist altern explor sampl random behavior polici exploit sampl ke step (with ke tunabl parameter) greedi polici learn small explor . consid onlin variant batch-growth method. assum process underli collect data dn follow unknown mix distribution. algorithm 6 illustr onlin variant apibrm greedi explor policy. algorithm allow definit paramet present offlin version: number transit kp n0 be- tween consecut polici improv explor schedul ke. polici fulli optimist kp = 1 polici updat sampl partial optimist 1 < kp kmax experi choos kmax = 10. extens studi paramet affect qualiti solut onlin lspi (busoniu, ernst, shutter, babuska, 2010) principl appli method. explor schedul control paramet ke decai factor decai chosen larg signific explor necessary. altern possibl implement onlin variant batch-growth method. wai perform learn altern explor trial explor factor f exploit trial 0. practic method work allow easili local optim solut start fix initi state s0 . need global optim valid initi state, necessari explor trough set initi state method 1 method 2 appli learn slower. 9. invert pendulum control problem invert pendulum benchmark, control problem consist balanc upright posit pendulum unknown length mass. appli forc cart pendulum cart attach (wang, tanaka, griffin, 1996). simplic challeng control task, benchmark wide test perform state art method function approxim rl. version problem degre freedom obtain fix pole axi rotation. state space s \ st = {(, ) r2} continu consist vertic angl angular veloc invert pendulum termin state st describ later. action allow = {am, 0, am} = 50n uniform nois [10, 10] ad chosen action. transit govern non-linear dynam 10 januari 28, 2015 journal experiment & theoret artifici intellig main simul time 10 sec simul time 50 sec simul time 200 sec simul time 1000 sec figur 1. invert pendulum: repres subsequ polici onlin apibrm method-1 (action discret grei level up) 0 100 200 300 400 500 600 700 800 900 1000 0 500 1000 1500 2000 2500 3000 number train episod s te ps best worst 0 100 200 300 400 500 600 700 800 900 1000 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0 simul time(s) s 0 100 200 300 400 500 600 700 800 900 1000 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0 simul time (s) s 0 100 200 300 400 500 600 700 800 900 1000 0 500 1000 1500 2000 2500 3000 number train episod s te ps worst best figur 2. invert pendulum: averag score offlin lspi (left) q-learn experi replai (right) adapt (lagoudaki parr, 2003) as: = g sin()ml()2 sin(2)/2 cos()u 4/3lm cos()2 (2) angular position, m mass pole, l length pole, m mass cart, u = a+a control action nois consist acceler appli cart, g graviti constant. paramet model simul report tabl 1. angular veloc restrict [4, 4]rads1 saturation. discrete-tim dynam obtain discret time t t + 1 chosen dt = 0.1s. t+1 |t+1| > m termin state st = {|| > m} reach fix m = /2. reward function r(st, at) defin r(st, at) = 1 || > m zero otherwise. discount factor chosen equal 0.95. dynam integr euler method 0.001 integr time step. gener data sampl consid episod start initi state s0 = (0, 0) random initi state stop pole leav region repres s\st mean enter termin state st . (lagoudaki parr, 2003) analysi benchmark report compar perform offlin lspi q-learning. case simul run 1000 11 januari 28, 2015 journal experiment & theoret artifici intellig main 0 200 400 600 800 1000 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 0.01 polici score simul time (s) s 0 200 400 600 800 1000 0 50 100 150 200 250 300 averag balanc time simul time (s) b la n ci n g t im e ( s) figur 3. invert pendulum: (left) averag score onlin api brm kp = 10 grid initi states; (right) averag balanc time grid initi state method-1 0 5 10 15 20 25 30 35 40 45 1 0 1 time (s) th et 0 5 10 15 20 25 30 35 40 45 5 0 5 time (s) th et t 0 5 10 15 20 25 30 35 40 45 50 0 50 time (s) ac tio nfigur 4. invert pendulum: state action repres subsequ learn trials. trial last 30 max consid minimum balanc reach. method-2 (onlin growth) fix initi state s0 = (0, 0) api brm learn local optim polici episod (20 simul time). (10000 samples) separ 1000 trial max 1s (10 samples) stop eventu reach termin state. offlin lspi us linear approxim architectur set 10 basi function (bfs) 3 actions, total 30 basi functions, approxim valu function. 10 bf includ constant term 9 rbf function arrang 3 3 grid 2dimension state space bfi(s) = esi 2/(22) 9 point grid {/4, 0,+/4} {1, 0,+1} = 1. train sampl collect start randomli perturb state close equilibrium state s0 = (0, 0) follow polici select action uniformli random. result shown figur 2 show perform term balanc step analysi detail (lagoudaki parr, 2003). episod allow run maximum 300 (3000 steps) continu balancing. run balanc period time consid successful. optim polici lspi good initi state s0 = (0, 0) wors initi states. simul benchmark onlin api brm run 1000 simul time collect 10000 samples. run split separ learn episod initi random initi state stop termin state reach 30 (300 steps). polici improv perform kp = 10 step (0.1s) greedi polici 0 = 1 reach valu = 0.1 350s. rbf kernel paramet = i3 = 0.5 regress paramet chosen c = 10 = 0.01 select grid search. figur 1 show subsequ polici repres run taken simul time t = 10s, 50s, 200s, 1000s. clearli gener abil svr make possibl captur structur approxim polici 50 simul time close resembl final polici obtain 1000 simul time. 12 januari 28, 2015 journal experiment & theoret artifici intellig main h d +w fcen cm mg x-axi goal contact t wheel wheel - ground goal (xb,yb) frame bike center goal (coord. = (xgoal ,ygoal)) (a) (b) figur 5. bike control problem: figur (a) repres bicycl seen line repres bicycle. center mass bicycle+cyclist cm height h ground, angl vertic bicycl repres total angl tilt cm. action d agent displac w nois simul imperfect balance. figur (b) repres bicycl seen above. angl handlebar displac normal, angl form bicycl frame x axi goal angl bicycl frame line join back-wheel ground contact center goal. t torqu appli cyclist handlebars. (xb, yb) contact point back-wheel ground (from (daniel et al., 2005)) 0 100 200 300 400 500 0.3 0.25 0.2 0.15 0.1 0.05 0 polici score simul time (s) s 0 100 200 300 400 500 0 50 100 150 200 250 300 350 400 450 500 550 averag balanc time simul time (s) b la n ci n g t im e ( s) figur 6. bike balancing: perform onlin api brm kp = 10 method-1 figur 1 show repres subsequ polici onlin apibrm method-1. figur 3 show perform final polici onlin api brm onlin learn process. perform measur evalu score grid initi state simul balanc 300 (3000 steps). q-learn requir state discret serious influenc perform estim state action valu properli state visit slow learning. contrari gener properti svr algorithm estim state valu unvisit state reason experi gain states. non parametr regress easili adapt differ situat gener parametr approxim work specif contest possibl eventu adapt changes. figur 4 report state action subsequ learn trials. trial last 30 max consid minimum balanc reach. method-2 (onlin growth) fix initi state s0 = (0, 0) optim local approxim 30 simul time. final number support vector necessari repres approxim action valu function set paramet approxim usual stai 5% total number collect sampl indic qualiti approximation. 13 januari 28, 2015 journal experiment & theoret artifici intellig main 10. bike balanc control problem consid control problem relat bicycl (randlov alstrom, 1998) move constant speed horizont plane (figur 5). bicycl balanc agent learn balanc bicycle. dynam compos seven state variabl s \ st = {(, , , , ) r5 | [m, m] [m, m] m = 15 rad m = 2.25 rad} plu termin state st . state relat bicycl posit bicycl plane. state variabl relat bicycl , (the angl radial speed vertic bicycle), , (the angl radial speed handlebar displac normal). || > m bicycl fallen reach termin state st . state variabl relat posit bicycl plane coordin (xb, yb) contact point tire horizont plane angl form bicycl x-axis. action space = {(u, t ) {0.02, 0, 0.02}{2, 0, 2}} compos 9 element depend torqu t appli handlebar displac d rider. nois uniformli distribut term dt = [0.02, 0.02] ad action d. continu time dynam describ follow differenti equat describ (randlov alstrom, 1998) detail dynam (randlov alstrom, 1998) paramet mean report figur 5, dynam hold valid |t+1| m |t+1| > m bicycl suppos fallen reach termin state st . suppos state variabl (xb, yb) observed. state variabl interven dynam state variabl reward function considered. consid relev variabl control problem partial observable. reward function bicycl balanc zero reward observed, bicycl fallen case reward equal -1. valu discount factor chosen problem equal 0.98. dynam integr euler method 0.001 integr time step. gener data sampl consid episod start initi state correspond bicycl stand go straight line s0 = (0, 0, 0, 0, 0) = (0, 0, 0, 0,0) fix valu chosen random 0 [, ] stop bicycl leav region repres s \ st mean termin state st . simul benchmark onlin apibrm run 500 simul time collect 50000 samples. run split separ learn episod initi random initi state s0 = (0, 0, 0, 0,0) 0 [, ] stop termin state reach 1s (100 steps). polici improv perform kp = 10 step (0.1s) greedi polici 0 = 1 reach valu = 0.1 200s. rbf kernel paramet = i7 = 1.5 regress paramet chosen c = 10 = 0.01 select grid search. figur 6 show perform final polici onlin apibrm onlin learn process. perform measur evalu score grid initi state s0 = {(0, 0, 0, 0,0} 0 [, ]. figur 7 report state action subsequ learn trials. trial last 50 max (5000 steps) consid minimum balanc reach goal. method-2 (onlin growth) fix initi state s0 = (0, 0, 0, 0,0) optim local approxim 50 simul time. lower figur 7 trajectori learn process final one. final number support vector necessari repres approxim action valu function set paramet approxim usual stai 5% total number collect sampl indic qualiti approximation. 14 januari 28, 2015 journal experiment & theoret artifici intellig main b c figur 7. bike balancing: (upper a) state action repres subsequ learn trials. trial last 50 max (5000 steps) consid suffici reach goal. method-2 (online-growth) small perturb fix initi state s0 = (0, 0, 0, 0, /2) api brm learn local optim polici episod (50 simul time). (lower) sketch trajectori (b zoom, c overall) time interv (0, 500s) bicycl (xb, yb) plane control final polici api brm 11. conclus develop model free brm approach call api brm abl optim polici continu state rl problem studi practic implement issues. particular, demonstr problem find optim polici minim bellman residu cast regress problem svr appropri- at rkhs. main contribut work experiment analysi non parametr approxim algorithm gener problem rl polici iter kernel methods. algorithm eventu converg optim polici mix distribut data samples. interest properti api brm algorithm are: api brm effici problem sampl experi sparse. algorithm base approxim polici iteration, power frame- work met success plan problems. open new research direc- tion us kernel base approxim polici iter context learning. api brm api algorithm make good us function approxim implic- 15 januari 28, 2015 journal experiment & theoret artifici intellig main itli construct approxim model kernels. api brm us increment svr allow estim approxim state action valu function rl. polici iter implicitli time new experi obtained. api brm complex strongli depend cost pai order solv svr essenti quadrat problem optimization. svr solv batch mode set train sampl dispos learn agent increment mode enabl addit remov train sampl effectively. final come theoret bound perform statist converg guarantee. acknowledg work partial support fi-dgr programm agaur eco/1551/2012. refer leemon baird. residu algorithms: reinforc learn function approximation. proceed twelfth intern confer machin learning, page 3037. morgan kaufmann, 1995. l. busoniu, d. ernst, b. shutter, r. babuska. onlin least-squar polici iter reinforc learn control. baltimore, editor, proceed american control confer acc-10, page 486491, 2010. lucian busoniu, alessandro lazaric, mohammad ghavamzadeh, remi munos, robert babuka, bart schutter. least-squar method polici iteration. marco wier mar- tijn otterlo, editors, reinforc learning, volum 12 adaptation, learning, opti- mization, page 75109. springer berlin heidelberg, 2012. isbn 978-3-642-27644-6. . url ernst daniel, geurt pierre, whenkel luis. tree base batch mode reinforc learning. journal machin learn research, 6:503556, 2005. michail g. lagoudaki ronald parr. least-squar polici iteration. journal machin learn research, 4:11071149, 2003. url jmlr/jmlr4.html#lagoudakisp03. mario martin. on-lin support vector machin regression. proceed 13th european confer machin learning, ecml 02, page 282294, london, uk, uk, 2002. springer- verlag. isbn 3-540-44036-4. url jett randlov paul alstrom. learn drive bycicl reinforc learn shaping. proceed fifth intern confer machin learning, page 463 471, 1998. h.o. wang, k. tanaka, m.f. griffin. approach fuzzi control nonlinear systems: stabil design issues. fuzzi systems, ieee transact on, 4(1):1423, feb 1996. 16