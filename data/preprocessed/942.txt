januari 28 , 2015 journal experiment & theoret artifici intellig main appear journal experiment & theoret artifici intellig vol .
00 , .
00 , month 20xx , 116 approxim polici iter use regular bellman residu minim g. espositoa m. martina universitat politecnica de catalunya , barcelona , spain ; ( v1.0 releas octob 2014 ) paper present approxim polici iter ( api ) method call apibrm use effect implement increment support vector regress ( svr ) approxim valu function abl general continu ( larg ) space reinforc learn ( rl ) problem .
rl ia methodolog abl solv complex uncertain decis problem usual model markov decis problem ( mdp ) .
api  brm formal non-parametr regular problem base outcom bellman residu minim ( brm ) abl minim varianc problem .
apibrm increment appli rl use on-lin agent interact framework .
base non-parametr svr api brm abl find global solut problem converg guarante optim solut .
valu function defin find optim polici specifi total reward agent might expect current state take one action .
therefor agent use valu function choos action take .
experiment evid perform well known rl benchmark present .
keyword : reinforc learn , support vector machin , approxim polici iter , regular , regress 1 .
introduct rl autonom agent interact environ learn take correct action everi situat order reach goal .
rl give method abl solv difficult uncertain sequenti decis problem , could quit challeng real-world applic .
rl problem often model mdp , deepli studi literatur .
main particular rl algorithm respect approach mdps rl agent learn optim polici experi without know paramet mdp .
usual find optim polici valu function defin specifi total reward agent might expect current state take one action .
agent choos action take use valu function .
howev one want handl problem continu larg state space , general use .
general properti rl algorithm import factor determin predict perform case .
main problem use general approxim rl converg guarante qualiti solut .
parametr method consid literatur , show advantag fast easi learn mechan .
howev , present inher problem case solut problem might express given architectur number chosen paramet .
candid non correspond author .
email : gesposit @ lsi.upc.edu 1 januari 28 , 2015 journal experiment & theoret artifici intellig main parametr valu function approxim svr known good properti general abil .
svr convex optim problem , suffer sub-optim , find alway global optim solut approxim problem .
non parametr learn method , also abl automat adapt complex problem .
use non parametr method approxim rl allow choos capac approxim function space mean fix kernel function number support determin need order find requir solut .
algorithm present paper instanc api .
like polici iter , algorithm repeat comput evalu function polici previous step use evalu function comput next improv polici .
order avoid need learn model , action valu function comput , make polici improv step simpl , like least-squar polici iter ( lspi ) algorithm ( lagoudaki parr , 2003 ) .
lspi reli least squar tempor differ learn ( lstd ) build algorithm use brm .
major novelti algorithm base exact increment svr context brm approxim polici iter .
idea use bellman residu polici iter goe back ( baird , 1995 ) propos comput approxim state valu function given model finite-st action mdp use quadrat loss function .
small bellman error might yield good approxim polici evalu function , turn may impli good final perform .
one major obstacl use brm learn without model sampl base approxim use least squar loss function base data singl trajectori behavior polici , unbias estim bellman residu .
algorithm use insensit loss function might solv problem .
main condit converg method ask trajectori suffici repres rapid mix .
also requir state trajectori follow stationari distribut .
mix condit essenti effici learn particular use exponenti mix condit .
2 .
background notat space  -algebra  defin (  ) set probabl measur  b (  ) space bound measur function w.r.t .
 b (  , l ) space bound measur function bound 0 < l <  .
henc may introduc definit 2.1 : ( continu state mdp ) continu state finit action dis- count mdp defin tupl ( , , p ,  ) , follow definit content :  measur state space st  denot state agent time t.  finit set avail action  denot action agent perform time t.  p : am ( r ) map evalu ( , )  a give distribut r denot p ( r ,  |s , ) .
 margin p defin p ( |s , ) =  r p ( dr , |s , ) denot probabl end state  perform action state ( transit probabl )  r ( |s , ) =  p (  , ds  |s , ) reward function denot expect reward agent transit state state  perform action .
 immedi expect reward defin r ( , ) =  r rr ( dr|s , ) = e [ r|s , ]    [ 0 , 1 ] discount factor .
2 januari 28 , 2015 journal experiment & theoret artifici intellig main stage action  select agent control process respons pair ( rt ,  ) drawn distribut p ( r ,  |st , ) i.e ( rt ,  )  p ( r ,  |st , ) rt reward agent receiv  next mdp state .
procedur continu lead random trajectori t = { s1 , a1 , r1 , s2 , a2 , r2 , ... }    denot space possibl trajectori .
mdp agent consist main action select polici =  ( st ) .
stationari stochast polici map state distribut action space t :  ( ) t ( a| ) denot probabl agent select action perform state time t. use  ( ) refer probabl distribut probabl mass function action state s. stochast polici also call soft commit singl action per state .
 ( a| ) stand probabl soft polici choos action state s. greedi polici soft polici 0    1 pick determinist particular action probabl 1  uniform random action probabl  .
use   ( |s ) indic action chosen accord probabl function state s. definit 2.2 : ( valu function ) agent follow polici  consid sequenc reward { rt :  1 } mdp start state ac- tion ( s1 , a1 )   ( , )  (  ) action valu function q defin q ( , ) = e {  t=1  t1rt| s1 = , a1 = ,  } optim action valu q ( , ) = sup q  ( , )  ( , )  a .
polici  optim whenev achiev best valu everi state polici  =  (  , q ) greedi w.r.t .
action valu function q   choos  ( ) = arg maxaa q ( , ) .
definit 2.3 : ( bellman oper ) bellman oper defin  : b (  )  b (  ) action valu function defin ( q ) ( , ) = r ( , ) +   p ( ds  |s , ) q (  ,  (  ) ) = e [ r +   a  (  |s  ) q (  ,  ) ] b (  ) repres space bound measur function .
given polici  fix point bellman oper- ator action valu function q = q .
bellman optim oper defin action valu function ( q ) ( , ) = r ( , ) +   p ( ds  |s , ) maxa  aq (  ,  ) definit 2.4 : ( reproduc kernel hilbert space ) consid subset measur function f :  r subset vector valu measur function f |a| : sa r|a| f |a| = { ( q1 , ... , q|a| ) : qi  f , = 1 , ... , |a| } call hypothesi space h chosen reproduc kernel hilbert space ( rkhs ) h :   r defin  inner product  ,  character symmetr posit definit function  : (  )  (  )  r call reproduc kernel , continu sa ( , )  sa follow reproduc properti hold : q ( , ) = q (  ) ,  (  , , ) h  (  , , )  h assum measur function space f |a| = h. h closur linear span set function span = {  ( , ) =  (  , , ) ( , )   } consid map  :   csa denot function assign valu  ( st , , , ) ( st , )  a .
kernel function  (  ,  ) play import role kernel-bas learn method map two element space input pattern real number taken state space mdp , thought similar measur input space .
deriv kernel method , kernel function aris natur inner product high-dimension featur space .
henc kernel satisfi sever import properti inner product : must symmetr posit semidefinit , mean associ gram matrix kij =  ( si , ai , sj , aj ) must posit semidefinit .
kernel satisfi properti said admiss .
import aspect method solv rl problem way data collect process .
data 3 januari 28 , 2015 journal experiment & theoret artifici intellig main collect set categor onlin offlin data process method categor batch increment .
onlin sampl set agent choos action sequenc  t ( |st ) direct influenc data stream generat dn = { ( s1 , a1 ,  1 , r1 ) , ... , ( sn , ,  n , rn ) } may assum stationari general non-i.i.d .
process .
stochast function t ( |s ) , whenev evalu state  dn defin stochast process n = { 1 ( |s  1 ) , ... , n ( |s  n ) } .
assum control mix condit non-i.i.d .
process , allow general strict i.i.d .
scenario .
may defin order multiset { ( s1 , a1 ) , ... , ( sn , ) } { ( r1 ,  1 ) , ... , ( r1 ,  1 ) } = bt ( st ) st  s ( ) s ( )  ( ) ( rt ,  )  p ( r ,  |st , ) .
b behavior stationari polici produc ( st , )   ( , )  ( , )  ( a ) result state action distribut .
collect data dn use defin empir oper thought empir approxim true oper .
definit 2.5 : ( empir bellman oper ) given polici  consid dn data set empir bellman oper defin ( t q ) ( st , ) = rt +   a  (  |s  ) q (  ,  ) empir bellman optim oper t  : zn  rn defin ( t q ) ( st , ) = rt + maxaaq (  ,  ) .
provid unbias estim bellman oper given polici  fix bound measur de- terminist function q : a r hold e [ t q ( st , ) |st , ] = q ( st , ) also e [ t q ( st , ) |st , ] = q ( st , ) 1   n. may also use symbol en [ f ] = 1 n n t=1 f ( zt ) shorthand empir version expect oper appli function f ( z ) use data set zn = { z1 , ... , zn } .
3 .
approxim polici iter use bellman residu minim polici iter method discov optim polici given mdp , pro- vide iter procedur polici space .
pi discov optim polici generat sequenc monoton improv polici .
iter consist two phase : polici evalu comput action valu function qk current polici k solv linear system bellman equat polici improv defin improv greedi polici k+1 q k k+1 = arg maxaaqk ( , ) .
valu function qk typic chosen qk  kqk , i.e. , approxim fix point k .
polici k+1 least good k bet- ter .
two step repeat chang polici case iter converg optim polici , often surpris small number iter .
guarante converg polici iter optim polici reli heavili upon tabular represent valu function , exact solut bellman equat , tabular represent polici .
exact represent method impract larg state action space .
case , approxim method use .
approxim polici iter framework introduc represent valu function polici .
crucial factor success approxim algorithm choic approxim architectur form polici iter known approxim polici iter ( api ) .
way implement api use brm case api proceed iter k evalu k choos qk bellman residu  br k = |qk  kqk| small ( i.e .
approxim fix point k ) .
api calcul k+1 =  (  , qk ) produc sequenc q0  1  q1 ... . sequenc { qk } k1k=0 bellman residu ( br ) polici approxim error ( ae ) defin iter 4 januari 28 , 2015 journal experiment & theoret artifici intellig main brk = qk  kqk  ae k = qk q k .
brm minim bellman error ( ) bellman residu q given distribut input data  use given loss function ` defin lbrm ` ( q ,  ) =  ` ( q ( , )  q ( , ) ) d ( , ) use sampl data setdn polici process n one tri evalu empir estim lbrm ` ( q , n , dn ) = en [ ` ( q ( st , ) t q ( st , ) ) ] .
brm problem solv contest api method use reproduc kernel hilbert space hypothesi space take linear combin form q ( , ) = n t=1 t ( st , , , ) , t  r. accord regular brm problem written q = arg min qh { lbrm ` ( q , n , dn ) + q 2 h } standard brm algorithm ( baird , 1995 ) use least squar loss function approxim action valu function data .
unfortun approach bring bias estim overcom problem common suggest use uncorrel , doubl sampl data set .
4 .
bellman residu minim svr hereaft introduc method aim solv brm problem use svr .
con- sider bellman residu br ( , ) = q q = qr ( , )  r ( , ) approx- imat function qr ( , ) = q ( , )    p ( ds  |s , )  a  (  |s  ) q (  ,  ) = e [ q ( , )    a  (  |s  ) q (  ,  ) |s , ] = e [ qr ( , ,  ) |s , ] r ( , ) = e [ r|s , ] brm use insensit loss function `  ( q  r  r ) = max ( 0 , |qr  r|   ) written lbrm ( q ,  ) = e [ `  ( qr  r ) ] = e [ `  ( q  q ) ] .
use data set n , dn empir estim be- come : lbrm ( q , n , dn ) = en [ `  ( q  r  r ) ] = en [ `  ( q  t q ) ] qr ( st , ,  ) = q ( st , )     ta t (  t|s  ) q (  ,  ) .
henc brm optim problem becom q = arg minqh { lbrm ( q , n , dn ) + q2h } regular term use norm hilbert space h. brm show remark sparsiti properti solut essenti reli train support vector .
lbrm ( q , n , dn ) unbias estim lbrm ( q ,  ) result evalu expect empir loss e [ lbrm ( q , n , dn ) |n , dn ] = e [ en [ `  ( qrr ) ] |n , dn ] = e [ en [ `  ( qt q ) ] |n , dn ]  e [ `  ( q r  r ) ] = lbrm ( q ,  ) use jensen  inequ ` ( e [ x ] )  e [ ` ( x ) ] hold convex function ` qr ( , ) = e [ q  r ( , ,  ) |s , ] r ( , ) = e [ r|s , ] q = e [ t q ] .
practic empir estim bias whenev slack present i.e .
error regress function fix thresh- old  .
unbias error contain resolut tube svr .
nevertheless choic svr paramet c  give way control effect .
5 .
api  brm dual batch solut consid subset observ sampl dn express approxim valu function use linear architectur q ( , ) =  ( , ) , w+ b w = ( w1 , ... , wd ) 5 januari 28 , 2015 journal experiment & theoret artifici intellig main weight vector  ( , ) = ( 1 ( , ) , ... , d ( , ) ) featur vector point ( , ) may build kernel function  ( st , , , ) =  ( st , ) ,  ( , )  .
action valu function belong hilbert space q  h weight vector w  h. use represent theorem also know function h express linear combin element span span = {  ( , ) =  (  , , ) ( , )   } express q ( , ) =  t ( , , st , ) .
use definit bellman oper q bellman residu train point fix polici  may write : br ( st , ) = q ( st , )   q ( st , ) = qr ( st , )  r ( st , ) = q ( st , )    p ( ds  |st , )   a  (  |s  )  q (  ,  )  r ( st , ) substitut function form q yield br ( st , ) =  ( st , ) , w  p ( ds  |st , )   a  (  |s  )   (  ,  ) , w + ( 1   ) b  r ( st , ) express bellman residu use weight w featur map  (  ) .
polici mdp dynam includ hilbert space h. howev altern way express bell- man residu trough combin two term bellman featur map- ping  ( st , ) =  ( st , )    p ( ds  |st , )  a  (  |s  )  (  ,  ) take ac- count structur mdp dynam .
bellman residu express br ( st , ) =  ( st , ) , w + ( 1   ) b  r ( st , ) bellman featur vector  ( st , ) may build bellman kernel   ( st , , , ) =  ( st , ) ,  ( , )  .
function qr weight vector w belong bellman hilbert space h .
use represent theorem function h express linear combin element span span = {  ( , ) =  (  , , ) ( , )   } express qr ( , ) =  t  ( , , st , ) .
choic polici well mdp dynam direct incorpor hilbert space h .
consid empir bellman oper t q bellman residu writ- ten br ( st , ,  ) = q ( st , )  t  q ( st , ) = qr ( st , ,  )  rt = q ( st , )     ta  (  t|s  )  q (  ,  )  rt substitut function form q yield hatbr ( st , ,  ) =  ( st , ) , w   ta  (  t|s  )  (  ,  ) , w+ ( 1 ) brt ex- press bellman residu use weight w featur map  (  ) .
em- piric bellman featur map  ( st , ,  ) =  ( st , )    ta  (  t|s  )  (  ,  ) empir bellman residu br ( st , ,  ) =  ( st , ,  ) , w + ( 1   ) b  rt empir bellman featur vector  ( st , ,  ) may build bellman kernel  ( st , ,  , , ,  ) =  ( st , ,  ) ,   ( , ,  )  .
use bellman kernel may find weight vector w use featur map  ( , ) search solut regress function qr ( , ) =  ( , ) , w+ b qr  h solv svr problem min w , b ,  ,  1 2 w2h + c n t=1 ( t +   ) ( 1 ) s.t .
r ( st , )   ( st , ) , w  ( 1  ) b  + t r ( st , ) +  ( st , ) , w+ ( 1  ) b  + t t , t  0 bellman kernel  ( st , , , ) reward r ( st , ) provid solv principl use standard svm packag .
6 .
api  brm comput complex svr power tool , comput storag requir increas rapid number train vector .
solv svr reli quadrat program ( qp ) optim , abl separ support vector rest train- 6 januari 28 , 2015 journal experiment & theoret artifici intellig main ing data .
svr implement test optim solut involv ( n2 ) dot product , solv qp problem direct involv invert kernel matrix , complex ( n3 ) n size train set .
henc computa- tional complex k-iter apibrm algorithm domin three factor : first comput gram matrix bellman kernel  ( zi , zj ) pair zi = ( si , ai ) , zj = ( sj , aj ) , second solv regular regress problem third number polici iter step use algorithm .
comput  pair involv enumer successor state si sj evalu base kernel  (  ,  ) pair successor state .
empir may defin averag branch factor finit state mdp  averag number possibl successor state state  limit continu state space assum  = 1 .
equival given dataset dn estim transit kernel p  , , branch factor  averag number term p  , non zero given ( , ) .
assum n sampl data set mdp empir present averag branch factor  state , comput singl element gram matrix requir ( 2 ) oper .
sinc gram matrix dimens n  n symmetr n ( n + 1 ) /2 uniqu element must comput .
therefor , total number oper comput full gram matrix ( kernel ) = ( 2n ( n+ 1 ) /2 ) .
gram matrix construct principl invers must evalu .
sinc gram matrix posi- tive definit symmetr , choleski decomposit might use , result total complex ( n3 ) .
result , total complex brm algorithm ( n 3 + 2n ( n + 1 ) /2 ) k polici iter step assum complex ( k ( n3 + 2n ( n + 1 ) /2 ) ) .
clear pessimist result thank sparsiti properti svr , assum averag number support vector nsv  n one may obtain posterior complex ( k ( n3sv+  2nsv ( nsv+1 ) /2 ) ) .
increment implement apibrm consid worst case , add new sampl need ( n3 ) o ( kernel ) train- ing sampl support vector ( martin , 2002 ) .
averag algorithm complex ( n2 ) increment api brm may assum ( k ( n2 + 2n ( n+ 1 ) /2 ) ) ) complex bound optimist posterior bound ( k ( n2sv +  2nsv ( nsv + 1 ) /2 ) ) ) nsv averag number support vector k polici iter .
7 .
api  brm algorithm implement speed learn depend most number support vector , influenc significa- tive perform .
first reason implement increment version apibrm algorithm .
anoth reason come fact version algorithm easili implement onlin set whenev agent may interact environ .
increment step ( least ) allow implement polici evalu updat approxim valu func- tion , one might perform polici improv updat polici .
fact , differ offlin case final perform matter , onlin learn perform improv everi transit sampl .
polici iter take requir account perform polici improv everi transi- tion sampl , accur evalu current polici complet .
practic implement onlin api  brm , use current behavior polici algorithm collect sampl interact system .
consequ explor ad polici becom soft .
alreadi mention greedi polici soft polici 0    1 pick determinist particular action probabl 1  uniform random action probabl 7 januari 28 , 2015 journal experiment & theoret artifici intellig main algorithm 1 onlin api brm greedi explor requir : (  ,  ,  , kp , k function ) l 0 initi q0 ( , ) ( 0 ) solv initi svr : q1  api brm ( 0 , d0 ,  ,  ) ( polici evalu ) store initi next state polici 0 measur initi state s0 time step k > 0 updat explor factor k choos action : ak = { k (  ) w.p .
1 k  random action w.p .
k } appli ak measur next state sk+1 reward rk+1 updat train sampl set : dk  dk1  ( sk , ak , rk+1 , sk+1 ) updat next state polici k  k1  k (  , sk+1 ) solv increment svr : qk  incrementalapi brm ( k , dk ,  ,  ) k = ( l + 1 ) kp updat polici l (  )   (  , qk1 ) end l l + 1 end return  .
henc polici improv implement without wait action valu function estim get close asymptot valu current polici .
henceforth estim updat continu without reset polici chang practic correspond multipl polici .
principl one may assum valu function estim remain similar subsequ polici al least chang much .
anoth possibl would rebuild action valu function estim scratch everi updat ( sort purg svr ) .
unfortun altern comput cost might necessari practic .
number transit consecut polici improv crucial paramet algorithm larg , avoid potenti bad polici use long .
hereaft make assumpt polici  induc stationari mix process mdp stationari distribut  .
mix process start arbitrari initi distribut follow current polici , state probabl distribut rapid tend stationari distribut markov chain .
guarante converg onlin version api  brm addit requir sampl follow stationari distribut state- action pair induc polici consid  .
intuit mean weight state-act pair ( , ) equal steady-st probabl pair along infinitely-long trajectori generat polici  .
moreov assum distribut  stationari result bellman equat uniqu solut bellman oper contract admit one uniqu fix point .
consid increment onlin api  brm algorithm , perform guarante reli small polici evalu error assum polici improv accur valu function avail .
practic mean polici evalu error larg might affect perform .
nevertheless , algorithm work fine practic sever standard rl benchmark .
8 januari 28 , 2015 journal experiment & theoret artifici intellig main 8 .
api  brm experi apibrm algorithm implement use combin matlab c routin test follow standard rl benchmark : invert pendulum , bicycl balanc ride .
simul implement use generat model capabl simul environ learn agent repres api brm algorithm .
domain standard benchmark rl literatur featur continu state space non-linear dynam .
moreov experi , compar perfor- manc api  brm algorithm learn method q-learn parametr linear approxim architectur lspi algorithm implement offlin ( lagoudaki parr , 2003 ) onlin ( busoniu , lazar , ghavamzadeh , muno , babuka , schutter , 2012 ) .
rank perform necessari introduc met- ric measur qualiti solut .
benchmark specif goal foreseen perform measur repres fulfil given task .
exampl invert pendulum , bicycl domain may assess qualiti polici abil avoid crash certain period time .
measur qualiti solut use stationari polici produc , comput expect return say higher expect return , better rl algorithm per- form .
done defin set initi state s0 comput averag expect return stationari polici chosen independ set tupl dn .
kind metric call score polici ( see ( daniel , pierr , lui , 2005 ) detail ) .
given learn polici  score defin score =  s0s0 r ( s0 ) |s0| r ( s0 ) empir estim r  ( ) = e [ n1 t=0  tr ( st ,  ( st ) ) |s0 = ] av- erag return .
order evalu score one estim averag empir return everi initi state s0  s0 monte-carlo simul .
consid benchmark non determinist averag score 10 differ simu- lation .
anoth import aspect keep mind rather larg flexibl method base upon general abil svr use incremental- iti .
general reli statist properti structur risk minim svr use suitabl kernel function .
experi pair zi = ( si , ai ) zj = ( sj , aj ) use rbf kernel  ( zi , zj ) = e  1 2 ( zizj ) t2 ( zizj )  diagon matrix specifi weight state-act vector compo- nent .
use kernel also allow manag possibl variant problem action space may consid continu eventu noisi .
even though studi statist properti use finit action space , practic algorithm may also work fine use continu action space .
part matrix  also defin svr paramet ( c ,  ) .
perform grid search find appropri set paramet (  , c ,  ) look result perform learn system .
fact , use differ set paramet might help find near-optim polici whose perform measur use score abil reach goal .
final , anoth import aspect may affect perform algorithm repre- sent way collect data therefor manag compromis need explor exploit learn polici .
thank flexibl method , may run experi use two differ method :  method-1 ( onlin api  brm ) : data generat offlin use random behavior polici produc set d0 tupl i.i.d .
use eventu set differ initi state s0 fix one s0 .
( step also avoid set direct q = 0 state ) .
data set use initi algorithm solv brm provid initi approxim valu function .
henc api brm algorithm proceed increment ad new experi improv polici kp step ( kp tunabl paramet ) use greedi polici .
explor part reli 9 januari 28 , 2015 journal experiment & theoret artifici intellig main paramet descript valu um g graviti constant 9.8 m/s2 pole mass 2.0 kg cart mass 8.0 kg l pole length 0.5  1/ ( m+m ) 0.1 kg1 dt simul step 0.1 r reward 0/-1  discount factor 0.95 tabl 1 .
paramet use simul invert pendulum control problem initi data set d0 part depend way manag explor  .
general one may forese exponenti decay  factor start valu 0  1 explor requir fix minimum valu  = 0.1 .
assum process under collect data dn follow unknown mix distribut .
 method-2 ( online-growth api  brm ) : anoth possibl consist altern explor sampl use random behavior polici exploit sampl everi ke step ( ke tunabl paramet ) use greedi polici learn small explor  .
consid onlin variant batch-growth method .
assum process under collect data dn follow unknown mix distribut .
algorithm 6 illustr onlin variant apibrm use greedi explor polici .
algorithm allow definit two paramet present offlin version : number transit kp  n0 be- tween consecut polici improv explor schedul ke .
polici fulli optimist whenev kp = 1 polici updat everi sampl partial optimist 1 < kp  kmax experi choos kmax = 10 .
extens studi paramet affect qualiti solut onlin lspi found ( busoniu , ernst , de shutter , babuska , 2010 ) principl might appli method .
explor schedul control paramet ke decay factor decay chosen larg signific amount explor necessari .
howev sever altern possibl also implement onlin variant batch-growth method .
way perform learn altern explor trial use explor factor f exploit trial use 0 .
practic method work well allow easili found local optim solut start fix initi state s0 .
nevertheless need find global optim valid initi state , becom necessari explor trough set initi state method 1 method 2 must appli learn becom slower .
9 .
invert pendulum control problem invert pendulum benchmark , control problem consist balanc upright posit pendulum unknown length mass .
done appli forc cart pendulum cart attach ( wang , tanaka , griffin , 1996 ) .
due simplic still challeng control task , benchmark wide use test perform state art method function approxim rl .
version problem one degre freedom obtain fix pole axi rotat .
state space \ st = { (  ,  )  r2 } continu consist vertic angl  angular veloc  invert pendulum termin state st describ later .
three action allow = { am , 0 , } = 50n uniform nois a  [ 10 , 10 ] might ad chosen action .
transit govern non-linear dynam 10 januari 28 , 2015 journal experiment & theoret artifici intellig main simul time 10 sec simul time 50 sec simul time 200 sec simul time 1000 sec figur 1 .
invert pendulum : repres subsequ polici found onlin apibrm use method-1 ( action discret three grey level show ) 0 100 200 300 400 500 600 700 800 900 1000 0 500 1000 1500 2000 2500 3000 number train episod te ps best worst 0 100 200 300 400 500 600 700 800 900 1000 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0 simul time ( ) co 0 100 200 300 400 500 600 700 800 900 1000 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0 simul time ( ) co 0 100 200 300 400 500 600 700 800 900 1000 0 500 1000 1500 2000 2500 3000 number train episod te ps worst best figur 2 .
invert pendulum : averag score offlin lspi ( left ) q-learn experi replay ( right ) adapt ( lagoudaki parr , 2003 ) system :  = g sin (  ) ml (  ) 2 sin ( 2 ) /2 cos (  ) u 4/3lm cos (  ) 2 ( 2 )  angular posit , mass pole , l length pole , mass cart , u = a+a control action nois consist acceler appli cart , g graviti constant .
paramet model use simul report tabl 1 .
angular veloc  restrict [ 4 , 4 ] rads1 use satur .
discrete-tim dynam obtain discret time + 1 chosen dt = 0.1s .
t+1 |t+1| > m termin state st = { || > m } reach fix m = /2 .
reward function r ( st , ) defin r ( st , ) = 1 || > m zero otherwis .
discount factor  chosen equal 0.95 .
dynam system integr use euler method 0.001s integr time step .
generat data sampl may consid episod start initi state s0 = ( 0 , 0 ) use random initi state stop pole leav region repres s\st mean enter termin state st .
( lagoudaki parr , 2003 ) analysi benchmark report compar perform offlin lspi q-learn .
case simul run 1000s 11 januari 28 , 2015 journal experiment & theoret artifici intellig main 0 200 400 600 800 1000 0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 0.01 polici score simul time ( ) co 0 200 400 600 800 1000 0 50 100 150 200 250 300 averag balanc time simul time ( ) b la n ci n g im e ( ) figur 3 .
invert pendulum : ( left ) averag score onlin api  brm kp = 10 grid initi state ; ( right ) averag balanc time grid initi state use method-1 0 5 10 15 20 25 30 35 40 45 1 0 1 time ( ) th et 0 5 10 15 20 25 30 35 40 45 5 0 5 time ( ) th et a 0 5 10 15 20 25 30 35 40 45 50 0 50 time ( ) ac tio nfigur 4 .
invert pendulum : state action repres subsequ learn trial .
trial last 30s max consid minimum balanc reach .
use method-2 ( onlin growth ) fix initi state s0 = ( 0 , 0 ) api brm learn local optim polici episod ( 20s simul time ) .
( 10000 sampl ) separ 1000 trial max 1s ( 10 sampl ) stop eventu reach termin state .
offlin lspi use linear approxim architectur set 10 basi function ( bfs ) one 3 action , thus total 30 basi function , approxim valu function .
10 bfs includ constant term 9 rbf function arrang 3  3 grid 2dimension state space bfi ( ) = esi 2/ ( 22 ) i 9 point grid { /4 , 0 , +/4 }  { 1 , 0 , +1 }  = 1 .
train sampl collect start random perturb state close equilibrium state s0 = ( 0 , 0 ) follow polici select action uniform random .
result shown figur 2 show perform term balanc step analysi detail ( lagoudaki parr , 2003 ) .
episod allow run maximum 300s ( 3000 step ) continu balanc .
run balanc period time consid success .
optim polici found lspi good enough use initi state s0 = ( 0 , 0 ) much wors initi state .
simul benchmark use onlin api  brm run 1000s simul time collect around 10000 sampl .
run split separ learn episod initi random initi state stop termin state reach otherwis 30s ( 300 step ) .
polici improv perform everi kp = 10 step ( 0.1s ) use greedi polici 0 = 1 reach valu  = 0.1 350s .
also use rbf kernel paramet  = i3  = 0.5 regress paramet chosen c = 10  = 0.01 select use grid search .
figur 1 show subsequ polici found repres run taken simul time = 10s , 50s , 200s , 1000s .
clear general abil svr make possibl captur structur approxim polici 50s simul time close resembl final polici obtain 1000s simul time .
12 januari 28 , 2015 journal experiment & theoret artifici intellig main  h +w fcen cm  mg x-axi  goal contact  front wheel back wheel - ground goal ( xb , yb ) frame bike center goal ( coord .
= ( xgoal , ygoal ) ) ( ) ( b ) figur 5 .
bike control problem : figur ( ) repres bicycl seen behind thick line repres bicycl .
center mass bicycle+cyclist cm height h ground ,  angl vertic bicycl  repres total angl tilt cm .
action agent displac w nois simul imperfect balanc .
figur ( b ) repres bicycl seen .
 angl handlebar displac normal ,  angl form bicycl frame x axi goal angl bicycl frame line join back-wheel ground contact center goal .
torqu appli cyclist handlebar .
( xb , yb ) contact point back-wheel ground ( ( daniel et al. , 2005 ) ) 0 100 200 300 400 500 0.3 0.25 0.2 0.15 0.1 0.05 0 polici score simul time ( ) co 0 100 200 300 400 500 0 50 100 150 200 250 300 350 400 450 500 550 averag balanc time simul time ( ) b la n ci n g im e ( ) figur 6 .
bike balanc : perform onlin api brm kp = 10 use method-1 figur 1 show repres subsequ polici found onlin apibrm use method-1 .
figur 3 show perform final polici found onlin api  brm along onlin learn process .
perform measur evalu score grid initi state simul balanc 300s ( 3000 step ) .
use q-learn requir state discret serious influenc perform estim state action valu proper state visit slow learn .
contrari general properti svr algorithm estim state valu unvisit state reason use experi gain state .
moreov non parametr regress easili adapt differ situat general parametr approxim may work well specif contest without possibl eventu adapt chang .
figur 4 report state action subsequ learn trial .
trial last 30s max consid minimum balanc reach .
use method-2 ( onlin growth ) fix initi state s0 = ( 0 , 0 ) optim local approxim found less 30s simul time .
final number support vector necessari repres approxim action valu function set paramet use approxim usual stay 5 % total number collect sampl also indic qualiti approxim .
13 januari 28 , 2015 journal experiment & theoret artifici intellig main 10 .
bike balanc control problem consid control problem relat bicycl ( randlov alstrom , 1998 ) move constant speed horizont plane ( figur 5 ) .
bicycl balanc agent learn balanc bicycl .
system dynam compos seven state variabl \ st = { (  ,  ,  ,  ,  )  r5 |   [ m , m ]   [ m , m ] m =  15 rad m =  2.25 rad } plus termin state st .
four state relat bicycl three posit bicycl plane .
state variabl relat bicycl  ,  ( angl radial speed vertic bicycl ) ,  ,  ( angl radial speed handlebar displac normal ) .
|| > m bicycl fallen reach termin state st .
state variabl relat posit bicycl plane coordin ( xb , yb ) contact point back tire horizont plane angl  form bicycl x-axi .
action space = { ( u , )  { 0.02 , 0 , 0.02 }  { 2 , 0 , 2 } } compos 9 element depend torqu appli handlebar displac rider .
nois system uniform distribut term dt = [ 0.02 , 0.02 ] ad action d. system continu time dynam describ follow differenti equat describ ( randlov alstrom , 1998 ) detail dynam found ( randlov alstrom , 1998 ) various paramet mean also report figur 5 , dynam hold valid |t+1|  m |t+1| > m bicycl suppos fallen reach termin state st .
suppos state variabl ( xb , yb ) observ .
sinc two state variabl interven dynam state variabl reward function consid .
henc may consid relev variabl make control problem partial observ .
reward function bicycl balanc zero reward alway observ , except bicycl fallen case reward equal -1 .
valu discount factor  chosen problem equal 0.98 .
dynam system integr use euler method 0.001s integr time step .
generat data sampl may consid episod start initi state correspond bicycl stand go straight line s0 = ( 0 , 0 , 0 , 0 , 0 ) = ( 0 , 0 , 0 , 0 , 0 ) fix valu  chosen random 0  [  ,  ] stop bicycl leav region repres \ st mean termin state st .
simul benchmark use onlin apibrm run 500s simul time collect around 50000 sampl .
run split separ learn episod initi random initi state s0 = ( 0 , 0 , 0 , 0 , 0 ) 0  [  ,  ] stop termin state reach otherwis 1s ( 100 step ) .
polici improv perform everi kp = 10 step ( 0.1s ) use greedi polici 0 = 1 reach valu  = 0.1 200s .
also use rbf kernel paramet  = i7  = 1.5 regress paramet chosen c = 10  = 0.01 select use grid search .
figur 6 show perform final polici found onlin apibrm along onlin learn process .
perform measur evalu score grid initi state s0 = { ( 0 , 0 , 0 , 0 , 0 } 0  [  ,  ] .
figur 7 report state action subsequ learn trial .
trial last 50s max ( 5000 step ) consid minimum balanc reach goal .
use method-2 ( onlin growth ) fix initi state s0 = ( 0 , 0 , 0 , 0 , 0 ) optim local approxim found less 50s simul time .
lower part figur 7 also show trajectori learn process well final one .
final number support vector necessari repres approxim action valu function set paramet use approxim usual stay 5 % total number collect sampl also indic qualiti approxim .
14 januari 28 , 2015 journal experiment & theoret artifici intellig main b c figur 7 .
bike balanc : ( upper ) state action repres subsequ learn trial .
trial last 50s max ( 5000 step ) consid suffici reach goal .
use method-2 ( online-growth ) small perturb fix initi state s0 = ( 0 , 0 , 0 , 0 , /2 ) api  brm may learn local optim polici episod ( 50s simul time ) .
( lower ) sketch trajectori ( b zoom , c overal ) time interv ( 0 , 500s ) bicycl ( xb , yb ) plane control final polici api brm 11 .
conclus develop model free brm approach call api brm abl find optim polici continu state rl problem studi practic implement issu .
particular , demonstr problem find optim polici minim bellman residu cast regress problem use svr appropri- ate rkhs .
main contribut work experiment analysi non parametr approxim algorithm general problem rl use polici iter kernel method .
algorithm eventu converg optim polici use mix distribut data sampl .
interest properti api brm algorithm : api brm quit effici problem sampl experi spars .
algorithm base approxim polici iter , power frame- work met success most among plan problem .
also open new research direc- tion use kernel base approxim polici iter context learn .
api brm api algorithm make good use function approxim implic- 15 januari 28 , 2015 journal experiment & theoret artifici intellig main it construct approxim model use kernel .
api  brm use increment svr allow estim approxim state action valu function rl .
polici iter done implicit time new experi obtain .
api brm complex strong depend cost one pay order solv svr essenti quadrat problem optim .
svr solv batch mode whole set train sampl dispos learn agent increment mode enabl addit remov train sampl effect .
final come theoret bound perform statist converg guarante .
acknowledg work partial support fi-dgr programm agaur eco/1551/2012 .
refer leemon baird .
residu algorithm : reinforc learn function approxim .
proceed twelfth intern confer machin learn , page 3037 .
morgan kaufmann , 1995 .
l. busoniu , d. ernst , b .
de shutter , r. babuska .
onlin least-squar polici iter reinforc learn control .
us baltimor , editor , proceed american control confer acc-10 , page 486491 , 2010 .
lucian busoniu , alessandro lazar , mohammad ghavamzadeh , remi muno , robert babuka , bart schutter .
least-squar method polici iter .
marco wier mar- tijn otterlo , editor , reinforc learn , volum 12 adapt , learn , opti- mizat , page 75109 .
springer berlin heidelberg , 2012 .
isbn 978-3-642-27644-6. .
url http : //dx.doi.org/10.1007/978-3-642-27645-3_3 .
ernst daniel , geurt pierr , whenkel lui .
tree base batch mode reinforc learn .
journal machin learn research , 6:503556 , 2005 .
michail g. lagoudaki ronald parr .
least-squar polici iter .
journal machin learn research , 4:11071149 , 2003 .
url http : //dblp.uni-trier.de/db/journals/ jmlr/jmlr4.html # lagoudakisp03 .
mario martin .
on-lin support vector machin regress .
proceed 13th european confer machin learn , ecml  02 , page 282294 , london , uk , uk , 2002 .
springer- verlag .
isbn 3-540-44036-4 .
url http : //dl.acm.org/citation.cfm ? id=645329.650050 .
jett randlov paul alstrom .
learn drive bycicl use reinforc learn shape .
proceed fifth intern confer machin learn , page 463 471 , 1998 .
h.o .
wang , k. tanaka , m.f .
griffin .
approach fuzzi control nonlinear system : stabil design issu .
fuzzi system , ieee transact , 4 ( 1 ) :1423 , feb 1996 .
16
