real-tim human-robot interact system base gestur assist scenario real-tim human-robot interact system base gestur assist scenario gerard canala , b , ,  , sergio escalerac , , cecilio angulob acomput vision center , campus uab , edifici , 08193 bellaterra ( cerdanyola ) , barcelona , spain bdept .
automat control , upc - barcelonatech , fme build , pau gargallo 5 , 08028 barcelona , spain cdept .
matematica aplicada analisi , ub , gran via de les cort catalan 585 , 08007 barcelona , spain dinstitut de robotica informatica industri , csic-upc , lloren artiga 4-6 , 08028 barcelona , spain abstract natur intuit human interact robot system key point develop robot assist peopl easi effect way .
paper , human robot interact ( hri ) system abl recogn gestur usual employ human non-verb communic introduc , in-depth studi usabl perform .
system deal dynam gestur wave nod recogn use dynam time warp approach base gestur specif featur comput depth map .
static gestur consist point object also recogn .
point locat estim order detect candid object user may refer .
point object unclear robot , disambigu procedur mean either verbal gestur dialogu perform .
skill would lead robot pick object behalf user , could present difficulti .
overal system which compos nao wifibot robot , kinecttm v2 sensor two laptop first evalu structur lab setup .
, broad set user test complet , allow assess correct perform term recognit rate , easi use respons time .
keyword : gestur recognit , human robot interact , dynam time warp , point locat estim correspond author .
email address : gerard.can @ cvc.uab.cat ( gerard canal ) , sergio @ maia.ub. ( sergio escalera ) , cecilio.angulo @ upc.edu ( cecilio angulo ) preprint submit comput vision imag understand april 1 , 2016 1 .
introduct autonom robot make way human inhabit environ- ment home workplac : entertain , help user domest activ daili live , help disabl peopl person care basic activ , would improv autonomi qualiti life .
order deploy robot system inhabit unstructur social space , robot endow communic skill user in- teract would intuit , eventu consid minim train .
besid , given great part human communic carri mean non-verb channel [ 1 , 2 ] , skill like gestur recogni- tion human behavior analysi reveal use kind robot system , would includ view understand surround human inhabit .
gestur recognit activ field research comput vision benefit mani machin learn algorithm , tempor warp [ 3 , 4 , 5 ] , hidden markov model ( hmms ) , support vector machin ( svms ) [ 6 ] , random forest classifi [ 7 ] deep learn [ 8 ] , mention .
moreov , gestur recognit person techniqu also propos [ 9 ] adapt system given user .
studi human comput interact ( hci ) specif human robot interact ( hri ) take advantag field .
henc , mani recent contribut [ 10 , 11 , 12 , 13 , 14 ] consid kinecttm-lik sensor recogn gestur given discrimin inform provid multi-mod rgb-depth data .
kinecttm base ap- plicat introduc [ 15 ] take order servic elder care robot .
static bodi postur analyz assist robot [ 16 ] detect whether user open toward robot interact .
communic gestur contrast daili live activ [ 17 ] intuit human robot inter- action .
novic user generat his/her gestur librari semi-supervis way [ 18 ] , recogn use non-parametr stochast seg- mentat algorithm .
[ 19 ] , user defin specif gestur mean messag human-robot dialogu , [ 20 ] framework defin user gestur control robot present .
deep neural network use [ 21 ] recogn gestur real time consid rgb inform .
point gestur , similar one propos paper , stud- ie most focus hand gestur [ 22 ] , use hand orient face pose [ 23 ] .
point direct estim [ 24 , 25 ] use gaze finger orient , deictic gestur interact peopl use refer object environ studi [ 26 ] .
relat point interact also use robot guidanc [ 27 ] .
work introduc real time human robot interact ( hri ) system whose object allow user communic robot easi , natur intuit gesture-bas fashion .
experiment setup compos humanoid robot ( aldebaran  nao ) wheel platform ( wifibot ) carri nao humanoid kinecttm sensor .
set-up , multi- robot system abl recogn static dynam gestur human 2 base geometr featur extract biometr inform dynam program techniqu .
gestur understand deictic visual indic user , robot assist him/her task pick object floor bring user .
order valid system extract robust conclus interact behavior , propos system test offlin experi , report high recognit rate , well extens set user test 67 peopl assess perform .
remaind paper organ follow : section 2 introduc method use gestur recognit human robot interact .
section 3 present experiment result includ offlin user test , final , section 4 conclud paper .
2 .
gestur base human robot interact aim studi gestur communic hri , robot system develop abl understand four differ gestur human user interact : wave ( hand rais move left right ) , point ( outstretch arm ) , head shake ( express disagr ) nod ( head gestur agreement ) .
overal robot system involv sever element : aldebaran  nao robot , small size humanoid robot suitabl interact hu- man user ; microsoft  kinecttm v2 sensor get rgb-depth visual data environ track user ; , given vision sensor ex- ceed nao  robot capabl ( size comput perform ) , nexter robot  wifibot wheel platform use carri sensor well nao , eas navig precis long rang .
fact , propos robot system take inspir darpa robot challeng 20151 humanoid robot drive car toward interest place exit car order finish work foot .
similar way , wheel robot ad system order carri sensor along littl humanoid , also exit complet task walk .
multi-robot setup allow nao use inform kinect  stm v2 sensor eas navig .
side , nao one charg direct interact user , also abl act environ , instanc , grasp object .
overal setup shown figur 1 , nao seat wifibot .
setup also includ laptop intel i5 processor deal kinecttm  data anoth intel core 2 duo laptop , send command robot use robot oper system ( ros ) 2 [ 28 ] .
depth map process use point cloud librari ( pcl ) 3 [ 29 ] , bodi track inform obtain use 1theroboticschallenge.org 2ros.org 3pointclouds.org 3 http : //theroboticschallenge.org/ http : //www.ros.org/ http : //pointclouds.org/ kinecttm v2 sdk .
figur 1 : robot system design work .
system program interact applic , test sever user differ age relat robot world ( see section 3.2 ) .
2.1 .
real time gestur recognit : interact robot section explain method use perform gestur recognit imag understand .
given applic system enhanc interact human user robot , defin gestur natur user possibl , avoid user train learn specif set gestur .
instead , robot understand gestur human would understand anoth human  gestur , repli visual stimulus real time .
consid set human gestur divid two categori , depend amount movement involv execut :  static gestur user place his/her limb specif posit stand , without dynam movement in- volv .
case , transmit inform obtain static pose configur .
point object exampl static gestur .
 dynam gestur , contrast , movement main gestur  featur .
transmit inform come type 4 movement well execut veloc .
may also contain partic- ular pose limb movement .
exampl dynam gestur wave salut someon gestur hand ask someon approach user  locat .
four differ gestur includ design system interact robot , three dynam remain one static .
dynam gestur wave , nod facial negat gestur .
static one point object .
categori tackl use differ approach .
next describ extract featur , gestur recognit method gestur  semant inform extract .
2.1.1 .
definit gestur specif featur gestur recognit perform base featur extract user bodi inform obtain depth map .
includ arm gestur possibl new gestur involv bodi part , skelet data obtain depth imag kinecttm sensor use kinecttm sdk v2.0 .
given limb gestur wave depend posit part bodi leg , rest bodi taken consider recognit perform .
, rather direct use joint coordin whole bodi , [ 4 , 30 ] , propos method take account involv limb distinct featur extract .
approach allow system recogn gestur time skelet data proper track sensor , includ situat sit ( instanc person wheelchair ) , well stand crouch .
applic abl recogn four gestur : point , wave , nod head negat .
point gestur  featur skeleton display figur 2a .
describ :  p , euclidean distanc hand hip joint bodi part .
featur discrimin point posit rest one arm may outstretch side bodi point place .
 p , elbow joint angl , defin angl vector elbow joint shoulder one vector elbow hand joint .
defin arm outstretch .
 p , posit hand joint .
given present setup overal structur robot system , featur account larg point gestur ( full arm ex- tend ) , one one would use point someth lay ground .
featur dynam wave gestur shown figur 2b .
defin : 5  w , euclidean distanc neck hand joint .
although necessari order perform test current set gestur , measur could normal divid longitud arm standard valu rang [ 0 , 1 ] handl bodi variat .
 w , elbow joint angl , defin point gestur .
elbow angl use featur requir normal- izat affect differ bodi height .
( ) point gestur fea- ture .
( b ) wave gestur featur dynam .
figur 2 : skelet gestur featur .
orient face provid sensor use describ nod gestur ( vertic movement head ) negat one ( horizont movement head ) .
three usual angular axe  pitch , roll yaw  use instead take absolut valu , deriv employ frame featur , oi , = oi , aoi1 , , oi , orient degre face frame accord axi .
moreov , one f frame use comput featur filter noisi orient estim , valu threshold given valu order end sequenc direct chang .
formal , featur frame axi , fi , , comput : fi , = ( |oi , a|  )  sign ( oi , ) .
( 1 ) figur 3 depict facial gestur .
2.1.2 .
dynam gestur recognit dynam time warp ( dtw ) [ 31 ] approach use detect dy- namic gestur .
dtw algorithm match two tempor sequenc find 6 figur 3 : facial gestur featur dynam .
vertic arrow repres nod gestur horizont one negat .
minimum align cost .
one sequenc refer gestur model gestur g , rg = { r1 , .
.
.
, rm } , in- put stream = { s1 , .
.
.
, s } , ri si featur vector .
featur depend gestur recogn : wave , ri = { wi ,  w } ri = { fi , pitch , fi , roll , fi , yaw } facial gestur .
sequenc align mean comput m n dynam program matrix , n length tempor window use discret infi- nite time , data keep enter system gestur identifi .
provid gestur spot need , minimum valu n two .
element mi , j  repres distanc subsequ { r1 , .
.
.
, ri } { s1 , .
.
.
, sj } , comput : mi , j = ( ri , sj ) +min ( mi , j1 , mi1 , j , mi1 , j1 ) , ( 2 ) (  ,  ) distanc metric choic .
differ distanc metric use implement .
instanc , ham distanc : dh ( ri , sj ) = o k=0 { rki 6= k j } , ( 3 ) number featur gestur , use facial gestur case .
weight l1 distanc employ case wave gestur , comput : dl1 ( ri , sj ) = o k=0 k|rki  k j | , ( 4 ) k posit weight constant .
7 gestur g consid recogn subsequ input data stream similar enough refer sequenc rg : mm , k  g , k , ( 5 ) g obtain use train method gestur g , detail section 3.1.1 .
order assur fulfil real time constraint , dtw execut multi-thread way differ gestur spread be- tween differ thread run gestur recognit method simultan , stop case one method find gestur input sequenc .
case need proper segment gestur begin-end manner , valid purpos , warp path found locat begin gestur sequenc .
warp path : w = { w1 , .
.
.
, wt } , ( 6 ) max ( , n )  < m+ n+ 1 , matrix pair index contigu element matrix defin map refer ges- ture rg subsequ input sequenc , subject follow constraint :  w1 = ( 1 , j ) wt = ( , j ) .
 wt1 = ( a , b ) wt = ( , b ) a a  1 b b  1 .
warp path w minim warp cost : cw ( ) = min ww   1t  t t=1 mwt   , ( 7 ) found matrix backtrack minimum path mm , j , m1 , k , k start point segment gestur j end .
2.1.3 .
static gestur recognit static approach select static gestur recognit , sens gestur consid recogn featur within certain valu given number contigu frame small movement involv .
number frame featur threshold obtain similar train method dynam case .
case , point gestur recogn , certain number frame f , elbow angl greater threshold tea indic arm outstretch distanc hand hip greater certain distanc td mean arm rest posit .
moreov , hand coordin use order check constraint 8 posit hold still move .
, gestur recogn follow constraint held fp frame :  p > td ,  p > tea , de (  p ,  p i1 )  0 , ( 8 ) de repres euclidean distanc .
system run static gestur recognit parallel dynam one , multi-thread way .
2.1.4 .
point locat estim point gestur recogn , inform need extract order perform associ task help user .
main inform deictic gestur give point locat , region surround space element interest user .
estim , floor plane descript , point direct coordin belong ground need .
first , arm posit obtain order know point direct .
, arm joint last ten frame gestur averag obtain mean direct avoid track error .
, coordin hand joint h elbow joint e use get point direct  eh = h  e vector .
even though kinecttm v2 sensor provid inform hand tip joint , direct provid elbow hand vector prove precis hand hand tip one preliminari test .
ground plane extract use plane estim method pcl librari [ 32 ] .
depth imag kinecttm obtain convert point cloud , plane segment use random sampl consensus ( ransac ) method [ 33 ] .
plane similar orthogon vector refer calibr plane use floor plane .
refer plane automat obtain system start segment plane depth imag keep paramet plane whose orthogon vector vertic axi ( axi ) sensor .
case camera parallel posit ground plane found fulfil condit , refer plane obtain user click three point ground graphic interfac , plane estim .
, ground point coordin obtain pick one element floor cloud .
therefor , let pf ground point ~nf = ( , b , c ) orthogon vector floor plane f = ax+by+cz+d = 0 , point point pp obtain : pp = h + ( pf h )  ~nf  eh  ~nf   eh .
( 9 ) exampl point locat estim shown figur 4a .
test user , observ bone correct track kinecttm sensor precis enough get accur 9 ( ) point locat esti- mation .
( b ) exampl user point deviat .
figur 4 : exampl point gestur .
point direct .
clear point gestur per- form hand front bodi .
also , user tend actual point farther object  locat , real point line inter- sect object , observ figur 4b .
order deal imprecis , correct point posit slight translat point locat backward .
2.1.5 .
near point object segment disambigu similar human respons point gestur , want robot look surround estim point locat detect possibl object user refer .
notic case care recogn actual object rather detect presenc .
perform first extract set point x scene point cloud xi  x select euclidean distanc de point point smaller certain valu r , de ( xi , pp )  r , x spheric point cloud radius r center point point pp .
extract floor plane , z = x \ { xi | xi  f } , object isol cluster algorithm appli sub point cloud z order join point object smaller point cloud per object .
cluster algorithm use euclidean cluster extract method [ 32 ] , start cluster pick point zi  z join neighbor zj  z de ( zi , zj ) < dth , dth user defin threshold .
process repeat neighbor 10 point found , case cluster ci obtain .
remain point cloud z process way get object cluster .
object found , centroid point comput mean coordin point cluster , 1|ci|  zci z , cluster  convex hull reconstruct order comput area .
allow system get notion posit space size ( see figur 4a ) .
howev , may case point locat clear near singl object , doubt refer one .
situat aris , spoken disambigu process start robot ask user object .
, robot may ask person point biggest object object clear differ size , otherwis ask relat posit , instanc ask question like  object right ?  .
user respond question yes utter , recogn use nao  built speech recognit softwar , perform equival facial gestur , robot know refer object two , may ask anoth question case three dubious object sight .
flowchart disambigu process includ supplementari materi .
2.2 .
robot interact human gestur recognit make robot system abl understand human gestur .
, human user must abl recogn robot interact success pleasant .
case , mean robot must work togeth order fulfil task respond user appropri way .
instanc , wifibot abl perform precis navig , wherea nao ideal interact speak user well act environ .
mean answer system visual stimuli made person expect , thus natur respons gestur .
figur 5 show flow applic normal use case .
applic program use state machin paradigm control workflow .
detail implement state machin shown supplementari materi .
wave gestur , expect respons would wave back user , perform similar gestur one made him/her mayb perform utter .
case point gestur , robot approach point locat analyz object present , tri deduc object user refer .
notic need user point place field view sensor , possibl point object farther away also make robot go point locat check object .
object known disambigu case doubt , nao goe wifibot ( figur 6 ) approach object , shown user perform gestur hand head expos understood object correct , seen figur 7 .
note could extend grasp object bring user .
11 figur 5 : exampl applic  use case .
3 .
experiment result order evalu design system , sever experi carri , includ offlin evalu method onlin evalu whole system extens set user test .
3.1 .
offlin evalu gestur recognit method evalu offlin set order valid perform method tune set paramet valu .
henc , small data set  hupba sequenc  generat label .
includ 30 sequenc 6 differ user ( 5 sequenc per user ) perform four gestur system abl recogn , well anoth arbitrari gestur choic ; perform random order .
gestur model use dynam gestur recognit modul specif record purpos one user perform gestur ideal way .
ideal way taken observ record sequenc , also take account observ gestur base system quotidian interact peopl .
model subject part subject data set .
order evalu system , two metric usual use domain adopt : jaccard index ( also known overlap ) defin j ( , b ) = |ab| |ab| , f1 score , comput f1score = 2tp 2tp+fp+fn .
12   figur 6 : nao  go wifibot approach object .
figur 7 : nao show point object .
3.1.1 .
paramet evalu result order comput perform measur , leave-one-subject-out cross valid ( losocv ) techniqu use .
, subject data set left grid search perform order tune best paramet differ method gestur system .
, paramet use sequenc left user perform metric obtain .
procedur repeat subject result averag everi subject sequenc order obtain final score .
carri paramet tune , interv valu test set record , keep perform better .
interv paramet use test includ dtw threshold wave  [ 6.75 , 9.5 ] , consid equal distribut valu step 0.25 , nod = negat  [ 4.5 , 20 ] step 0.5 .
distanc weight 13 wave gestur   [ 0.1 , 0.55 ] step 0.05 .
facial gestur  paramet test orient deriv threshold  [ 5 , 30 ] step 5 number frame sampl f  [ 1 , 20 ] increment 1 unit .
static gestur , threshold number frame td  [ 0.1 , 0.45 ] step 0.5 tea  [ 2.0 , 2.55 ] step 0.05 .
rang chosen empir perform initi test use sequenc includ variat gestur , record purpos .
figur 8 show obtain result standard deviat differ- ent user .
figur 8a plot result f1 measur differ overlap threshold decid amount overlap enough consid tp .
meanwhil , figur 8b show result use jaccard index measur differ number  care  frame .
observ , wave point gestur one better recognit rate , point slight better accord jaccard index .
facial gestur , nod present better perform negat measur .
facial gestur present wors perform due fact mani user perform gestur subt differ length vari consider way term orient .
also get hamper distanc user camera orient valu subtl farther user .
even though , get losocv f1 score 0.6  0.61 ( mean  standard deviat loso subject ) nod gestur 0.610.15 negat one overlap threshold 0.4 , result accept get natur interact real time system .
focus jaccard index plot figur 8b , observ best mean perform obtain 7  care  frame use , reach 0.65 0.07 overlap .
use  care  frame comput jaccard index make sens natur interact applic goal segment gestur frame level detect gestur , despit frame detect start end .
use 7 frame ( three previous begin , begin frame three ) enough solv tempor differ detect label data .
3.2 .
user test evalu order evalu system  perform , test differ user real scenario .
opinion collect use easi con- sider accord need extern intervent part communic .
test user select differ age group educ back- ground , might never seen humanoid robot , analyz behavior check task fulfil .
test took place differ envi- ronment , tri keep user known comfort scenario , includ two high school , communiti center elder social associ .
total 67 user particip experi .
14 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 overlap threshold f 1 e u f1 measur result wave nod negat point mean ( ) f1 measur result .
0 5 10 15 20 25 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 number `` care '' frame j c c rd n e x e u jaccard index measur result wave nod negat point mean ( b ) overlap ( jaccard index ) measur result .
figur 8 : offlin perform evalu result .
15 screenplay test follow : user stand front robot system two three object place ground , around three meter far .
user first wave robot , point object elect , answer facial gestur robot ask question disambigu .
otherwis , user ask perform facial gestur end test .
procedur usual repeat twice user , fill questionnair experi end .
video show execut exampl system includ supplementari materi .
object two milk bottl cooki box , gestur recog- nition paramet obtain use train mechan previous explain , time  hupba sequenc  use tune paramet .
object cluster extract , radius 55 centimet around point locat use , suitabl valu use object .
figur 9 show user perform test differ environ .
3.2.1 .
user  survey analysi section highlight interest result obtain user  questionnair test .
result analyz three age group .
figur 10 show bar plot relev question , aggreg age group .
tabl 1 includ answer numer question questionnair .
mean  sd question min max 9-34 year 35-60 year 61-86 year wave  respons speed 1 5 3.79 0.74 3.89 0.90 4.00 1.05 point  respons speed 1 5 3.66 0.91 3.88 1.02 4.00 1.41 figur point object 1 5 4.00 1.16 3.76 1.09 3.55 1.75 nao clear show guess 1 5 4.32 0.97 4.12 0.99 3.82 1.72 natur interact 2 5 3.57 0.63 3.53 1.00 4.14 0.90 tabl 1 : numer user  answer survey ( answer number 1 5 ) .
summari , user age 9 86 year , averag 34.823.98 .
divid three group : 9 34 , 35 60 61 86 year , youngest user last group age 71 .
gender quit balanc , 55 % user male , seen figur 10a .
moreov , zero small previous contact kind robot .
wave gestur agre natur user , age group , even though user problem reproduc need explan would wave anoth way .
respons obtain robot one would expect consid quick enough , mean robot act natur way need help understand respons gave , seen figur 10b , 10c tabl 1 .
result point gestur quit similar , 16 ( ) user high school .
( b ) user anoth high school .
( c ) user communiti center .
( ) user elder social associ .
figur 9 : exampl user perform test .
17 natur quit fast equival result differ age group , even though user expect robot someth object grasp open bottl ( figur 10d , 10e ) .
moreov , user thought point time enough 35 % user felt much time ( although kept point object robot said gestur alreadi recogn ) , shown figur 10f .
nao  respons , robot miss right object case , thought clear show object robot understood without ambigu , seen tabl 1 .
facial gestur perform user , felt comfort , nod exagger .
fact , 46 % peopl youngest group made nod gestur felt unnatur exagger , shown figur 10g .
negat gestur similar respons ( see figur 10h ) .
general , facial gestur present disadvantag long hair peopl hair cover face perform ( special negat case ) , impli face tracker lost face gestur recogn .
88 % user thought easi answer yes/no question system .
final , overal interact felt quit natur , seen tabl 1 , much user felt frustrat due system misunderstand gestur , seen figur 10i .
user know robot moment test shown figur 10j , case due languag difficulti , robot spoke english4 .
henc , 36 % user speak english need extern support translat .
92 % user state enjoy test ( 100 % elder group ) , vast major user thought applic kind use assist peopl household environ , special elder one reduc mobil , depict figur 10l .
moreov , almost thought easi communic task gestur manner , figur 10k show .
last question ask possibl gestur addit system .
interest respons includ gestur call come back , start , stop indic nao sit wifibot .
3.3 .
system time recognit rate order obtain object evalu metric , 30 addit test perform six user ( five gestur per user ) conduct .
respons time differ gestur along recognit rate , well execut time object detect modul extract .
tabl 2 tabl 3 show obtain result .
seen , respons time tabl 2 , span end gestur start robot respons , quit suitabl natur interact , gestur answer less two second averag .
4most user  mother tongu either spanish catalan .
18 9  34 35  60 61  86 0 3 6 9 12 15 18 21 24 27 30 33 36 39 age ( year ) n u b e r f u e rs user age distribut male femal ( ) natur unnatur hard perform 0 4 8 12 16 20 24 28 32 36 40 44 48 52 answer n u b e r f u e rs wave gestur natur 9  34 year 35  60 year 61  86 year ( b ) yes 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 answer n u b e r f u e rs expect wave  respons 9  34 year 35  60 year 61  86 year ( c ) natur unnatur hard perform 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 answer n u b e r f u e rs point gestur natur 9  34 year 35  60 year 61  86 year ( ) yes 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 answer n u b e r f u e rs expect point  respons 9  34 year 35  60 year 61  86 year ( e ) yes 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 answer n u b e r f u e rs thought point long 9  34 year 35  60 year 61  86 year ( f ) figur 10 : user answer questionnair .
19 natur exagger unnatur hard perform 0 2 4 6 8 10 12 14 16 18 20 22 24 answer n u b e r f u e rs nod gestur opinion 9  34 year 35  60 year 61  86 year ( g ) natur exagger unnatur hard perform 0 1 2 3 4 5 6 7 8 9 answer n u b e r f u e rs negat gestur opinion 9  34 year 35  60 year 61  86 year ( h ) yes 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 answer n u b e r f u e rs felt frustrat test 9  34 year 35  60 year 61  86 year ( ) yes 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 answer n u b e r f u e rs felt confus point 9  34 year 35  60 year 61  86 year ( j ) yes 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 answer n u b e r f u e rs easi tell task 9  34 year 35  60 year 61  86 year ( k ) yes 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 answer n u b e r f u e rs use household environ 9  34 year 35  60 year 61  86 year ( l ) figur 10 : user answer questionnair .
20 item time ( second ) mean  sd recognit rate wave gestur 1.72  0.62 83.33 % point gestur 1.91  0.67 96.67 % nod gestur 1.99  0.49 73.33 % negat gestur 1.47  0.47 33.33 % object detect 0.53  0.29 63.33 % tabl 2 : respons execut time recognit rate differ gestur object detect 30 test .
dynam gestur recognit time span end gestur system respons , static one start gestur object respons .
gestur time measur use standard chronomet oper test control .
object detect , compris time order robot segment object respons wifibot  laptop , comput less second .
look recognit rate , best recogn gestur point one .
negat gestur one lowest recognit rate , case offlin result , main face well track face sideward camera .
system also show high recognit rate object detect even though error , detail tabl 3 .
caus rate wrong point locat estim 3.33 % object detect wrong object detect 16.67 % disambigu failur 3.33 % navig error ( reach place ) 13.33 % tabl 3 : error rate caus object detect step 30 test .
4 .
conclus work , present multi-robot system design interact human user real time gestur base manner .
system proof concept show import interact phase order abl assist user special need , elder handicap peopl .
con- sequent , could interact robot way use human be , robot use inform provid user help .
instanc , robot could pick someth floor without need actual recogn object know person refer deictic gestur .
21 includ gestur recognit method base kinecttm v2 sensor take account dynam gestur , recogn dtw use specif featur face bodi , static gestur deictic one refer someth present environ .
multi-robot system shown effect way combin effort special robot , one carri weight sensor comput power precis navig , abl speak interact natur way user .
collabor perform task lead success system interact .
furthermor , extens set user test carri 67 user littl contact robot abl perform test minim extern indic , result natur interact case .
offlin test also show high recognit rate perform real time gestur detect spot specif record data set .
nevertheless , differ element system detect point direct could improv futur work .
instanc , use accur hand pose estim like one propos [ 34 , 35 , 36 ] may allow direct finger use obtain point direct , probabl result precis locat estim .
facial gestur anoth element could high improv , first tri use better facial tracker proper handl side view ( clear affect detect negat gestur ) , also explor ad kind featur .
acknowledg would like thank la garriga  town council , youth center radio silenc help user test communic organ , well follow entiti , associ peopl locat la garriga : associacio de gent gran l  esplai de l  espaicaixa ,  la torr del fanal  com- muniti center , institut villa romana , escola sant llus goncaga , pujol- buckl famili allow us perform test facil .
special thank also dr. marta daz guidelin user test analys , joan guasch josep maria canal help wifibot adapta- tion , vctor vlchez proofread .
work partial support spanish ministri economi competit , project tin 2012-38416-c03-01 tin2013-43478-p .
research fellow gerard canal thank fund grant issu catalunya - la pedrera foundat .
refer [ 1 ] j. devito , m. hecht , nonverb communic reader , waveland press , 1990 .
22 [ 2 ] c. breazeal , c. kidd , a. thomaz , g. hoffman , m. berlin , effect non- verbal communic effici robust human-robot team- work , : proceed ieee/rsj intern confer in- tellig robot system , 2005 .
( iro 2005 ) , 2005 , pp .
708713 .
doi:10.1109/iros.2005.1545011 .
[ 3 ] a. hernandez-vela , m. a. bautista , x. perez-sala , v. ponce-lopez , s. es- calera , x. baro , o. pujol , c. angulo , probability-bas dynam time warp bag-of-visual-and-depth-word human gestur recog- nition rgb-d , pattern recognit letter 50 ( 0 ) ( 2014 ) 112121 , depth imag analysi .
doi:10.1016/j.patrec.2013.09.009 .
[ 4 ] m. rey , g. domnguez , s. escalera , featur weight dynam time warp gestur recognit depth data , : proceed 2011 ieee intern confer comput vision workshop ( iccv workshop ) , 2011 , pp .
11821188 .
doi:10.1109/iccvw.2011.6130384 .
[ 5 ] k. kulkarni , g. evangelidi , j. cech , r. horaud , continu action recog- nition base sequenc align , intern journal comput vision 112 ( 1 ) ( 2015 ) 90114 .
doi:10.1007/s11263-014-0758-9 .
[ 6 ] b. liang , l. zheng , multi-mod gestur recognit use skelet joint motion trail model , : l. agapito , m. m. bronstein , c. rother ( ed .
) , comput vision - eccv 2014 workshop , vol .
8925 lectur note comput scienc , springer intern publish , 2015 , pp .
623638 .
doi:10.1007/978-3-319-16178-5_44 .
[ 7 ] n. camgz , a. kindiroglu , l. akarun , gestur recognit use templat base random forest classifi , : l. agapito , m. m. bronstein , c. rother ( ed .
) , comput vision - eccv 2014 workshop , vol .
8925 lectur note comput scienc , springer intern publish , 2015 , pp .
579594 .
doi:10.1007/978-3-319-16178-5_41 .
[ 8 ] d. wu , l. shao , deep dynam neural network gestur segmenta- tion recognit , : l. agapito , m. m. bronstein , c. rother ( ed .
) , comput vision - eccv 2014 workshop , vol .
8925 lectur note comput scienc , springer intern publish , 2015 , pp .
552571 .
doi:10.1007/978-3-319-16178-5_39 .
[ 9 ] a. yao , l. van gool , p. koh , gestur recognit portfolio personal- izat , : 2014 ieee confer comput vision pattern recog- nition ( cvpr ) , 2014 , pp .
19231930 .
doi:10.1109/cvpr.2014.247 .
[ 10 ] o. lope , m. rey , s. escalera , j. gonzalez , spheric blur shape model 3-d object pose recognit : quantit analysi hci applic smart environ , ieee transact cyber- netic 44 ( 12 ) ( 2014 ) 23792390 .
doi:10.1109/tcyb.2014.2307121 .
23 http : //dx.doi.org/10.1109/iros.2005.1545011 http : //dx.doi.org/10.1016/j.patrec.2013.09.009 http : //dx.doi.org/10.1109/iccvw.2011.6130384 http : //dx.doi.org/10.1007/s11263-014-0758-9 http : //dx.doi.org/10.1007/978-3-319-16178-5_44 http : //dx.doi.org/10.1007/978-3-319-16178-5_41 http : //dx.doi.org/10.1007/978-3-319-16178-5_39 http : //dx.doi.org/10.1109/cvpr.2014.247 http : //dx.doi.org/10.1109/tcyb.2014.2307121 [ 11 ] s. iengo , s. rossi , m. staffa , a. finzi , continu gestur recognit flexibl human-robot interact , : proceed 2014 ieee inter- nation confer robot autom ( icra ) , ieee , 2014 , pp .
48634868 .
doi:10.1109/icra.2014.6907571 .
[ 12 ] h. kim , s. hong , h. myung , gestur recognit algorithm move kinect sensor , : proceed 2013 ieee ro-man , 2013 , pp .
320 321. doi:10.1109/roman.2013.6628475 .
[ 13 ] a. ramey , v. gonzalez-pacheco , m. a. salich , integr low-cost rgb-d sensor social robot gestur recognit , : proceed 6th intern confer human-robot interact , hri  11 , acm , new york , ny , usa , 2011 , pp .
229230 .
doi:10.1145/1957656 .
1957745 .
[ 14 ] t. fujii , j. hoon lee , s. okamoto , gestur recognit system human- robot interact applic robot servic task , : proceed intern multiconfer engin comput scientist ( imec 2014 ) , vol .
, intern associ engin , newswood limit , 2014 , pp .
6368 .
[ 15 ] x. zhao , a. m. naguib , s. lee , kinect base call gestur recog- nition take order servic elder care robot , : proceed- ing 23rd ieee intern symposium robot human interact communic ( ro-man 2014 ) , 2014 , pp .
525530 .
doi : 10.1109/roman.2014.6926306 .
[ 16 ] d. mccoll , z. zhang , g. nejat , human bodi pose interpret classi- ficat social human-robot interact , intern journal social robot 3 ( 3 ) ( 2011 ) 313332 .
doi:10.1007/s12369-011-0099-6 .
[ 17 ] a. chrungoo , s. manimaran , b. ravindran , activ recognit nat- ural human robot interact , : m. beetz , b. johnston , m.-a .
william ( ed .
) , social robot , vol .
8755 lectur note comput sci- enc , springer intern publish , 2014 , pp .
8494 .
doi:10.1007/ 978-3-319-11973-1_9 .
[ 18 ] e. bernier , r. chellali , i. m. thouvenin , human gestur segment base chang point model effici gestur interfac , : proceed 2013 ieee ro-man , 2013 , pp .
258263 .
doi:10.1109/roman .
2013.6628456 .
[ 19 ] d. michel , k. papoutsaki , .
a. argyro , gestur recognit support- ing interact human social assist robot , : g. be- bis , r. boyl , b. parvin , d. koracin , r. mcmahan , j. jerald , h. zhang , s. drucker , c. kambhamettu , m. el choubassi , z. deng , m. carlson ( ed .
) , advanc visual comput , vol .
8887 lectur note comput scienc , springer intern publish , 2014 , pp .
793804 .
doi:10.1007/978-3-319-14249-4_76 .
24 http : //dx.doi.org/10.1109/icra.2014.6907571 http : //dx.doi.org/10.1109/roman.2013.6628475 http : //dx.doi.org/10.1145/1957656.1957745 http : //dx.doi.org/10.1145/1957656.1957745 http : //dx.doi.org/10.1109/roman.2014.6926306 http : //dx.doi.org/10.1109/roman.2014.6926306 http : //dx.doi.org/10.1007/s12369-011-0099-6 http : //dx.doi.org/10.1007/978-3-319-11973-1_9 http : //dx.doi.org/10.1007/978-3-319-11973-1_9 http : //dx.doi.org/10.1109/roman.2013.6628456 http : //dx.doi.org/10.1109/roman.2013.6628456 http : //dx.doi.org/10.1007/978-3-319-14249-4_76 [ 20 ] m. obaid , f. kistler , m. hring , r. bhling , e. andr , framework user- defin bodi gestur control humanoid robot , intern journal social robot 6 ( 3 ) ( 2014 ) 383396 .
doi:10.1007/s12369-014-0233-3 .
[ 21 ] p. barro , g. parisi , d. jirak , s. wermter , real-tim gestur recognit use humanoid robot deep neural architectur , : 2014 14th ieee-ra intern confer humanoid robot ( humanoid ) , 2014 , pp .
646651 .
doi:10.1109/humanoids.2014.7041431 .
[ 22 ] j. l. raheja , a. chaudhari , s. maheshwari , hand gestur point loca- tion detect , optik - intern journal light electron optic 125 ( 3 ) ( 2014 ) 993  996. doi:10.1016/j.ijleo.2013.07.167 .
[ 23 ] m. pateraki , h. baltzaki , p. trahania , visual estim point target robot guidanc via fusion face pose hand orienta- tion , comput vision imag understand 120 ( 0 ) ( 2014 ) 1  13. doi:10.1016/j.cviu.2013.12.006 .
[ 24 ] c. park , s. lee , real-tim 3d point gestur recognit mobil robot cascad hmm particl filter , imag vision comput 29 ( 1 ) ( 2011 ) 51  63. doi:10.1016/j.imavis.2010.08.006 .
[ 25 ] d. droeschel , j. stuckler , s. behnk , learn interpret point gestur time-of-flight camera , : proceed 2011 6th acm/iee intern confer human-robot interact ( hri ) , 2011 , pp .
481488 .
[ 26 ] c. matuszek , l. bo , l. zettlemoy , d. fox , learn unscript deictic gestur languag human-robot interact , : proceed 28th nation confer artifici intellig ( aaai ) , quebec citi , quebec , canada , 2014 .
[ 27 ] a. jevtic , g. doisi , y. parmet , y. edan , comparison interact modal- iti mobil indoor robot guidanc : direct physic interact , person follow , point control , ieee transact human-machin system 45 ( 6 ) ( 2015 ) 653663 .
doi:10.1109/thms.2015.2461683 .
[ 28 ] m. quigley , k. conley , b. p. gerkey , j. faust , t. foot , j. leib , r. wheeler , a. y. ng , ros : open-sourc robot oper system , : icra workshop open sourc softwar , 2009 .
[ 29 ] r. b. rusu , s. cousin , 3d : point cloud librari ( pcl ) , : pro- ceed ieee intern confer robot autom ( icra ) , shanghai , china , 2011 .
[ 30 ] t. arici , s. celebi , a. s. aydin , t. t. temiz , robust gestur recog- nition use featur pre-process weight dynam time warp- ing , multimedia tool applic 72 ( 3 ) ( 2014 ) 30453062 .
doi : 10.1007/s11042-013-1591-9 .
25 http : //dx.doi.org/10.1007/s12369-014-0233-3 http : //dx.doi.org/10.1109/humanoids.2014.7041431 http : //dx.doi.org/10.1016/j.ijleo.2013.07.167 http : //dx.doi.org/10.1016/j.cviu.2013.12.006 http : //dx.doi.org/10.1016/j.imavis.2010.08.006 http : //dx.doi.org/10.1109/thms.2015.2461683 http : //dx.doi.org/10.1007/s11042-013-1591-9 http : //dx.doi.org/10.1007/s11042-013-1591-9 [ 31 ] h. sako , s. chiba , dynam program algorithm optim spo- ken word recognit , ieee transact acoust , speech signal process 26 ( 1 ) ( 1978 ) 4349 .
doi:10.1109/tassp.1978.1163055 .
[ 32 ] r. b. rusu , cluster segment , : semant 3d object map everyday robot manipul , vol .
85 springer tract advanc robot , springer berlin heidelberg , 2013 , ch .
6 , pp .
7585 .
doi:10 .
1007/978-3-642-35479-3_6 .
[ 33 ] m. a. fischler , r. c. boll , random sampl consensus : paradigm model fit applic imag analysi autom car- tographi , communun acm 24 ( 6 ) ( 1981 ) 381395 .
doi : 10.1145/358669.358692 .
[ 34 ] j. tompson , m. stein , y. lecun , k. perlin , real-tim continu pose recoveri human hand use convolut network , acm transact graphic ( tog ) 33 ( 5 ) ( 2014 ) 169:1169:10. doi:10.1145/2629500 .
[ 35 ] f. kirac , y. e. kara , l. akarun , hierarch constrain 3d hand pose estim use regress forest singl frame depth data , pattern recognit letter 50 ( 2014 ) 91  100 , depth imag analysi .
doi : http : //dx.doi.org/10.1016/j.patrec.2013.09.003 .
[ 36 ] t. sharp , c. keskin , d. robertson , j. taylor , j. shotton , d. kim , c. rhe- mann , i. leichter , a. vinnikov , y. wei , d. freedman , p. koh , e. krupka , a. fitzgibbon , s. izadi , accur , robust , flexibl real-tim hand track- ing , : proceed 33rd annual acm confer human fac- tor comput system , chi  15 , acm , new york , 2015 , pp .
3633 3642. doi:10.1145/2702123.2702179 .
gerard canal receiv bachelor degre comput sci- enc universitat politecnica de catalunya ( upc ) 2013 .
obtain master degre artifici intellig universitat politecnica de catalunya ( upc ) , universitat de barcelona ( ub ) universitat rovira virgili ( urv ) , 2015 .
main research interest includ develop novel assist technolog base social robot involv comput vision .
current pursu ph.d. assist human-robot interact use comput vision techniqu .
sergio escalera obtain ph.d. degre multi-class vi- sual categor system comput vision center , uab .
obtain 2008 best thesi award comput scienc universitat autonoma de barcelona .
lead human pose recoveri behavior analysi group .
as- sociat professor depart appli mathemat analysi , universitat de barcelona .
also member comput vision center campus uab .
director 26 http : //dx.doi.org/10.1109/tassp.1978.1163055 http : //dx.doi.org/10.1007/978-3-642-35479-3_6 http : //dx.doi.org/10.1007/978-3-642-35479-3_6 http : //dx.doi.org/10.1145/358669.358692 http : //dx.doi.org/10.1145/358669.358692 http : //dx.doi.org/10.1145/2629500 http : //dx.doi.org/http : //dx.doi.org/10.1016/j.patrec.2013.09.003 http : //dx.doi.org/http : //dx.doi.org/10.1016/j.patrec.2013.09.003 http : //dx.doi.org/10.1145/2702123.2702179 chalearn challeng machin learn .
vice-chair iapr tc-12 : multimedia visual inform system .
research interest includ , be- tween other , statist pattern recognit , visual object recognit , hci system , special interest human pose recoveri behavior analysi multi-mod data .
cecilio angulo receiv m.s .
degre mathemat univers barcelona , spain , ph.d. degre scienc universitat politecnica de catalunya - barcelonatech , spain , 1993 2001 , respect .
1999 2007 , universitat politecnica de catalunya , assist professor .
nowaday as- sociat professor depart automat control , univers .
2011  also serv director master  degre automat control robot .
 current director knowledg engin research group respons research project area social cognit robot .
cecilio angulo author 250 technic public .
research interest includ cognit robot , machin learn algorithm social robot applic .
27 introduct gestur base human robot interact real time gestur recognit : interact robot definit gestur specif featur dynam gestur recognit static gestur recognit point locat estim near point object segment disambigu robot interact human experiment result offlin evalu paramet evalu result user test evalu user 's survey analysi system time recognit rate conclus
