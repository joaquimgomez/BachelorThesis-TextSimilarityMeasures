journal artifici intellig research 1 (1993) 1-15 submit 6/91; publish 9/91 behavior convolut net featur extract dario garcia-gasulla ferran pare armand vilalta jonatan moreno barcelona supercomput center (bsc) omega building, c. jordi girona, 1-3 08034 barcelona, catalonia eduard ayguad jesu labarta ulis cort barcelona supercomput center (bsc) universitat politecnica catalunya - barcelonatech (upc) toyotaro suzumura ibm t.j. watson research center, new york, usa barcelona supercomput center (bsc) abstract deep neural network represent learn techniques. training, deep net capabl gener descript languag unpreced size machin learning. extract descript languag code train cnn model (in case imag data), reus purpos field interest, provid access visual descriptor previous learnt cnn process million im- ages, requir expens train phase. contribut field (commonli known featur represent transfer transfer learning) pure empir far, extract cnn featur singl layer close output test perform feed classifier. approach provid consist results, relev limit classif tasks. complet differ approach, paper statist measur discrimin power singl featur deep cnn, character class 11 datasets. seek provid new insight behavior cnn features, particularli on convolu- tional layers, relev applic knowledg represent reasoning. result confirm low middl level featur behav differ high level features, certain conditions. cnn featur knowledg represent purpos presenc absence, doubl inform singl cnn featur provide. studi nois featur include, propos threshold approach discard it. insight direct applic gener cnn embed spaces. 1. introduct imag classif success applic deep learning. perfor- manc deep learn network challeng like imagenet larg scale visual recog- nition competit base capabl model build exception rich represent languag given dataset. languag solv c1993 ai access foundation. right reserved. garcia-gasulla, pares, & vilalta problem like classif detection, achiev state-of-the-art perform (he et al., 2016). coherently, deep learn model frequent defin represent learn techniqu (lecun et al., 2015). unfortunately, build represent language, deep network requir lot data lot comput effort, reduc number problem model directli appli to. deep learning, field research commonli known transfer learn tri reus represent languag learnt problem solv another. formal us notat introduc pan yang (2010). notat main compon summar follows; domain d defin set data instanc given probabl distribut (e.g., imag certain resolut certain distribut pixel values), task t defin set label target function (e.g., label assign images, function classifi imag accordingly). transfer learn deep learn frequent initi deep network featur learnt sourc problem (t ,ds), network optim later fine-tun process target problem (tt ,dt ). significantly, approach shown produc better result train network (tt ,dt ) scratch (i.e., random initialization) (yosinski et al., 2014). accord pan yang (2010), approach exampl induct transfer learning, label data avail tt ts . altern us transfer learn us neural network train ts featur extractor tt , order us machin learn method result representations. so, repres dt data languag learnt ts task, enabl us pre-train deep network represent (not deep network model themselves) dataset lack size requir train method (azizpour et al., 2016; sharif razavian et al., 2014). accord pan yang (2010) case exampl featur represent transfer (the analog term transfer learn featur extract wide used). notic approach transfer learn tackl unsupervis learn problem cluster (gui & morency, 2015), enabl us featur obtain unsupervis learn task (e.g., autoencoders). context convolut neural network (cnns), attempt featur rep- resent transfer focus reus activ obtain layer close output cnn (typic fully-connect layer). compar lower-level layers, high-level layer provid better result out-of-the-box feed clas- sifier cluster algorithm (azizpour et al., 2016; sharif razavian et al., 2014; gui & morency, 2015; donahu et al., 2014). regardless results, convolut layer cnn encod larg visual knowledg vari complex (yosinski et al., 2015; garcia-gasulla et al., 2017a), knowledg successfulli ex- ploit far. gener purpos featur represent transfer maxim representativeness, hypothes optim represent include, degree, inform larger varieti layer (i.e., ones). relevant, particularli knowledg represent reason fields, understand differ convolut layer fulli connect layers, learnt cnn properli exploited. 2 behavior convolut net featur extract context, paper analyz behaviour featur deep cnn purpos featur extraction. us deep cnn (vgg16, simonyan & zisserman, 2014) pre-train larg dataset ((t ,ds) = imagenet 2012 ) build imag represen- tation altern dataset ((tt ,dt ) {mit67, f lowers102, cub200, ...}), studi behavior individu featur domain defin dataset. first, section 2 in- troduc previou contribut transfer learn field, focus featur extraction. section 3 introduc dataset cnn model experiments, imag embed build featur extract process. basi studi sta- tistic distanc methods, review section 4. section 5, introduc behavior statist distanc current problem, distribut compose. distribut analyz section 6, impact nois measur discuss section 7. consist find given differ sourc task shown section 8. finally, conclus drawn studi summar section 9. 2. relat work cnn model defin larg number parameters, requir lot data instanc (typic images) optimization. releas larg visual datasets, hand-mad featur (perronnin et al., 2010) produc best result vision tasks. nowadays, cnn train dataset imagenet voc2012 (everingham et al., 2012), learn power visual descriptors, allow outperform previous competit solut improv fisher vector visual task (chatfield et al., 2014). donahu et al. (2014) present studi behavior convolut filter featur extract process. work, author studi cnn (alexnet architec- ture compos 5 convolut layer 3 fulli connect layers) train imagenet 2012 (t ,ds) transfer learning. qualitatively, author observ featur fulli connect layer outperform featur lower layer task separ concept accord wordnet hierarchy. author evalu featur extract convolut layer fulli connect layer datasets. result indic featur fulli connect layer train support vector machin (svm) achiev state-of-the-art result relat tasks. contribut sharif razavian et al. (2014) goe similar direction, overfeat network architectur pre-train imagenet 2012 (t ,ds). author fo- cu fulli connect layer, perform data augment increas qualiti featur (crop rotat samples), component-wis power transformation. appli l2-normal result vectors, svm train appli wide varieti task (tt {imag classification, fine grain recognition, attribut detection, visual instanc retrieval,...}) domain (dt {voc2007, flowers102, cub200,...) achiev competit result them. task (imag classi- fication), featur layer evalu separ svm, fulli connect layer obtain best results. differ approach depict yosinski et al. (2014), goal studi transfer featur purpos fine tune deep neural network target task dataset. regard, author distanc sourc target task strongli relat depth optim layer transfer learn process. 3 garcia-gasulla, pares, & vilalta azizpour et al. (2016) empir evalu paramet affect trans- fer learn process featur extraction. paramet consid relat architectur train initi cnn (network depth width, dis- tribut train data, optim parameters), relat transfer learn process (fine-tuning, network layer extracted, spatial pool dimensional- iti reduction). paramet evalu 17 visual recognit tasks, identifi set good paramet depend distanc sourc task target task. represent layer (which layer build embedding) author second fulli connect layer produc best result cases, feed svm classification. deep residu network (resnets) evolut tradit cnn includ branch paths. unlik cnns, stack layer sequentially, resnet implement short- cut connect eas converg train process, allow train network layer (up thousands). mahmood et al. (2016) explor us resnet featur extraction, particularli solv imag classif problems. re- sult indic resnet competit altern classic cnn architectures, context featur extraction. long et al. (2015) propos new deep adapt network (dans) architectur solv problem domain adapt convolut neural networks. architectur convolut layer paramet reus modification, weight convolut layer fine-tun new task. fulli connect weight tailor fit specif task multipl kernel maximum mean discrep (mk-mmd). however, approach consid problem solv differ target task impact reusabl pre-train features. 3. sourc target problem featur represent transfer context deep learn particularli us target problem (tt ,dt ) includ label data train cnn represent language. context, (tt ,dt ) problem solv featur craft differ (t ,ds) problem, qualiti result embed represent strongli depend similar (d ,ts) (dt ,tt ) are. languag learnt (t ,ds) lack vocabulari properli character particular (tt ,dt ) (e.g., ds defin black white images, dt includ color patterns), result embed poor qualiti learn appli deficient. mind, (t ,ds) typic chosen captur rang visual pattern broad possible, languag like includ featur relev differ target tasks, capabl character wide varieti imag domains. regard, cnn train imagenet 2012 dataset (russakovski et al., 2015) good candid sourc problem, 1,000 categori compos ts requir huge varieti visual patterns, larg number imag avail ds guarante learnt model gener differ domains. 4 behavior convolut net featur extract 3.1 cnn architectur complet specifi ts need label space y object predict function f(). case function f() defin train cnn (i.e., architectur parameters). popular cnn architectures, featur extract (see section2). goal explor behavior convolut layer featur extract process, us architectur follow canon scheme layer (i.e., conv/pool/conv/pool/.../fc). time, wish us model capabl learn rich represent languag level (i.e., deep network). combin requir lead us vgg16 architectur sourc featur (simonyan & zisserman, 2014). vgg16 compos 13 convolut layer (with 5 pool layers) 3 fully-connect layer (see tabl 1 detail architecture). except figur 2, 3, 4 7 obtain vgg19 architecture, illustr purposes. architectur authors, detail paper. differ vgg16 have 3 extra convolut layer conv3 4, conv4 4 conv5 4. result obtain vgg19 architectur consist on obtain vgg16 experiments. model publicli avail author web page1. 3.2 target dataset defin sourc task domain (t ,ds), let introduc publicli avail dataset consid target (tt ,dt ) studi transfer learning: 1. mit indoor scene recognit dataset (quattoni & torralba, 2009) (mit67 ) con- sist differ indoor scene 67 categories. main challeng resid class depend global spatial properti rel presenc objects. 2. caltech-ucsd birds-200-2011 dataset (wah et al., 2011) (cub200 ) fine-grain dataset contain imag 200 differ speci birds. 3. oxford flower dataset (nilsback & zisserman, 2008) (flowers102 ) fine-grain dataset consist 102 flower categories. 4. oxford-iiit-pet dataset (parkhi et al., 2012) (catsdogs) fine-grain dataset cover 37 differ breed cat dogs. 5. stanford dog dataset (khosla et al., 2011) (stanforddogs) fine-grain dataset contain imag 120 breed dog imagenet 2012. 6. caltech-101 dataset (fei-fei et al., 2007) (caltech101 ) classic dataset 101 object categori contain clean imag low level occlusion. 7. caltech-256 dataset (griffin et al., 2007) (caltech256 ) contain 256 object categori larger minimum number imag class caltech101. 8. food-101 dataset (bossard et al., 2014) (food101 ) larg dataset 101 popular food categories. particular case us test split data reduc size avoid label-nois present train split. 1. deep/ 5 garcia-gasulla, pares, & vilalta 9. describ textur dataset (cimpoi et al., 2014) (textures) databas textur categor accord list 47 term inspir human perception. 10. oulu knot dataset (silven et al., 2003) (wood) contain knot imag spruce wood, classifi accord nordic standards. old dataset industri applica- tion consid challeng human experts. 11. us valid split imagenet 2012 (russakovski et al., 2015) (imagenet) target problem comparison purposes. notic imag compos dataset differ train set imagenet 2012, differ distribut impli domain different, similar (d ' dt ). disambigu refer imagenet 2012 talk dataset imagenet talk target task tt . tabl 1: detail vgg16 architectur (simonyan & zisserman, 2014). layer: number filter (i.e., neuron uniqu set parameters), learnabl paramet (i.e., weight biases), neuron featur represent transfer process. layer #filter #paramet #neuron # embed featur input 150k - conv1 1 64 1.7k 3.2m 64 conv1 2 64 36k 3.2m 64 pool1 max pool 802k - conv2 1 128 73k 1.6m 128 conv2 2 128 147k 1.6m 128 pool2 max pool 401k - conv3 1 256 300k 802k 256 conv3 2 256 600k 802k 256 conv3 3 256 600k 802k 256 pool3 max pool 200k - conv4 1 512 1.1m 401k 512 conv4 2 512 2.3m 401k 512 conv4 3 512 2.3m 401k 512 pool4 max pool 100k - conv5 1 512 2.3m 100k 512 conv5 2 512 2.3m 100k 512 conv5 3 512 2.3m 100k 512 pool5 max pool 25k - fc6 4,096 103m 4k 4,096 fc7 4,096 17m 4k 4,096 output 1,000 4m 1k - total 13,416 138m 15m 12,416 6 behavior convolut net featur extract tabl 2: properti dataset experi averag dataset #imag #class #imag class #imag class |i| |c| |ic| |ic| imagenet 50,000 1000 50 50 cub200 11,788 200 41 - 60 59 wood 438 7 14 - 179 63 flowers102 8,189 102 40 - 258 80 caltech101 9,146 101 31 - 800 91 mit67 6,700 67 100 100 textur 5,640 47 120 120 caltech256 30,607 256 80 - 800 120 stanforddog 20,580 120 150 - 200 172 catsdog 7,349 37 184 - 200 199 food101 25,250 101 250 250 dataset sizes, number class number imag class specifi tabl 2. experi train model datasets, mean requir provid train test splits. instead, merg split us data available. 3.3 embed given sourc target problem (t ,ds), (tt ,dt ), paramet modifi construct embed space. paramet explor azizpour et al. (2016). case, us main paramet consid coher study. first, imag represent built result process 10 crop imag (4 corner middl crop, mirrored) cnn averag result activations. frequent methodolog featur extract (sharif razavian et al., 2014; azizpour et al., 2016), provid robust result representations. second, perform spatial averag pool convolut layer obtain singl valu filter. transform reduc number featur embedding, rel spatial inform (i.e., result featur determin visual pattern imag average, regardless exact location), maintain descript power (i.e., featur separ account embedding). spatial pool methodolog recurr solut field (sharif razavian et al., 2014; azizpour et al., 2016). wish explor behavior convolut layers, embed contain 16 convolut layer avail vgg16 (from conv1 1 conv5 3). comparison purpos extract fulli connect layer (fc6,fc7), contrast behavior convolut fulli connect features. notic spatial pool perform convolut layer appli fc layers. compon result embedding, compos 12,416 values, shown tabl 1. remain document, mention embed refer representation. 7 garcia-gasulla, pares, & vilalta 4. statist distanc method previou studi us convolut layer featur extract transfer learn- ing pure empirical, base perform specif classifi (most fre- quently, svm) featur extract singl layer cnn (see section 2). approach shown provid consist results, limit classification, strongli influenc choic classifi (e.g., classifi perform better certain number variables, affect differ noise). paper propos differ approach evalu behavior cnn features. instead evalu perform specif machin learn algorithm embed- ding, measur descript power cnn featur statistically, studi behavior differ class compos dataset. goal learn descript natur cnn features, knowledg represent reason methodolog adapt accordingly. detail, approach consist evalu characterist featur embed is, target class consid datasets. words, want evalu descript power group featur (which featur select problem) analyz discrimin power singl feature. cnn neuron crisp behavior w.r.t. class (i.e., neuron activ binarili depend class), origin train task. instead, cnn neuron provid fuzzi piec inform class. contextu inform provid individu features, consid activ given class target task tt (inner-class behaviour), compar activ happen rest class dataset tt (outer-class behaviour). insight featur perform repres singl class dataset. inner/out class behaviour visual histogram featur activ (see left plot figur 1). statist speaking, rescal histogram den- siti estim approxim true probabl densiti function (pdf). sophist method avail (scott, 2015) us histogram sake com- putat simplicity. studi behavior given cnn featur given class (a featur class pair), compar correspond inner/out densiti estimations. statist distanc consid purpos well-known kullback-leibl (kl) divergence. kullback-leibl diverg measur pdf, p q, differ follow equat 1, point domain. dkl(p,q) = p (i)ln p (i) q(i) (1) histogram approxim pdfs, possibl fit pdf (e.g., normal distribution, uniform distribution, etc) histogram. is, however, inconveni histogram differ featur fit differ pdfs, featur properli fit pdf. mutual inform measur inform random variables, x y , share. understood expect kullbackleibl diverg univari distribut p(x) condit distribut p(x|y). inform gain greater 8 behavior convolut net featur extract figur 1: left: inner/out class densiti estim featur n458 layer conv4 3, class pink-yellow dahlia (60) flowers102 dataset. right: accumul distribut kolmogorov-smirnov statist (dks) data. right line indic maximum distance, correspond kolmogorov-smirnov statistic. differ distribut p(x|y) p(x) grows. experiments, y analog belong class c. thus, p(x|y) repres inner-class distribution, p(x|y) repres outer-class distribution. p(x|y) approxim p(x) number sampl class bigger class c, formal |ic| |ic| p(x|y) ' p(x). case, usual task high number class evenli distribut samples, mutual inform approxim kullback-leibl divergence. bhattacharyya distanc altern kl measur distanc discret probabl distributions. analogously, measur densiti estim p q follow equat 2, discret point domain x. db(p,q) = ln ( ix p (i)q(i) ) (2) compar densiti estimations, bhattacharyya distanc directli data, have choos fit pdf. however, mathemat rang posit ([0,)), make bhattacharyya distanc unabl identifi densiti estim below. analysi know inner-class behaviour higher outer-class behavior vice versa, situat provid differ insights. kolmogorov-smirnov statist (dks) measur distanc empir distribut function (edf) p q. point domain x, dk evalu distanc p (i) q(i), obtain maximum. formal defin equat 3 graphic displai right plot figur 1. reduc comput cost evalu dk , discret domain edf 100 bins, decreas domain resolut 1%. notic that, edf cumul distribution, dk directli comput set valu (one pair inner/out class behaviours). dks(p,q) = sup ix |p (i)q(i)| (3) 9 garcia-gasulla, pares, & vilalta contrast bhattacharyya distance, dk s mathemat rang [0, 1]. us sign variant dk sign indic edf point p q differ most. variant extend rang dk [1, 1] allow differenti inner class behavior outer class behavior (dk > 0) vice versa (dk < 0). dk = 0 mean distribut identical, dk = 1 dk = 1 mean distribut intersect point. kolmogorov-smirnov statist requir fit pdf (unlik kullback-leibl divergence) desirable. reasons, follow experi us sign version kolmogorov-smirnov statist (dks). 5. statist distanc behavior statist distanc analysi base inner/out class dk . section introduc behavior dk values, distribut valu layer-wis comput given datasets. follow section 5.1 contain detail studi distribut perspectives. simpli put, distanc dks(f, c) ' 0 mean distribut activ featur f imag belong class c (i.e., ic) ident distribut valu featur f imag belong class (i.e., ic). dks(f, c) > 0 featur valu imag ic tend higher rest imag ic, impli visual element repres featur f commonli ic imag ic images. similarly, dks(f, c) < 0, featur valu gener lower ic ic, impli element repres featur f rare ic imag compar rest dataset. illustr behavior explor featur highest dk valu mit67 dataset (i.e., closer 1). figur 2 show dks(f, c) valu differ layers, indic class c larg dk valu occurs. particular featur encoding, plot 9 imag crop imagenet 2012 valid set (i.e., imagenet) produc highest activ valu feature. imag dataset provid better featur characterizations, cnn featur origin train classes. exampl figur 2, featur have high dk valu greenhous class show plants, grass fields, regardless layer depth. featur high dk valu buffet class identifi food plates, featur high dk valu cloister identifi gothic arches. case featur conv3 3 n145, present high dk valu florist class, crop produc high activ correspond color pattern contrast surroundings. analog studi posit dk valu figur 2, consid lowest dk valu (i.e., closer -1). initially, expect featur have lowest dk given class c identifi element appear imag c. example, hypothet class whale expect extrem neg dk featur identifi wheel. however, dk valu comput context dataset (i.e., indic inner/out class disparity) assumpt incomplete. matter fact, featur have lowest dk given class c identifi element appear imag c rare compar frequenc rest images. example, dataset compos class whale clownfish, featur lowest dk valu class whale correspond 10 behavior convolut net featur extract conv3 4 n202 conv4 2 n89 conv5 4 n277 fc7 n1426 f v u li za ti o n class greenhous greenhous florist greenhous d+k 0.9144 0.9059 0.9287 0.9515 conv3 3 n145 conv4 3 n293 conv5 4 n449 fc7 n1779 f v u li za ti o n class florist florist buffet cloister d+k 0.8641 0.8923 0.9259 0.9174 figur 2: 8 embed featur high dk (among 10) mit67 dataset vgg19 architecture. class produc high dk shown feature. featur correspond specif neuron (for fulli connect layers) filter (for convolut layers) origin cnn model. illustr captur visual pattern feature, 9 crop imag imagenet 2012 valid set produc highest activ valu neuron filter. imag crop match neuron recept field. have highest dk valu class clownfish, like featur identifi orang relat patterns. hand, featur identifi uncommon pattern class (i.e., wheel) dk valu close zero classes, inner/out class distribut similar. illustr behavior extrem neg dk values, figur 3 show featur extrem neg dk valu (among 10 lowest) differ class cub200 dataset. particular featur (the n1946 fc7 layer) appar special recogn fly animals, shown set imag imagenet val- idat set produc maximum featur activ (see row figur 3). deeper analysi feature, base method yosinski et al., 2015, indic 11 garcia-gasulla, pares, & vilalta central color figur clutter background influenti featur activation. nevertheless, accord dk study, featur produc neg valu class birds. explan li particular class featur produc extrem neg dk values: class correspond bird live water coastal environment, dull color (see second row figur 3). feature, hand, appar special identifi color fly anim ly branches. case, extrem neg dk valu neuron analog identifi fly anim dull color absenc visual features. exampl behavior flowers102 dataset shown figur 4. again, featur produc 10 neg dk valu repres dataset (class flowers), specif classes. featur encod visual pattern correspond radial orang red pattern (featur n1449 fc7), focus wide petal (featur n3529 fc7). clearly, class flower highli neg dk valu properties. hence, abnorm absenc featur (e.g., spear thristl class shown figur 4), roughli character flower radial pistil wide petals. fc7 n1946 f ea tu v u li za ti o n sa m p le class gadwal brown pelican white pelican heermann gull dk 0.8880 0.8736 0.8624 0.8553 figur 3: exampl filter extrem neg dk valu differ class cub200 dataset vgg19 architecture. row illustr visual pattern captur feature, method figur 2. second row contain sampl imag cub200 dataset differ classes, classes. 12 behavior convolut net featur extract fc7 n1449 fc7 n3529 f ea tu v u li za ti o n sa m p le class 35 alpin sea holli 10 globe thistl 14 spear thistl 14 spear thistl 29 artichok dk 0.9517 0.9373 0.9293 0.9466 0.9438 figur 4: exampl filter have extrem neg dk valu differ class flowers102 dataset vgg19 architecture. row illustr visual pattern captur feature, method figur 2. second row contain sampl imag flowers102 dataset differ classes, classes. exampl illustr lack featur activ convei relev information. notic behavior neg dk valu depend context provid dataset, extrem neg valu class happen featur consist high valu rest dataset. statistically, extrem neg valu featur happen small set classes, since, set class grew, inner/out class dispar decrease, make dk closer zero. capabl extract knowledg lack data novel particularli relev featur represent transfer, featur origin design target task. setting, presenc absenc visual pattern provid relev inform character images. let discuss relev behavior fine-grain datasets, contain class belong small, similar famili entities. extrem neg dk valu identifi infrequ low featur activations, need featur frequent dataset (e.g., fly anim bright color live tree frequent featur birds). happen fine-grain datasets, common featur data. however, broad dataset includ wider visual varieti class (e.g., imagenet, mit67 ), fewer featur frequent class infrequ few. hence, harder obtain extrem neg dk values. 13 garcia-gasulla, pares, & vilalta 5.1 statist distanc distribut introduc essenti behavior posit neg dk , consid overal distribut dk valu dataset. plot dks(f, c) valu produc dataset, obtain clear bimod distribution, separ posit (d+ks) neg (dks) values. figur 6 example. modal resembl log-norm distribution. repres distribut dk valu layer dataset singl plot, figur 5 flatten distribut displai correspond mode error bars. discuss result distributions, let defin term us follow sections. data represent good model target domain dt consid highli descriptive, capabl character associ data. hand, data represent good model target task tt consid highli discriminative, capabl separ associ labels. categor features, highli descript on help build rich represent domain, highli discrimin on help solv classif task. context study, discrimin featur f w.r.t. class c shown close -1 1 dks(f, c) (a discrimin featur expect produc differ valu ic ic). unfortunately, descript featur illustr term dk values, descript origin domain task label (which dk measures). refer discrimin featur refer definition. figur 5: inner/out class dk distribut layer 11 differ dataset em- bedding. distribut d+k d ks similar log-norm distribut repres mode central point error bar enclos 68% accumul probabl (equival normal distribution). 14 behavior convolut net featur extract let discuss distribut dk valu shown figur 5. regardless layer depth, featur discrimin task (either posit negatively) dk valu close zero. separ d + ks d ks decreas deeper layer (the fully-connect ones), particularli task differ sourc task. deeper featur special sourc task, featur turn irrelev new task, produc similar activ valu ic ic, turn result dk valu closer zero. indeed, distanc d+k d ks decreas dataset essenti subset sourc task, imagenet, caltech101 caltech256. behavior dk distribut fully-connect layer discuss section6.2. investig variabl behavior dk distribut base layer depth, figur 6 show distribut separ plots: featur convolut layer featur fulli connect layers. accord plot figur 6, featur convolut layer equal discrimin datasets, task direct subset sourc task (e.g., imagenet). fur- thermore, number degre posit discrimin featur symmetr number degre neg discrimin features. indic convolu- tional featur contain similar inform exploit modalities. behavior dk distribut convolut layer discuss section6.1. insight coher find state-of-the-art, indic featur high-level layer specif discriminant, particularli target task close sourc task (azizpour et al., 2016). however, result indic featur low-level layer gener discrimin origin considered. open door us knowledg represent purpos relat problem unsupervis learning. 5.2 desir distribut dk valu get detail analysis, let consid character us featur perspect dk distributions. help motiv conclus draw consequ analysis. mention before, featur high absolut dk valu given task discrimi- nativ class task. consid featur layer together, figur 6, desir distribut densiti concentr possibl extrem x axis. plot figur 6 convolut featur averag separ irrelev dk = 0. however, bibliographi plenti experi fully-connect layer shown outperform convolu- tional layer classification. explan phenomenon correl dk valu discrimin linear, discrimin grow rapidli approach dk = 1 dk = 1. result, have featur dk = 0.3 good have singl featur dk = 0.5. higher discrim power fully-connect featur report bibliographi certain dataset support distribut dk valu (bottom plot figur 6) which, datasets, slightli higher longer tail plu distribut convolut featur (top plot figur 6). dataset (e.g., wood, flowers102 ) convolut featur discrim power fully-connect features. 15 garcia-gasulla, pares, & vilalta figur 6: dk distanc distribut convolut (top) fulli connect (bottom) layer 11 differ dataset embedding. x axi indic dk valu featur class, y axi indic occurr percentag correspond- ing total features/class pair (f, c). 6. analysi statist behavior section discuss observ plot introduc previou section. follow section separ analyz behavior convolut layer fully-connect layers. 6.1 analysi convolut layer figures, dataset order averag number imag class. shown figur 5, clear correl number dk valu obtain low convolut layer (less imag class caus extrem dk values). correl decreas layer depth non-exist fully-connect layers. 16 behavior convolut net featur extract middl higher layer affect properti like dataset similarity, later. start analysi distribut convolut layer shown figur 5 fo- cuse unusu long bar wood and, certain degree, flowers102 datasets. behaviour obviou convolut layer d+k modality, attenu later layer symmetr dk modality. have rel imag class (a particular class wood dataset 14 samples), flowers102 dataset, special wood dataset, compos class differ small texture-lik characteristics. low level convolut layer known learn filter similar gabor filter color blob (yosinski et al., 2014), appropri solv sort problems. factor explain featur disproportion discrimin datasets. textur dataset displai behavior, dataset specif textur patterns, answer li composit dataset. addit have imag class, imag textur dataset displai textur imag level, look pixel imag (a low convolut layer do) imposs identifi textur (e.g., larg portion imag label wrinkl singl wrinkle). coherently, discrimin featur dataset on middl upper convolut layers. exampl behavior low level convolut layers, figur 7 show featur layer conv1 1 conv1 2 produc high d+k valu specif class flowers102 dataset. particular featur correspond horizont gradients, vertic gradient edg detectors, featur appear infrequ imag class compar rest flower dataset. behavior wood flowers102 dataset convolut layers, distribut dk valu stabl general. plot figur 6 show low-level convolut featur behav similarli dataset (includ imagenet). featur optim classif imagenet 2012 classes, roughli discrimin imagenet rest datasets. provid evid transfer learn fine tune produc good result (yosinski et al., 2014), indic featur layer ubiquit knowledg representation. dataset behav clearli differ extrem valu low-level layer featur (by have higher tail) wood dataset, reason previous discussed: detail class small sampl size. flowers102 caltech101 tail height average, dataset includ properti (both case flowers102, limit sampl size case caltech101 ). 6.2 analysi fulli connect layer let consid distribut dk valu fulli connect layer plot figur 6. dataset food101 distinct distribution, larg spike dk valu close 0 (close 8% featur class pair fall bin) dk valu close -1 1. behaviour like directli relat variabl domain, number imag class (food101 most, 250). inconsist food, larg number sampl lead differ activ class (e.g., caesar salad 17 garcia-gasulla, pares, & vilalta 35 alpin sea holli featur visual neuron conv1 2 n22 conv1 2 n8 conv1 2 n16 conv1 1 n60 d+k 0.8665 0.8663 0.8641 0.8393 figur 7: exampl low-level featur (from layer conv1 1 conv1 2) high d+k valu (among 10) given class flowers102 dataset vgg19 architecture. row contain sampl imag correspond class. second row contain featur visual imagenet, obtain process figur 2. includ differ ingredi present differ ways). variat lead indistinguish inner-class outer-class behaviors, turn result dk valu close zero. particular behavior food101 dataset extrapol dataset larger number imag class, catsdog stanforddog caltech256. dataset similar sourc task ts (imagenet2012 ), result indic similar task relev properti behavior fully-connect features. support distribut correspond caltech101 caltech256 datasets. differ averag number instanc class |ic| (91 - 120), total size (9,146 - 30,607) number class (101 - 256) dk distribut ident (see plot (a) figur 8). explor consideration, categor 11 dataset 3 groups, base degre overlap imagenet2012 : (a) dataset class direct subset on imagenet2012. group includ imagenet, stanforddogs, catsdogs, caltech101 caltech256. (b) dataset class partial intersect on imagenet2012. group includ cub200, flowers102 food101. (c) dataset class complet disjoint on imagenet2012. group includ wood, mit67 textures. 18 behavior convolut net featur extract (a) dataset subset imagenet2012 (b) dataset intersect imagenet2012 (c) dataset disjoint imagenet2012 figur 8: distribut dk valu featur fulli connect layer. plot subset datasets, group similar imagenet2012. x axi indic dk values, y axi indic occurr percentag total. 19 garcia-gasulla, pares, & vilalta figur 8 show distribut dk valu fully-connect features, plot separ groups. group (a) group distribut dk valu get close zero y axi dk = 0 dataset group. impli that, datasets, singl feature-class pair ident inner outer-class distribution. words, dataset fully-connect featur somewhat discrimin classes. dataset show behavior imagenet, caltech101 caltech256. wide spectrum dataset directli contain sourc task imagenet2012. significantly, happen regardless number class (1,000, 101 256 respectively). hand, dataset group (a) happen (stanforddog catsdogs) limit certain domain (dogs, cat dog respectively). dataset subset sourc task, fully-connect featur discrimin class. irrelev featur class pair like correspond featur character type element imagenet2012 restrict domain dataset (e.g., character non-liv things). indiscrimin featur prevent dk distribut reach zero dk = 0 point. impact includ wide spectrum class w.r.t. have indiscrimin featur support dataset fourth lowest percentag feature/class pair close dk = 0. textur dataset (see panel (c) figur 8), which, appar littl common sourc imagenet2012 task, includ wide varieti textur come plants, animals, man-mad objects, etc. general, bimod distribut group (a) (b) imbalanc group (c), dk distribut account significantli larger proport total area. hand, distribut valu group (c) closer distribut valu convolut featur (see plot figur 6), modal symmetrical. imbalanc behavior group (a) (b) explain natur fully-connect features, optim origin train strongli activ small subset class inhibit vast majority. result higher tail d+k side. hand, balanc behavior group (c) indic cases, instead activ strongli set classes, featur activ moder larger classes. particularli interesting, indic fully-connect featur treat convolut featur target task complet differ sourc task. 7. level nois threshold section 6, discuss distribut dk valu dataset level, assum dk valu evenli distribut class compos dataset. however, case, subset class compos dataset larg set relev features, subset class under-repres featur character them. answer question, figur 9 plot accumul distribut d+k valu class. black line repres singl class dataset. graph accumulative, show featur d+k valu greater x axi valu class. thus, dk = 0.2 (on x axis) plot number featur meet dk > 0.2 (on y axis) class. 20 behavior convolut net featur extract imagenet cub200 wood flowers102 caltech101 mit67 textur figur 9: dk posit distanc (d + ks) accumul distribut 11 differ dataset (black) embedding. line correspond differ class dataset. result dataset random label shown red. dash line mark t+ threshold introduc section 7.1. figur 9 show certain varianc class dataset given dk threshold. impli class richli character embed others, suspected. class reach 0 y axi dk = 0.4, reach 0 remark dk = 0.9. check have 0 featur dk = 0.4 impli 21 garcia-gasulla, pares, & vilalta caltech256 stanforddog catsdog food101 figur 9: (cont.) dk posit distanc (d + ks) accumul distribut 11 differ dataset (black) embedding. line correspond differ class dataset. result dataset random label shown red. dash line mark t+ threshold introduc section 7.1. class character embedding, figur behavior dataset random label (in red). obtain assign imag random label, keep total number instanc class unmodifi (i.e., shuffl real labels). notic process keep unchang properti like number class imbal number instanc class. random label observ character embed produc pure noisi class characterist target task. shown 11 dataset figur 9, random class drop 0 featur dk = 0.1 dk = 0.3. gap black red line allow assert class repres meaningfulli (i.e., randomness) certain point. trigger question portion curv prune maxim discrimin minim noise. equival ask minimum dk valu consid relev choos featur character class. 7.1 threshold measur main goal paper studi viabil convolut featur featur represent transfer. however, unspecificity, convolut featur gener nois sens provid inform relat target labels. plot figur 9 show inner outer class distribut random class provid insight actual magnitud noise. 22 behavior convolut net featur extract consid definit threshold t+ t dk , d + ks < t + dk > t safe discard featur represent transfer process. threshold allow determin featur like relev class, cancel signific noise. defin threshold impli trade-off, t+ t close zero result represent larger descript power, t+ t close 1 -1 respect result represent minimum noise. reliabl threshold (thi analog t+ t), propos maxim distanc dataset correspond version random label (a shown subfigur figur 9). defin distanc averag number featur have dk > x x rang [0, 1]. analog compute, point x axi subfigur figur 9, averag y axi valu black/r lines. averag black line behavior regular dataset, averag red line behavior random version. obtain differ valu obtain averag distanc (davg) dataset random version. formally, averag distanc valu dk = x is: davg(x) = cc |dks(f, c) > x| |c| ccrand |dks(f, c ) > x| |crand| f embed (4) c known class (labels) associ data, crand randomli associ class (random-labels). vertic bar | | denot set cardinality. given distanc measur davg(x), defin threshold t + t valu dk = x maxim davg(x) respect sub-domain d + ks d ks . tabl 3 show threshold 11 datasets. tabl 3: t+ t threshold defin maximum averag distanc dataset explored. d+k d ks region comput separately. fifth column show maximum davg distanc dataset random version. dataset t+ davg(t +) t davg(t ) imagenet 0.150 3,100 -0.121 4,368 cub200 0.185 2,289 -0.155 2,417 wood 0.176 3,087 -0.156 2,901 flowers102 0.141 3,186 -0.120 3,572 caltech101 0.148 3,480 -0.124 4,737 mit67 0.123 3,269 -0.106 3,745 textur 0.101 3,738 -0.089 4,553 caltech256 0.112 3,813 -0.096 5,172 catsdog 0.115 3,353 -0.098 4,727 stanforddog 0.110 3,055 -0.092 4,535 food101 0.072 3,555 -0.062 4,075 23 garcia-gasulla, pares, & vilalta clear correl t+ t number sampl class |ic|. indeed, logarithm curv fit t+ t respect |ic| obtain coeffici determin r2 0.82 0.84 respectively. indic number imag class good indic level nois expected. factor overshadow relev aspects, level similar tasks, import task exactli (i.e., imagenet). interest featur embed remain relev prune noisi on applic threshold. 12,416 featur embedding, approxim 3,300 featur t 4,000 t+ remain average. threshold correspond dataset plot vertic dash line figur 9, show class (all black lines) minim repres appli it. studi degre nois layer-wise, figur 10 plot percentag dks(f, c) valu kept t+ t threshold set layers. dataset group (a) (i.e., imagenet, stanforddogs, catsdogs, caltech101 caltech256 ), prune caltech101 caltech256 stanforddog catsdog imagenet cub200 wood flowers102 mit67 textur food101 figur 10: dataset layer, percentag feature/class pair remain pruning, origin label (blue) random label (green) (i.e., featur dk certain class higher t + lower t). 24 behavior convolut net featur extract feature-class pair evenli distribut convolut fully-connect layers. indic nois embed datasets. dataset group (b) (c), prune feature-class pair fully-connect layer signifi- cantli larger prune pair convolut layers. caus higher specif high level features, frequent irrelev character class differ sourc task. result us for, given target task, deter- mine featur layer build embedding. 8. consist sourc task section valid result consist sourc problem cnn training. purpos us vgg16 network architectur train places2 dataset (zhou et al., 2016). places2 task ts unrel imagenet 2012 contain larg set sampl (1.8 million). however, domain ds places2 dataset wide imagenet 2012, focus scene categori instead objects. analog figur 5, figur 11 show distribut dk valu layer embed creat places2 dataset. overall, distribut similar obtain imagenet2012 embedding. focus convolut layer confirm observ correl averag number instanc class |ic| dk values. particular behaviour wood flowers102 present. moreover, convolut layer conv1 1 conv3 3 look practic datasets. similarity, reinforc hypothesi generalist natur convolut features, regardless sourc target tasks. figur 11: inner/out class dk distribut layer 10 differ dataset embed creat places2 sourc task. detail figur 5 25 garcia-gasulla, pares, & vilalta case fully-connect features, behavior mit67 analog group (a) imagenet2012 embedding, mit67 closest task sourc task (i.e., places2 ). target task intersect sourc task, stanforddog catsdogs, displai complet opposit activity. remark differ embed second-to-last fully-connect layer, dk diverg contract significantli dataset (includ mit67 ). clear explan phenomenon, hypothes differ object characterist need classifi holist class places2 (i.e., scenes) caus featur fc6 layer (where inform aggregated) extrem specif sourc task. 9. conclus cnn featur represent transfer studi past perform classifi (most commonly, svm). contribut measur layer per- form discrimin class task cnn origin train for. contribut know that, consid together, featur compos fulli connect layer defin discrimin embed spaces. contrast contributions, purpos paper analyz behavior featur layer individually, measur relev knowledg representation. explor inner/out class activ feature, class datasets. conclus draw studi coher current state-of-the-art, novel. outlin all: typically, featur characterist given class presence, shown featur class absence, provid differ type inform modality. particularli relev fine-grain datasets, common featur characterist certain class absenc (e.g., bird dull color live water). novel contribut us knowledg represent reason purpos (section 5). featur convolut layer fulli connect layer highli specific, characterist class irrelev it. featur rest convolut layer convei variat information, characterist class presenc absence. motiv us distinct knowledg extract approaches, depend layer depth (section 5.1). certain tasks, convolut featur outperform fully-connect featur dis- crimin correspond labels. overall, result indic featur low-level layer gener discrimin origin considered, open door us knowledg represent purpos relat problem unsupervis learn (section 5.2). low middl level featur similar behavior dataset train (imagenet) rest target datasets. indic cnn fea- ture layer knowledg represent wide varieti dataset fine-tuning. previous propos bibliographi (section 6.1). 26 behavior convolut net featur extract previous claim bibliography, relev fully-connect featur strongli relat similar task origin problem network train for. however, wide spectrum task (i.e., contain sort classes) kei factor. wide spectrum domain similar sourc task guarante indiscrimin fully-connect featur (section 6.2). behavior fully-connect featur target task intersect sourc task similar behavior convolut features. context, set featur treat analog knowledg represent purpos (section 6.2). discrimin featur layer embed class dataset evaluated. mean that, knowledg represent setting, class indescribable, showcas rich represent languag built cnn layer (section 7). behavior random dataset obtain estim inner/out class distanc account noise. conserv threshold littl varianc defin datasets, appli threshold half featur embed remain relev (section 7.1). context key, featur extract knowledg represent processes. signific featur activ (or lack of) depend dataset reference. represent data neural network embed consid context abl exploit possibl modal information. conclusions, work provid methodolog identifi relev fea- ture (either presenc absence) deep cnn. appli approach present here, defin full-network embed (an embed layer network) outperform tradit single-lay embed classif task (garcia-gasulla et al., 2017b), improv perform multimod pipelin imag caption imag retriev task (vilalta et al., 2017). acknowledg work partial support joint studi agreement no. w156463 ibm/bsc deep learn center agreement, spanish govern programa severo ochoa (sev-2015-0493), spanish ministri scienc technolog tin2015-65316-p project generalitat catalunya (contract 2014-sgr-1051), core research evolut scienc technolog (crest) program japan scienc technolog agenc (jst). 27 garcia-gasulla, pares, & vilalta refer azizpour, h., razavian, a. s., sullivan, j., maki, a., & carlsson, s. (2016). factor transfer gener convnet representation. ieee transact pattern analysi machin intelligence, 38 (9), 17901802. bossard, l., guillaumin, m., & van gool, l. (2014). food-101min discrimin compo- nent random forests. european confer vision, pp. 446461. springer. chatfield, k., simonyan, k., vedaldi, a., & zisserman, a. (2014). return devil details: delv deep convolut nets. arxiv preprint arxiv:1405.3531. cimpoi, m., maji, s., kokkinos, i., mohamed, s., & vedaldi, a. (2014). describ textur wild. proceed ieee confer vision pattern recognition, pp. 36063613. donahue, j., jia, y., vinyals, o., hoffman, j., zhang, n., tzeng, e., & darrell, t. (2014). decaf: deep convolut activ featur gener visual recognition.. icml, vol. 32, pp. 647655. everingham, m., van gool, l., williams, c. k. i., winn, j., & zisserman, a. (2012). pascal visual object class challeng 2012 (voc2012) results.. fei-fei, l., fergus, r., & perona, p. (2007). learn gener visual model train examples: increment bayesian approach test 101 object categories. vision imag understanding, 106 (1), 5970. garcia-gasulla, d., ayguade, e., labarta, j., bejar, j., cortes, u., suzumura, t., & chen, r. (2017a). visual embed unsupervis extract abstract semantics. cognit system research, 42, 7381. garcia-gasulla, d., vilalta, a., pares, f., moreno, j., ayguade, e., labarta, j., cortes, u., & suzumura, t. (2017b). out-of-the-box full-network embed convolut neural networks. arxiv preprint arxiv:1705.07706. griffin, g., holub, a., & perona, p. (2007). caltech-256 object categori dataset.. gui, l., & morency, l.-p. (2015). learn transfer deep convnet represent group-spars factorization. proceed ieee intern confer vision. he, k., zhang, x., ren, s., & sun, j. (2016). deep residu learn imag recognition. proceed ieee confer vision pattern recognition, pp. 770778. khosla, a., jayadevaprakash, n., yao, b., & li, f.-f. (2011). novel dataset fine-grain imag categorization: stanford dogs. proc. cvpr workshop fine-grain visual categor (fgvc), vol. 2. lecun, y., bengio, y., & hinton, g. (2015). deep learning. nature, 521 (7553), 436444. long, m., cao, y., wang, j., & jordan, m. (2015). learn transfer featur deep adapt networks. bach, f., & blei, d. (eds.), proceed 32nd inter- nation confer machin learning, vol. 37 proceed machin learn research, pp. 97105, lille, france. pmlr. 28 behavior convolut net featur extract mahmood, a., bennamoun, m., an, s., & sohel, f. (2016). resfeats: residu network base featur imag classification. arxiv preprint arxiv:1611.06656. nilsback, m.-e., & zisserman, a. (2008). autom flower classif larg number classes. vision, graphic & imag processing, 2008. icvgip08. sixth indian confer on, pp. 722729. ieee. pan, s. j., & yang, q. (2010). survei transfer learning. ieee transact knowledg data engineering, 22 (10), 13451359. parkhi, o. m., vedaldi, a., zisserman, a., & jawahar, c. (2012). cat dogs. vision pattern recognit (cvpr), 2012 ieee confer on, pp. 34983505. ieee. perronnin, f., sanchez, j., & mensink, t. (2010). improv fisher kernel large- scale imag classification. european confer vision, pp. 143156. springer. quattoni, a., & torralba, a. (2009). recogn indoor scenes. proceed ieee confer vision pattern recognition, pp. 413420. ieee. russakovsky, o., deng, j., su, h., krause, j., satheesh, s., ma, s., huang, z., karpathy, a., khosla, a., bernstein, m., berg, a. c., & fei-fei, l. (2015). imagenet larg scale visual recognit challenge. intern journal vision (ijcv), 115 (3), 211252. scott, d. w. (2015). multivari densiti estimation: theory, practice, visualization. john wilei & sons. sharif razavian, a., azizpour, h., sullivan, j., & carlsson, s. (2014). cnn featur off-the- shelf: astound baselin recognition. proceed ieee confer vision pattern recognit workshops, pp. 806813. silven, o., niskanen, m., & kauppinen, h. (2003). wood inspect non-supervis clustering. machin vision applications, 13 (5), 275285. simonyan, k., & zisserman, a. (2014). deep convolut network large-scal imag recognition. arxiv preprint arxiv:1409.1556. vilalta, a., garcia-gasulla, d., pares, f., ayguade, e., labarta, j., cortes, u., & suzumura, t. (2017). full-network embed multimod embed pipeline. arxiv preprint arxiv:1707.09872. wah, c., branson, s., welinder, p., perona, p., & belongie, s. (2011). caltech-ucsd birds- 200-2011 dataset. tech. rep. cns-tr-2011-001, california institut technology. yosinski, j., clune, j., bengio, y., & lipson, h. (2014). transfer featur deep neural networks?. advanc neural inform process system 27, pp. 33203328. yosinski, j., clune, j., nguyen, a., fuchs, t., & lipson, h. (2015). understand neural network deep visualization. arxiv preprint arxiv:1506.06579. zhou, b., khosla, a., lapedriza, a., torralba, a., & oliva, a. (2016). places: imag databas deep scene understanding. arxiv preprint arxiv:1610.02055. 29