improv svm classif imbalanc dataset introduc new bias haydemar nunez central univers venezuela , venezuela lui gonzalez-abril universidad de sevilla , espana cecilio angulo technic univers catalonia , spain may 22 , 2017 abstract support vector machin ( svm ) learn imbalanc dataset , well learn machin , show poor perform minor class svm design induc model base overal error .
improv perform kind problem , low-cost post-process strategi propos base calcul new bias adjust function learn svm .
propos bias consid proport size class order improv perform minor class .
solut avoid introduc tune new paramet , also mod- ifi standard optim problem svm train .
experiment result 34 dataset , differ imbal de- gree , show propos method actual improv classifi- cation imbalanc dataset , use standard error measur base sensit g-mean .
furthermor , perform compar well-known cost-sensit smote scheme , with- ad complex comput cost .
keyword : support vector machin , post-process , bias , cost- sensit strategi , smote 1 1 .
introduct major problem face classif learn algorithm imbal class dataset .
appear mani ex- ampl one sever class , remain class .
domain situat aris medic diagnosi , text classif , fraud detect credit card usag , detect communic network intrus , among other .
sinc usual repres target classi- ficat task , scenario import obtain model ex- hibit high predict perform minor class .
howev , standard learn algorithm tend produc hypothesi good perform major class , construct classif model base error whole train set , independ repres balanc class .
solv problem , mechan exist allow algo- rithm show good perform minor class .
effect , sever strategi propos , re-balanc dataset sam- pling techniqu , construct classifi take account cost error differ class , combin ( ensembl ) result sev- eral classifi train differ data distribut ( garcia 2009 ; lopez , fernandez , garca , palad , herrera 2013 ; sun , wong , kamel 2009 ) .
case svm , learn mechan becom interest option deal imbalanc dataset , svm build classif model base subset train instanc ( cristianini shawe- taylor 2000 ; vapnik 1999 ) .
howev , like machin learn techniqu , svm minim error dataset generat model , bias toward major class imbal sever .
enhanc perform svm problem imbalanc class , sever solut propos .
gen- eral applic , like sampl techniqu re-balanc dataset pre- process stage ; , specif , consid svm  particular featur like base cost-sensit learn ( batuwita palad 2013 ) .
re- search paper suggest use post-process stage order reduc bias toward major class classifi learn svm ( garcia 2009 ) .
2 follow last research line , strategi svms base cal- culat new bias threshold propos .
new bias consid class  proport dataset allow tune origin function learn svm improv perform minor class .
pro- pose solut neither introduc new paramet , modifi origin optim problem svm train .
paper organ follow : section 2 briefli introduc svm learn mechan provid overview strategi improv perform kind problem .
section 3 , propos post- process procedur determin new bias detail .
section 4 , present experi perform verifi applic propos , along analysi result , comparison perform new approach cost-sensit scheme .
final , conclus research present .
2 .
svm imbalanc dataset svm base statist learn theori appli suc- cess classif regress problem differ domain ( cris- tianini shawe-taylor 2000 ; oneto , ridella , anguita 2016 ; vapnik 1999 ) .
hypothesi space learn machin hyperplan ( linear decis surfac ) .
train look decis function maximum margin separ class .
thus , binari clas- sific task set train data z = { ( x1 , y1 ) , .
.
.
, ( xn , yn ) } , xi  x  < , yi  = { +1 , 1 } , decis function f ( x ) = w xb , optim hyperplan determin follow , min w , b 1 2 w2 + c n i=1 i s. t. { yi ( w  xi  b ) + i  1 , i  0 , = 1 .
.
.
n ( 1 ) w vector hyperplan defin orient , b bias determin posit .
slack variabl i measur error instanc violat constraint yi ( w  xi  b )  1 .
user-defin paramet c determin trade-off maxim 3 margin minim error , i.e .
higher valu c , svm focus minim error .
dual form , optim problem solv , max i < n i=1 i  1 2 n , k=1 iyikyk xi  xk s.t .
  0  i  c , = 1 .
.
.
n n i=1 iyi = 0 , ( 2 ) lead follow decis function , f ( x ) = sign ( n i=1 iyi xi  x b ) .
( 3 ) construct nonlinear decis boundari , input vector pro- ject inner product space higher dimens use basi set nonlinear function .
new space optim hyperplan determin .
use theori kernel satisfi mercer  theorem , oper perform direct input space use xi  xj = k ( xi , xj ) .
, decis function formul , f ( x ) = sign ( n i=1 iyik ( xi , xj )  b ) .
( 4 ) among train vector , associ weight i greater zero ( 3 ) ( 4 ) .
element lie decis margin known support vector ( sv ) .
unsign valu f ( x ) measur distanc exampl x hyperplan , sign determin class label ( posit negat ) .
moder imbalanc dataset , empir result show , un- like machin learn algorithm , svm produc good hypothesi without modif ( akbani , kwek , japkowicz 2004 ; imam , ting , kamruzzaman 2006 ; wu chang 2005 ) .
one explan phenomenon svm use set support vector construct clas- sific model , negat instanc far decis border taken account svm affect , even 4 numer one .
howev , svm overcom problem imbal- anc data distribut imbalanc .
case , observ hyperplan separ learn svm close minor class , result low perform general exampl class , comparison major class ( batuwita palad 2013 ; ghodsi 2010 ; liu , , huang 2006 , wu chang 2005 ) .
2.1 strategi svm imbalanc dataset sever strategi propos improv perform svm imbalanc dataset .
describ introduc section accord moment appli learn process .
2.1.1 pre-process strategi base re-sampl techniqu balanc dataset .
one way over-sampl data minor class ; henc , new instanc creat order increas proport dataset .
contrast , under-sampl seek reduc size major class remov subset data .
general-purpos procedur , target particular machin learn techniqu .
one common use smote , employ k-nearest neighbor techniqu over-sampl minor class ( chawla , bowyer , hall , kegelmey 2002 ; vilarino , spyridono , vitria , radeva 2005 ) .
other strategi appli cluster algorithm sub-sampl major class ( li , yu , bi huang 2014 ; yu , debenham , jan , simoff 2006 ; zhou , ha , wang 2010 ) .
also strategi svm seek increas minor class consid margin area two class ( castro , carvalho , braga 2009 ) .
work base use svm obtain posit support vector , over-sampl data ( hernandez- santiago , cervant , lopez-chau , garca-lamont ; wang 2008 ) .
featur also exploit build under-sampl algorithm ( tang , zhang , chawla , krass 2009 ; wang 2014 ) , svm use build new dataset compos inform negat support vector posit data .
solut use sampl method 5 ensembl ( kang cho 2006 ; liu et al .
2006 ; wask , benediktsson , sveinsso 2009 ; yang , zhang , zhou , zomaya 2011 ; sukhanov , merent , debe , hahn zoubir 2015 ) .
furthermor , propos seek over-sampl train use activ learn ( ertekin 2013 ) .
2.1.2 train strategi strategi includ propos modifi standard op- timize problem svm train order incorpor inform relat proport class dataset .
one approach cost- sensit learn incorpor learn problem inform relat penalti associ wrong predict class .
case svm , cost inform two type error introduc formul learn problem , use two regular- izat paramet , c+ c , associ error posit negat class , respect ( ver-opoulo , campbel , cristianini 1999 ; cohen , hilario , sax , hugonnet , geissbuhl 2006 ) , min w , b 1 2 w2 + c+  i|yi=+1 i + c   i|yi=1 i s. t. { yi ( w  xi  b ) + i  1 , i  0 , = 1 .
.
.
n ( 5 ) work also ad new restrict slack variabl i , order control margin separ two class ( ghodsi 2010 ; yang , wang , yang , yu 2008 ) .
differ approach present batuwita palad ( 2010 ) , one regular paramet c use , inform cost error incorpor alloc differ weight variabl i .
propos solut combin cost-sensit learn techniqu ( akbani et al .
2004 ; muscat , mahfouf , zughrat , yang , thornton , khondabiand , sortano 2014 ; wang japkowicz , 2010 ; zieba , tomczak , lubicz , swiatek 2014 ) .
propos modifi kernel matrix accord observ imbal distribut data , kba algorithm ( wu chang 2005 ) .
ramrez allend ( 2012 ) method propos train two one-class svm , one fit class , aggreg decis nest manner boundari improv .
final , 6 , wu , silva , zhao , qian ( 2015 ) , model-bas approach integrat- ing cost-sensit learn gaussian mixtur model imbalanc classif problem propos .
2.1.3 post-process strategi general , approach orient either , toward modifi weight vector w function decis determin new bias , order adjust decis boundari learn svm provid good margin separ posit class .
exampl , z-svm method propos imam et al .
( 2006 ) , determin valu new paramet z , solv ad optim problem .
optim paramet weight contribut support vector minor class vector w decis function obtain train .
li , hu , hirasawa ( 2008 ) , bias decis function modifi calcul offset  averag unsign valu generat f ( x ) support vector .
similar strategi use shanahan roma ( 2003 ) , new offset calcul appli beta-gamma algorithm .
studi suggest re-interpret output svm .
exampl , fuzzi decis function appli li et al .
( 2008 ) , whose paramet estim observ distribut dataset .
wang zheng ( 2008 ) , decis process incorpor post-process modul , whose construct base method inform theori defin new bias classif .
3 .
novel post-process strategi base bias strategi propos improv perform svm imbal- anc dataset general requir tune new paramet sampl rate number k select neighbor .
method computa- tional expens consid construct sever classifi ( method base ensembl ) , base iter algorithm , mod- ifi kernel matrix ( kba ) sampl techniqu requir sever over-train step .
cost-sensit approach , standard svm optim problem must modifi cost error class 7 must known .
moreov , produc over-fit model ( wang japkowicz 2010 ) .
hand , empir shown hyperplan learn svm presenc imbalanc dataset approxim orient ideal hyperplan ( ghodsi 2010 ; liu et al .
2006 ; wu chang 2005 ) .
reduc general minor class would inde associ bias b , posit instanc lie far ideal limit , i.e. , svm learn boundari much close class .
studi , present sun , lim , liu ( 2009 ) domain text classif , suggest increas research strategi determin new threshold svm  decis function .
modif base distribut class dataset , also direct affect standard svm train .
follow latter research line , novel post-process strategi base calcul new bias propos paper .
proport among class dataset consid , henc adjust function learn svm order improv perform minor class .
propos solut involv tune new paramet .
fur- thermor , neither requir modifi standard optim problem train svm , addit step re-train .
propos , base develop present gonzalez-abril , angulo , velasco , ortega ( 2008 ) , modifi , train , separ margin hyperplan toward major class order achiev better general perform data minor class .
new bias calcul follow ( nunez , gonzalez-abril , angulo 2011 ) .
let z = { ( x1 , y1 ) , .
.
.
, ( xn , yn ) } train set , xi  x  < , yi  = { +1 , 1 } .
also , let z1 z2 dataset belong posit class ( + ) negat one ( - ) , respect .
standard formul bias , linear separ case indic bias could obtain , bs =  +  2 ( 6 ) ( bs = bstandard )  maximum valu hyperplan without bias appli set negat instanc z2 ,  minimum valu hyperplan without bias appli entri minor set z1 , 8 ,  = max xkz2 n i=1 ik ( xi , xk ) ,  = min xkz1 n i=1 ik ( xi , xk ) .
( 7 ) let us indic bias b chosen b =  , instanc posit class correct label .
furthermor ,  smallest valu ensur 100 % correct classif train vector ( gonzalez- abril , nunez , angulo , velasco 2014 ) .
definit bs ( 6 ) extend take account proport class dataset .
henc , n1 n2 number pattern class ( + ) ( - ) respect , new proport bias bp defin , bp = n1 +n2 n1 +n2 .
( 8 ) henc , imbalanc problem , n1  n2 , new bias move decis limit toward negat class , thus increas margin separ posit class .
moreov , maximum minimum valu hyperplan without bias reach support vector , consid point calcul   ,  = max xksv2 n i=1 ik ( xi , xk ) ,  = min xksv1 n i=1 ik ( xi , xk ) ( 9 ) sv1 sv2 set support vector class ( + ) ( - ) , respect .
new decis function would simpli express follow , f ( x ) = sign ( n i=1 iyik ( xi , xj )  b ) .
( 10 ) furthermor , svm decis function generat support vector ( inform instanc classif task ) , addit modif propos exist : consid number support vector posit negat class , nsv1 nsv2 , respect , rather valu n1 n2 .
henc , second bias bp1 also propos , bp1 = nsv1 +nsv2 nsv1 +nsv2 .
( 11 ) 9 let us see relationship among bias n1  n2 .
case , optim problem usual provid number support vector nsv1 < nsv2 , defin r = n2/n1 r = nsv2/nsv1 , 1  r  r ( fact check section 4 ) .
henc , result , |  bp| = 1 1 +r |  | , |  bp1| = 1 1 + r |  | , |  bs| = 1 2 |  | , ( 12 ) 0  |  bp|  |  bp1|  |  bs| ( 13 ) , bs farther  bp1 , turn , farther away  bp .
thus , check decis function move away zone posit sampl , increas , later demonstr , accuraci class .
therefor , new bias move hyperplan learn svm obtain better classif perform posit class , consid proport class : greater imbal , greater margin separ minor class .
4 .
experiment result analysi perform post-process strategi propos test 34 dataset uci repositori ( frank asuncion 2010 ) .
characterist dataset shown tabl 1 .
label ( + ) assign class shown bracket , label ( - ) remain data .
perform classifi obtain use new bias measur use sensit geometr mean ( g-mean ) ( garcia , 2009 ) .
sensit measur posit accuraci , indic mani exampl minor class correct classifi ; g-mean evalu perform term sensit specif ( negat accuraci ) follow , g-mean =  sensit  specif ( 14 ) 10 tabl 1 : uci dataset use experiment .
dataset order extrem moder imbal .
dataset number % dataset number % instanc posit posit instanc posit posit winequ white ( 3 ) 4868 20 0.41 user model ( 1 ) 258 24 9.30 abalon ( 19 ) 4177 32 0.77 sat ( 4 ) 4435 415 9.36 winequ red ( 8 ) 1593 18 1.18 satimag ( 4 ) 6435 626 9.70 page-block ( 5 ) 5473 115 2.10 euthyroid 2000 238 11.90 yeast ( 7 ) 1483 35 2.36 glass ( 7 ) 214 29 13.55 thyroid ( 1 ) 3772 93 2.47 segment ( 1 ) 2310 330 14.29 nursey ( 3 ) 12960 328 2.53 hepat 129 24 18.60 fault ( 5 ) 1941 55 2.83 column 310 60 19.35 winequ white ( 4 ) 4864 163 3.35 cmc ( 2 ) 1473 333 22.61 yeast ( 5 ) 1483 51 3.44 dna 2000 464 23.20 mun ( 3 ) 8124 292 3.59 vehicl ( 1 ) 846 199 23.52 letter ( ) 20000 789 3.95 transfus 748 178 23.80 car ( 3 ) 1728 69 3.99 haberman 306 81 26.50 derma ( 2 ) 358 21 5.87 german 1000 300 30.00 ecoli ( 5 ) 336 20 5.95 waveform ( 0 ) 5000 1657 33.00 balanc ( 2 ) 625 49 7.24 pima 768 268 34.00 gtc 2126 176 8.28 tictac ( 2 ) 958 332 34.66 sensit allow us show well posit class classifi g-mean show balanc accuraci posit neg- ativ class .
also , accuraci includ , standard metric .
svm train , usual rbf kernel use , well matlab  bioinformat toolbox process .
valu  ( rbf width ) c ( regular term ) obtain explor two-dimension grid :  = { 20 , 21 , .
.
.
, 26 } , c = { 20 , 21 , .
.
.
, 210 } best valu accuraci classifi ( svm , cost-sensit svm smote svm ) use .
averag valu accuraci , g-mean sensit shown tabl 2 , dataset , use ten-fold cross-valid like empir exper- iment repeat procedur 10 time order ensur good statist behavior .
result statement establish :  work imbalanc dataset , evalu metric like g-mean sensit measur classifi perform independ data distribut , elect correct kind problem .
exampl , svm accuraci valu 0.99 abalon dataset .
howev , complet fail classifi posit class , re- flect valu g-mean .
11 tabl 2 : averag valu accuraci , g-mean sensit dataset , use ten-fold cross-valid ( 100 replic ) .
dataset accuraci g-mean sensit svm bp bp1 svm bp bp1 svm bp bp1 wineq white ( 3 ) .996 .968 .969 .120 .449 .449 .038 .235 .235 abalon ( 19 ) .992 .491 .619 .000 .625 .552 .000 .828 .592 wineq red ( 8 ) .989 .506 .813 .000 .603 .296 .000 .835 .280 page-block ( 5 ) .984 .761 .892 .534 .858 .803 .296 .974 .821 yeast ( 7 ) .978 .649 .961 .300 .749 .831 .143 .891 .723 thyroid ( 1 ) .992 .928 .992 .851 .95 .886 .733 .976 .794 nursey ( 3 ) .993 .981 .984 .878 .983 .602 .774 .986 .37 fault ( 5 ) .955 .938 .951 .323 .594 .432 .166 .417 .247 wineq white 4 ) .963 .764 .921 .336 .621 .568 .128 .513 .353 yeast ( 5 ) .965 .464 .953 .000 .628 .623 .000 .907 .453 mun ( 3 ) .954 .953 .954 .572 .903 .587 .349 .858 .366 letter ( ) .998 .994 .998 .975 .995 .994 .951 .996 .991 car ( 3 ) .960 .973 .968 .000 .941 .595 .000 .911 .39 derma ( 2 ) .968 .951 .957 .923 .944 .935 .864 .936 .906 ecoli ( 5 ) .986 .968 .974 .851 .950 .902 .775 .945 .855 balanc ( 2 ) .921 .774 .845 .749 .830 .851 .623 .909 .867 gtc .980 .882 .975 .912 .920 .950 .804 .970 .924 user model ( 1 ) .979 .981 .985 .885 .968 .971 .791 .956 .956 sat ( 4 ) .948 .868 .943 .787 .894 .837 .634 .929 .727 satimag ( 4 ) .945 .911 .942 .811 .889 .83 .678 .865 .716 euthyroid .907 .761 .889 .703 .809 .809 .517 .887 .727 glass ( 7 ) .951 .879 .953 .855 .898 .868 .768 .935 .792 segment ( 1 ) .996 .993 .996 .988 .994 .991 .977 .995 .984 hepat .852 .721 .741 .643 .732 .744 .562 .855 .84 column .867 .872 .868 .758 .873 .714 .645 .883 .57 cmc ( 2 ) .759 .630 .714 .527 .618 .594 .320 .607 .452 dna .967 .949 .957 .948 .957 .96 .914 .972 .967 vehicl ( 1 ) .986 .983 .985 .982 .985 .985 .975 .988 .986 transfus .779 .748 .778 .540 .614 .557 .327 .462 .350 haberman .719 .634 .661 .464 .617 .609 .273 .606 .512 german .762 .623 .708 .667 .660 .695 .517 .804 .680 waveform ( 0 ) .897 .873 .895 .877 .884 .863 .824 .921 .783 pima diabet .755 .742 .753 .672 .725 .676 .518 .687 .530 tictac ( 2 ) .983 .997 .997 .974 .996 .997 .950 .995 .998 12  dataset , despit imbal , origin svm get reason model ( e.g .
ecoli ) ; case , fail ( abalon , winequ , yeast ) .
 new bias improv perform standard svm dataset respect g-mean sensit metric .
furthermor , perform sensit metric use bp bias better employ bp1 , dataset except tic-tac dataset .
fact due tic-tac dataset uniqu 34 dataset r  r true .
compar perform post-process strategi report literatur , , smote cost-sensit scheme use train svm list uci dataset .
comparison made bp bias .
matlab bioinformat  toolbox provid cost- sensit scheme valu c+ c ( 5 ) calcul c : c+ = c n 2n1 , c = c n 2n2 .
( 15 ) worth note , c = c +n1+c n2 n1+n2 , , similar formula bias bp chang c + c   , respect .
result obtain use evalu metric , well ten-fold cross-valid structur , shown tabl 3 .
moreov , comparison proport support vector learn decis function relat number train data offer scheme tabl 4 .
ratio measur complex svm classifi .
therefor , conclud , smote cost sen- sitiv approach provid decis function complex henc , aforement , produc over-fit model .
order measur similar result scheme , friedman test appli ( demser 2006 ) .
non-parametr test use detect signific differ multipl classifi .
obtain p-valu accuraci , g-mean sensibl 0.1076 , 0.0150 0.0083 , respect .
result , confid level fix 5 % , conclud follow :  accuraci measur , long p-valu friedman test 0.1076 , conclud signific evid equival three method .
13 tabl 3 : comparison novel method vs cost-sensit smote approach .
averag valu accuraci , g-mean sensit dataset , use ten-fold cross-valid .
dataset accuraci g-mean sensit bp cost smote bp cost smote bp cost smote wineq white ( 3 ) .968 .950 .963 .449 .147 .497 .235 .050 .328 abalon ( 19 ) .491 .739 .584 .625 .682 .605 .828 .648 .646 wineq red ( 8 ) .506 .787 .789 .603 .653 .588 .835 .650 .458 page-block ( 5 ) .761 .932 .808 .858 .795 .704 .974 .690 .619 yeast ( 7 ) .649 .896 .902 .749 .875 .876 .891 .860 .853 thyroid ( 1 ) .928 .975 .989 .950 .957 .885 .976 .934 .795 nursey ( 3 ) .982 .982 .955 .983 .991 .977 .986 1.00 1.00 fault ( 5 ) .938 .945 .697 .594 .353 .627 .417 .178 .579 wineq white ( 4 ) .764 .839 .897 .621 .657 .627 .513 .515 .441 yeast ( 5 ) .464 .867 .863 .628 .843 .871 .911 .834 .775 mun ( 3 ) .953 .962 .964 .903 .951 .981 .858 .939 .998 letter ( ) .994 .997 .998 .995 .997 .995 .996 .995 .991 car ( 3 ) .973 .899 .903 .941 .946 .948 .911 1.00 1.00 derma ( 2 ) .951 .931 .904 .944 .935 .917 .936 .944 .939 ecoli ( 5 ) .968 .943 .945 .950 .976 .959 .945 1.00 .978 balanc ( 2 ) .774 .584 .813 .830 .781 .801 .909 .684 .796 gtc .882 .972 .887 .920 .941 .894 .970 .907 .904 user model ( 1 ) .981 .950 .949 .968 .933 .926 .956 .961 .904 sat ( 4 ) .868 .919 .921 .894 .892 .863 .929 .861 .799 satimag ( 4 ) .911 .934 .919 .889 .847 .863 .865 .749 .803 euthyroid .761 .897 .887 .809 .826 .883 .887 .782 .881 glass ( 7 ) .879 .896 .945 .898 .905 .897 .935 .863 .842 segment ( 1 ) .993 .995 .996 .994 .994 .991 .995 .992 .985 hepat .721 .817 .695 .732 .740 .589 .855 .720 .440 column .872 .864 .866 .873 .874 .887 .883 .898 .926 cmc ( 2 ) .630 .668 .737 .618 .643 .595 .607 .556 .435 dna .949 .971 .965 .957 .959 .955 .972 .941 .937 vehicl ( 1 ) .983 .983 .964 .985 .983 .972 .988 .985 .988 transfus .748 .698 .522 .614 .633 .548 .462 .546 .615 haberman .634 .721 .642 .617 .485 .572 .606 .359 .490 german .623 .728 .714 .660 .691 .657 .804 .629 .557 waveform ( 0 ) .873 .884 .853 .884 .888 .856 .921 .901 .866 pima diabet .742 .733 .634 .725 .725 .649 .687 .694 .723 tictac ( 2 ) .997 .983 .984 .996 .983 .982 .995 .987 .978  respect g-mean metric , p-valu 0.0150 , fried- man test detect signific differ .
furthermor , test indi- cate signific differ post-process strategi cost-sensit svm method .
nevertheless , evi- 14 tabl 4 : proport support vector relat number train data .
dataset number support vector dataset number support vector bp cost smote bp cost smote wineq white ( 3 ) 1.79 7.93 16.52 user model ( 1 ) 12.02 17.83 6.78 abalon ( 19 ) 6.08 55.99 52.78 sat ( 4 ) 15.04 19.07 27.10 wineq red ( 8 ) 6.28 36.66 38.32 satimag ( 4 ) 9.09 10.29 18.64 page-block ( 5 ) 8.31 18.19 59.13 euthyroid 22.00 38.00 31.25 yeast ( 7 ) 4.18 27.91 28.79 glass ( 7 ) 17.76 19.16 13.55 thyroid ( 1 ) 2.49 11.24 9.92 segment ( 1 ) 4.07 4.89 48.72 nursey ( 3 ) 3.36 5.14 11.39 hepat 32.56 47.29 38.37 fault ( 5 ) 6.08 6.65 54.53 column 25.80 26.77 29.51 wineq white ( 4 ) 11.47 30.26 31.37 cmc ( 2 ) 44.26 64.09 41.98 yeast ( 5 ) 7.35 55.02 28.66 dna 37.35 39.85 18.05 mun ( 3 ) 5.87 52.90 5.85 vehicl ( 1 ) 9.69 9.34 22.87 letter ( ) 1.36 2.18 1.27 transfus 42.65 57.62 58.82 car ( 3 ) 8.22 17.48 17.53 haberman 52.28 65.03 38.23 derma ( 2 ) 14.80 25.42 30.31 german 45.70 50.60 36.25 ecoli ( 5 ) 7.74 31.25 19.05 waveform ( 0 ) 22.84 24.12 43.88 balanc ( 2 ) 17.44 53.92 21.28 pima diabet 48.95 52.60 55.86 gtc 7.38 8.98 67.38 tictac ( 2 ) 35.59 36.01 18.47 denc classifi smote signific differ .
 respect sensit measur , p-valu 0.0083 , , friedman test detect signific differ .
furthermor , test indic us signific differ among post- process strategi two method .
therefor , conclud post-process strategi best strategi order maxim sensit metric .
5 .
conclus futur work experiment result dataset differ degre imbal , conclud svm perform signific improv use new bias consid proport class .
import benefit propos approach standard optim problem associ svm modifi .
neither new paramet must tune , comput cost practic insignific .
compar strategi cost-sensit 15 smote approach , bias modif approach achiev superior perform term sensit , classif function far less complex term number support vector .
futur work , theoret framework studi movement bias workspac accord definit develop .
refer akbani , r. , kwek , s. , japkowicz , n. ( 2004 ) ,  appli support vector machin imbalanc dataset  , proceed 15th european confer machin learn ecml  2004 , pp .
3950 .
batuwita , r. palad , v. ( 2010 ) ,  fsvm-cil : fuzzi support vector machin class imbal learn  , ieee transact fuzzi sys- tem , 18 , 558571 .
batuwita , r. palad , v. ( 2013 ) ,  class imbal learn method support vector machin  , imbalanc learn : foundat , al- gorithm , applic , pp .
83-99 , berlin , germani : john wiley & son .
castro , c. l. , carvalho , m. a. , braga , a. p. ( 2009 ) ,  improv algorithm svms classif imbalanc data set  , proceed- ing 11th intern confer enginn applic neural network eann 2009 , pp .
108118 .
chawla , n. v. , bowyer , k. w. , hall , l. o. , kegelmey , w. p. ( 2002 ) ,  smote : synthet minor over-sampl techniqu  , journal artifici intellig research , 16 , 321357 .
cohen , g. , hilario , m. , sax , h. , hugonnet , s. , geissbuhl , .
( 2006 ) ,  learn imbalanc data surveil nosocomi infect  , artifici intellig medicin , 37 , 718 .
cristianini , n. shawe-taylor , j .
( 2000 ) , introduct support vector machin kernel-bas learn method , new york , ny : cambridg univers press , 1st edit .
demser , j .
( 2006 ) ,  statist comparison classifi multipl data set , journal machin lean research , 7 , 130 .
ertekin , s. ( 2013 ) ,  adapt oversampl imbalanc data classifi- cation  , inform scienc system , lectur note electr engin , 264 , 261269 .
16 frank , a. asuncion , .
( 2010 ) , uci machin learn repositori , uni- versiti california , school inform comput scienc .
irvin .
archive.ics.uci.edu/ml gonzalez-abril , l. , nunez , h. , angulo , c. , velasco , f. ( 2014 ) ,  gsvm : svm handl imbalanc accuraci class bi-classif problem  , appli soft comput , 17 , 23-31 .
gonzalez-abril , l. , angulo , c. , velasco , f. , ortega , j .
.
( 2008 ) ,  note bias svms multiclassif  , ieee trans- action neural network , 19 ( 4 ) , 723725 .
, h. garcia , e. .
( 2009 ) ,  learn imbalanc data  , ieee transact knowledg data engin , 21 ( 9 ) , 12631284 .
, h. ghodsi , .
( 2010 ) ,  rare class classif support vector machin  , proceed 20th intern confer pattern recog- nition , icpr  10 , pp .
548551 .
hernandez-santiago , j. , cervant , j. , chau , a. l. , garcia- lamont , f. ( 2012 ) ,  enhanc perform svm skew data set excit support vector  , proceed 13th ibero-american confer artifici intellig iberamia 2012 , pp .
101110 .
imam , t. , ting , k. m. , kamruzzaman , j .
( 2006 ) ,  z-svm : svm improv classif imbalanc data  , proceed 19th aus- tralian confer artifici intellig aus-ai 2006 , pp .
264273 .
kang , p. cho , s. ( 2006 ) ,  eus svms : ensembl under-sampl svms data imbal problem , lectur note comput scienc , 4232 , 837846 .
li , b. , hu , j. , hirasawa , k. ( 2008 ) ,  improv support vector ma- chine soft decision-mak boundari  , proceed 26th iast intern confer artifici intellig applic aia  08 , pp .
4045 .
li , p. , yu , x. , bi , t. t. , huang , j. l. ( 2014 ) ,  imbalanc data svm classif method base cluster boundari sampl dt-knn prune  , intern journal signal process , imag process pattern recognit , 7 ( 2 ) , 61-68 .
liu , y. , , a. , huang , x .
( 2006 ) ,  boost predict accuraci imbalanc dataset svm ensembl  , proceed 10th pacific- asia confer knowledg discoveri data mine pakdd 2006 , pp .
107118 .
17 http : //archive.ics.uci.edu/ml lopez , v. , fernandez , a. , garcia , s. , palad , v. , herrera , f. ( 2013 ) ,  insight classif imbalanc data : empir- ical result current trend use data intrins characterist  , inform scienc , 250 , 113141 .
muscat , r. , mahfouf , m. , zughrat , a. , yang , y. y. , thornton , s. , khondabi , a. v. , sortano , s. ( 2014 ) ,  hierarch fuzzi support vector machin ( svm ) rail data classif  , proceed 19th ifac world congress , pp .
1065210657 .
nguyen , h. m. , cooper , e. w. , kamei , k. ( 2011 ) ,  borderlin over- sampl imbalanc data classif  , intern journal knowl- edg engin soft data paradigm , 3 , 421 .
nunez , h. , gonzalez-abril , l. , angulo , c. ( 2011 ) ,  post-process strategi svm learn unbalanc data  , proceed 19th european symposium artifici neural network esann  2011 , pp .
195 200 .
oneto , l. , ridella , s. , anguita , d. ( 2016 ) .
 tikhonov , ivanov morozov regularizationfor support vector machin learn , machin learn , 3 , 103136 .
ramirez , f. allend , h. ( 2012 ) ,  dual support vector domain de- scription imbalanc classif  , artifici neural network machin learn icann 2012 , lectur note comput scienc , 7552 , 710717 .
shanahan , j. g. roma , n. ( 2003 ) ,  improv svm text classif perform threshold adjust  , lectur note comput scienc , 2837 , pp .
361372 .
sukhanov , s. , merent , a. , debe , c. , hahn , j. , zoubir , .
( 2015 ) ,  bootstrap-bas svm aggreg class imbal problem  , pro- ceed 23rd european signal process confer eusipco 2015 , pp 155169 .
sun , a. , lim , e.-p. , liu , .
( 2009 ) ,  strategi imbalanc text classif use svm : compar studi  , decis support sys- tem , 48 , 191201 .
sun , y. , wong , a. c. , kamel , m. s. ( 2009 ) ,  classif imbal- anc data : review  , intern journal pattern recognit artifici intellig , 23 , 687719 .
tang , y. , zhang , y.-q. , chawla , n. v. , krasser , s. ( 2009 ) ,  svms 18 model high imbalanc classif  , ieee transact sys- tem , man cyberneticspart b , 39 , 281288 .
vapnik , v. n. ( 1999 ) , natur statist learn theori ( inform scienc statist ) , new york , ny : springer .
veropoulo , k. , campbel , c. , cristianini , n. ( 1999 ) ,  control- ling sensit support vector machin  , proceed 16th intern joint confer artifici intellig ijcai 1999 , pp .
5560 .
vilarino , f. , spyridono , p. , vitria , j. , radeva , p. ( 2005 ) ,  ex- periment svm stratifi sampl imbalanc problem : detect intestin contract  , proceed 3rd intern confer advanc pattern recognit icapr 2005 , vol .
2 , pp .
783 791 .
wang , b. x. japkowicz , n. ( 2010 ) ,  boost support vector machin imbalanc data set , knowledg inform system , 25 , 120 .
wang , h. zheng , h. ( 2008 ) ,  improv support vector machin classif imbalanc biolog dataset  , proceed 4th intern confer intellig comput icic 2008 , pp .
6370 .
wang , h.-i .
( 2008 ) ,  combin approach smote biased-svm imbalanc dataset  , proceed intern joint confer neural network ijcnn 2008 , pp .
228231 .
wang , q .
( 2014 ) ,  hybrid sampl svm approach imbalanc data classif  , abstract appli analysi , articl id 972786 .
wask , b. , benediktsson , j .
a. , sveinsson , j. r. ( 2009 ) ,  classi- fy remot sens data support vector machin imbalanc train data  , proceed 8th intern workshop multipl classifi system mcs09 , pp .
375384 .
wu , g. chang , e. .
( 2005 ) ,  kba : kernel boundari align con- sider imbalanc data distribut  , ieee transact knowledg data engin , 17 , 786795 .
yang , c.-y. , wang , j. , yang , j.-s. , yu , g.-d. ( 2008 ) ,  imbalanc svm learn margin compens  , proceed 5th intern symposium neural network : advanc neural network isnn  08 , pp .
636644 .
yang , p. , zhang , z. , zhou , b .
b. , zomaya , a. .
( 2011 ) ,  sampl subset optim classifi imbalanc biolog data  , pro- 19 ceed 15th pacific-asia confer advanc knowledg discoveri data mine pakdd 2011 , vol .
2 , pp .
333344 .
yu , t. , debenham , j. , jan , t. , simoff , s. ( 2006 ) ,  combin vector quantize support vector machin imbalanc dataset  , artifici intellig theori practic .
ifip 19th world comput congress , vol .
217 , chapter 9 , pp .
8188 .
zhou , b. , ha , m. , wang , c. ( 2010 ) ,  improv algorithm un- balanc data svm  , advanc intellig soft comput , fuzzi inform engin , 78 , pp .
549-555 .
zieba , m. , tomczak , j. m. , lubicz , m. , swiatek , j .
( 2014 ) ,  boost svm extract rule imbalanc data applic predic- tion post-op life expect lung cancer patient  , appli soft comput , 14 ( part ) , 99-108 .
20
