untitl ieee/acm transact audio, speech, languag processing, vol. 25, no. 4, april 2017 807 deep learn backend singl multisess i-vector speaker recognit omid ghahabi javier hernando abstractth lack label background data make big per- formanc gap cosin probabilist linear discrimi- nant analysi (plda) score baselin techniqu i-vector speaker recognition. unsupervis clus- tere techniqu estim labels, accur predict true label assum sev- eral sampl speaker background data true reality. paper, author us deep learn (dl) perform gap given unla- bele background data. goal, author propos impostor select algorithm univers model adapt process hybrid base deep belief network deep neural network discrimin model target speaker. order insight behavior dl techniqu single- multisess speaker enrol tasks, ex- periment carri paper scenarios. experi nation institut standard technolog 2014 i-vector challeng 46% perform gap, term minimum decis cost function, fill propos dl-base system. furthermore, score combin propos dl-base plda estim label cover 79% gap. index termsdeep learning, deep neural network, deep belief network, i-vector, speaker recognition. i. introduct recent compact represent speech utterancesknown i-vector [1] state-of-the-art text-independ speaker recognition. com- mon score techniqu decid i-vector belong speaker cosin probabilist linear discrimi- nant analysi (plda) [2], [3]. plda score lead superior perform cost need speaker-label back- ground data. moreover, need sampl back- ground speaker spoken differ session condit work efficiently. recent challeng speaker recognition, organ nation institut standard manuscript receiv june 15, 2016; revis octob 27, 2016 januari 8, 2017; accept januari 16, 2017. date public februari 8, 2017; date current version march 1, 2017. work support spanish project deepvoic (tec2015-69266-p) european project camomil (pcin-2013-067). associ editor coordin review manuscript approv public dr. bin ma. author talp research center, depart sig- nal theori communications, universitat politecnica catalunya barcelonatech, barcelona 08034, spain (e-mail: color version figur paper avail onlin digit object identifi 10.1109/taslp.2017.2661705 technolog (nist), perform gap common score techniqu label background data avail [4]. un- supervis automat label techniqu like propos [5], [6], appropri estim true label assum sampl speaker background data true real- ity. plda estim label perform reason [5], [6], result far plda actual label [7]. hand, success us deep learn (dl) speech processing, specif speech recognit (e.g., [8][12]), inspir commun us dl techniqu speaker recognit well. gener ap- proaches, like restrict boltzmann machin (rbm) deep belief network (dbn), discrimin ones, like deep neural network (dnn), purpose. possibl us dl techniqu speaker recognit com- bine state-of-the-art i-vector approach. kind combin considered. dl techniqu i-vector extract process [13][17] appli i-vector backend [18][23]. dnn i-vector extract algorithm main goals. first, univers background model (ubm) replac dnn, typic train acoust model speech recognit [13], [14], [16], [24], [25]. sec- ond, convent spectral featur replac append so-cal dnn bottleneck featur [15], [16]. signific perform gain report case shown append bottleneck featur spectral on gaus- sian ubm acoust model lead higher qualiti i-vector [15], [16]. besides, i-vector computation, dl techniqu differ purposes. example, differ combina- tion rbm propos [18], [19] classifi i-vector [20] learn speaker channel factor sub- space plda simulation. rbm [26] dnn [27] increas discrimin power i-vector given speaker-label background data. [21][23] dbn integr adapt process provid better initial- izat dnn order discrimin target models. attempt extract compact represent speech signal given spectral featur [28][30] gmm supervector [31]. work, author us deep architectur backend i-vector classif order perform 2329-9290 2017 ieee. translat content mine permit academ research only. person us permitted, republication/redistribut requir ieee permission. standards/publications/rights/index.html information. 808 ieee/acm transact audio, speech, languag processing, vol. 25, no. 4, april 2017 gap cosin (unlabeled-based) plda (labeled- based) score baselin system given unlabel background data. [21], [22], author advantag unsupervis learn dbn train global model refer univers dbn (udbn) dnn supervis learn model target speaker discriminatively. provid balanc training, impostor select algorithm cope train data, udbn-adapt process proposed. compar [21], [22], deep architectur differ number layer explor singl multi-sess speaker enrol tasks. paramet global model normal adaptation. normal scale paramet facilit train net- work specif hidden layer used. layer pre-train propos [21] work. reason emphas layer connec- tion weight avoid lower hidden layer learn input data. fact import hidden layer used. addition, new experi base unsupervis label techniqu plda [6] perform paper potenti baselin label background data available. preliminari experi perform nist sre 2006 [32] effect contribution. take advan- tage conclus obtain preliminari experiments, set experi carri newer challeng databas nist 2014 i-vector challeng [4]. exper- iment result perform 2014 i-vector challeng propos dl-base fill 46% perform gap cosin oracl plda score system term mindcf similar plda score result obtain unsupervis estim labels. score combin propos dl-base plda estim label fill 79% gap. rest paper organ follows. section ii give brief background overview i-vectors, plda, deep learn techniqu experiments. section iii present propos dl-base backend i-vector classification. section iv describ propos impostor select algorithm order balanc training. section v show cope data train tar- model. section vi vii discuss experiment result obtain nist sre 2006 nist 2014 i-vector challenge, respectively. section viii conclud paper. ii. background a. i-vector plda shown gaussian mixtur model (gmm) adapt univers background model (ubm) repres featur vector speech signal adequ [33]. mean vector adapt gmm stack build supervector s, model follow [1], s = su + tx (1) su speaker- session-independ mean super- vector typic ubm, t total variabl matrix, fig. 1. (a) dnn, (b) dbn, (c) dbn training/dnn pre-training. x low rank vector latent variables. mean posterior distribut x refer i-vector [1]. posterior distribut condit baum-welch statist given speech utterance. t matrix train expectation-maxim (em) algorithm given central baum-welch statist background speech ut- terances. words, i-vector low rank vector, typic 400 600, repres speech ut- terance. detail [1]. main score techniqu i-vector cosin [1], [34] probabilist linear discrimin analysi (plda) [2], [3]. plda effici techniqu perform score session variabl compensation. i- vector suffici low dimension, modifi version plda propos [3] typic used. assum i-vector decompos as, = m + + (2) m global offset, column eigenvoices, latent vector have standard normal prior, residu vector normal distribut zero mean covari matrix. model paramet estim larg collect speaker-label background data em algorithm [2]. class i-vector covari matrices, depend model parameters, store scoring. b. deep learn deep learn (dl) refer branch machin learn techniqu attempt learn high level featur data. 2006 [35], [36], dl new area research applic machin learn signal processing. deep learn architectur speech process (e.g., [11], [12], [37][39]). deep neural network (dnn), deep belief network (dbn), restrict boltzmann machin (rbm) main techniqu work discrimin model target speaker given input i-vectors. dnn feed-forward neural network multipl hid- den layer (fig. 1(a)). train discrimin back-propag algorithm given class label input vec- tors. train algorithm tri minim loss function class label outputs. classif tasks, ghahabi hernando: deep learn backend singl multisess i-vector speaker recognit 809 cross-entropi loss function soft- max commonli activ function output layer [40]. typically, paramet dnn initi small random numbers. recently, shown effici techniqu paramet initi [41][43]. techniqu consist initi dnn dbn parameters, refer unsu- pervis pre-train hybrid dbn-dnn [9], [44]. empir shown pre-train stage set weight network closer optimum solut ran- dom initi [41][43]. dbn gener model multipl hidden layer stochast unit visibl layer repres data vector (fig. 1(b)). layer undirect layer top-down direct connect gener data. effici greedi layer wise algorithm train dbn paramet [36]. case, dbn divid two-lay sub-network treat rbm (fig. 1(c)). rbm built visibl unit trained, paramet frozen output given rbm input vectors. process repeat layer reached. rbm gener model construct undirect layer stochast hidden visibl units. rbm train base maximum likelihood criterion stochast gradient descent algorithm [9], [36]. gradient estim approxim version contrast diverg (cd) algorithm call cd-1 [35], [36]. theoret practic detail [35], [36], [45]. train algorithm given [31]. networks, possibl updat parame- ter process train example, effici divid input data (batch) smaller size batch (minibatch) updat paramet averag- ing gradient minibatch. paramet updat procedur repeat avail input data processed. iter call epoch. iii. propos deep learn backend i-vector success us i-vector speaker recognit dl techniqu speech process applic encourag research commun combin techniqu speaker recognition. kind combin considered. dl techniqu i-vector extract process, appli backend. work, dl technolog backend two-class hybrid dbn-dnn train target speaker increas discrimin target i-vector/ i-vector speaker (non-targets/impostors) (fig. 2). pro- pose network initi speaker-specif paramet adapt global model, refer univer- sal deep belief network (udbn). cross-entropi be- tween class label output minim back-propag algorithm. dnn usual need larg number input sampl train efficiently. gener rule, deeper network requir fig. 2. propos deep learn architectur train speaker model. input data. speaker recognition, target speaker enrol sampl (singl session task) multipl sampl (multi-sess task). cases, number target sampl limited. network train limit data highli probabl overfitted. hand, number target impostor sampl highli unbalanced, i.e., target sampl thousand impostor samples. learn unbalanc data result bias dnn major class. words, dnn higher predict accuraci major class. fig. 3 show block diagram propos approach discrimin model target speakers. main contribut propos work tackl problems. balanc train block attempt decreas number impostor sampl and, contrary, increas number target on reason effect way. in- form impostor sampl target speaker select propos impostor select algorithm. afterwards, select impostor cluster cluster centroid consid final impostor sampl target speaker model. impostor centroid target sampl divid equal minibatch provid balanc impostor tar- data minibatch. hand, dbn adapt block propos compens lack input data. dbn train need label data, background i-vector build udbn. paramet udbn adapt balanc data obtain target speaker. end, given target/impostor labels, adapt dbn balanc data, dnn discrimin train target speaker. contribut describ detail follow sections. iv. balanc train speaker model propos method final dis- criminative, need posit neg data inputs. nevertheless, problem posit neg data highli unbalanc case, lead bias major class. straightfor- ward wai deal unbalanc data problem explor [46][48] [49], [50]. commonli method data sam- pling. data major class undersampl and, contrary, data minor class oversampled. effect techniqu highli depend data structure. 810 ieee/acm transact audio, speech, languag processing, vol. 25, no. 4, april 2017 fig. 3. block-diagram train/test phase propos deep learn backend i-vectors. propos approach shown fig. 3, impostor decreas steps, select clus- tering. hand, target sampl in- creas replic combination. that, bal- anc target impostor sampl distribut equal minibatches. a. impostor select cluster object decreas larg number neg sam- ple reason way. propos main steps. first, impostor i-vector inform train dataset selected. inform impostor means, case, impostor repres given target statist close target dataset. real applications, make sens select impostor global close enrol speakers. target speaker chang significantly, select im- postor re-select accord new target dataset. second, number select impostor sampl high comparison number target ones, cluster k-mean algorithm cosin distanc criterion. centroid cluster final neg samples. select method inspir data-driven back- ground data select techniqu propos [51]. techniqu given avail impostor supervectors, sup- port vector machin (svm) classifi train target speaker. number time impostor select support vector, train svm models, call impostor support vector frequenc [51]. impostor exampl higher frequenc select refin impostor dataset. however, svm train target speaker com- putation costly. moreover, final discrimin model dnns, worth emploi techniqu such. instead, propos us cosin similar effici fast criterion compar i-vectors. compar target i-vector impostor i-vector background data set. n impostor close target i-vector treat like support vector [51]. fig. 4. step propos impostor select algorithm. impostor higher frequenc select inform impostors. n select impostor re- fer local global select impostor work. paramet n determin experimentally. algorithm shown fig. 4 summar follows, 1) set impostor frequenc fm = 0 impostor i-vector m , 1 m m 2) target i-vector , 1 a) comput cosin (i ,m ), 1 m m b) select n impostor highest score c) select impostor fm fm + 1 3) sort impostor descend order base fm 4) select impostor final ones. cosin (i ,m ) cosin score target i- vector impostor i-vector m background dataset, m number impostor target i- vectors, respectively. note, case multi-sess target enrollment, averag avail i-vector target speaker consid algorithm. final select impostor local, global, pool them. local pool used, comput cost higher k-mean cluster run target model independently. propos similar algorithm [23] select process depend background data. ghahabi hernando: deep learn backend singl multisess i-vector speaker recognit 811 fig. 5. exampl propos balanc train dnn multi-sess speaker verif task. minibatch target i-vector differ impostor shown dnns. randomli select subset background data algorithm target train database. order process statist reliable, process repeat time impostor frequenc accumul iterations. algorithm [23]. shown algorithm perform sim- ilar algorithm us train target set select process background databas larg [23]. b. target replic combin order balanc posit neg samples, number target sampl increas number impostor cluster centroid obtain section iv-a. singl session enrol task, i-vector target speaker simpli replic number cluster centroids. replic target i-vector act exactli pre-train process dnn sampl nois creat rbm train [45]. moreover, adapt supervis learn stage replic version target impostor class have weight network paramet updated. multi-sess task, avail i-vector target speaker combined, i.e., averag n i-vector consid new target i-vector. number posit neg sampl bal- anced, distribut equal minibatches. words, minibatch contain number impostor targets. target sampl multi-sess task combined, target sampl differ impostor on shown network minibatch (fig. 5). op- timum number impostor cluster minibatch determin experiment section vi vii. v. univers dbn adapt unlik dnns, need label data training, dbn necessarili need label data inputs. hence, unsupervis train global model refer univers dbn (udbn) [21]. udbn train feed background i-vector differ speakers. train procedur carri layer layer rbm describ section ii-b. input i-vector real-valued, fig. 6. comparison adapt connect weight visibl hidden unit differ speakers. gaussian-bernoulli rbm (grbm) [9], [45] train connect weight visibl hidden layer units. rest connect weight train bernoulli-bernoulli rbms. shown pre-train techniqu initi dnn better simpli random number [41][43]. however, input sampl available, pre-train achiev good model. case, propos [21] adapt udbn paramet balanc data obtain target speaker. adapt carri train dbn initi paramet udbn given balanc data target speaker. adapt dbn initi final dnn target models. or- der avoid overfitting, iter consid adaptation. suppos udbn learn speaker channel variabl background data. therefore, udbn provid meaning initi point dbn simpl random initialization. studi [42] shown pre-train robust respect random initializa- tion seed. us udbn paramet make target model independ random seeds. contrast [21], [22], work normal udbn paramet adaptation. normal carri simpli scale maximum absolut valu connec- tion weight 0.01. way, connect weight dynam rang similar typic random ini- tialization. additionally, bia term multipli 0.01 closer zero. bia term usual set zero connect weight randomli initialized. udbn paramet normal facilit train network specif hidden layer used. way, learn rate number epoch tune random initi dnn adapt dnn supervis learn stage. fig. 6 show comparison adapt udbn connect weights, input layer hidden layer, 812 ieee/acm transact audio, speech, languag processing, vol. 25, no. 4, april 2017 differ speakers. seen figure, speaker- specif initi point set adapt process dnn target model. given target/impostor labels, minibatch stochast gradient descent back-propag carri fine-tuning. softmax logist sigmoid activ function label layer hidden layers, respectively. propos comput output score log pos- terior ratio (lpr) form as, (target|) = log p (target|) log p (non-target|) (3) p (target|) p (non-target|) are, respectively, posterior probabl target non-target class given test i-vector . lpr comput help gaussian true fals score distribut us score fusion. addition, fine-tun process effici momentum factor smooth updates, weight decai regular penal larg weights. layer pre-train propos [21] work. reason give emphasi layer connect weight avoid lower layers, closer input, learn input data. fact import higher number hidden layer used. vi. experi nist sre 2006 nist sre 2006 [32] effect pro- pose contribut shown fig. 3 singl multi- session speaker verif tasks. experiments, built scratch includ voic activ detect (vad) featur i-vector extraction. take advantag conclus section, nist 2014 i- vector challeng databas [4] section vii compar perform propos recent state-of-the-art baselin systems. a. baselin databas core test condit sre 2006 singl session task 8 convers train condit multi-sess task. cases, train test signal approxim two-minut total speech duration. 816 target model 51,068 trial singl session 699 target model 31,080 trial multi-sess task. speech signal two-minut approxim durat nist sre 2004 2005 background data contain 6,063 speech signal 1,070 distinct speakers. frequenc filter (ff) featur [52] exper- iments. ffs, like mel frequenc cepstral coeffici (mfcc), decorrel version log filter bank energi (fbe) [52]. shown ff featur achiev perform equal better mfcc [52]. featur extract 10 ms 30 ms ham window. number static ff featur 16 delta ff delta log energy, 33-dimension featur vector built. featur ex- traction, speech signal subject energy-bas silenc remov process. gender-independ ubm repres diagon covariance, 512-compon gmm. i- vector 400-dimensional. i-vector extract process carri aliz open sourc softwar [53]. ubm, t matrix, plda paramet train back- ground data. plda baselin system gender-independ 250-dimension speaker space. plda experiments, i-vector length normalized. perform evalu detect error tradeoff (det) curves, equal error rate (eer), minimum decis cost function (mindcf) de- fine follow [32], dcf (t) = 0.1 pm (t) + 0.99 pf (t) (4) miss rate pm rel number target trial decid incorrectly, fals alarm rate pf rel number non-target trial decid incorrectly, t threshold dcf computed. b. singl session experi dnn experiments, size hidden layer set 512. dnn hidden layer explor experiments. layer data increas comput complex signific gain. number minibatch number impostor centroid set experiment 3 12, respectively. minibatch includ impostor centroid replic target samples. worth note compar speech recognit train data typic high, size number minibatch appli- cation. however, gradient stabl train work well. dnn baselin system, train dnn target speaker impostor background data ran- dom initialization. case, background i-vector cluster k-mean algorithm centroid consid impostor samples. work, us uniform distribut u (0, 0.01) random initi experiment result show achiev slightli better per- formanc normal distribut n (0, 0.01) prior work [21]. tune paramet network fix experiments. dnn-3l stand hidden layer dnn. paramet n , number local global select impostor propos impostor select method, need determin experimentally. fig. 7 illustr vari- abil eer term paramet hidden layer dnns. similar behavior observ mindcf curves. dnn exampl shown figur initi ran- domly. base figure, dnn-1l set n 10 2,000, respectively. similar curv plot network n set 10 set 300 500 dnn-2l dnn-3l, respectively. experiment result show main improv adapt process come adapt con- nection weight input layer hidden layer dnns. adapt layer ghahabi hernando: deep learn backend singl multisess i-vector speaker recognit 813 fig. 7. determin paramet propos impostor select algorithm hidden layer dnns. n are, respectively, number local global nearest impostor i-vector target i-vectors. tabl effect propos idea fig. 3 perform propos dnn system impostor select adapt eer (%) mindcf (104 ) # hidden layer # hidden layer 1 2 3 1 2 3 8.55 7.76 7.59 381 353 351 8.06 7.12 7.09 360 327 326 7.43 7.47 7.45 339 343 339 6.81 6.97 6.99 315 317 313 fusion cosin 6.83 6.88 6.64 308 309 299 fusion plda 4.98 5.03 4.76 253 248 230 result obtain core test condit nist sre 2006. cosin plda baselin system achiev (eer=7.18%, mindcf=324) (eer=4.78%, mindcf=253), respectively. signific impact performance. order decreas probabl overfit adaptation, separ net- work adapt minibatch paramet obtain network averaged. tabl summar effect propos contribu- tion. impostor select improv perform great extent networks. tri global, local, pool global local select impostor k-mean cluster best perform obtain global select impostors. biggest improv adapt process observ dnn hidden layer. best result obtain impostor select adapt techniqu 8-20% 10-17% rela- tive improv term eer mindcf, respectively, compar baselin dnns. biggest rel improve- ment achiev dnn-1l. row tabl fusion dnn system cosin (eer=7.18%, mindcf=0.0324) plda (eer=4.78%, mindcf=0.0253) baselin systems. score mean varianc normal simpli summed. fusion cosin baselin dnn system improv result dnn-3l achiev best result correspond 8% rela- tive improv eer mindcf comparison cosin score baselin system. nevertheless, dnn-3l tabl ii effect propos idea fig. 3 perform propos dnn system impostor select adapt eer (%) mindcf (104 ) # hidden layer # hidden layer 1 2 3 1 2 3 4.58 4.58 4.38 208 213 217 4.02 4.07 3.86 183 201 194 4.24 4.30 4.20 202 207 202 3.68 3.83 3.50 170 189 172 fusion cosin 3.61 3.77 3.45 161 169 162 fusion plda 2.46 2.62 2.36 111 121 112 result obtain nist sre 2006, 8-session enrol task. cosin plda baselin system achiev (eer=4.2%, mindcf=191) (eer=2.27%, mindcf=105), respectively. score improv plda result specif mindcf 9% rel improvement. combin score dnn differ number hidden layers, gain observed. c. multi-sess experi configur singl session task appli multi-sess one. number minibatch set 3. minibatch, 8 target i-vector accompany- ing 8 impostor cluster centroid shown network. therefore, size minibatch total number impostor cluster 16 24, respectively. com- binat i-vector target speaker help train networks, replic target i-vector minibatch shown fig. 5. train net- work paramet tune singl session experiments. result summar tabl ii. 12% rel improv achiev dnn emploi impostor select techniqu propos work. pa- ramet obtain singl session task, re-select impostor new multi-sess data set. adapta- tion process improv perform 8%. singl session task, adapt effect one-hidden- layer dnns. networks, paramet hidden layer adapt improv observ adapt layers. best result obtain dnn-3l propos techniqu combined. show 20% rel improv eer mindcf comparison baselin three-lay dnns. propos three-hidden-lay dnn perform cosin (eer=4.2%, mindcf=0.0191) plda (eer=2.27%, mindcf=0.0105) baselin systems, 17% 10% rel improv term eer mindcf, respectively, compar cosin scoring. fusion cosin baselin improv result cases, improv observ combin plda scores. 814 ieee/acm transact audio, speech, languag processing, vol. 25, no. 4, april 2017 vii. experi nist 2014 i-vector challeng databas provid nist 2014 speaker recog- nition i-vector challeng [4] experi section. speech signals, i-vector given directli nist challeng train, test, develop speaker recognit systems. enabl compar- ison readili consist front-end type background data [4]. challenge, speaker recognit system evalu phases: speaker label background data known known systems. cosin plda score techniqu nist baselin system unlabel label background data available, re- spectively. goal evalu tech- niqu perform gap baselin system label background data available. a. baselin databas convent telephon speech record nist sre 2004 2012 comput i-vector challeng [7]. unlik nist sre 2006 experiments, durat speech signal i-vector approxim 2 minutes, challeng i-vector extract speech utter vari durat mean 39.6 seconds. set 600- dimension i-vector provided: development, train, test consist 36,572, 6,530, 9,634 i-vectors, respectively. number target speaker model 1,306 i-vector available. target model score test i-vector and, therefore, total number trial 12,582,004. trial divid nist randomli select subsets: progress subset (40%), evalu subset (60%). perform evalu mindcf obtain [4], dcf (t) = pm (t) + 100 pf (t) (5) main baselin system consid work background i-vector labeled: cosin plda estim labels. plda actual label oracl comparison. them, i-vector whiten length normal prior evalu averag i-vector target speaker singl target model. cosin baselin averag i-vector length normal shown plda system re-norm affect perform [7]. plda system gender-independ 400- dimension speaker space. order best plda actual labels, background i-vector extract speech signal shorter 30 second discard plda train [7]. plda estim labels, stage unsupervis cluster techniqu estim speaker label background data. stage cluster algorithm similar mean shift base algorithm propos [54] successfulli challeng [6]. second stage, closer cluster obtain stage combined. stages, i-vector join base cosin similar consid threshold set 0.29 experi [6]. end, cluster contain 4 50 i-vector selected. [6], i-vector 20 second speech discard plda train case. possibl train plda estim label repeat stage unsupervis cluster algorithm plda similar measurement, time consum signific gain observ practice. experiment result baselin compar perform report [6] [5]. b. multi-sess experi architectur sre 2006 multi-sess experi- ment experi modifi- cation. size hidden layer set 400. minibatch consist 5 impostor centroid 5 target samples. to- tal number impostor centroid 15 target model. dnn-1l dnn-3l work better dnn-2l sre 2006 experiments, implement network nist i-vector challenge. dnn-1l dnn-3l train learn rate 0.002 0.07 number epoch 30 300, respectively. momentum weight decai set, respectively, 0.9 0.001 dnns. unlabel background i-vector udbn training. learn rate number epoch udbn train set 0.02 200 grbm, 0.06 120 rest rbms, respectively. momentum, weight decay, minibatch size set, respectively, 0.9, 0.0002, 100 rbms. dnn-3l adapt layers. learn rate number epoch adapt set, respectively, 0.001 10 layer 0.0001 20 second layer. discuss section iv-a, background data set big like challenge, result slightli better train data set select algorithm. hand, gener rule challeng us train data allow impostor selection. therefore, order fair comparison result particip sites, us background i-vector impostor select algorithm (section iv-a). sre 2006 experiments, tri global, local, pool global local select impostor k-mean cluster best perform obtain pooling. global impostor selection, n set 4,500 100 dnn-1l dnn-3l, respectively. al- gorithm iter 20 times. afterwards, global select impostor pool 500 local impostor target speaker k-mean clustering. tabl iii compar perform propos dnn system baselin system term mindcf eer, figs. 8 9 compar oper point term det curves. circl figur oper point correspond mindcfs. worth note nist 2014 i-vector challeng perform system evalu term mindcf. however, includ eer tabl better comparison. ghahabi hernando: deep learn backend singl multisess i-vector speaker recognit 815 tabl iii comparison perform propos dnn baselin system nist 2014 i-vector challeng unlabel background data progress set evalu set eer (%) mindcf eer (%) mindcf [1] cosin 4.78 0.386 4.46 0.378 [2] plda (estim labels) 3.85 0.300 3.46 0.284 [3] propos dnn-1l 5.13 0.327 4.61 0.320 [4] propos dnn-3l 4.55 0.305 4.11 0.300 fusion [2] & [4] 2.99 0.260 2.70 0.243 label background data [5] plda (actual labels) 2.23 0.226 2.01 0.207 fusion [2] & [5] 2.04 0.220 1.85 0.204 fusion [4] & [5] 2.13 0.221 2.00 0.196 fusion [2] & [4] & [5] 1.88 0.204 1.74 0.190 fig. 8. comparison perform propos dnn-3l baselin system progress set nist 2014 i-vector challenge. seen table, propos dnn-3l perform better dnn-1l, conclud sre 2006 experiments. propos dnn-3l achiev compar perform plda estim label term mindcf (with 21% rel improv compar cosin scoring), lower perform term eer. words, shown figs. 8 9, propos dnn-3l perform closer plda actual label cosin lower fals alarm (fa) probabilities. higher fa probabilities, wai around. propos dnn plda actual label achiev perform fa probabl 0.01, lower 0.01 propos dnn outperform plda actual labels. seen advantag propos have better perform lower fa probabl import higher secur purposes. fig. 9. comparison perform propos dnn-3l baselin system evalu set nist 2014 i-vector challenge. interest point combin dnn-3l plda estim label score level improv result great extent oper points. score fusion carri bosari toolkit [55]. combin weight train progress trial set evalu set. result rel improv compar cosin baselin 36% term mindcf evalu set. improv us background label consider compar 45% rel improv obtain plda actual labels. us speaker label background data goal work, interest propos dl-base backend plda estim label help oracl plda system, us actual labels. seen tabl iii, case dnn-3l plda estim labels, combin oracl plda improv results. improv higher term eer plda estim label higher term mindcf dnn-3l systems. nevertheless, combin system achiev best performance, correspond 8% 13% rel improv term mindcf eer, respectively, compar plda actual labels. viii. conclus hybrid architectur base deep belief network (dbn) deep neural network (dnn) propos work discrimin model target speaker i-vector speaker verification. main object per- formanc gap cosin oracl plda score system label background data available. main contribut propos dnn 816 ieee/acm transact audio, speech, languag processing, vol. 25, no. 4, april 2017 effici particular task. firstly, inform im- postor i-vector select cluster provid balanc training. secondly, dnn initi speaker specif paramet adapt global model, refer univers dbn (udbn). order insight behavior techniqu singl multi-sess speaker enrol tasks, ex- periment carri scenarios. experi perform nist sre 2006, mainli development, nist 2014 i-vector challenge, mainli evaluation. shown propos hybrid fill approxim 46% perform gap cosin ora- cle plda score system term mindcf. propos outperform baselin plda estim labels, score fusion highli effect cover 79% gap. reason propos outperform baselin plda explicitli compens session variabl carri plda. thus, expect ad explicit session model propos hybrid model improv performance, scope paper. refer [1] n. dehak, p. kenny, r. dehak, p. dumouchel, p. ouellet, front-end factor analysi speaker verification, ieee trans. audio, speech, lang. process., vol. 19, no. 4, pp. 788798, 2011. [2] s. j. d. princ j. h. elder, probabilist linear discrimin analysi infer identity, proc. 2007 ieee 11th int. conf. comput. vis., 2007, pp. 18. [3] p. kenny, bayesian speaker verif heavi tail priors, proc. odyssey, speaker lang. recognit. workshop, 2010. [4] nist, nist speaker recognit i-vector machin learn chal- lenge, 2014. [online]. available: ivectorchallenge_2013-11-18_r0.p df [5] e. khoury, l. el shafey, m. ferras, s. marcel, hierarch speaker cluster method nist i-vector challenge, proc. odyssey, speaker lang. recognit. workshop, 2014, pp. 254259. [6] s. novoselov, t. pekhovsky, k. simonchik, stc speaker recognit nist i-vector challenge, proc. odyssey, speaker lang. recog. workshop, 2014, pp. 231240. [7] c. greenberg et al. nist 2014 speaker recognit i-vector machin learn challenge, proc. odyssey, speaker lang. recog. workshop, 2014, pp. 224230. [8] a. mohamed, d. yu, l. deng, investig full-sequ train deep belief network speech recognition, proc. interspeech, 2010, pp. 28462849. [9] g. e. dahl, d. yu, l. deng, a. acero, context-depend pre- train deep neural network large-vocabulari speech recognition, ieee trans. audio, speech, lang. process., vol. 20, no. 1, pp. 3042, jan. 2012. [10] a. mohamed, g. e. dahl, g. hinton, acoust model deep belief networks, ieee trans. audio, speech, lang. process., vol. 20, no. 1, pp. 1422, jan. 2012. [11] g. hinton et al., deep neural network acoust model speech recognition: share view research groups, ieee signal process. mag., vol. 29, no. 6, pp. 8297, nov. 2012. [12] a. senior, h. sak, i. shafran, context depend phone model lstm rnn acoust modelling, proc. ieee int. conf. acoust., speech, signal process., 2015, pp. 45854589. [13] y. lei, n. scheffer, l. ferre, m. mclaren, novel scheme speaker recognit phonetically-awar deep neural network, proc. ieee int. conf. acoust., speech, signal process., 2014, pp. 16951699. [14] p. kenny, v. gupta, t. stafylakis, p. ouellet, j. alam, deep neural network extract baum-welch statist speaker recognition, proc. odyssey, 2014, pp. 293298. [15] m. mclaren, y. lei, l. ferre, advanc deep neural network approach speaker recognition, proc. ieee int. conf. acoust., speech, signal process., 2015, pp. 48144818. [16] f. richardson, d. reynolds, n. dehak, deep neural network ap- proach speaker languag recognition, ieee signal process. lett., vol. 22, no. 10, pp. 16711675, oct. 2015. [17] y. liu, y. qian, n. chen, t. fu, y. zhang, k. yu, deep featur text-depend speaker verification, speech commun., vol. 73, pp. 113, oct. 2015. [18] t. stafylakis, p. kenny, m. senoussaoui, p. dumouchel, preliminari investig boltzmann machin classifi speaker recognition, proc. odyssey, 2012, pp. 109116. [19] m. senoussaoui, n. dehak, p. kenny, r. dehak, p. dumouchel, attempt boltzmann machin speaker verification, proc. odyssey, 2012, pp. 117121. [20] t. stafylakis, p. kenny, m. senoussaoui, p. dumouchel, plda us- ing gaussian restrict boltzmann machin applic speaker verification, proc. interspeech, 2012, pp. 16921695. [21] o. ghahabi j. hernando, deep belief network i-vector base speaker recognition, proc. ieee int. conf. acoust., speech, signal process., 2014, pp. 17001704. [22] o. ghahabi j. hernando, i-vector model deep belief net- work multi-sess speaker recognition, proc. odyssey, 2014, pp. 305310. [23] o. ghahabi j. hernando, global impostor select dbn multi-sess i-vector speaker recognition, advanc speech languag technolog iberian languag (lectur note artifici intelligence). berlin, germany: springer, nov. 2014. [24] w. m. campbell, deep belief network vector-bas speaker recognition, proc. interspeech, 2014, pp. 676680. [25] d. garcia-romero, xiaohui zhang, a. mccree, d. povey, improv- ing speaker recognit perform domain adapt challeng deep neural networks, proc. 2014 ieee spoken lang. technol. workshop, dec. 2014, pp. 378383. [26] s. novoselov, t. pekhovsky, k. simonchik, a. shulipa, rbm-plda subsystem nist i-vector challenge, proc. interspeech, 2014, pp. 378382. [27] y. z. isik, h. erdogan, r. sarikaya, s-vector: discrimin rep- resent deriv i-vector speaker verification, proc. eur. signal process. conf., nice, france, aug. 2015, pp. 20972101. [28] v. vasilakakis, s. cumani, p. laface, speaker recognit mean deep belief networks, proc. biometr technol. forens sci., 2013. [29] e. variani, xin lei, e. mcdermott, i. lopez moreno, j. gonzalez- dominguez, deep neural network small footprint text-depend speaker verification, proc. 2014 ieee int. conf. acoust., speech signal process., 2014, pp. 40524056. [30] p. safari, o. ghahabi, j. hernando, featur speaker vector mean restrict boltzmann machin adaptation, proc. odyssey, 2016, pp. 366371. [31] o. ghahabi j. hernando, restrict boltzmann machin supervec- tor speaker recognition, proc. int. conf. acoust., speech, signal process., 2015, pp. 48044808. [32] nist, nist year 2006 speaker recognit evalu plan, 2006. [online]. available: index.htm [33] d. a. reynolds, t. f. quatieri, r. b. dunn, speaker verif adapt gaussian mixtur models, digit. signal process., vol. 10, no. 1, pp. 1941, jan. 2000. [34] n. dehak, r. dehak, j. glass, d. reynolds, p. kenny, cosin simi- lariti score score normal techniques, proc. odyssey, speaker lang. recognit. workshop, 2010, pp. 7175. [35] g. e. hinton r. salakhutdinov, reduc dimension data neural networks, science, vol. 313, no. 5786, pp. 504507, jul. 2006. [36] g. e. hinton, s. osindero, y-w. teh, fast learn algorithm deep belief nets, neural comput., vol. 18, no. 7, pp. 15271554, 2006. [37] z-h. ling, l. deng, d. yu, model spectral envelop re- strict boltzmann machin deep belief network statist para- metric speech synthesis, ieee trans. audio, speech, lang. process., vol. 21, no. 10, pp. 21292139, oct. 2013. [38] x-l. zhang j. wu, deep belief network base voic activ de- tection, ieee trans. audio, speech, lang. process., vol. 21, no. 4, pp. 697710, apr. 2013. [39] tara n. sainath et al. deep convolut neural network large-scal speech tasks, neural netw., vol. 64, pp. 3948, apr. 2015. ghahabi hernando: deep learn backend singl multisess i-vector speaker recognit 817 [40] z.-h. ling et al. deep learn acoust model parametr speech generation: systemat review exist techniqu futur trends, ieee signal process. mag., vol. 32, no. 3, pp. 3552, 2015. [41] h. larochelle, y. bengio, j. louradour, p. lamblin, explor strate- gi train deep neural networks, j. mach. learn. res., vol. 10, pp. 140, jun. 2009. [42] e. dumitru, p. manzagol, y. bengio, s. bengio, p. vincent, difficulti train deep architectur effect unsuper- vise pre-training, proc. 12th int. conf. artif. intell. statist., 2009, pp. 153160. [43] d. erhan, y. bengio, a. courville, p. manzagol, p. vincent, s. bengio, unsupervis pre-train help deep learning? j. mach. learn. res., vol. 11, pp. 625660, mar. 2010. [44] l. deng d. yu, deep learning: method applications. delft, netherlands: publishers, jun. 2014. [45] g. e. hinton, practic guid train restrict boltzmann ma- chines, neural networks: trick trade (lectur note com- puter science, 7700). berlin, germany: springer, jan. 2012, pp. 599619. [46] h. e. a. garcia, learn imbalanc data, ieee trans. knowl. data eng., vol. 21, no. 9, pp. 12631284, sep. 2009. [47] n. thai-nghe, z. gantner, l. schmidt-thieme, cost-sensit learn- ing method imbalanc data, proc. 2010 int. joint conf. neural netw., jul. 2010, pp. 18. [48] t. m. khoshgoftaar, j. van hulse, a. napolitano, supervis neural network modeling: empir investig learn imbal- anc data label errors, ieee trans. neural netw., vol. 21, no. 5, pp. 813830, 2010. [49] v. lopez, a. fernandez, s. garca, v. palade, f. herrera, in- sight classif imbalanc data: empir result cur- rent trend data intrins characteristics, inform. sci., vol. 250, pp. 113141, nov. 2013. [50] s. barua, m. m. islam, x. yao, k. murase, mwmote-major weight minor oversampl techniqu imbalanc data set learn- ing, ieee trans. knowl. data eng., vol. 26, no. 2, pp. 405425, feb. 2014. [51] m. mclaren, r. vogt, b. baker, s. sridharan, data-driven back- ground dataset select svm-base speaker verification, ieee trans. audio, speech, lang. process., vol. 18, no. 6, pp. 14961506, aug. 2010. [52] c. nadeu, d. macho, j. hernando, time frequenc filter filter-bank energi robust hmm speech recognition, speech commun., vol. 34, no. 12, pp. 93114, apr. 2001. [53] a. larcher et al. aliz 3.0 open sourc toolkit state-of-the-art speaker recognition, proc. interspeech, 2013, pp. 27682771. [54] m. senoussaoui, p. kenny, t. stafylakis, p. dumouchel, studi cosin distance-bas mean shift telephon speech diariza- tion, ieee/acm trans. audio, speech, lang. process., vol. 22, no. 1, pp. 217227, jan. 2014. [55] n. brummer e. villiers, bosari toolkit user guide: theory, al- gorithm code binari classifi score processing, 2011. [online]. available: omid ghahabi receiv m.sc. degre elec- trical engin shahid beheshti university, tehran, iran, 2009. current work ph.d. degre technic univers cat- alonia (upc), barcelona, spain. 2009 2011, speech process group, research center intellig signal processing, tehran, iran. 2011 2016, research speech process group, signal theori com- munic department, upc. late 2016, eml european media laboratori gmbh, heidelberg, germany, speech technologist. research interest include, limit to, speaker recognit diarization, speech signal processing, deep learning. author coauthor journal confer paper topics. member research center languag speech technolog applications, barcelona, spain. javier hernando receiv m.s. ph.d. degre telecommun engin technic univers catalonia (upc), barcelona, spain, 1988 1993, respectively. 1988, depart signal theori communications, upc, current professor director research center languag speech. academ year 20022003, visit research pana- sonic speech technolog laboratory, santa barbara, ca, usa. led upc team eu- ropean, spanish, catalan projects. research interest includ robust speech analysis, speech recognition, speaker verif localization, oral dialogue, multimod interfaces. author coauthor 200 public book chapters, review articles, confer paper topics. receiv 1993 extraordinari ph.d. award upc. << /ascii85encodepag fals /allowtranspar fals /autopositionepsfil true /autorotatepag /none /bind /left /calgrayprofil (grai gamma 2.2) /calrgbprofil (srgb iec61966-2.1) /calcmykprofil (u.s. web coat \050swop\051 v2) /srgbprofil (srgb iec61966-2.1) /cannotembedfontpolici /warn /compatibilitylevel 1.4 /compressobject /off /compresspag true /convertimagestoindex true /passthroughjpegimag true /createjobticket fals /defaultrenderingint /default /detectblend true /detectcurv 0.0000 /colorconversionstrategi /srgb /dothumbnail true /embedallfont true /embedopentyp fals /parseiccprofilesincom true /embedjobopt true /dscreportinglevel 0 /emitdscwarn fals /endpag -1 /imagememori 1048576 /lockdistillerparam true /maxsubsetpct 100 /optim true /opm 0 /parsedsccom fals /parsedsccommentsfordocinfo true /preservecopypag true /preservedicmykvalu true /preserveepsinfo fals /preserveflat true /preservehalftoneinfo true /preserveopicom fals /preserveoverprintset true /startpag 1 /subsetfont fals /transferfunctioninfo /remov /ucrandbginfo /preserv /useprologu fals /colorsettingsfil () /alwaysemb [ true /algerian /arial-black /arial-blackital /arial-bolditalicmt /arial-boldmt /arial-italicmt /arialmt /arialnarrow /arialnarrow-bold /arialnarrow-boldital /arialnarrow-ital /arialunicodem /baskoldfac /batang /bauhaus93 /bellmt /bellmtbold /bellmtital /berlinsansfb-bold /berlinsansfbdemi-bold /berlinsansfb-reg /bernardmt-condens /bodonimtpostercompress /bookantiqua /bookantiqua-bold /bookantiqua-boldital /bookantiqua-ital /bookmanoldstyl /bookmanoldstyle-bold /bookmanoldstyle-boldital /bookmanoldstyle-ital /bookshelfsymbolseven /britannicbold /broadwai /brushscriptmt /californianfb-bold /californianfb-ital /californianfb-reg /centaur /centuri /centurygoth /centurygothic-bold /centurygothic-boldital /centurygothic-ital /centuryschoolbook /centuryschoolbook-bold /centuryschoolbook-boldital /centuryschoolbook-ital /chiller-regular /colonnamt /comicsansm /comicsansms-bold /cooperblack /couriernewps-bolditalicmt /couriernewps-boldmt /couriernewps-italicmt /couriernewpsmt /estrangeloedessa /footlightmtlight /freestylescript-regular /garamond /garamond-bold /garamond-ital /georgia /georgia-bold /georgia-boldital /georgia-ital /haettenschweil /harlowsolid /harrington /hightowertext-ital /hightowertext-reg /impact /informalroman-regular /jokerman-regular /juiceitc-regular /kristenitc-regular /kuenstlerscript-black /kuenstlerscript-medium /kuenstlerscript-twobold /kunstlerscript /latinwid /lettergothicmt /lettergothicmt-bold /lettergothicmt-boldobliqu /lettergothicmt-obliqu /lucidabright /lucidabright-demi /lucidabright-demiital /lucidabright-ital /lucidacalligraphy-ital /lucidaconsol /lucidafax /lucidafax-demi /lucidafax-demiital /lucidafax-ital /lucidahandwriting-ital /lucidasansunicod /magneto-bold /maturamtscriptcapit /mediciscriptltstd /microsoftsansserif /mistral /modern-regular /monotypecorsiva /ms-mincho /msreferencesansserif /msreferencespecialti /niagaraengraved-reg /niagarasolid-reg /nuptialscript /oldenglishtextmt /onyx /palatinolinotype-bold /palatinolinotype-boldital /palatinolinotype-ital /palatinolinotype-roman /parchment-regular /playbil /pmingliu /poorrichard-regular /ravi /showcardgothic-reg /simsun /snapitc-regular /stencil /symbolmt /tahoma /tahoma-bold /tempussansitc /timesnewromanmt-extrabold /timesnewromanmtstd /timesnewromanmtstd-bold /timesnewromanmtstd-boldcond /timesnewromanmtstd-boldit /timesnewromanmtstd-cond /timesnewromanmtstd-condit /timesnewromanmtstd-ital /timesnewromanps-bolditalicmt /timesnewromanps-boldmt /timesnewromanps-italicmt /timesnewromanpsmt /times-roman /trebuchet-boldital /trebuchetm /trebuchetms-bold /trebuchetms-ital /verdana /verdana-bold /verdana-boldital /verdana-ital /vinerhanditc /vivaldii /vladimirscript /webd /wingdings2 /wingdings3 /wingdings-regular /zapfchancerystd-demi /zwadobef ] /neveremb [ true ] /antialiascolorimag fals /cropcolorimag true /colorimageminresolut 150 /colorimageminresolutionpolici /ok /downsamplecolorimag true /colorimagedownsampletyp /bicub /colorimageresolut 150 /colorimagedepth -1 /colorimagemindownsampledepth 1 /colorimagedownsamplethreshold 1.50000 /encodecolorimag true /colorimagefilt /dctencod /autofiltercolorimag fals /colorimageautofilterstrategi /jpeg /coloracsimagedict << /qfactor 0.76 /hsampl [2 1 1 2] /vsampl [2 1 1 2] >> /colorimagedict << /qfactor 0.40 /hsampl [1 1 1 1] /vsampl [1 1 1 1] >> /jpeg2000coloracsimagedict << /tilewidth 256 /tileheight 256 /qualiti 15 >> /jpeg2000colorimagedict << /tilewidth 256 /tileheight 256 /qualiti 15 >> /antialiasgrayimag fals /cropgrayimag true /grayimageminresolut 150 /grayimageminresolutionpolici /ok /downsamplegrayimag true /grayimagedownsampletyp /bicub /grayimageresolut 300 /grayimagedepth -1 /grayimagemindownsampledepth 2 /grayimagedownsamplethreshold 1.50000 /encodegrayimag true /grayimagefilt /dctencod /autofiltergrayimag fals /grayimageautofilterstrategi /jpeg /grayacsimagedict << /qfactor 0.76 /hsampl [2 1 1 2] /vsampl [2 1 1 2] >> /grayimagedict << /qfactor 0.40 /hsampl [1 1 1 1] /vsampl [1 1 1 1] >> /jpeg2000grayacsimagedict << /tilewidth 256 /tileheight 256 /qualiti 15 >> /jpeg2000grayimagedict << /tilewidth 256 /tileheight 256 /qualiti 15 >> /antialiasmonoimag fals /cropmonoimag true /monoimageminresolut 1200 /monoimageminresolutionpolici /ok /downsamplemonoimag true /monoimagedownsampletyp /bicub /monoimageresolut 600 /monoimagedepth -1 /monoimagedownsamplethreshold 1.50000 /encodemonoimag true /monoimagefilt /ccittfaxencod /monoimagedict << /k -1 >> /allowpsxobject fals /checkcompli [ /none ] /pdfx1acheck fals /pdfx3check fals /pdfxcompliantpdfonli fals /pdfxnotrimboxerror true /pdfxtrimboxtomediaboxoffset [ 0.00000 0.00000 0.00000 0.00000 ] /pdfxsetbleedboxtomediabox true /pdfxbleedboxtotrimboxoffset [ 0.00000 0.00000 0.00000 0.00000 ] /pdfxoutputintentprofil (none) /pdfxoutputconditionidentifi () /pdfxoutputcondit () /pdfxregistrynam () /pdfxtrap /fals /createjdffil fals /descript << /ch <feff4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002> /cht <feff4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002> /dan <feff004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e> /deu <feff00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e> /esp <feff005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e> /fra <feff005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e> /ita (utilizzar quest impostazioni crear documenti adob pdf adatti visualizzar e stampar documenti aziendali modo affidabile. documenti pdf creati possono esser aperti acrobat e adob reader 5.0 e versioni successive.) /jpn <feff30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002> /kor <feffc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e> /nld (gebruik deze instellingen om adob pdf-documenten te maken waarme zakelijk documenten betrouwbaar kunnen worden weergegeven en afgedrukt. gemaakt pdf-documenten kunnen worden geopend met acrobat en adob reader 5.0 en hoger.) /nor <feff004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e> /ptb <feff005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e> /suo <feff004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e> /sve <feff0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e> /enu (use set creat pdf match "suggested" set pdf specif 4.0) >> >> setdistillerparam << /hwresolut [600 600] /pages [612.000 792.000] >> setpagedevic