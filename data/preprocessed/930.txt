journal artifici intellig research 55 ( 2016 ) 1-15 submit 11/15 ; publish 01/16 featur paradigm : deep learn machin translat marta r. costa-jussa marta.ruiz @ upc.edu universitat politecnica de catalunya , barcelona abstract last year , deep learn algorithm high revolution sever area includ speech , imag natur languag process .
specif field machin translat ( mt ) remain invari .
integr deep learn mt vari re-model exist featur standard statist system develop new architectur .
among differ neural network , research work use feed- forward neural network , recurr neural network encoder-decod schema .
architectur abl tackl challeng low-resourc morpholog variat .
manuscript focus describ neural network integr enhanc differ aspect model statist mt , includ languag model , word align , translat , reorder , rescor .
, report new neural mt approach togeth descript foundat relat work recent approach use subword , charact train multilingu languag , among other .
final , includ analysi correspond challeng futur work use deep learn mt .
1 .
introduct inform societi continu evolv toward multilingu : e.g .
differ languag english gainin import web ; strong societi , like european , continu multilingu .
differ languag , domain , languag style combin potenti sourc inform .
context , machin translat ( mt ) , task automat translat text sourc languag target languag , gain relev .
industri academi strong investig field progress incred speed .
progress may direct attach introduct deep learn .
basic , deep learn evolut neural network compos multiple- layer model , neural network machin learn system capabl learn task train exampl without requir explicit program task .
mt one applic deep learn succeed recent .
although neural network propos mt late nineti ( forcada & neco , 1997 ; castano & casacuberta , 1997 ) , integr differ part statist mt sinc 2006 , 2013 2014 first competit neural mt system propos ( kalchbrenn & blunsom , 2013 ; sutskev , vinyal , & le , 2014 ; cho , van merrienbo , gulcehr , bahdanau , bougar , schwenk , & bengio , 2014b ) , 2015 , neural mt reach state-of-the-art ( bahdanau , cho , & bengio , 2015 ) .
c2016 ai access foundat .
right reserv .
1.1 mt approach deep learn mt approach main follow rule-bas corpus-bas strategi .
rule- base mt system date back earli 70s initi systran ( philipson , 2017 ) eurotra ( maegaard , 1989 ) .
idea behind rule-bas approach transforma- tion sourc target done mean perform analysi sourc text , transfer ( hand-craft rule ) new sourc represent target representa- tion generat final target text .
corpus-bas approach learn larg amount text .
one popular success approach statist one , particular , phrase-bas mt system ( koehn , och , & marcu , 2003 ) .
statist approach benefit train larg dataset .
normal , statist mt use parallel text level sentenc , use co-occurr extract bilingu dictionari , final , use monolingu text comput languag model estim fluent translat text target languag .
main limit statist mt reli parallel corpora .
rule-bas mt , limit requir mani linguist resourc , lot human expert time .
consider amount research tri hybrid two approach ( costa-jussa , 2015 ) .
anoth type mt approach , popular decad 80s , interlingua- base , focus find univers represent languag .
howev , approach fallen disus challeng expens manual find univers represent languag .
1.2 mt deep learn recent appear new train optim algorithm neural network , i.e .
deep learn techniqu ( hinton , osindero , & teh , 2006 ; bengio , 2009 ; goodfellow , ben- gio , & courvill , 2016 ) , avail larg quantiti data increas comput power capac benefit introduct deep learn mt .
deep learn learn represent multipl level abstract complex ( bengio , 2009 ) .
lot excit around deep learn achiev breakthrough , e.g .
automat extract composit imag line face ( lee , gross , ranganath , & ng , 2009 ) , imagenet classif ( krizhevski , sutskev , & hinton , 2012 ) reduc error rate speech recognit around 10 % ( grave , moham , & hinton , 2013 ) .
lot recent activ scientif communiti use deep learn mt refelect , exampl , explos number work relev confer 2014 date .
manuscript present overview earli stage deep learn start featur function statist mt ( schwenk , costa-jussa , & fonollosa , 2006 ) becom entir new paradigm , achiev state-of-the-art result ( jean , cho , memisev , & bengio , 2015 ) within one-year develop .
2 1.3 manuscript  contribut organ manuscript focus collect describ research done introduc deep learn mt .
differ previous survey ( zhang & zong , 2015 ) , de- tail deep learn techniqu , instead provid briefli descript make manuscript self-contain .
center attent :  overview integr deep learn mt report mt aspect improv differ type neural network ;  detail new neural mt architectur , cite foundat work well discuss recent advanc face challeng aspect encount neural mt architectur ;  depict analysi strength weak deep learn mt .
rest manuscript organ follow .
section 2 briefli defin type neural network most use enhanc mt includ feed-forward , recurr neural network encoder-decod schema .
, section 3 classifi deep learn introduc mt enhanc translat languag model , word align , contextu inform , reorder rescor .
section 4 review emerg deep learn architectur mt : neural mt togeth descript recent advanc area .
section 5 underlin main challeng use deep learn mt depict main strength weak .
final , last section bring discuss role futur deep learn mt field point work direct .
2 .
brief descript neural network type section briefli describ neural network type use mt , detail neural network type refer complet studi ( goodfellow , bengio , & courvill , 2017 ) .
neural network defin type statist learn algorithm use estim function larg number input .
neural network organis layer , includ input output layer , , consid one sever hidden layer .
layer compos neuron elementari unit network .
neuron receiv one sever input neuron perform weight sum input pass non-linear function ( activ ) produc output .
among main advantag , algorithm :  extract abstract data , current , provid best perform multipl domain applic learn data ;  requir featur engin sinc algorithm learn data ;  easili adapt new problem , i.e .
deep learn architectur ap- pli one particular applic use applic ( kaiser , gomez , shazeer , vaswani , parmar , jone , & uszkoreit , 2017 ) .
3 among main drawback , algorithm :  train complex model requir larg amount data , huge number param- eter high comput cost ;  requir challeng task determin right architectur topolog network adequ task .
section introduc main neural network type employ mt , either complement statist approach part new neural mt ap- proach .
main feed-forward recurr neural network encoder- decod schema .
extend multipl hidden layer unit input output layer constitut deep neural network ( dnns ) .
network mt task most train supervis learn framework , algorithm learn huge collect exampl ( i.e .
parallel text level sentenc ) .
consequ , one main big issu deep learn architectur deal larg vocabulari .
train speed goe vocabulari increas .
main reason deep learn architectur use mt ( relat task ) normal requir softmax function generat final probabl output word .
comput soft- max function involv take sum score word vocabulari , feasibl larg vocabulari .
see section 3 4 , main two direct face problem : either reduc vocabulari size use charact subword unit instead word ; use approxim self-norm reduc comput time , model size .
2.1 feed-forward neural network feed-forward neural network ( fnns ) , see figur 1 ( ) , connect input hidden node output without loop .
basic , network classifi singl multi-lay perceptron .
former consist function map input output valu .
latter consist sever fulli connect layer direct graph .
layer sever node , node neuron non-linear activ function .
convolut neural network ( cnns ) popular type fnns cnns , whose connect pattern neuron inspir overlap individu neuron anim cortex ( lecun & bengio , 1998 ) .
convolut oper mathemat way describ connect pattern .
mention 3 basic properti cnns top fnns : local connect ( adjac neuron connect ) , paramet share ( replic unit share parameter ) maxpool unit form subsampl .
2.2 recurr neural network recurr neural network ( rnns ) , see figur 1 ( b ) , anoth class neural network .
main characterist connect unit form direct cycl , generat intern state dynam tempor behavior .
fnns typic reli fixed-s context window make markov assumpt word depend n previous word .
hand , rnns abl use intern memori get rid 4 markov assumpt condit previous word , high relev languag model mt .
differ type rnns , manuscript , focus use first neural mt system .
much detail explan type neural network found ( goodfellow et al. , 2016 ) .
long short term memori ( lstm ) lstm network ( hochreit & schmidhub , 1997 ) direct cycl structur differ structur repeat cycl .
repeat cycl three neural network gate ( input , memory/forget output ) allow discard keep inform solv problem rnn face vanish gradient , discov ( hochreit , 1991 ; bengio , simard , & frasconi , 1994 ) .
intuitev , vanish gradient problem may appear use gradient-bas backpropag method .
train weight algorithm , weight updat use gradient error function .
point , rnns , chain rule appli entir histori sequenc appli mani time may caus gradient tend zero ( special , use activ function tanh sigmoid ) .
sinc lstm initi propos , success use wide rang sequenc applic ( grave , 2013 ) .
gate rnn altern lstms gate rnn ( chung , gulcehr , cho , & bengio , 2015 ; cho et al. , 2014b ) , main differ instead three gate lstms , grus two gate ( reset updat ) .
grus less paramet train compar lstms may help train faster general better less data ( chung , gulcehr , cho , & bengio , 2014 ) .
bi-direct rnn bi-direct rnn use finit sequenc label sequenc  element use past futur context ( schuster & paliw , 1997 ) .
case , predict comput combin rnn output process sequenc left right rnn output process sequenc right left .
2.3 encoder-decod type architectur inspir autoencod tri predict input ( goodfellow et al. , 2016 ) .
encoder-decod architectur general idea autoencod allow differ input output data .
encoder-decod architectur aim learn represent ( encod ) input data , decod represent minim amount error recov output data .
main purpos intern represent dimension reduct capabl ex- tract relev featur dataset .
schemat represent shown figur 1 ( c ) .
3 .
statist mt system deep learn statist mt ( lopez , 2008 ) focus find probabl target text given sourc text .
train , approach use parallel monolingu corpus extract sever model ( i.e .
translat , languag reorder ) infer combin beam-search decod .
translat model bilingu dictionari word sequenc score base co-ocurr extract parallel corpus , 5 figur 1 : feed-forward neural network ( ) ; recurr neural network ( b ) autoen- coder ( c ) schema .
automat align level word , usual use ibm model ( brown , pietra , pietra , & mercer , 1993 ) .
aim model provid accur translat sourc sequenc .
languag model monolingu dictionari word sequenc object model probabl target sequenc .
final , reorder model score chang order sourc target .
model assign differ weight optim procedur maximis translat qualiti , measur automat metric , e.g .
bleu ( papineni , rouko , ward , & zhu , 2002 ) .
addit , system may benefit rescor n-best list deal decod search error .
statist mt enhanc neural network differ level .
section cover neural network improv languag model ; word align ; translat model ; reorder ; rescor .
figur 2 show proport work model ( note sourc statist cite paper current manuscript ) .
summari section , tabl 1 show main relat work introduc deep learn standard statist mt .
3.1 languag model languag model task score sequenc word .
approach neural net- work languag model long histori ( elman , 1990 ; bengio , ducharm , vincent , & janvin , 2003 ) .
subsect cover way neural network enhanc monolingu languag model statist mt system , bilingu languag model report follow section 3.3 .
( schwenk et al. , 2006 ) use continu space languag model inspir classic approach ( bengio et al. , 2003 ) improv n-gram-bas mt system ( marino , banch , crego , de gispert , lambert , fonollosa , & costa-jussa , 2006 ) .
neural network multi- 6 layer perceptron , train classifi , input project , hidden output layer .
project layer input word repres distribut encod input word use linear activ function .
hidden layer use hyperbol tangent activ function output layer softmax layer .
case , continu space languag model use rescor phrase-bas n-gram-bas mt system .
follow , ( mikolov , 2012 ) use recurr neural network languag model rescor n-best list .
regard problem limit vocabulari ( mention section 2 ) , ( hu , auli , gao , & gao , 2014 ) use two rnn-base minimum translat unit model .
author focus address issu data sparsiti limit context size leverag continu represent unbound histori recurr network .
frame problem sequenc model task minim unit , furthermor , model minimum translat unit bag-of-word .
result show approach complementari strong rnn-base languag model base sole target word .
( niehu & waibel , 2012 ) propos simplifi neural languag model base re- strict boltzmann machin integr mt decod .
fulli integr lead improv n-best rescor .
also integr mt decod , ( vaswani , zhao , fossum , & chiang , 2013 ) use fnn architectur use rectifi linear unit noise-contrast estim , requir repeat summat whole vocabulari enabl train neural network larger dataset .
( devlin , zbib , huang , lamar , schwartz , & makhoul , 2014 ) introduc neural network joint languag model augment n-gram languag model ( stolck , 2002 ) m-word sourc window .
moreov , network self-norm , allow increas vocabulari size .
( auli & gao , 2014 ) show rnn languag model optim use bleu criterion .
addit , author effici integr rnn decod .
given decod speed use n-gram lm still state-of-the-art , approach calcul neural probabl n-gram format ( wang , utiyama , goto , sumita , zhao , & lu , 2013 ) .
framework joint translat reorder consist train sequenc encod translat word reorder inform time , ( guta , alkhouli , peter , wuebker , & ney , 2015 ) compar perform n-gram , feedforward recurr neural network direct translat .
addit , recent , studi investig effect sever variat neural languag model inform .
( luong , kayser , & man , 2015a ) investig whether deep neural languag model three four layer outper- form fewer layer term translat qualiti , reach signific improv joint condit sourc target context .
( costa-jussa , espana , madhyastha , escolano , & fonollosa , 2016 ) use character-awar languag model ( kim , jernit , sontag , & rush , 2016 ) rescor n-best list outperform result enhanc phrase-bas system .
3.2 word align word align key task statist mt system sinc identifi word-by-word relationship given pair sentenc correspond translat .
ibm model 7 ( brown et al. , 1993 ) one popular probabilist formul problem use success giza++ implement ( och & ney , 2003 ) .
recent , appear approach use deep learn perform task .
( yang , liu , li , zhou , & yu , 2013 ) use methodolog dnn use speech recognit learn extract lexic translat inform .
model integr multi-lay neural network hidden markov model ( hmm ) framework , extract context depend lexic translat .
model train bilingu corpus use monolingu data pre-train word embed .
improv qualiti classic ibm model .
( tamura , watanab , & sumita , 2014 ) improv previous work use rnn allow unlimit align histori .
3.3 translat model given work literatur , distinguish studi done bilingu translat model , phrase-bas model syntax-bas model .
main differ reli fact bilingu translat model follow languag model structur bilingu unit ( marino et al. , 2006 ) , phrase-bas model use bilingu unit context ( koehn et al. , 2003 ) , , final , syntax-bas model incorpor explicit represent syntax pars sourc and/or target sentenc follow type grammar e.g .
( yamada & knight , 2001 ) .
bilingu languag model earli approach use neural network bilingu translat model normal two-step system , mean n-best list propos tradit way , continu space model use rescor list .
( schwenk , costa-jussa , & fonollosa , 2007 ) propos project bilingu unit onto continu space .
, project allow estim translat probabl continu represent .
case , author use fully-connect multi-lay perceptron .
bilingu unit act neural network input .
author face problem comput complex solv limit vocabulari .
( zamora-martnez , bleda , & schwenk , 2010 ) appli neural languag model bilingu monolingu languag model , relev , decod extend neural languag model viterbi , give better result rescor .
( son , allauzen , & yvon , 2012 ) propos similar architectur , author use two vector input layer come sourc target languag .
two represent combin hidden layer .
also order deal larger vocabulari , output structur cluster tree , word belong one class associ sub-class .
( wu , watanab , & hori , 2014 ) tackl sparsiti problem factor bilingu tupl sourc target use recurr model .
phrase-bas translat model ( schwenk , 2012 ) use fnns estim trans- lation probabl use continu represent discuss fulli integr decod .
( gao , , yih , & deng , 2014 ) report novel phrase translat model score bilingu phrase distanc featur vector continu space .
continu space learn multi-lay neural network weight learn bleu score .
( sundermey , alkhouli , wuebker , & ney , 2014 ) present two word-bas phrase-bas approach recurr translat model .
former 8 assum one-to-on align sourc target sentenc .
latter model phrasal translat probabl avoid sparsiti issu use singl word input output unit .
furthermor , addit unidirect formul , author experi- ment bidirect network take full sourc sentenc account predict .
particular relev challeng phrase-bas translat model fact tak- ing account larger context produc translat .
standard translat provid context translat , general limit short context .
therefor , follow focus work aim success employ larger context phrase-bas translat model .
( zou , socher , cer , & man , 2013 ) learn bilingu embed larg unlabel corpus , util mt word align constrain translat equival .
new embed ad semant featur phrase-bas system signific outperform baselin system .
( cui , zhang , liu , chen , li , zhou , & yang , 2014 ) propos learn topic represent parallel data use encoder-decod techniqu pre-train fine-tun .
pre-train fine-tun allow partial initi error function point easier train .
( martnez , espana-bonet , tiedemann , & marquez , 2014 ) use distribut vector represent word ( mikolov , le , & sutskev , 2013 ) handl ambigu word .
author identifi content word differ translat .
content word , author take window two previous two follow word comput vector represent .
comput linear combin vector obtain context vector .
, calcul score base similar among vector everi possibl translat option .
( costa-jussa , gupta , rosso , & banch , 2014 ) use deep learn encoder-decod structur learn similar correspond train test sentenc integr similar measur new featur phrase-bas system .
syntax-bas translat model ( meng , lu , wang , li , jiang , & liu , 2015 ) sum- mariz relev sourc inform convolut architectur , guid target inform , , architectur integr dependency-to-str translat system ( xie , mi , & liu , 2011 ) .
( zhai , zhang , zhou , & zong , 2014 ) use rnn perform structur predict bracket transduct grammar mt system ( wu , 1995 ) .
3.4 reorder word reorder respond phenomena word take differ posit sourc target sentenc involv translat .
becom one challeng aspect mt larg bodi research work address issu ( bisazza & federico , 2016 ) .
main two option appli reorder model within statist mt framework integr model within decod formul preprocess .
former perform reorder direct search target languag .
latter reorder sourc word way better match target word order ( costa-jussa & fonollosa , 2006 ) .
9 reorder decod .
( li , liu , sun , izuha , & zhang , 2014a ) propos recurs autoencoder-bas itg-bas translat ( wu , 1995 ) , type syntax-bas model .
( li , liu , sun , izuha , & zhang , 2014b ) propos neural reorder model condit reorder probabl word current previous phrase pair .
( kanouchi , sudoh , & komachi , 2016 ) use also recurs autoencod architectur use phrase translat word align inform test phrase-bas system .
specif , allevi data sparsiti problem , author build one classifi phrase pair use four recurs autoencod softmax layer .
phrase pair repres continu space vector use also recurs autoencod .
differ , ( setiawan , huang , devlin , lamar , zbib , schwartz , & makhoul , 2015 ) develop new neural network featur model non-loc translat phenomena relat word reorder- ing improv featur tensor neural network .
author use hypothesis- enumer featur estim probabl generat target word source- enumer featur estim probabl sourc word .
reorder preprocess .
( mice baron & attardi , 2015 ) propos class rnn model exploit sourc depend syntax .
( yu & zhu , 2015 ) propos rnn-base rule sequenc model captur arbitrari distanc contextu inform estim probabl rule sequenc .
( cui , wang , & li , 2016 ) present lstm-base neural reorder model direct model word pair align .
figur 2 : introduct neural network statist mt .
work appli main translat languag model ( base paper cite manuscript ) .
10 tabl 1 : deep learn statist mt : main relat work system model main relat work statistical-bas languag model ( schwenk et al. , 2006 ; schwenk , 2010 ) ( niehu & waibel , 2012 ; vaswani et al. , 2013 ) ( wang et al. , 2013 ; luong et al. , 2015a ) ( mikolov , 2012 ; devlin et al. , 2014 ) ( auli & gao , 2014 ; hu et al. , 2014 ) ( guta et al. , 2015 ; costa-jussa et al. , 2016 ) word align ( yang et al. , 2013 ) ( tamura et al. , 2014 ) translat ( schwenk et al. , 2007 ; zamora-martnez et al. , 2010 ) ( son et al. , 2012 ; schwenk , 2012 ) ( gao et al. , 2014 ; meng et al. , 2015 ) ( sundermey et al. , 2014 ) ( zou et al. , 2013 ; martnez et al. , 2014 ) ( mikolov et al. , 2013 ; cui et al. , 2014 ) ( costa-jussa et al. , 2014 ) ( wu et al. , 2014 ; zhai et al. , 2014 ) reorder ( setiawan et al. , 2015 ) ( mice baron & attardi , 2015 ) ( yu & zhu , 2015 ) ( cui et al. , 2016 ) ( li et al. , 2014a , 2014b ; kanouchi et al. , 2016 ) ( setiawan et al. , 2015 ) rescor ( neubig et al. , 2015 ; stahlberg et al. , 2016a ) 3.5 rescor rescor task re-rank list tentat translat ( provid decod ) use differ knowledg inform one use model decod .
research work area previous posterior neural mt system .
previous , most use neural lm tm rescor statistical-bas system report section languag translat model 3.1 3.3 , respect .
posterior , refer use neural mt system ( detail next section ) purpos .
direct , ( neubig , morishita , & nakamura , 2015 ) rescor n- best list syntax-bas system nmt ( stahlberg , hasler , & byrn , 2016a ) improv approach use lattic instead .
4 .
neural machin translat neural mt system neural network train predict target sentenc given sourc sentenc .
section , inspir ( cho , 2015 ) , describ probabilist train frame- work new approach , togeth review main foundat work recent advanc neural mt .
summari section , tabl 2 show main relat work neural mt .
4.1 probabilist train framework train framework core neural mt aim maxim probabl target sentenc given sourc sentenc .
particular , neural mt system map sourc sentenc = s1 , ... , si ( word ) target sentenc = t1 , ... , tj ( j word ) parametr condit distribut p ( tn|sn ) train sentenc 11 corpus set n = 1 ... n .
, learn algorithm maxim follow object function : argmax 1 n n n=1 log p ( tn|sn ) ( 1 )  differ model paramet .
deal variable-length input output , rnns ( see section 2.2 ) abl maintain intern state read sequenc input , translat sequenc word , therebi allow input length .
4.2 encoder-decod base rnns mention earlier , neural mt system use encoder-decod schema , maxim shown equat 1 .
section describ explicit encod decod base rnns oper .
encod follow next step : 1 .
build word one-hot vector , binari vector singl element set 1 ( wi ) .
2 .
project one-hot vector continu represent .
encod project vector matrix e whose column word sourc vocabulari row number dimens chosen ( mi = ewi ) .
project generat continu vector sourc word , element vector later updat maxim log-prob correct output sentenc .
3 .
build sequenc summar rnn , hi = 0 ( hi1 , mi ) ,  activ function rnn  paramet .
visual vector , seen similar sentenc close togeth summary-vector space ( sutskev et al. , 2014 ) .
decod , basic invers encod , follow next step : 1 .
comput intern hidden state decod zi =  ( hi , ui1 , zi1 ) , hi repres summari whole sourc sentenc , ui1 previous translat word , zi1 previous hidden state decod .
2 .
comput next word probabl , first score word k given hidden state zi e ( k ) = wikzi + bk , , simplifi bias , score normal obtain probabl use softmax .
3 .
predict next word .
choos ith word , go back first step comput decod  intern hidden state , score normal target word select next ( i+ 1 ) th word , repeat select end-of-sent word .
simpl architectur led notabl improv achiev state-of-the-art qualiti translat , explain follow section .
12 figur 3 : neural mt architectur .
4.3 foundat work earli research neural mt found work like ( forcada & neco , 1997 ; castano & casacuberta , 1997 ) , main limit comput power short data .
former build state-spac represent input string unfold obtain correspond output string .
latter use elman simpl rnn ( elman , 1990 ) go sourc target .
first propos neural mt model main use previous encoder-decod architectur ( sutskev et al. , 2014 ; cho et al. , 2014b ) .
explain previous section 4.2 , architectur allow encod sourc text fixed-length vector decod fixed-length vector target text .
encod decod train singl architectur parallel corpus .
main problem type architectur compress sourc sentenc fixed-length vector .
( cho , van merrienbo , bahdanau , & bengio , 2014a ) analys new approach show neural mt perform relat well short sentenc without unknown word , perform degrad rapid increment sentenc length number unknown word .
13 address long sentenc issu , i.e .
main caus encod input sentenc singl fixed-lentgh vector , ( bahdanau et al. , 2015 ) propos new mechan decod decid part sourc sentenc pay attent .
attent mechan reliev encod compress sourc sentenc fixed- length vector , allow neural translat model deal better long sentenc .
see schemat represent encoder-decod attent figur 3 .
case ( pouget-abadi , bahdanau , van merrienbo , cho , & bengio , 2014 ) , au- thor propos way address challeng long sentenc automat segment input sentenc phrase easili translat neural network transla- tion model .
4.4 recent advanc sinc neural mt young paradigm , still larg room improv .
seem perform foundat neural mt work larg depend languag pair quantiti train resourc .
exampl , wmt 2015 , neural mt beat phrase-bas system one task ( bojar , chatterje , federmann , haddow , huck , hokamp , koehn , logacheva , monz , negri , post , scarton , specia , & turchi , 2015 ) .
howev , one year later , use subword unit ( sennrich , haddow , & birch , 2016b ) enlarg train data , neural mt system outperform phrase-bas system larg number task ( bojar , chatterje , federmann , graham , haddow , huck , jimeno yepe , koehn , logacheva , monz , negri , neveol , neve , popel , post , rubino , scarton , specia , turchi , verspoor , & zampieri , 2016 ) .
follow describ popular recent advanc appli neural mt focus solv big challeng neural mt , includ : manag long train ; cover en- tire sourc sentenc translat ; correct deal morpholog variat larg vocabulari ; manag low-resourc pair languag ; make train effici .
encoder-decod architectur .
encod decod recent design mean three differ success architectur .
first , use rnns ( mention section 4.2 ) use recurr process sequenc variabl length .
main disadvantag network process text strict order ( either left-to-right right-to-left ) comput expens sinc parallel ( cho et al. , 2014b ) .
second approach ( convolut neural network ) overcom limit network process element time .
network allow comput vector represent sequenc word way deal input sentenc allow learn hierarch structur ( gehr , auli , grangier , yarat , & dauphin , 2017 ) .
final , third approach use self-attent mechan allow model depend without limit posit ( vaswani , shazeer , parmar , uszkoreit , jone , gomez , kaiser , & polosukhin , 2017 ) .
attent mechan variat .
mention previous section 4.3 attention- base mechan solv limit encod fixed-length vector entir input .
sever propos improv origin propos ( bahdanau et al. , 2015 ) .
( tu , lu , liu , liu , & li , 2016 ) propos maintain coverag vector keep track attent histori .
help prevent over-transl ( word unnecessarili 14 translat multipl time ) under-transl ( word mistaken untransl ) .
variat propos , ( yang , hu , deng , dyer , & smola , 2016 ) propos model sequenc attent level word rnn check fix window previous decis .
given attent current time tend correl previous attent , ( cohn , hoang , vymolova , yao , dyer , & haffari , 2016 ) propos add inform last attent make next decis .
attent also use sentenc case call intra self-attent .
type attent use first translat work ( vaswani et al. , 2017 ) new structur multi-head attent allow focus attent differ part sentenc time .
larg vocabulari , subword , character-bas .
mention section 2 , com- pute final probabl output word cost larger vocabulari .
( jean et al. , 2015 ) propos model reduc limit target vocabulari use sampl re- duce complex comput normal constant output word probabil- iti neural languag model ( bengio & senec , 2008 ) .
( luong , sutskev , le , vinyal , & zaremba , 2015b ) address problem rare word use post-process step translat out-of-vocabulari word use dictionari .
differ , approach , base intuit various word class translat via smaller unit word , use word segment techniqu empir show subword model im- prove back-off dictionari baselin unknown word ( sennrich , haddow , & birch , 2016a ) .
furthermor , approach direct deal charact ( costa-jussa & fonollosa , 2016 ; lee , cho , & hofmann , 2016 ) even byte ( costa-jussa , escolano , & fonollosa , 2017b ) reduc vocabulari minimum .
work use character-bas embed train convolut network highway network ( srivastava , greff , & schmidhub , 2015 ) .
multilingu low-resourc .
low-resourc alway limita- tion train competit corpus-bas mt system .
neural mt propos sever way tackl problem .
one hand , ( zoph , yuret , may , & knight , 2016 ) propos train high-resourc languag pair transfer learn paramet low-resourc languag pair .
hand , sever propos train system multilingu resourc .
relev work direct includ system train one- to-mani languag ( dong , wu , , yu , & wang , 2015 ) , simultan translat sentenc one sourc languag multipl target languag ; many-to-on languag ( zoph & knight , 2016 ) , standard neural mt model train mani sourc one singl target languag ; many-to-mani ( firat , cho , & bengio , 2016 ) , neural model train mani sourc mani target languag .
recent advanc includ system abl zero-shot translat ( johnson , schuster , le , krikun , wu , chen , thorat , viega , wattenberg , corrado , hugh , & dean , 2016 ) , mean without parallel corpus languag b , system abl learn translat among languag .
, general , phrase-bas mt handl low-resourc set better shown ( koehn & knowl , 2017 ) .
ad prior/linguist knowledg .
although idea neural mt deep learn- ing general ad minimum prior knowledg possibl system , ap- proach shown ad kind linguist knowledg use .
syntact 15 tabl 2 : deep learn neural mt : main relat work system relat progress main relat work neural-bas foundat ( forcada & neco , 1997 ; castano & casacuberta , 1997 ) ( sutskev et al. , 2014 ; cho et al. , 2014a ) ( cho et al. , 2014b ; bahdanau et al. , 2015 ) ( pouget-abadi et al. , 2014 ) enc-dec ( gehr et al. , 2017 ; vaswani et al. , 2017 ) attent ( tu et al. , 2016 ; yang et al. , 2016 ) ( cohn et al. , 2016 ; vaswani et al. , 2017 ) subword ( jean et al. , 2015 ; luong et al. , 2015b ) ( sennrich et al. , 2016b ; costa-jussa et al. , 2017b ) ( costa-jussa & fonollosa , 2016 ; lee et al. , 2016 ) multilingu ( zoph et al. , 2016 ; zoph & knight , 2016 ; firat et al. , 2016 ) ( johnson et al. , 2016 ) linguist ( eriguchi et al. , 2016 ; stahlberg et al. , 2016a ) ( sennrich & haddow , 2016 ) product ( crego et al. , 2016 ; wu et al. , 2016 ; levin et al. , 2017 ) knowledg ad ( eriguchi , hashimoto , & tsuruoka , 2016 ; stahlberg , hasler , wait , & byrn , 2016b ) ( sennrich & haddow , 2016 ) train morpholog linguist featur word embed neural mt model .
system product .
although effici neural mt challeng sinc train system may last week ( special use rnns ) , alreadi success neural mt system product .
detail descript system found ( crego , kim , klein , rebollo , yang , senellart , akhanov , & et al. , 2016 ; wu , schuster , chen , le , norouzi , & et al. , 2016 ; levin , dhanuka , khalil , kovalev , & khalilov , 2017 ) .
5 .
neural mt analysi : strength weak deep learn introduc standard statist mt system ( see section 3 ) new mt approach ( see section 4 ) .
section make analysi main strength weak neural mt approach ( see summari figur 5 ) .
analysi help toward plan futur direct neural mt .
strength main inher strength neural mt model compon joint train allow end-to-end optim .
anoth relev strength , given architectur base creat interme- diat represent , neural model could eventu evolv toward machine-learnt interlingua approach ( johnson et al. , 2016 ) .
interlingua represent would key outperform mt low-resourc languag pair well effici deal mt high multilingu environ .
addit , neural mt shown abl learn differ basic unit granular- iti .
subword-bas represent , e.g .
( sennrich et al. , 2016b ; costa-jussa & fonollosa , 2016 ; lee et al. , 2016 ) , allow neural mt model open-vocabulari translat seg- ment word .
among differ altern build subword unit , byte pair encod , data compres techniqu , shown perform effici ( sennrich et al. , 2016b ) .
charact allow take advantag intra-word inform implement sourc side ( costa-jussa & fonollosa , 2016 ) sourc target side ( lee et al. , 2016 ) .
16 figur 4 : strength weak analysi neural mt .
final , new paradigm allow multimod machin translat ( elliott , frank , & hasler , 2015 ) , allow take advantag imag inform translat end-to-end speech translat architectur ( weiss , chorowski , jait , wu , & chen , 2017 ) , reduc concaten error .
weak main inher weak neural mt difficulti trace error , long train time high comput cost .
weak high comput cost train test model .
train face gpus ( graphic process unit ) expens .
final , ad weak relat interpret model fact model work vector , matric tensor instead word phrase .
there- fore , abil train neural model scratch requir background machin learn comput scienc easi users/compani abl compre- hend/interpret .
difficult adopt paradigm .
small compani may prefer consolid paradigm like phrase rule-bas .
17 6 .
summari , conclus futur work deep learn integr standard statist mt system differ level ( i.e .
languag model , word align , translat , reorder rescor ) differ perspect achiev signific improv case .
field deep learn advanc quick worth notic neural-bas techniqu work today may replac new one near futur .
addit , entir new paradigm propos : neural mt .
curious , approach propos almost simultanea popular phrase-bas system ( forcada & neco , 1997 ; castano & casacuberta , 1997 ) .
propos name differ connectionist mt , given comput power requir prohibit time data avail enough train complex system , idea abandon .
nowaday , thank gpus , comput power limit inform societi provid larg quantiti data allow train larg number paramet model .
difficult quantifi much mt improv neural approach .
vari languag pair task .
exampl , result wmt 2016 evalu ( bojar et al. , 2016 ) show neural mt achiev best result ( term human evalu ) languag direct german-english , english romanian , english-german , czech-english , english-czech ; other like romanian-english , russian-english , english-russian , english-finnish .
neural mt may affect larg languag differ , low resourc variat train versus test domain ( aldon , 2016 ; costa- jussa , aldon , & fonollosa , 2017a ; costa-jussa , 2017 ; koehn & knowl , 2017 ) .
interpret mt system never difficult .
evolut mt , first lost rule ( transit rule statistical-bas approach ) recent , lost translat unit ( transit statist neural-bas approach ) .
nowaday , new neural-bas approach mt open new question , e.g .
machine-learnt interlingua someth attain ?
minim unit translat ?
manuscript recompil systemat foundat work use deep learn- ing mt progress incred fast .
deep learn influenc mani area natur languag process expect use techniqu con- troversi .
adventur envisag neural algorithm go impact mt futur seem stay proven recent news big compani adopt neural mt approach e.g .
google1 systran ( crego et al. , 2016 ) .furthermor , deep learn alreadi take field dramat shown appear first end-to-end speech-to-text translat ( weiss et al. , 2017 ) multimod mt ( elliott et al. , 2015 ) , interlingua-bas represent ( firat , cho , sankaran , vural , & bengio , 2017 ) unsupervis mt ( artetx , labaka , agirr , & cho , 2017 ; lampl , denoy , & ranzato , 2017 ) .
1. http : //www.nature.com/news/deep-learning-boosts-google-translate-tool-1.20696 18 acknoledg work support spanish ministerio de economa competitividad , eu- ropean region develop fund agencia estat de investigacion , postdoctor senior grant ramon cajal , contract tec2015-69266-p ( mineco/fed , eu ) contract pcin-2017-079 ( aei/mineco ) .
refer aldon , d. ( 2016 ) .
sistema de traduccion neuron usando bitmap .
b.s .
thesi , universitat politecnica de catalunya .
artetx , m. , labaka , g. , agirr , e. , & cho , k. ( 2017 ) .
unsupervis neural machin translat .
corr , abs/1710.11041 .
auli , m. , & gao , j .
( 2014 ) .
decod integr expect bleu train recurr neural network languag model .
proceed 52nd annual meet associ comput linguist ( volum 2 : short paper ) , pp .
136142 , baltimor , maryland .
associ comput linguist .
bahdanau , d. , cho , k. , & bengio , .
( 2015 ) .
neural machin translat joint learn align translat .
corr , abs/1409.0473 .
bengio , y. , & senec , j. s. ( 2008 ) .
adapt import sampl acceler train neural probabilist languag model .
tran .
neur .
netw. , 19 ( 4 ) , 713722 .
bengio , y. , simard , p. , & frasconi , p. ( 1994 ) .
learn long-term depend gra- dient descent difficult .
tran .
neur .
netw. , 5 ( 2 ) , 157166 .
bengio , .
( 2009 ) .
learn deep architectur ai .
found .
trend mach .
learn. , 2 ( 1 ) , 1127 .
bengio , y. , ducharm , r. , vincent , p. , & janvin , c. ( 2003 ) .
neural probabilist languag model .
j. mach .
learn .
res. , 3 , 11371155 .
bisazza , a. , & federico , m. ( 2016 ) .
survey word reorder statist machin trans- lation : comput model languag phenomena .
comput .
linguist. , 42 ( 2 ) , 163205 .
bojar , o. , chatterje , r. , federmann , c. , graham , y. , haddow , b. , huck , m. , ji- meno yepe , a. , koehn , p. , logacheva , v. , monz , c. , negri , m. , neveol , a. , neve , m. , popel , m. , post , m. , rubino , r. , scarton , c. , specia , l. , turchi , m. , verspoor , k. , & zampieri , m. ( 2016 ) .
find 2016 confer machin translat .
proceed first confer machin translat , pp .
131198 , berlin , germani .
associ comput linguist .
bojar , o. , chatterje , r. , federmann , c. , haddow , b. , huck , m. , hokamp , c. , koehn , p. , logacheva , v. , monz , c. , negri , m. , post , m. , scarton , c. , specia , l. , & turchi , m. ( 2015 ) .
find 2015 workshop statist machin translat .
pro- ceed tenth workshop statist machin translat , pp .
146 , lisbon , portug .
associ comput linguist .
19 brown , p. f. , pietra , v. j. d. , pietra , s. a. d. , & mercer , r. l. ( 1993 ) .
mathemat statist machin translat : paramet estim .
comput .
linguist. , 19 ( 2 ) , 263311 .
castano , m. a. , & casacuberta , f. ( 1997 ) .
connectionist approach mt..
proc .
eurospeech confer .
cho , k. ( 2015 ) .
natur languag understand distribut represent .
corr , abs/1511.07916 .
cho , k. , van merrienbo , b. , bahdanau , d. , & bengio , .
( 2014a ) .
properti neural machin translat : encoderdecod approach .
proceed ssst-8 , eighth workshop syntax , semant structur statist translat , pp .
103111 , doha , qatar .
associ comput linguist .
cho , k. , van merrienbo , b. , gulcehr , c. , bahdanau , d. , bougar , f. , schwenk , h. , & bengio , .
( 2014b ) .
learn phrase represent use rnn encoder-decod statist machin translat .
proceed 2014 confer empir method natur languag process , emnlp 2014 , octob 25-29 , 2014 , doha , qatar , meet sigdat , special interest group acl , pp .
17241734 .
chung , j. , gulcehr , c. , cho , k. , & bengio , .
( 2014 ) .
empir evalu gate recurr neural network sequenc model .
corr , abs/1412.3555 .
chung , j. , gulcehr , c. , cho , k. , & bengio , .
( 2015 ) .
gate feedback recurr neu- ral network .
proceed 32nd intern confer intern confer machin learn - volum 37 , icml  15 , pp .
20672075 .
jmlr.org .
cohn , t. , hoang , c. d. v. , vymolova , e. , yao , k. , dyer , c. , & haffari , g. ( 2016 ) .
incor- porat structur align bias attent neural translat model .
proceed 2016 confer north american chapter associa- tion comput linguist : human languag technolog , pp .
876885 , san diego , california .
associ comput linguist .
costa-jussa , m. r. ( 2015 ) .
much hybrid machin translat need ? .
journal associ inform scienc technolog , 66 ( 10 ) , 21602165 .
costa-jussa , m. r. ( 2017 ) .
catalan-spanish neural machin translat ?
analysi , com- parison combin standard rule phrase-bas technolog .
pro- ceed eacl workshop : vardial , valencia .
costa-jussa , m. r. , aldon , d. , & fonollosa , j .
a. r. ( 2017a ) .
chinese-spanish neural machin translat enhanc charact andword bitmap font .
machin trans- lation , accept public .
costa-jussa , m. r. , escolano , c. , & fonollosa , j .
.
( 2017b ) .
byte-bas neural machin translat .
proc .
1st workshop subword charact level model nlp , pp .
154158 .
costa-jussa , m. r. , & fonollosa , j .
a. r. ( 2006 ) .
statist machin reorder .
proceed- ing 2006 confer empir method natur languag process , emnlp  06 , pp .
7076 , stroudsburg , pa , usa .
associ comput lin- guistic .
20 costa-jussa , m. r. , & fonollosa , j .
a. r. ( 2016 ) .
character-bas neural machin transla- tion .
proceed 54th annual meet associ comput linguist ( volum 2 : short paper ) , pp .
357361 , berlin , germani .
associ comput linguist .
costa-jussa , m. r. , gupta , p. , rosso , p. , & banch , r. e. ( 2014 ) .
english-to-hindi system descript wmt 2014 : deep source-context featur mose .
proceed ninth workshop statist machin translat , pp .
7983 , baltimor , maryland , usa .
associ comput linguist .
costa-jussa , m. , espana , c. , madhyastha , p. , escolano , c. , & fonollosa , j .
( 2016 ) .
talpupc spanishenglish wmt biomed task : bilingu embed char-bas neural languag model rescor phrase-bas system .
proceed wmt , berlin .
crego , j. m. , kim , j. , klein , g. , rebollo , a. , yang , k. , senellart , j. , akhanov , e. , & et al. , p. b .
( 2016 ) .
systran  pure neural machin translat system .
corr , abs/1610.05540 .
cui , l. , zhang , d. , liu , s. , chen , q. , li , m. , zhou , m. , & yang , m. ( 2014 ) .
learn topic represent smt neural network .
proceed 52nd annual meet associ comput linguist ( volum 1 : long paper ) , pp .
133143 , baltimor , maryland .
associ comput linguist .
cui , y. , wang , s. , & li , j .
( 2016 ) .
lstm neural reorder featur statist machin translat .
proceed 2016 confer north american chapter associ comput linguist : human languag technolog , pp .
977982 , san diego , california .
associ comput linguist .
devlin , j. , zbib , r. , huang , z. , lamar , t. , schwartz , r. , & makhoul , j .
( 2014 ) .
fast robust neural network joint model statist machin translat .
proceed 52nd annual meet associ comput linguist ( volum 1 : long paper ) , pp .
13701380 , baltimor , maryland .
associ comput linguist .
dong , d. , wu , h. , , w. , yu , d. , & wang , h. ( 2015 ) .
multi-task learn multipl languag translat .
acl .
elliott , d. , frank , s. , & hasler , e. ( 2015 ) .
multi-languag imag descript neural sequenc model .
corr , abs/1510.04709 .
elman , j. l. ( 1990 ) .
find structur time .
cognit scienc , 14 ( 2 ) , 179  211 .
eriguchi , a. , hashimoto , k. , & tsuruoka , .
( 2016 ) .
tree-to-sequ attent neural machin translat .
proceed 54th annual meet associ comput linguist ( volum 1 : long paper ) , pp .
823833 , berlin , germani .
firat , o. , cho , k. , & bengio , .
( 2016 ) .
multi-way , multilingu neural machin translat share attent mechan .
proceed 2016 confer north american chapter associ comput linguist : human languag technolog , pp .
866875 , san diego , california .
21 firat , o. , cho , k. , sankaran , b. , vural , f. t. y. , & bengio , .
( 2017 ) .
multi-way , mul- tilingu neural machin translat .
accept public comput speech languag , special issu deep learn machin translat .
forcada , m. l. , & neco , r. p. ( 1997 ) .
recurs hetero-associ memori translat .
proceed intern work-confer artifici natur neural network : biolog artifici comput : neurosci technolog , iwann  97 , pp .
453462 , london , uk , uk .
springer-verlag .
gao , j. , , x. , yih , w. , & deng , l. ( 2014 ) .
learn continu phrase represent translat model .
proceed 52nd annual meet associ comput linguist , acl 2014 , june 22-27 , 2014 , baltimor , md , usa , volum 1 : long paper , pp .
699709 .
gehr , j. , auli , m. , grangier , d. , yarat , d. , & dauphin , y. n. ( 2017 ) .
convolut sequenc sequenc learn .
corr , abs/1705.03122 .
goodfellow , i. , bengio , y. , & courvill , .
( 2016 ) .
deep learn .
mit press .
goodfellow , i. , bengio , y. , & courvill , .
( 2017 ) .
deep learn .
mit press .
grave , .
( 2013 ) .
generat sequenc recurr neural network .
corr , abs/1308.0850 .
grave , a. , moham , a.-r. , & hinton , g. ( 2013 ) .
speech recognit deep recurr neural network .
2013 ieee intern confer acoust , speech signal process , pp .
66456649 .
ieee .
guta , a. , alkhouli , t. , peter , j.-t. , wuebker , j. , & ney , h. ( 2015 ) .
comparison be- tween count neural network model base joint translat reorder sequenc .
proceed 2015 confer empir method natur languag process , pp .
14011411 , lisbon , portug .
associ comput linguist .
hinton , g. e. , osindero , s. , & teh , y.-w. ( 2006 ) .
fast learn algorithm deep belief net .
neural comput. , 18 ( 7 ) , 15271554 .
hochreit , s. ( 1991 ) .
untersuchungen zu dynamischen neuronalen netzen .
diploma , tech- nisch universitat munchen , 91 .
hochreit , s. , & schmidhub , j .
( 1997 ) .
long short-term memori .
neural comput. , 9 ( 8 ) , 17351780 .
hu , y. , auli , m. , gao , q. , & gao , j .
( 2014 ) .
minimum translat model recurr neural network .
proceed 14th confer european chapter associ comput linguist , pp .
2029 , gothenburg , sweden .
associ comput linguist .
jean , s. , cho , k. , memisev , r. , & bengio , .
( 2015 ) .
use larg target vocabulari neural machin translat .
proceed 53rd annual meet associ comput linguist 7th intern joint confer natur languag process ( volum 1 : long paper ) , pp .
110 , beij , china .
associ comput linguist .
22 johnson , m. , schuster , m. , le , q. v. , krikun , m. , wu , y. , chen , z. , thorat , n. , viega , f. b. , wattenberg , m. , corrado , g. , hugh , m. , & dean , j .
( 2016 ) .
googl  mul- tilingu neural machin translat system : enabl zero-shot translat .
corr , abs/1611.04558 .
kaiser , l. , gomez , a. n. , shazeer , n. , vaswani , a. , parmar , n. , jone , l. , & uszkoreit , j .
( 2017 ) .
one model learn .
corr , abs/1706.05137 .
kalchbrenn , n. , & blunsom , p. ( 2013 ) .
recurr continu translat model .
proceed 2013 confer empir method natur languag pro- cess , pp .
17001709 , seattl , washington , usa .
associ comput linguist .
kanouchi , s. , sudoh , k. , & komachi , m. ( 2016 ) .
neural reorder model consid phrase translat word align phrase-bas translat .
proceed 3rd workshop asian translat ( wat2016 ) , pp .
94103 , osaka , japan .
cole 2016 organ committe .
kim , y. , jernit , y. , sontag , d. , & rush , a. m. ( 2016 ) .
character-awar neural lan- guag model .
proceed 30th aaai confer artifici intellig ( aaai  16 ) .
koehn , p. , & knowl , r. ( 2017 ) .
six challeng neural machin translat .
acl workshop neural machin translat .
koehn , p. , och , f. j. , & marcu , d. ( 2003 ) .
statist phrase-bas translat .
pro- ceed 2003 confer north american chapter associ comput linguist human languag technolog - volum 1 , naacl  03 , pp .
4854 , stroudsburg , pa , usa .
associ comput linguist .
krizhevski , a. , sutskev , i. , & hinton , g. e. ( 2012 ) .
imagenet classif deep convolut neural network .
proceed 25th intern confer neural inform process system , nip  12 , pp .
10971105 , usa .
curran associ inc. lampl , g. , denoy , l. , & ranzato , m. ( 2017 ) .
unsupervis machin translat use monolingu corpora .
corr , abs/1711.00043 .
lecun , y. , & bengio , .
( 1998 ) .
handbook brain theori neural networks.. chap .
convolut network imag , speech , time seri , pp .
255258 .
mit press , cambridg , , usa .
lee , h. , gross , r. , ranganath , r. , & ng , a. .
( 2009 ) .
convolut deep belief network scalabl unsupervis learn hierarch represent .
proceed 26th annual intern confer machin learn , icml  09 , pp .
609616 , new york , ny , usa .
acm .
lee , j. , cho , k. , & hofmann , t. ( 2016 ) .
fulli character-level neural machin translat without explicit segment .
corr , abs/1610.03017 .
levin , p. , dhanuka , n. , khalil , t. , kovalev , f. , & khalilov , m. ( 2017 ) .
toward full- scale neural machin translat product : booking.com use case .
corr , abs/1709.05820 .
23 li , p. , liu , y. , sun , m. , izuha , t. , & zhang , d. ( 2014a ) .
neural reorder model phrase-bas translat .
proceed cole 2014 , 25th intern confer comput linguist : technic paper , pp .
18971907 , dublin , ireland .
dublin citi univers associ comput linguist .
li , p. , liu , y. , sun , m. , izuha , t. , & zhang , d. ( 2014b ) .
neural reorder model phrase-bas translat .
proceed cole 2014 , 25th intern confer comput linguist : technic paper , pp .
18971907 , dublin , ireland .
dublin citi univers associ comput linguist .
lopez , .
( 2008 ) .
statist machin translat .
acm comput .
surv. , 40 ( 3 ) , 8:18:49 .
luong , m.-t. , kayser , m. , & man , c. d. ( 2015a ) .
deep neural languag model machin translat .
proceed nineteenth confer comput natur languag learn , beij , china .
luong , t. , sutskev , i. , le , q. , vinyal , o. , & zaremba , w. ( 2015b ) .
address rare word problem neural machin translat .
proceed 53rd annual meet associ comput linguist 7th intern joint confer natur languag process ( volum 1 : long paper ) , pp .
11 19 , beij , china .
associ comput linguist .
maegaard , b .
( 1989 ) .
eurotra : machin translat project european communiti .
perspect artifici intellig , ii .
marino , j .
b. , banch , r. e. , crego , j. m. , de gispert , a. , lambert , p. , fonollosa , j .
a. r. , & costa-jussa , m. r. ( 2006 ) .
n-gram-bas machin translat .
comput .
linguist. , 32 ( 4 ) , 527549 .
martnez , e. , espana-bonet , c. , tiedemann , j. , & marquez , l. ( 2014 ) .
word  vector represent meet machin translat .
proc .
8th workshop syntax , semant structur , doha , qatar .
meng , f. , lu , z. , wang , m. , li , h. , jiang , w. , & liu , q .
( 2015 ) .
encod sourc languag convolut neural network machin translat .
proceed 53rd annual meet associ comput linguist 7th inter- nation joint confer natur languag process ( volum 1 : long paper ) , pp .
2030 , beij , china .
associ comput linguist .
mice baron , a. v. , & attardi , g. ( 2015 ) .
non-project dependency-bas pre-reord recurr neural network machin translat .
proceed 53rd annual meet associ comput linguist 7th inter- nation joint confer natur languag process ( volum 1 : long paper ) , pp .
846856 , beij , china .
associ comput linguist .
mikolov , t. ( 2012 ) .
statist languag model base neural networks.. mikolov , t. , le , q. v. , & sutskev , .
( 2013 ) .
exploit similar among languag machin translat .
corr , abs/1309.4168 .
neubig , g. , morishita , m. , & nakamura , s. ( 2015 ) .
neural rerank improv subject qualiti machin translat : naist wat2015 .
proceed 2nd work- shop asian translat , wat 2015 , kyoto , japan , octob 16 , 2015 , pp .
3541 .
24 niehu , j. , & waibel , .
( 2012 ) .
continu space languag model use restrict boltz- mann machines.. och , f. j. , & ney , h. ( 2003 ) .
systemat comparison various statist align model .
comput linguist , 29 ( 1 ) , 1951 .
papineni , k. , rouko , s. , ward , t. , & zhu , w.-j .
( 2002 ) .
bleu : method automat evalu machin translat .
proceed 40th annual meet associ comput linguist , acl  02 , pp .
311318 , stroudsburg , pa , usa .
associ comput linguist .
philipson , j .
( 2014 ; access septemb 2017 ) .
systran : brief histori machin transla- tion.. pouget-abadi , j. , bahdanau , d. , van merrienbo , b. , cho , k. , & bengio , .
( 2014 ) .
over- come curs sentenc length neural machin translat use automat segment .
proceed ssst-8 , eighth workshop syntax , semant structur statist translat , pp .
7885 , doha , qatar .
associ compu- tation linguist .
schuster , m. , & paliw , k. ( 1997 ) .
bidirect recurr neural network .
tran .
sig .
proc. , 45 ( 11 ) , 26732681 .
schwenk , h. ( 2010 ) .
continuous-spac languag model statist machin translat .
pragu bull .
math .
linguist , 93 , 137146 .
schwenk , h. ( 2012 ) .
continu space translat model phrase-bas statist ma- chine translat .
cole 2012 , 24th intern confer comput linguist , proceed confer : poster , 8-15 decemb 2012 , mumbai , in- dia , pp .
10711080 .
schwenk , h. , costa-jussa , m. r. , & fonollosa , j .
a. r. ( 2006 ) .
continu space languag model iwslt 2006 task .
2006 intern workshop spoken languag translat , iwslt 2006 , keihanna scienc citi , kyoto , japan , novemb 27-28 , 2006 , pp .
166173 .
schwenk , h. , costa-jussa , m. r. , & fonollosa , j .
a. r. ( 2007 ) .
smooth bilingu n-gram translat .
emnlp-conl 2007 , proceed 2007 joint confer empir method natur languag process comput natur lan- guag learn , june 28-30 , 2007 , pragu , czech republ , pp .
430438 .
sennrich , r. , & haddow , b .
( 2016 ) .
linguist input featur improv neural machin translat .
proceed first confer machin translat , pp .
83 91 , berlin , germani .
sennrich , r. , haddow , b. , & birch , .
( 2016a ) .
edinburgh neural machin translat system wmt 16 .
proceed first confer machin translat , pp .
371376 , berlin , germani .
associ comput linguist .
sennrich , r. , haddow , b. , & birch , .
( 2016b ) .
neural machin translat rare word subword unit .
proceed 54th annual meet associ comput linguist ( volum 1 : long paper ) , pp .
17151725 , berlin , germani .
associ comput linguist .
25 setiawan , h. , huang , z. , devlin , j. , lamar , t. , zbib , r. , schwartz , r. , & makhoul , j .
( 2015 ) .
statist machin translat featur multitask tensor network .
proceed 53rd annual meet associ comput linguist 7th intern joint confer natur languag process ( volum 1 : long paper ) , pp .
3141 , beij , china .
associ comput linguist .
son , l. h. , allauzen , a. , & yvon , f. ( 2012 ) .
continu space translat model neural network .
proceed 2012 confer north american chap- ter associ comput linguist : human languag technolog , naacl hlt  12 , pp .
3948 , stroudsburg , pa , usa .
associ comput linguist .
srivastava , r. k. , greff , k. , & schmidhub , j .
( 2015 ) .
highway network .
corr , abs/1505.00387 .
stahlberg , f. , hasler , e. , & byrn , b .
( 2016a ) .
edit distanc transduc action : univers cambridg english-german system wmt16 .
proceed first confer machin translat , pp .
377384 , berlin , germani .
associ comput linguist .
stahlberg , f. , hasler , e. , wait , a. , & byrn , b .
( 2016b ) .
syntact guid neural machin translat .
corr , abs/1605.04569 .
stolck , .
( 2002 ) .
srilm-an extens languag model toolkit .
proceed interna- tional confer spoken languag process , pp .
257286 .
sundermey , m. , alkhouli , t. , wuebker , j. , & ney , h. ( 2014 ) .
translat model bidirect recurr neural network .
proceed 2014 confer em- piric method natur languag process ( emnlp ) , pp .
1425 , doha , qatar .
associ comput linguist .
sutskev , i. , vinyal , o. , & le , q. v. ( 2014 ) .
sequenc sequenc learn neural network .
advanc neural inform process system 27 : annual confer- enc neural inform process system 2014 , decemb 8-13 2014 , montreal , quebec , canada , pp .
31043112 .
tamura , a. , watanab , t. , & sumita , e. ( 2014 ) .
recurr neural network word align- ment model .
proceed 52nd annual meet associ com- putat linguist ( volum 1 : long paper ) , pp .
14701480 , baltimor , maryland .
associ comput linguist .
tu , z. , lu , z. , liu , y. , liu , x. , & li , h. ( 2016 ) .
model coverag neural machin translat .
proceed 54th annual meet associ compu- tation linguist ( volum 1 : long paper ) , pp .
7685 , berlin , germani .
associ comput linguist .
vaswani , a. , shazeer , n. , parmar , n. , uszkoreit , j. , jone , l. , gomez , a. n. , kaiser , l. , & polosukhin , .
( 2017 ) .
attent need .
corr , abs/1706.03762 .
vaswani , a. , zhao , y. , fossum , v. , & chiang , d. ( 2013 ) .
decod large-scal neural languag model improv translat .
proceed 2013 confer em- piric method natur languag process , emnlp 2013 , 18-21 octob 2013 , 26 grand hyatt seattl , seattl , washington , usa , meet sigdat , special interest group acl , pp .
13871392 .
wang , r. , utiyama , m. , goto , i. , sumita , e. , zhao , h. , & lu , b.-l. ( 2013 ) .
convert continuous-spac languag model n-gram languag model statist ma- chine translat .
proceed 2013 confer empir method natur languag process , pp .
845850 , seattl , washington , usa .
associ comput linguist .
weiss , r. j. , chorowski , j. , jait , n. , wu , y. , & chen , z .
( 2017 ) .
sequence-to-sequ model direct translat foreign speech .
corr , abs/1703.08581 .
wu , d. ( 1995 ) .
trainabl coars bilingu grammar parallel text bracket .
pro- ceed third workshop larg corpora ( vlc ) .
wu , y. , schuster , m. , chen , z. , le , q. v. , norouzi , m. , & et al. , w. m. ( 2016 ) .
googl  neural machin translat system : bridg gap human machin translat .
corr , abs/1609.08144 .
wu , y. , watanab , t. , & hori , c. ( 2014 ) .
recurr neural network-bas tupl sequenc model machin translat .
proceed cole 2014 , 25th intern confer comput linguist : technic paper , pp .
19081917 , dublin , ireland .
dublin citi univers associ comput linguist .
xie , j. , mi , h. , & liu , q .
( 2011 ) .
novel dependency-to-str model statist ma- chine translat .
proceed 2011 confer empir method natur languag process , pp .
216226 , edinburgh , scotland , uk .
associ comput linguist .
yamada , k. , & knight , k. ( 2001 ) .
syntax-bas statist translat model .
proceed- ing 39th annual meet associ comput linguist , acl  01 , pp .
523530 , stroudsburg , pa , usa .
associ comput linguist .
yang , n. , liu , s. , li , m. , zhou , m. , & yu , n. ( 2013 ) .
word align model context depend deep neural network .
proceed 51st annual meet associ comput linguist ( volum 1 : long paper ) , pp .
166175 , sofia , bulgaria .
associ comput linguist .
yang , z. , hu , z. , deng , y. , dyer , c. , & smola , a. j .
( 2016 ) .
neural machin translat recurr attent model .
corr , abs/1607.05108 .
yu , h. , & zhu , x .
( 2015 ) .
recurr neural network base rule sequenc model statisti- cal machin translat .
proceed 53rd annual meet associ comput linguist 7th intern joint confer natur languag process ( volum 2 : short paper ) , pp .
132138 , beij , china .
associ- ation comput linguist .
zamora-martnez , f. , bleda , m. j. c. , & schwenk , h. ( 2010 ) .
n-gram-bas machin trans- lation enhanc neural network french-english btec-iwslt  10 task .
2010 intern workshop spoken languag translat , iwslt 2010 , pari , franc , decemb 2-3 , 2010 , pp .
4552 .
27 zhai , f. , zhang , j. , zhou , y. , & zong , c. ( 2014 ) .
rnn-base deriv structur predict smt .
proceed 52nd annual meet associ compu- tation linguist ( volum 2 : short paper ) , pp .
779784 , baltimor , maryland .
associ comput linguist .
zhang , j. , & zong , c. ( 2015 ) .
deep neural network machin translat : overview .
ieee intellig system , 15411672 .
zoph , b. , & knight , k. ( 2016 ) .
multi-sourc neural translat .
corr , abs/1601.00710 .
zoph , b. , yuret , d. , may , j. , & knight , k. ( 2016 ) .
transfer learn low-resourc neural machin translat .
corr , abs/1604.02201 .
zou , w. y. , socher , r. , cer , d. , & man , c. d. ( 2013 ) .
bilingu word embed phrase-bas machin translat .
proceed 2013 confer empir method natur languag process , pp .
13931398 , seattl , washington , usa .
associ comput linguist .
28
