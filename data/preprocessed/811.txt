upcommon portal del coneix obert la upc aquesta s una cpia la versi author final draft d'un articl publicat la revista pattern recognit letters. url d'aquest document upcommon e-prints: paper publicat / publish paper: gupta, parth , costa-juss, marta r. , rosso, paolo, banchs, rafael e. (2016) deep source-context featur lexic select statist machin translation. pattern recognit letters, vol. 75, 2016, pg. 24-29. doi:10.1016/j.patrec.2016.02.014. 1 pattern recognit letter journal homepage: www.elsevier.com deep source-context featur lexic select statist machin translat parth guptaa, marta r. costa-jussab,, paolo rossoa, rafael e. banchsc aprhlt research center, universitat politecnica valencia btalp research center, universitat politecnica catalunya, barcelona centro investigacion en computacion, instituto politecnico nacional, mexico chuman languag technology, institut infocomm research, singapor abstract paper present methodolog address lexic disambigu standard phrase-bas sta- tistic machin translat system. similar sourc context select appropri translat units. inform introduc novel featur phrase-bas model select translat unit extract train sentenc similar sentenc translate. similar comput deep autoencod representation, allow obtain effect low-dimension embed data statist signific bleu score improve- ment differ task (english-to-spanish english-to-hindi). c 2016 elsevi ltd. right reserved. 1. introduct sourc context usual relev translat texts. however, standard phrase-bas statist machin translat (smt) system us sourc context limit word compos translat units. source-context inform special necessari translat differ domains. also, source-context inform im- portant sourc languag sourc word form (spelling) translat differ form target words. address differ motivations, sourc context in- format introduc phrase-bas differ perspectives: lexic semant topic adapt (section 2). us differ classif techniqu decid mean word multipl translations. explor differ topic featur functions. paper, propos enhanc context-awar translat unit take account semant con- text provid sourc sentenc translat (section 3). allow introduc new featur function translat unit inform similar input sentenc translat sourc sentenc correspond author e-mail: (marta r. costa-jussa) translat unit extract from. methodolog pro- pose evalu work base sourc con- text similar approach present (banch costa-jussa, 2011) us latent semant analysi (lsa) comput sim- ilar differ contexts. differ work, introduc us auto-encod construct deep represen- tation sentenc reduc space comput simi- lariti sentenc (use sourc context trans- lation units). algorithm test intern evalu workshop statist machin translat 2014 (costa-jussa et al., 2014). evalu us fea- ture learn deep autoencod model frame- work assess semant similar sentenc (sec- tion 4). deep learn shown outperform compar gener model like mention lsa (hin- ton salakhutdinov, 2006) lda (salakhutdinov hinton, 2009; srivastava et al., 2013). introduct unsupervis pretrain (hinton salakhutdinov, 2006; erhan et al., 2010), deep autoencoders, work estim similar sentenc contextu latent space, effici trained. deep learn algorithm im- plement gpu highli scalable. domain adapt train model, import issu con- textual similar methods, effect handl deep learn method glorot et al. (2011); bengio (2012). similar method like lsa, context matrix factor- iz scratch adaptation. methodology, 2 research highlight (required) creat highlights, type highlight \item command. introduct source-context deep featur standard phrase-bas statist machin translat system. comput sentence-similar mean auto-encoders. effect low-dimension embed data. goal improv translat output term lexic selection. experi standard data collect english- spanish english-hindi translat task propos method perform significantli (statisticallly) better baselin (section 5). present thorough analysi scalabl aspect propos method. rest paper organ follows. section 2 re- port overview relat work introduc sourc context inform deep learn standard smt systems. section 3 present phrase-bas model ex- tend sourc context information. section 4 explain deep represent sentences, better com- pute similar sourc contexts. section 5 describ experi proof relev techniqu section 6 concludes. 2. relat work main novelti paper ad sourc context knowledg mean deep learn techniqu standard phrase-bas smt system, overview rele- vant work (without aim completeness) area. 2.1. ad sourc context smt mention previou section, address dif- ferent motivations, sourc context inform intro- duce phrase-bas differ perspectives: lexic semant topic adaptation. lexic semant works, carpuat wu (2005) in- troduc word sens disambigu techniques. bonet et al. (2009) train local classifi linguist context infor- mation translat phrase. haqu (2010) us differ syntac- tic lexic featur propos incorpor in- format neighbour word report complet state-of-the-art introduc sourc context phrase-bas reader refer to. topic adapt perspective, work basic fo- cu address challeng translat differ do- mains. example, banch costa-jussa (2011) us latent semant analysi (lsa) comput similar differ- ent contexts. recently, chen et al. (2013) comput phrase pair featur vector space represent captur do- main similar development. hasler et al. (2014) us latent dirichlet alloc (lda) comput topic featur functions. 2.2. deep learn techniqu smt 10 years, increas studi mt us differ strategi base deep learning. worth notic huge explos work topic big confer acl, naacl emnlp. approach try modifi featur model standard smt system. work propos novel mt architectures. work ad deep learn smt system us continuous-spac neural languag models, e.g. schwenk et al. (2006); vaswani et al. (2013). on smooth bilingu languag model inspir previ- ou ones, e.g. schwenk et al. (2007); zamora-martnez et al. (2010) that, liu et al. (2013) us deep learn algorithm improv translat target languag model mt son et al. (2012); kalchbrenn blunsom (2013). recent work us deep learn model phrase probabilities, e.g. cho et al. (2014); new reorder models, e.g. li et al. (2013); new differ featur lu et al. (2014). differ neural ar- chitectur face bilingu translat present e.g. sundermey et al. (2014); kalchbrenn blunsom (2013). 2.3. dimesion reduct techniqu similar estima- tion field similar estim continu space advanc recent past. earli model base lsa (dumai et al., 1988) laid foundat dimension- aliti reduct techniqu incorpor context form correl matrix. formul exploit advanc linear model orient principl compo- nent analysi (opca) (platt et al., 2010) s2net (yih et al., 2011). non-linear extens outperform linear counterpart includ us deep autoencod (hinton salakhutdinov, 2006; srivastava et al., 2013; gupta et al., 2014). work, exploit deep autoencod base model estim sourc context similarity. 3. extend phrase-bas model section describ standard phrase-bas smt sys- tem methodolog integr sourc context theoret practic point view. 3.1. phrase-bas smt given sourc string sj1 = s1 . . . s j . . . sj translat target string ti1 = t1 . . . ti . . . ti , phrase-bas smt aim choose, possibl target strings, string highest probability: ti1 = argmax ti1 p(ti1| j 1) j number word target sourc sentence, respectively. phrase-bas seg- ment sourc sentenc segments, translat segment phrase contain sourc target se- quenc word (s1..sn|||t1..tm). finally, compos target sentence. standard implement phrase- base us featur probabl com- bine rel frequenc the: target languag model, word phrase bonu source-to-target target- to-sourc lexic model reorder model koehn et al. (2007). 3.2. theoret integr methodolog idea extend concept translat unit phrase (p) defin unit elements: phrase-sourc (ps), phrase-target (pt) source-sent (ss). p = {ps|||pt|||ss} (1) 3 definit ident source-target phrase pair extract differ train sentenc (or sourc sentences) regard differ translat units. accord this, related context consid ad- dition (hereinafter, source-context) featur function (scf ) phrase input sentence. p = {ps|||pt|||sc f } (2) source-context featur function consist similar measur input sentenc translat sourc context compon avail translat unit illustr fig. 1. s1: hotel book room t1: el hotel reservaba ma habitacion s2: everybodi want write book t2: todo el mundo quier escribir libro sobr s mismo input: read nice book book : libro book : reservar s2 s1 input fig. 1. illustr propos similar featur help choos trans- lation units. sc f includ phrase addit stan- dard featur functions, i.e. condit (cp) posterior (pp) probability, lexic weight (l1, l2) phrase bonu (pb). therefore, extend phrases. p = {ps|||pt|||cp, pp, l1, l2, pb, sc f } (3) schema similar previou work banch costa- jussa (2011). differ previou work, comput- ing similar input sentenc translat origin sentences, comput cosin distanc deep represent sentences, explain sec- tion 4. 3.3. practic integr implement source-context featur function dynam de- pend input sentenc translated. moment, featur function integr standard phrase-bas smt describ follow procedure. fig. 2 show procedur implement source- context featur function. train (ts) valid (vs) (either develop test) sentence, comput sim- ilar measur build similar matrix (w) train valid set. then, sentenc valid set (vsn) extract phrase list (pn) tm = m train sentenc (ts) vn = n valid sentenc (vs) tsm 2 tm vsn 2 vn wmn = ! (tsm, vsn) = similarity(tsm, vsn) end end vsn 2 vn pn = phrase list 2 tm decod p = phrase entri ||| tsm 2 pn p 2 pn p p ||| wmn end translat vsn pn end fig. 2. source-context featur implement algorithm. decoding. phrase entri (p) phrase list extend translat unit contain train sentenc (tsm) extracted. then, phrase entri assign correspond source-context similar ma- trix w vsn tsm, posit wmn. finally, sentenc valid set vsn translat cor- respond extend phrase tabl (pn) includ source-context feature. flow depict fig. 3. 4. deep represent sentenc repres sentenc latent space non- linear dimension reduct technique. method base deep autoencod architectur allow obtain ef- fectiv low-dimension embed text data. autoen- coder network tri learn approxim ident function output similar input. input output dimens network (n). autoencod approxim ident function steps: i) reduction, ii) reconstruction. reduct step take input v 2 rn map h 2 rm m < n seen function h = g(v) g : rn ! rm. hand, reconstruct step take output reduct step h map v 2 rn wai v v consid v = f (h) function f : rm ! rn. autoencod seen f (g(v)) v. neural network base implement autoen- coder, visibl layer correspond input v hid- den layer correspond h. m suffic small autoencod abl deriv power low-dimension repre- sentat data latent space hinton salakhutdinov (2006). variant autoencoders: i) sin- gle hidden layer, ii) multipl hidden layers. singl hidden layer, optim solut remain pca project ad non-linear hid- den layer bourlard kamp (1988). pca limit overcom stack multipl encoders, constitut call deep architecture. deep construct lead 4 sourc target autoencod parallel corpu phrase tabl lexic selector - - - .. - - - - .. - . . - - - .. - input sourc sentenc p = {ps|||pt|||scf} ps pt ss sourc project hs hinput scf = cosine(hs, hinput) one-tim index fig. 3. workflow system. truli non-linear power reduc space representa- tion hinton salakhutdinov (2006). deep architectur constitut stack multipl restrict boltzmann machin (rbm) other. let visibl unit v 2 {0, 1}n binari bag-of-word repre- sentat text document hidden unit h 2 {0, 1}m hidden latent variables. energi state {v,h} follows, e(v,h) = nx i=1 aivi mx j=1 b jh j x i, j vih jwi j (4) vi, h j binari state visibl unit hidden unit j, ai, b j bias wi j weight them. then, easi sampl data direct shown below, p(vi = 1|h) = (ai + x j h jwi j) (5) p(h j = 1|v) = (b j + x viwi j) (6) (x) = 1/(1 + exp(x)) logist sigmoid function. architectur autoencod shown fig. 4. la- tent space represent h(2) sentenc s obtain shown eq. 6. sentenc latent space compar mean cosin similar shown below: !(s1, s2) = cosine(h(2)s1 |vs1,h (2) s2 |vs2) (7) 5. experi section describ experiment framework test introduct deep context featur standard phrase-bas smt system. report detail data set used, baselin system, train deep structur similar sentenc extracted, improv techniqu term bleu score papineni et al. (2002) and, finally, scalabl technique. 5.1. data set baselin english-to-spanish parallel corpu extract bible, publicli avail constitut excel corpu experi test pro- pose methodolog provid rich varieti contexts. corpu contain 30,000 sentenc train 800,000 words, 500 sentenc develop test sets. additionally, larger data set, english-to-hindi corpu avail wmt 2014 bojar et al. (2014). train sentenc 300,000 sentences, 3,500,000 words, 429 sentenc develop 500 sen- tenc test. baselin standard state-of-the-art phrase-bas built mose toolkit koehn et al. (2007). follow option train system, include: grow-diagonal-final-and word align symmetrization, lex- ical reordering, rel frequenc (condit pos- terior probabilities) phrase discounting, lexic weight phrase bonu translat model (with phrase length 10), 5-gram languag model kneser-nei smooth word bonu model. order com- pare techniqu built contrast context 5 code layer output layerinput layer (a) (b) v h (1) h (2) fig. 4. architectur autoencoder. (a) deep format stack rbms. (b) unrol fine-tuning. featur base lsa banch costa-jussa (2011) baseline. system comput 2intel xeon e52670 v3 2,3ghjz 12n processor server. 5.2. autoencod train model sentenc autoencod framework consid vocabulari remov frequent term appear k sentenc train partit dataset. remov stopword appli stemmer. bibl wmt14 dataset, consid vocabulari size (n) 3543 (k=5) 7299 (k=20) respect 1. autoencod pretrain contrast di- vergenc (cd) step size 1 (hinton, 2002). minibatch size 20 100 pretrain fine-tun re- spectively. architectur autoencod n-500-128- 500-n 2 shown figur 4. weight decai pre- vent overfitting. additionally, order encourag sparsiti hidden units, kullback-leibl sparsiti regular used. gpu3 base implement autoencod train model took 45 minut bibl dataset 4.5 hour wmt14 dataset. 5.3. latent semant analysi lsa basic perform singular valu decomposit sentence-term matrix d line princip compon analysi (pca) (dumai et al., 1988). lsa obtain k prin- cipal compon d consid project space sentenc compar space. inher idea semant similar term (dimens d) correspond similar latent compon sentenc near reduc comparison space. method look eigenproblem formul below: cvj = jv j, (8) 1the valu k decid consid size dataset size vocabulari 2differ architectur tri rule higher layer larger previou layer (becaus sparsiti data) statist differ result observed. tri layer n-500-250-128-250-500-n produc wors results, 3-layers. 3nvidia geforc gtx titan memori 6 gib 2688 cuda core where, j jth largest eigenvalue, v j correspond eigenvector c correl matrix (dt d). lsa us k eigenvector projection. 5.4. result tabl 1 show improv term bleu papineni et al. (2002) ad deep context baselin english-to-spanish (en2es) english-to-hindi (en2hi), re- spectiv develop test sets. note en2 qualiti higher en2hi easier translat task higher train cor- pus. shown tables, propos method perform significantli better baselin lsa method translat task consistently. en2 en2hi dev test dev test baselin 36.81 37.46 9.42 14.99 +lsa 37.20 37.84 9.83 15.12 +deep 37.28 38.19 10.40 15.43 tabl 1. bleu score en2 en2hi translat tasks. depict statist signific (p-value<0.05) wrt baselin lsa respec- tively. notic result en2 en2hi consist improved. argu hindi span- ish higher vocabulari variat compar english, richer morphology. benefit ad source-context inform better reflect case sourc phrase target word translations. improv translat prove deep represent help find adequ contextu similar train test sen- tences. bleu score improv task translat directions. analysi translat out- put present tabl 2 asiya 4 show exampl translat improv term lexic select goal methodolog present paper. exampl shown tabl 2. tabl 3, analys method improves. notic tabl 3 probabl sens 4 6 translat sourc brake band baselin pero el rompio la tropa +deep pero el rompio la cuerda refer pero el rompio la atadura sourc soft depth baselin ghraiyo\ s m laym ron lgt +deep ghraiyo\ s m laym cfk refer ghraiyo\ s koml cfk tabl 2. manual analysi translat outputs. ad deep featur allow adequ lexic selection. cp pp scf bands|||tropa 0.31 0.17 0.01 bands|||cuerda 0.06 0.07 0.23 cry|||rona 0.23 0.06 0.85 cry|||cfk 0.15 0.04 0.90 tabl 3. probabl valu phrase-bas system) word band spanish translations; word nd hindi translations. band consid dataset tropas, liter mean troups. idea propos source-context featur us contextu similar input sentenc (in) sentenc train set addit sourc inform decoding. therefore, given entir input sentence: kept bound chain fet- ter ; brake bands, method abl infer correct sens word band (i.e. case cuer- das, liter mean ropes, synonym refer ataduras, liter mean ty ropes) consid- er similar train sentences: (s1) lord sent band chalde , band syr- ian (s2) shall band thee , shall bind thee them. case, !(s2, in) > !(s1, in) seen ta- ble 3. similarly, hindi example, frequent sens word rona, liter mean exampl tabl 2 refer sens cfk, mean scream. method identifi context sc f (cry|||cfk) > sc f (cry|||rona). source-context featur capabl choos better translat unit given context correct trans- lation seen train data. 5.5. scalabl compon method: i) incorpor source-context featur tune phase mt project train sentenc latent space; ii) sim- ilar estim input sentenc train sen- tenc latent space. step computation expens one-tim offline, big con- cern. similar estim online, effici comput multi-cor cpu gpu es- sential matrix multiplication. however, plan integr similar estim translat decoding. 6. conclus work shown novel methodolog exploit deep represent techniqu effect includ deep learn- ing base contextu similar estim method han- dle sourc context incorpor end-to-end smt system. propos method show statist signific improve- ment compar strong baselin system english-to- spanish english-to-hindi translat tasks. manual analysi clearli illustr advantag choos- ing appropri translat unit take account in- format input sentenc context deep relat train sentences. present method scale run-time. interest work includ shorter contexts, experi deeper auto-encod better integr dynam featur translat decoding. also, speed-up search divid featur space chunk search hierarchically, perform cluster us kd-trees. acknowledg work support wiq-ei (irs grant n. 269180) diana-appl (tin2012- 38603-c02-01) project vlc/campu microclust multimod interact inintellig systems. work second author support spanish ministerio economa y competitividad, contract tec2012-38939-c03- 02 european region develop fund (erdf/feder) seventh framework program european commiss intern outgo fel- lowship mari curi action (imtrap-2011-29951). refer banchs, r.e., costa-jussa, m.r., 2011. semant featur statist ma- chine translation, in: proceed fifth workshop syntax, seman- tic structur statist translation, pp. 126134. bengio, y., 2012. deep learn represent unsupervis trans- fer learning, in: icml unsupervis transfer learning, pp. 1736. bojar, o., diatka, v., rychl, p., stranak, p., suchomel, v., tamchyna, a., ze- man, d., 2014. hindencorp - hindi-english hindi-onli corpu ma- chine translation, in: proceed ninth intern confer languag resourc evalu (lrec14), reykjavik, iceland. espana bonet, c., gimenez, j., marquez, l., 2009. discrimin phrase- base model arab machin translation. transact asian lan- guag inform process 8, 15:115:20. url: acm.org/10.1145/1644879.1644882, doi:10.1145/1644879. 1644882. bourlard, h., kamp, y., 1988. auto-associ multilay percep- tron singular valu decomposition. biolog cybernet 59, 291 294. url: doi:10. 1007/bf00332918. carpuat, m., wu, d., 2005. word sens disambigu vs. statist machin translation, in: proceed 43rd annual meet associ comput linguistics, associ comput linguistics, stroudsburg, pa, usa. pp. 387394. url: 3115/1219840.1219888, doi:10.3115/1219840.1219888. chen, b., kuhn, r., foster, g., 2013. vector space model adapt statist machin translation, in: proceed 51st annual meet associ comput linguist (volum 1: long papers), associ comput linguistics, sofia, bulgaria. pp. 12851293. url: 7 cho, k., van merrienboer, b., gulcehre, c., bahdanau, d., bougares, f., schwenk, h., bengio, y., 2014. learn phrase represent rnn encoder-decod statist machin translation, in: proceed 2014 confer empir method natur languag processing, emnlp 2014, octob 25-29, 2014, doha, qatar, meet sigdat, special group acl, pp. 17241734. costa-jussa, m.r., gupta, p., rosso, p., banchs, r.e., 2014. english-to-hindi descript wmt 2014: deep source-context featur moses, in: proceed ninth workshop statist machin translation, associ comput linguistics, baltimore, maryland, usa. pp. 7983. url: dumais, s.t., furnas, g.w., landauer, t.k., deerwester, s., harshman, r., 1988. latent semant analysi improv access textual informa- tion, in: proceed sigchi confer human factor com- pute systems, acm, new york, ny, usa. pp. 281285. url: //doi.acm.org/10.1145/57167.57214, doi:10.1145/57167. 57214. erhan, d., bengio, y., courville, a.c., manzagol, p.a., vincent, p., bengio, s., 2010. unsupervis pre-train help deep learning? journal machin learn research 11, 625660. glorot, x., bordes, a., bengio, y., 2011. domain adapt large-scal sentiment classification: deep learn approach, in: icml, pp. 513520. gupta, p., bali, k., banchs, r.e., choudhury, m., rosso, p., 2014. queri ex- pansion mixed-script inform retrieval, in: 37th intern acm sigir confer research develop inform re- trieval, sigir 14, gold coast , qld, australia - juli 06 - 11, 2014, pp. 677686. haque, r., 2010. integr source-languag context log-linear mod- el statist machin translation. ph.d. thesis. dublin citi university. hasler, e., blunsom, p., koehn, p., haddaw, b., 2014. dynam topic adap- tation phrase-bas mt, in: proceed european as- sociat comput linguistics, associ comput linguistics, gothenburg, sweden. url: anthology/p13-1126. hinton, g., salakhutdinov, r., 2006. reduc dimension data neural networks. scienc 313, 504 507. hinton, g.e., 2002. train product expert minim contrast divergence. neural comput 14, 17711800. kalchbrenner, n., blunsom, p., 2013. recurr continu translat models, in: proceed 2013 confer empir method natur languag processing, associ comput linguistics, seattle, washington, usa. pp. 17001709. url: anthology/d13-1176. koehn, p., hoang, h., birch, a., callison-burch, c., federico, m., bertoldi, n., cowan, b., shen, w., moran, c., zens, r., dyer, c., bojar, o., constantin, a., herbst, e., 2007. moses: open sourc toolkit statist machin translation, in: proceed 45th annual meet acl in- teract poster demonstr sessions, pp. 177180. li, p., liu, y., sun, m., 2013. recurs autoencod itg-bas trans- lation, in: proceed 2013 confer empir method natur languag processing, associ comput linguistics, seattle, washington, usa. pp. 567577. url: org/anthology/d13-1054. liu, l., watanabe, t., sumita, e., zhao, t., 2013. addit neural network statist machin translation, in: proceed 51st annual meet associ comput linguist (volum 1: long papers), associ comput linguistics, sofia, bulgaria. pp. 791801. url: lu, s., chen, z., xu, b., 2014. learn new semi-supervis deep auto- encod featur statist machin translation, in: proceed 52nd annual meet associ comput linguist (volum 1: long papers), associ comput linguistics, bal- timore, maryland. pp. 122132. url: anthology/p14-1012. papineni, k., roukos, s., ward, t., zhu, w.j., 2002. bleu: method automat evalu machin translation, in: proceed 40th annual meet associ comput linguistics, pp. 311 318. platt, j.c., toutanova, k., tau yih, w., 2010. translingu document represen- tation discrimin projections, in: emnlp, pp. 251261. salakhutdinov, r., hinton, g.e., 2009. replic softmax: undirect topic model, in: nips, pp. 16071614. schwenk, h., costa-jussa, m.r., fonollosa, j.a.r., 2006. continu space languag model iwslt 2006 task, in: 2006 intern workshop spoken languag translation, iwslt 2006, keihanna scienc city, ky- oto, japan, novemb 27-28, 2006, pp. 166173. schwenk, h., r. costa-jussa, m., r. fonollosa, j.a., 2007. smooth bilin- gual n-gram translation, in: proceed 2007 joint confer empir method natur languag process comput natur languag learn (emnlp-conll), associ computa- tional linguistics, prague, czech republic. pp. 430438. url: //www.aclweb.org/anthology/d/d07/d07-1045. son, l.h., allauzen, a., yvon, f., 2012. continu space translat mod- el neural networks, in: proceed 2012 confer north american chapter associ comput linguistics: human languag technologies, associ comput linguis- tics, stroudsburg, pa, usa. pp. 3948. url: citation.cfm?id=2382029.2382036. srivastava, n., salakhutdinov, r., hinton, g.e., 2013. model document deep boltzmann machines, in: uai. sundermeyer, m., alkhouli, t., wuebker, j., ney, h., 2014. translat mod- el bidirect recurr neural networks, in: proceed 2014 confer empir method natur languag process (emnlp), associ comput linguistics, doha, qatar. pp. 14 25. url: vaswani, a., zhao, y., fossum, v., chiang, d., 2013. decod large- scale neural languag model improv translation, in: proceed 2013 confer empir method natur languag processing, emnlp 2013, 18-21 octob 2013, grand hyatt seattle, seattle, washing- ton, usa, meet sigdat, special group acl, pp. 13871392. yih, w., toutanova, k., platt, j.c., meek, c., 2011. learn discrimin project text similar measures, in: proceed fifteenth confer comput natur languag learning, conll 2011, portland, oregon, usa, june 23-24, 2011, pp. 247256. zamora-martnez, f., bleda, m.j.c., schwenk, h., 2010. n-gram-bas ma- chine translat enhanc neural network french-english btec-iwslt10 task, in: 2010 intern workshop spoken lan- guag translation, iwslt 2010, paris, france, decemb 2-3, 2010, pp. 4552. url: 10/slta_045.html.