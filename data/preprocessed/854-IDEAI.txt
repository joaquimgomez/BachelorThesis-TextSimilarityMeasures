restrict boltzmann machin vector represent speech speaker recognit avail onlin www.sciencedirect.com speech & languag 47 (2017) 1629 www.elsevier.com/locate/csl restrict boltzmann machin vector represent speech speaker recognitioni taggedpd1x xomid ghahabi d2x x*, d3x xjavier hernando d4x x taggedptalp research center, depart signal theori communications, universitat politecnica catalunya, barcelona 08034, spain receiv 26 octob 2016; receiv revis form 2 2017; accept 19 june 2017 taggedpabstract years, i-vector state-of-the-art techniqu speaker recognition. recent advanc deep learn (dl) technolog improv qualiti i-vector dl techniqu us computation expens need phonet label background data. aim work develop effici altern vector represent speech keep comput cost low possibl avoid phonet labels, accessible. propos vector base gaussian mixtur model (gmm) restrict boltzmann machin (rbm) refer gmmrbm vectors. role rbm learn total speaker session variabl background gmm supervectors. rbm, refer univers rbm (urbm), transform unseen supervector propos low dimension vectors. us differ activ function train urbm differ- ent transform function extract propos vector investigated. end, variant rectifi linear unit (relu) refer variabl relu (vrelu) proposed. experi core test condit 5 nist sre 2010 compar result convent i-vector achiev clearli lower comput load vector extract process. 2017 authors. publish elsevi ltd. open access articl cc by-nc-nd license. ( taggedpkeywords: restrict boltzmann machine; deep learning; variabl rectifi linear unit; speaker recognition; gmmrbm vector; i-vector 1. introduct taggedpth low dimension represent speech utter base factor analysi techniqu well-known i-vector (dehak et al., 2011a). past years, i-vector shown great perform speaker recognit applic (e.g., dehak et al., 2011b; bahari et al., 2012; xia liu, 2012). commonli score techniqu i-vector cosin distanc (dehak et al., 2010; 2011a) probabilist linear discrimin analysi (plda) (princ elder, 2007; kenny, 2010). plda score lead superior per- formanc need speaker-label background data costli access easily. paper recommend accept roger moore. * correspond author. e-mail address: (o. ghahabi). 0885-2308/ 2017 authors. publish elsevi ltd. open access articl cc by-nc-nd license. ( o. ghahabi j. hernando / speech & languag 47 (2017) 1629 17 taggedpmotiv success us deep learn (dl) speech process applic (e.g., moham et al., 2010; dahl et al., 2012; moham et al., 2012; hinton et al., 2012; senior et al., 2015), dl techniqu speaker recognit differ purposes. example, dl techniqu appli backend i-vector (stafylaki et al., 2012b; senoussaoui et al., 2012; stafylaki et al., 2012a; novoselov et al., 2014; ghahabi hernando, 2014a; 2014b; 2017), i-vector extract algorithm (lei et al., 2014; kenni et al., 2014; mclaren et al., 2015; richardson et al., 2015; liu et al., 2015; campbell, 2014; garcia-romero et al., 2014), emploi compact represent speech signal (vasilakaki et al., 2013; variani et al., 2014; liu et al., 2015; ghahabi hernando, 2015; safari et al., 2016) discrimin featur classif (safari et al., 2015). taggedpdl technolog i-vector extract algorithm ways. first, deep neural network (dnn) acoust model typic gaussian mixtur model (gmm) (lei et al., 2014; kenni et al., 2014; campbell, 2014; richardson et al., 2015; garcia-romero et al., 2014; liu et al., 2015). second, convent spectral featur replac append so-cal dnn bottleneck featur dnn gmm acoust model (mclaren et al., 2015; richardson et al., 2015; liu et al., 2015). shown best result obtain spectral featur append bottleneck featur gmm acoust model (mclaren et al., 2015; richardson et al., 2015; lozano-diez et al., 2016). however, main problem us dnn acoust model bottleneck featur extractor increas highli comput cost i-vector extract process. moreover, case phonet label requir dnn training, accessible. taggedpon hand, work tri us dl techniqu build compact representa- tion speech signal convent i-vector algorithm. vasilakaki et al. (2013), variani et al. (2014), liu et al. (2015), deep architectur train background featur vectors. featur vector given utter forward-propag mean posterior probabl particular hidden layer (variani et al., 2014) pca dimens reduc version (liu et al., 2015), pca dimens reduc version mean vector (vasilakaki et al., 2013) consid new compact representation. safari et al. (2016), paramet adapt network stack build supervec- tor. dimens new supervector reduc pca. ghahabi hernando (2015), author gmm supervectors, featur vectors, input restrict boltzmann machin (rbm). rbm dimens reduct stage scenario. liu et al. (2015) variani et al. (2014) shown success text-depend speaker recognition, signif- icant improv report text-independ tasks. moreover, work dl techniqu featur vector domain costly. taggedpth aim work develop effici framework vector represent speech keep comput cost low possibl avoid phonet labels. order achiev goal, global rbm refer univers rbm (urbm) train given background gmm supervectors. urbm tri learn total session speaker variabl background supervectors. transform unseen supervector lower dimension vector refer gmmrbm vectors. taggedpcompar preliminari work present ghahabi hernando (2015), whiten supervector domain, computation costly, replac warp featur vector domain. chang make possibl obtain higher speaker recognit accuracy, special lower dimension vectors. moreover, effect type activ function train urbm type transform function gmmrbm vector extract investigated. end, variat linear rectifi unit (relu), refer variabl relu (vrelu), propos train urbm, linear function transform vector extract stage. taggedpth core condit nist sre 2006 (nist, 2006) develop core condit 5 nist sre 2010 (nist, 2010) bigger background data test evaluation. experi evalu set show propos gmmrbm vector achiev compar perform convent i-vector lower comput cost requir vector extraction. conclus valid cosin plda scoring. moreover, combin gmmrbm vector i-vector score level improv perform more. taggedpth rest paper organ follows. section 2 give brief background overview convent i-vector plda. section 3 describ propos gmmrbm vectors. section 4 investig effect 18 o. ghahabi j. hernando / speech & languag 47 (2017) 1629 taggedpactiv transform function used, respectively, urbm train gmmrbm vector extract discuss database, baselin systems, experiment results. section 5 conclud paper. 2. convent i-vector taggedpan i-vector (dehak et al., 2011a) low rank vector represent speech utterance. featur vector speech signal repres gaussian mixtur model (gmm) adapt univers background model (ubm). mean vector adapt gmm stack build supervector s. supervector model follows, s subm tn 1 subm speaker- session-independ mean supervector, typic ubm, t total variabl matrix, n vector latent variables. posterior distribut n condit baumwelch statis- tic given speech utterance. mean posterior distribut refer i-vector v comput follows, v t ts1n ut 1 t ts 1 efu 2 n u diagon matrix contain zeroth order baumwelch statistics, efu supervector central order statistics, s diagon covari matrix initi subm updat factor analysi training, t denot transpos operation. tmatrix train expectationmaxim (em) algorithm given baumwelch statist background speech utterances. detail dehak et al. (2011a). taggedptwo main score techniqu i-vector cosin (dehak et al., 2010; 2011a) probabilist linear dis- crimin analysi (plda) (princ elder, 2007). plda effect techniqu perform score session variabl compensation. assum i-vector decompos as, v mfz 3 m global offset, column f eigenvoices, z latent vector have standard normal prior, residu vector e normal distribut zero mean covari matrix. model paramet estim larg collect speaker-label background data em algorithm princ elder (2007). class i-vector covari matric depend model paramet store scoring. 3. propos gmmrbm vector taggedprecently, advanc deep learn (dl) improv qualiti i-vectors, dl techniqu us computation expens need phonet label background data. propos work altern vector-bas represent speaker computation expens manner us phonet speaker labels. taggedprbm good potenti purpos good represent power unsuper- vise computation low cost. fact, rbm gener network fulli connect layer visi- ble hidden stochast units. work, assum input visibl unit gmm supervector output hidden unit low dimension vector look for. rbm train given back- ground gmm supervector refer univers rbm (urbm). role urbm learn total session speaker variabl background supervectors. differ type unit activ function train urbm mention section 3.2 evalu section 4 application. train urbm, visiblehidden connect weight matrix transform unseen gmm supervector lower dimension vector refer gmmrbm vector work. taggedpfig. 1 show block-diagram propos framework. process divid main stage detail follow sections, correspond block fig. 1. first, gmm supervector built warp spectral featur given ubm, normal ubm parameters. second, fig. 1. block-diagram propos gmmrbm vector framework. w b paramet univers rbm (urbm), m global mean, andh whiten matrix obtain background gmmrbm vectors. o. ghahabi j. hernando / speech & languag 47 (2017) 1629 19 taggedpbackground gmm supervector train urbm option normal provid appropri transform matrix supervector propos low dimension vectors. third, given unseen gmm supervector paramet urbm, gmmrbm vector extracted. 3.1. featur warp gmm supervector taggedpa shown block fig. 1, input speech signal character spectral featur vectors. after- wards, featur warp appli map distribut individu featur gaussian distribut time interv base cumul distribut function (cdf). method assum compon featur vector independ process individu separ stream. cdf match perform slide window size n central frame window warped. featur given window sort ascend order. given compon valu x central frame rank r (1 r n), warp valu bx satisfi (pelecano sridharan, 2001; xiang et al., 2002), r 1=2=n z bx 1 f zdz 4 left approxim cdf valu x, right cdf valu bx; f (z) probabil- iti densiti function (pdf) standard normal distribut (pelecano sridharan, 2001; xiang et al., 2002). taggedpit shown featur warp help compens undesir session variabl speech signal (pelecano sridharan, 2001) gaussian featur distributions. shown experiment result section featur warp high impact perform propos gmmrbm vectors. taggedpwarp featur model gmm adapt background model (ubm). mean vector adapt gmm stack build supervector. order increas discrimin power, supervector model-norm mean supervector diagon covari matrix ubm (subm subm), s 0 s1=2ubm s subm: 5 taggedpmodel normal help have zero mean unit varianc supervector prior assumpt train rbm real-valu input describ section. 3.2. univers rbm train taggedpnorm supervector obtain background data train urbm (block b fig. 1). role urbm learn session speaker variabl background supervectors. urbm 20 o. ghahabi j. hernando / speech & languag 47 (2017) 1629 taggedpparamet transform unseen supervector lower dimension gmmrbm vectors. differ visibl hidden units, activ function train rbm (hinton, 2012). input applic real-valu supervectors, visibl unit gaussian. however, sigmoid rectifi linear unit (relu) hidden layer train urbm. men- tion hinton (2012) prove experiments, train rbm linear hidden visibl unit highli unstable. therefore, pure linear hidden unit discard work. given urbm parameters, reason transform function transform unseen supervectors. section, prob- lem us tradit sigmoid function activ transform address potenti solut proposed. variant relu, refer variabl relu (vrelu), propos application. shown section 4 propos vrelu suffer problem sigmoid relu. taggedpfig. 2 show histogram posterior probabl hidden unit urbm nonlinear transformations. urbm train tradit sigmoid activ function. typic sigmoid function log sigmoid function, ghahabi hernando (2015), emploi trans- formation. hidden unit similar behaviors. seen figure, posterior probabl distribut hidden unit sigmoid transform compress zero far gaussian distribut ideal propos gmmrbm vectors. fact degrad perform significantly. fig. 2. histogram hidden unit valu (bottom) (left) transform sigmoid log sigmoid functions. urbm train sigmoid hidden units. o. ghahabi j. hernando / speech & languag 47 (2017) 1629 21 taggedpth behavior better case log sigmoid function distribut transform lin- ear function, problem valu zero transformation. whiten transform posterior probabl correct distribut extent, per- formanc low specif sigmoid transformation. taggedpa potenti solut chang mean varianc posterior probabl distributions, transformation, fall activ nonlinear part transform functions. easili perform urbm paramet normal propos follows, cw w max i;j jwijj 6 bbi b bi b 7 w visiblehidden connect weights, b vector hidden bia terms, b paramet control, respectively, varianc mean posterior probabl distribut hidden unit nonlinear transformation, wij (i, j) element w, bi b ith element mean valu b, respectively. taggedpfig. 3 show chang b distribut posterior probabl hidden unit desir interval. section 4 movement improv qualiti gmmrbm vector urbm train sigmoid hidden units. taggedpanoth altern unit relu. relu kind linear unit neg valu zero out. urbm train relu input transform linear function training, problem occur. however, section 4, problem distribut posterior probabl hidden unit asymmetr mean value, appropri plda scoring. therefore, propos work variant relu, refer variabl relu (vrelu). vrelu, unit valu threshold t zero out, fix threshold zero relu. threshold t randomli select normal distribut n(0, 1) hidden unit input sampl train iteration. fact, vrelu defin follows, f x x x> t 0 x t ; t 2n0; 1 8 taggedpfig. 4 compar relu vrelu posit neg valu t. shown section 4 vrelu solv asymmetr problem posterior probabl distribut great extent and, therefore, work better relu plda score used. taggedpth train algorithm rbm sigmoid hidden unit ghahabi hernando (2015). following, explain rbm train algorithm propos vrelu. fig. 5 show train steps. connect weight w randomli initi n(0, 0.01) visibl hidden bia term (a b, respectively) set zero. given normal supervector s0, posterior probabl lower fig. 3. histogram posterior probabl hidden unit urbm normal urbm (with differ pair b) nonlinear transformation. histogram obtain background dataset development. fig. 4. comparison relu propos vrelu. epoch, hidden unit input sample, t randomli select normal distribut zero mean unit variance. (b) (c) exampl vrelu t posit negative, respectively. 22 o. ghahabi j. hernando / speech & languag 47 (2017) 1629 taggedpdimension hidden vector h calcul eq. (8). afterwards, supervector reconstruct given hidden unit values. reconstruct supervector s0r recalcul posterior probabl hidden units. steps, mark fig. 5, provid inform updat paramet network. actually, train process base maximum likelihood criterion stochast gradient descent algorithm (hinton salakhutdinov, 2006; hinton et al., 2006), gradient estim approxim version contrast diverg (cd) algorithm call cd1 (hinton et al., 2006; hinton, 2012). taggedpth train process summar follows, taggedp initi network paramet (w, b, a) taggedp cd1 step taggedp1. h f bws0 9 taggedp2. s 0 r aw th 10 taggedp3. hr f bws0r 11 taggedp updat network paramet taggedp1. dw h s0ht s0rhtr t 12 taggedp2. da h s0 s0r 13 taggedp3. db h h hr 14 taggedpwher h learn rate f (.) vrelu function calcul eq. (8). taggedpadditionally, momentum factor smooth updates, weight decai regular penal larg weights. paramet updat process minibatch updat procedur repeat minibatch processed. fig. 5. train univers rbm (urbm) given background gmm supervectors. o. ghahabi j. hernando / speech & languag 47 (2017) 1629 23 3.3. gmmrbm vector extract taggedpgiven gmm supervector block urbm paramet block b fig. 1, gmmrbm vector extract block c follows, vr ws1=2ubm s subm 15 taggedpa linear transform function used, hidden unit bia term b easili discard visiblehidden connect weight w transformation. reformul eq. (15) base zeroth order baumwelch statistics, have, vr ws1=2ubm n 1uefu 16 relev factor map adapt ad ton u. taggedplik case i-vectors, result gmmrbm vector mean normal whiten mean vector whiten matrix obtain background data, h v d 1=2v t 17 h whiten matrix, v matrix eigenvectors, d diagon matrix correspond eigenvalues, small constant regular factor ad avoid larg valu practice. 3.4. comput load compar i-vector taggedpth comparison eqs. (2) (16) impli clearli gmmrbm vector extract need comput load. compar comput load term number product oper requir extract i-vector gmmrbm vector size base eqs. (2) (16). consid comput cost multipl matric n m m k order onmk matrix invers size n n order on3; fact n u diagon ws1=2ubm eq. (16) t ts 1 eq. (2) comput offline, minimum comput load i-vector gmmrbm vector extract on3 2n2 2nm 1m; respectively, n dimens i-vector/ gmmrbm vector m size supervector. taggedpfig. 6 compar minimum comput load extract i-vector gmmrbm vector differ valu n m. figur impli number oper requir extract gmmrbm vector 106 108 compar i-vector requir 108 1011 operations. comput load higher import onlin applic frequenc vector extract high. fig. 6. comparison number product oper requir extract i-vector gmmrbm vector term (a) size i-vector/gmmrbm vector n (b) size supervector m. 24 o. ghahabi j. hernando / speech & languag 47 (2017) 1629 4. experiment result taggedpth detail database, setup baselin propos approaches, experiment result given section. baselin system base convent i-vector score cosin plda techniques. propos gmmrbm vector build accord block-diagram fig. 1. effect featur warping, urbm normalization, type activ transform functions, score combin cosin plda techniqu shown section. 4.1. setup, baseline, databas taggedptwo set databas experiments. development, core test condit nist 2006 sre evalu (nist, 2006) used. includ 816 target model 51,068 trials. train test phases, durat speech signal approxim minutes. background data includ 6063 speech file collect nist 2004 2005 sre corpora. background data train ubm, urbm, plda, t whiten matrices. taggedpfor evaluation, nist 2010 sre (nist, 2010), core test-common condit 5, includ differ num- ber trial involv normal vocal effort convers telephon speech train test, used. back- ground data collect form nist sre 20042008 includ 37,600 speech utter 18,140 signal label plda training. taggedpfrequ filter (ff) featur (nadeu et al., 2001) experiments. like mel-frequ cepstral coeffici (mfcc), ff decorrel version log filter bank energi (fbe) (nadeu et al., 2001). shown ff featur achiev perform equal better mfcc (nadeu et al., 2001). featur extract 10 ms 30 ms ham window. number static ff featur 16 delta ff delta log energy, 33-dimension featur vector built. featur extraction, speech signal sub- ject energy-bas silenc remov process. featur extraction, 3-second slide window fea- ture warping. taggedp open sourc softwar (larcher et al., 2013) build i-vector baselin system cosin plda score techniqu employed. dimens i-vector 400 plda size develop data 250 evalu 400. gender-independ ubm repres diagonal-covari 512-compon gmm. taggedpin propos gmmrbm vector framework, gmm adapt ubm relev factor 16. mean vector adapted. dimens supervector is, therefore, 512 33 = 16,896. urbm hidden layer size 400 8000 train creat gmmrbm vectors. bigger train sigmoid activ function compar result report ghahabi her- nando (2015). urbm hidden layer size 400 train sigmoid, relu, propos vrelu. learn rate, number epochs, minibatch size, weight decay, momentum urbm, train vrelu, set 0.0014, 40, 50, 2 103; 0.9, respectively. taggedpperform evalu equal error rate (eer), minimum decis cost function (mindcf) calcul cm 10;cfa 1;pt 0:01 develop experi (nist, 2006) cm 1;cfa 1;pt 0:001 evalu experi (nist, 2010). 4.2. result taggedpa mention section 1, main differ work prior work ghahabi hernando (2015) discard whiten supervector level make us featur warp instead. result report tabl 1 impli featur warp used, whiten supervector level helps. exactli ghahabi hernando (2015). however, featur vector warped, whiten supervector effect anymore. moreover, time memori consuming. best result obtain featur warp used. taggedpfig. 7 show histogram compon gmmrbm vector obtain urbm, train sigmoid activ function. however, sigmoid, log sigmoid, linear transform function vector extraction. histogram compon similar behaviors. sigmoid log sigmoid tabl 1 effect featur warp whiten input gmm supervector propos gmmrbm framework. number parenthes indic dimens gmmrbm vectors. result obtain develop databas cosin scoring. input rbm output rbm raw featur warp featur eer mindcf eer mindcf whiten supervector gmmrbm vector (8000) 7.58 0.0346 6.90 0.0331 raw supervector gmmrbm vector (8000) 7.92 0.0379 6.89 0.0323 raw supervector gmmrbm vector (400) 10.45 0.0475 8.08 0.0383 o. ghahabi j. hernando / speech & languag 47 (2017) 1629 25 taggedptransformations, histogram present urbm normal urbm parameters. normaliza- tion paramet b eqs. (6) (7) set 0.05 0.5, respectively. approxim posterior distribut interv 2 2 (fig. 3) correspond activ nonlinear part sigmoid log sigmoid transform functions. sigmoid log sigmoid, urbm normal help have gaussian-lik histograms. shown later, increas perform gmmrbm vector urbm train sigmoid hidden units. taggedpfig. 8 show histogram gmmrbm vector urbm train relu vrelu linear transform cases. figur impli histogram asymmetr case fig. 7. comparison histogram compon background gmmrbm vector obtain sigmoid activ function transform function (a) sigmoid, (b) log sigmoid, (c) linear. fig. 8. comparison histogram compon background gmmrbm vector obtain (a) relu (b) propos vrelu activ function linear transform function. 26 o. ghahabi j. hernando / speech & languag 47 (2017) 1629 taggedprelu train process hidden unit encourag have posit values. hand, random threshold t propos vrelu make possibl have posit neg hidden valu train process. improv histogram shown fig. 8. taggedpt 2 compar perform gmmrbm vector extract differ urbm transform functions. comparison base cosin plda scoring. expected, wors result sigmoid hidden unit transform function. urbm normal improv significantli perfor- manc vectors. us log sigmoid perform better sigmoid discuss figs. 2 7. urbm normal improv perform case improv sigmoid transformation. urbm train sigmoid hidden unit paramet linear transform input supervectors, perform wors log sigmoid cosin scor- ing better plda scoring. us relu train urbm linear function transformation, keep perform good log sigmoid cosin score improv plda result obtain sigmoid urbm linear transformation. urbm train vrelu improv plda result slightli more. later vrelu work better relu unseen evalu set cosin plda scoring. taggedpt 3 compar perform gmmrbm vectors, obtain urbm train relu vrelu, tradit i-vector evalu set. us propos vrelu show better perform us relu cosin plda scoring. fact impli variabl threshold t vrelu increas gener power urbm addit correct histograms. table, per- formanc best gmmrbm vector compar i-vector cosin plda scoring. signific achiev comput load gmmrbm vector extract tra- dition i-vector extract discuss section 3.4. end, best result achiev score fusion i-vector gmmrbm vector show 77.5% 46.5% rel improv term eer mindcf, respectively, compar i-vectors. score fusion, bosari toolkit (brummer villiers, 2011) used. fusion weight train develop set. tabl 2 effect type hidden unit train urbm transform function extract gmmrbm vectors. result obtain develop databas vector dimens 400. vrelu refer propos variabl relu. hidden unit transform cosin plda eer (%) mindcf eer (%) mindcf sigmoid sigmoid 13.55 0.0570 11.05 0.0517 sigmoid (normal urbm) 8.67 0.0407 6.08 0.0338 log sigmoid 8.08 0.0383 6.51 0.0316 log sigmoid (normal urbm) 7.85 0.0366 6.28 0.0317 linear 8.24 0.0382 5.86 0.0317 relu linear 7.82 0.0372 5.58 0.0305 vrelu linear 7.82 0.0373 5.52 0.0297 tabl 3 perform comparison propos gmmrbm vector convent i-vector evalu set core test condition-common 5 nist 2010 sre. gmmrbm vector i-vector size 400. cosin plda eer (%) mindcf eer (%) mindcf [1] i-vector 6.270 0.05450 4.096 0.04993 [2] gmmrbm vector (train relu) 6.638 0.06228 4.517 0.05085 [3] gmmrbm vector (train vrelu) 6.497 0.06099 3.907 0.05184 fusion [1] [3] 5.791 0.05238 3.814 0.04673 o. ghahabi j. hernando / speech & languag 47 (2017) 1629 27 5. conclus taggedpw present work new vector represent speech text-independ speaker recognition. gmm supervector transform univers rbm (urbm) lower dimension vectors, refer gmmrbm vectors. role urbm learn total speaker session variabl back- ground gmm supervectors. us differ hidden unit train urbm differ transform function vector extract investigated. variant linear rectifi unit (relu), refer var- iabl relu (vrelu), proposed. variabl threshold defin unit correct histogram gmmrbm vector lead higher gener power urbm. experiment result core test- common condit 5 nist 2010 sre perform gmmrbm vector compar tradit i-vector cosin plda score comput load. moreover, best result obtain score fusion gmmrbm vector i-vectors. acknowledg taggedpthi work fund spanish project deepvoic (tec2015-69266-p) european project camomil (pcin-2013-067). like thank miquel india help extract tradit i-vector nist sre 2010 data. refer taggedpbahari, m.h., mclaren, m.l., hamme, h.v., leeuwen, d.a.v., 2012. ag estim telephon speech i-vectors. in: proceed 2012 annual confer intern speech commun associ (interspeech). taggedpbrummer, n., villiers, e., 2011. bosari toolkit user guide: theory, algorithm code binari classifi score processing. [online]. avail- able: taggedpcampbell, w.m., 2014. deep belief network vector-bas speaker recognition. in: proceed 2014 annual confer intern speech commun associ (interspeech), pp. 676680. taggedpdahl, g., yu, d., deng, l., acero, a., 2012. context-depend pre-train deep neural network large-vocabulari speech recognition. ieee trans. audio speech lang. process. 20 (1), 3042. doi: 10.1109/tasl.2011.2134090. taggedpdehak, n., dehak, r., glass, j., reynolds, d., kenny, p., 2010. cosin similar score score normal techniques. in: proceed 2010 speaker languag recognit workshop (odyssey). taggedpdehak, n., kenny, p., dehak, r., dumouchel, p., ouellet, p., 2011a. front-end factor analysi speaker verification. ieee trans. audio speech lang. process. 19 (4), 788798. doi: 10.1109/tasl.2010.2064307. taggedpdehak, n., torres-carrasquillo, p., reynolds, d., dehak, r., 2011b. languag recognit i-vector dimension reduction. in: proceed- ing 2011 annual confer intern speech commun associ (interspeech). citeseer, pp. 857860. taggedpgarcia-romero, d., zhang, x., mccree, a., povey, d., 2014. improv speaker recognit perform domain adapt challeng deep neural networks. in: proceed 2014 ieee spoken languag technolog workshop (slt), pp. 378383. taggedpghahabi, o., hernando, j., 2014a. deep belief network i-vector base speaker recognition. in: proceed 2014 ieee intern confer acoustics, speech signal process (icassp), pp. 17001704. taggedpghahabi, o., hernando, j., 2014b. i-vector model deep belief network multi-sess speaker recognition. in: proceed 2014 speaker languag recognit workshop (odyssey), pp. 305310. taggedpghahabi, o., hernando, j., 2015. restrict boltzmann machin supervector speaker recognition. in: proceed 2015 ieee interna- tional confer acoustics, speech signal process (icassp), pp. 48044808. taggedpghahabi, o., hernando, j., 2017. deep learn backend singl multisess i-vector speaker recognition. ieee/acm trans. audio speech lang. process. 25 (4), 807817. doi: 10.1109/taslp.2017.2661705. 28 o. ghahabi j. hernando / speech & languag 47 (2017) 1629 taggedphinton, g., 2012. practic guid train restrict boltzmann machines. neural networks: trick trade. lectur note science. springer berlin heidelberg, pp. 599619. taggedphinton, g., deng, l., yu, d., dahl, g., mohamed, a., jaitly, n., senior, a., vanhoucke, v., nguyen, p., sainath, t., 2012. deep neural network acoust model speech recognition: share view research groups. ieee signal process. mag. 29 (6), 8297. taggedphinton, g., osindero, s., teh, y.-w., 2006. fast learn algorithm deep belief nets. neural comput. 18 (7), 15271554. doi: 10.1162/ neco.2006.18.7.1527. taggedphinton, g., salakhutdinov, r., 2006. reduc dimension data neural networks. scienc 313 (5786), 504507. doi: 10.1126/sci- ence.1127647. taggedpkenny, p., 2010. bayesian speaker verif heavi tail priors. in: proceed 2010 speaker languag recognit workshop (odyssey). taggedpkenny, p., gupta, v., stafylakis, t., ouellet, p., alam, j., 2014. deep neural network extract baumwelch statist speaker recogni- tion. in: proceed 2014 speaker languag recognit workshop (odyssey), pp. 293298. taggedplarcher, a., bonastre, j.-f., fauve, b., lee, k., lvy, c., li, h., mason, j., parfait, j.-y., 2013. aliz 3.0 open sourc toolkit state-of-the-art speaker recognition. in: proceed 2013 annual confer intern speech commun associ (interspeech), pp. 27682771. taggedplei, y., scheffer, n., ferre, l., mclaren, m., 2014. novel scheme speaker recognit phonetically-awar deep neural network. in: proceed 2014 ieee intern confer acoustics, speech signal process (icassp). taggedpliu, y., qian, y., chen, n., fu, t., zhang, y., yu, k., 2015. deep featur text-depend speaker verification. speech commun. 73, 113. taggedplozano-diez, a., silnova, a., matejka, p., glembek, o., plchot, o., pesan, j., burget, l., gonzalez-rodriguez, j., 2016. analysi optim bottleneck featur speaker recognition. in: proceed 2016 speaker languag recognit workshop (odyssey), pp. 352 357. taggedpmclaren, m., lei, y., ferre, l., 2015. advanc deep neural network approach speaker recognition. in: proceed 2015 ieee inter- nation confer acoustics, speech signal process (icassp). taggedpmohamed, a., dahl, g., hinton, g., 2012. acoust model deep belief networks. ieee trans. audio speech lang. process. 20 (1), 14 22. doi: 10.1109/tasl.2011.2109382. taggedpmohamed, a., yu, d., deng, l., 2010. investig full-sequ train deep belief network speech recognition. in: proceed 2010 annual confer intern speech commun associ (interspeech), pp. 28462849. taggedpnadeu, c., macho, d., hernando, j., 2001. time frequenc filter filter-bank energi robust hmm speech recognition. speech com- mun. 34 (12), 93114. doi: 10.1016/s0167-6393(00)00048-0. taggedpnist, 2006. nist year 2006 speaker recognit evalu plan. [online]. available: taggedpnist, 2010. nist year 2010 speaker recognit evalu plan. [online]. available: evaluation_2010. taggedpnovoselov, s., pekhovsky, t., simonchik, k., shulipa, a., 2014. rbm-plda subsystem nist i-vector challenge. in: proceed 2014 annual confer intern speech commun associ (interspeech), pp. 378382. taggedppelecanos, j., sridharan, s., 2001. featur warp robust speaker verification. in: proceed 2001 speaker languag recognit workshop (odyssey). crete, greece, pp. 213218. taggedpprince, s., elder, j., 2007. probabilist linear discrimin analysi infer identity. in: proceed eleventh ieee interna- tional confer vision, (iccv 2007). taggedprichardson, f., reynolds, d., dehak, n., 2015. deep neural network approach speaker languag recognition. ieee signal process. lett. 22 (10), 16711675. taggedpsafari, p., ghahabi, o., hernando, j., 2015. featur classif mean deep belief network speaker recognition. in: proceed 2015 european signal process confer (eusipco), pp. 21622166. taggedpsafari, p., ghahabi, o., hernando, j., 2016. featur speaker vector mean restrict boltzmann machin adaptation. in: proceed- ing 2016 speaker languag recognit workshop (odyssey), pp. 366371. taggedpsenior, a., sak, h., shafran, i., 2015. context depend phone model lstm rnn acoust modelling. in: proceed 2015 ieee intern confer acoustics, speech signal process (icassp), pp. 45854589. taggedpsenoussaoui, m., dehak, n., kenny, p., dehak, r., dumouchel, p., 2012. attempt boltzmann machin speaker verification. in: pro- ceed 2012 speaker languag recognit workshop (odyssey). taggedpstafylakis, t., kenny, p., senoussaoui, m., dumouchel, p., 2012a. plda gaussian restrict boltzmann machin applic speaker verification. in: proceed 2012 annual confer intern speech commun associ (interspeech). taggedpstafylakis, t., kenny, p., senoussaoui, m., dumouchel, p., 2012b. preliminari investig boltzmann machin classifi speaker recogni- tion. in: proceed 2012 speaker languag recognit workshop (odyssey). taggedpvariani, e., lei, x., mcdermott, e., lopez moreno, i., gonzalez-dominguez, j., 2014. deep neural network small footprint text-depend speaker verification. in: proceed 2014 ieee intern confer acoustics, speech signal process (icassp), pp. 40524056. taggedpvasilakakis, v., cumani, s., laface, p., 2013. speaker recognit mean deep belief networks. in: proceed 2013 biometr tech- nologi forens science, pp. 5257. taggedpxia, r., liu, y., 2012. i-vector space model emot recognition. in: proceed 2012 annual confer intern speech commun associ (interspeech), pp. 22272230. taggedpxiang, b., chaudhari, u.v., navrtil, j., ramaswamy, g.n., gopinath, r.a., 2002. short-tim gaussian robust speaker verification. in: proceed 2002 ieee intern confer acoustics, speech signal process (icassp), pp. 681684. omid ghahabi receiv m.sc. degre electr engin shahid beheshti university, tehran, iran, 2009. current work ph.d. degre technic univers catalonia (upc), barcelona, spain. 2009 2011, speech process group, research center intellig signal processing, tehran, iran. 2011 2016, research speech process group, signal theori commun department, upc. late 2016, eml european media laboratori gmbh, heidelberg, germany, o. ghahabi j. hernando / speech & languag 47 (2017) 1629 29 speech technologist. research interest includ speaker languag recognition, speaker diarization, speech signal processing, deep learning. author coauthor journal confer paper topics. member research center languag speech technolog applications, barcelona, spain. javier hernando receiv m.s. ph.d. degre telecommun engin technic univers catalonia (upc), barcelona, spain, 1988 1993, respectively. 1988, depart sig- nal theori communications, upc, current professor director research center languag speech. academ year 20022003, visit research panason speech technolog laboratory, santa barbara, ca, usa. led upc team european, spanish, catalan projects. research interest includ robust speech analysis, speech recognition, speaker verif localization, oral dialogue, multimod interfaces. author coauthor 200 public book chapters, review articles, confer paper topics. receiv 1993 extraordinari ph.d. award upc. restrict boltzmann machin vector represent speech speaker recognit 1. introduct 2. convent i-vector 3. propos gmm-rbm vector 3.1. featur warp gmm supervector 3.2. univers rbm train 3.3. gmm-rbm vector extract 3.4. comput load compar i-vector 4. experiment result 4.1. setup, baseline, databas 4.2. result 5. conclus acknowledg refer