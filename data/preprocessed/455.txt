siam j. sci .
comput .
c 2015 societi industri appli mathemat vol .
00 , .
0 , pp .
00000000 multilevel balanc domain decomposit extrem scale  santiago badia  , alberto f. martin  , javier princip  abstract .
paper present fully-distribut , communicator-awar , recurs , interlevel-overlap message-pass implement multilevel balanc domain decomposit constraint ( mlbddc ) precon- dition .
implement high reli subcommun order achiev desir effect coarse-grain overlap comput communic , communic communic among level hierarchi ( name inter-level overlap ) .
essenti , main communic split mani non-overlap subset mpi task ( i.e. , mpi subcommun ) level hierarchi .
provid special resourc ( core memori ) devot level , care re-schedul map comput communic al- gorithm let high degre overlap exploit among level .
subroutin associ data structur express recurs , therefor mlbddc precondition arbitrari number level built re-us signific recurr part code .
approach lead excel weak scalabl result soon level-1 task mask coarser-level duti .
provid model indic choos number level coarsen ratio consecut level determin qualit scalabl limit given choic .
carri comprehens weak scalabl analysi propos implement 3d laplacian linear elast problem .
excel weak scalabl result obtain 458,752 ibm bg/q core 1.8 million mpi task , first time exact domain decomposit precondition ( base spars direct solver ) reach scale .
1 .
introduct .
simul scientif engin problem govern partial dif- ferenti equat ( pdes ) involv solut spars linear system .
time spent implicit simul linear solver relat overal execut time grow size problem number core [ 22 ] .
order satisfi ever increas demand realiti complex- iti simul , scientif comput must advanc develop numer algorithm implement effici exploit largest amount comput resourc , massiv parallel linear solver key compon process .
growth comput power pass increas number core chip , instead make core faster .
next generat supercomput , abl reach 1 exaflop/ , expect reach billion core .
thus , futur scientif comput strong relat abil effici exploit extrem core count [ 1 ] .
numer algorithm compon scalabl effici run extrem scale supercomput .
extrem core count , must reduc communic synchron among core , overlap communic comput .
largest scale , linear solver base precondit krylov subspac method .
algorithm scalabl precondition includ ( algebra ) multigrid ( mg ) [ 30 ] domain decomposit ( dd ) algorithm [ 31 ] .
howev , theoret properti enough practic weak scalabl , sinc precondition must allow massiv scalabl implement .
today  scalabl algorithms/implement present practic limit parallel , e.g. , due small , coars problem solv hierarch process dd/amg , loss sparsiti denser communic pattern coarser level amg [ 7 ] .
dd precondition make explicit use partit global mesh , e.g. , finit element ( fe ) integr , sub-mesh ( subdomain ) , provid natur framework develop fast robust parallel solver tailor distributed-memori machin .
one-level dd algorithm involv solut local problem nearest-neighbor communic .
( second level ) coars correct ( coupl subdomain ) requir algorithm scalabl , also harm practic ( cpu time ) weak scalabl .
two-level dd algorithm includ balanc neumann- centr internacion de metod numer l  enginyeria ( cimn ) , parc mediterrani de la tecnologia , upc , estev terrada 5 , 08860 castelldefel , spain ( { sbadia , amartin , princip } @ cimne.upc.edu ) .
universitat politecnica de catalunya , jordi girona 1-3 , edifici c1 , 08034 barcelona , spain .
this work fund european research council fp7 programm idea start grant .
258443 - comfus : comput method fusion technolog fp7 numexa project grant agreement 611636 .
a. f. martn also partial fund generalitat de catalunya program  ajut per la incorporacio , amb caract tempor , de person investigador junior les universitat publiqu del sistema universitari catala pdj 2013  .
acknowledg prace award us access fermi base bologna ( itali ) cineca , gcs juqueen base julich ( germani ) jsc .
grate acknowledg jsc staff general , dirk broemmel particular , support porting/debug fempar depend to/on juqueen .
1 neumann precondition ( bnn ) [ 23 ] , balanc dd constraint precondition ( bddc ) [ 13 ] , feti-dp precondition [ 15 ] .
practic scalabl limit two-level dd implement determin coars solver comput , whose size increas ( best ) linear respect number subdomain .
coars problem rapid becom bottleneck algorithm increas number core , sinc irremedi produc sever parallel effici loss ( core involv coars solver comput idl ) .
weak scalabl sustain even incur notabl parallel effici loss , increas- ing number core deal coars solver , e.g. , mean message-pass spars direct solver one mump [ 2 ] .
complex type solver quadrat three dimension problem , quadrat increas number core need keep constant comput time .
howev , scalabl spars direct solver limit hundr core , harm overal weak scalabl two-level bddc method point [ 3 ] .
salient featur bddc method fact constrain neumann dirichlet local problem , well coars problem , comput inexact way , e.g. , use one amg cycl without affect algorithm scalabl method [ 14 ] .
use inexact coars solver also possibl feti-dp method , modif [ 20 ] .
sinc amg solver maintain weak scalabl much message-pass spars direct method , inexact version bddc feti-dp method exhibit improv weak scalabl , even though implement still incur parallel effici loss comment .
inexact feti-dp method , local problem comput direct solver coars problem approxim use amg , exploit [ 20 ] .
bddc precondition salient properti permit overcom parallel over- head , make excel candid extrem scale solver design : ( p1 ) allow mathemat support extrem aggress coarsen coars matrix similar sparsiti pattern origin system matrix .
memory-constrain supercom- puter , order 105 spars direct method [ 5 ] ( see sect .
5 ) .
( p2 ) coars fine compon comput parallel , sinc basi coars space construct way orthogon fine compon space respect inner product endow system matrix [ 5 ] .
( p3 ) due fact coars matrix similar structur origin system matrix , multilevel extens algorithm possibl [ 25,32 ] .
properti ( p1 ) readili exploit bddc implement .
effici exploit ( p2 ) , i.e. , orthogon coars fine space , trivial .
howev , properti make possibl parallel comput coars fine correct , i.e. , overlap time .
[ 5 ] , classifi duti exact ( i.e. , use spars direct solver ) bddc-pcg algorithm fine coars duti .
duti re-schedul achiev maximum degre overlap preserv data depend .
actual implement idea requir signific code refactor , sinc involv switch spmd ( singl program multipl data ) mpmd ( multipl program multipl data ) parallel execut mode ; core divid fine grid duti coars grid duti .
bulk-asynchron approach reduc synchron among core , overlap communications/comput , follow exascal solver paradigm [ 1 ] .
exploit [ 5 ] , perform scalabl analys 3d poisson linear elast problem pair state-of-the-art multicore-bas distributed-memori machin ( helio curi ) .
excel weak scalabl attain 27k core reason high local problem size ; local coars problem solv use multi-thread spars direct solver pardiso [ 28 ] .
, clear reduct comput time memori requir inexact solver compar spars direct one made possibl get [ 6 ] excel weak scalabl result inexact/overlap implement two-level bddc precondition , 93,312 core 20 billion unknown juqueen .
regard ( p3 ) , multilevel bddc ( mlbddc ) algorithm propos [ 25 ] , coars problem next bddc level approxim bddc approxim .
implement mlbddc method exploit ( p2 ) found [ 29 ] .
even though cpu cost coars problem reduc , sinc coars problem still serial respect fine compon , implement [ 25 ] still suffer parallel effici loss .
, sinc condit number 2 bound slight increas number level [ 25 ] , detail numer experi [ 29 ] conclus prove benefit multilevel extens ( see , e.g. , tabl 4 , 7 , 8 [ 29 ] ) .
work , extend approach [ 5 ] two-level bddc method mlbddc .
exact two-level method serial coars solver effect till ten thousand core .
beyond point , coars problem mask anymor fine duti .
order go larger core count , extend approach [ 5 ] two-level bddc method mlbddc , i.e. , exploit ( p2 ) ( p3 ) .
present fully-distribut , communicator-awar , recurs , interlevel- overlap message-pass implement mlbddc precondition .
fully-distribut ( versus central ) mean data structur ( thus associ computations/commun ) distribut among core , includ coarse-grid problem level mlbddc precondit hierarchi .
communicator-awar refer fact code high reli subcommun order achiev desir effect coarse-grain overlap comput communic , communic communic among level hierarchi ( name inter-level overlap ) .
essenti , main communic split mani non-overlap subset mpi task ( i.e. , mpi subcommun ) level hierarchi .
given intermedi k-th level , coarse-grid problem built level k-1 distribut among subset mpi task devot level k. provid special hardwar resourc ( core memori ) devot level , care re-schedul map comput communic algorithm let high degre overlap exploit among level .
final , recurs implement mean subroutin associ data structur express recurs , therefor mlbddc precondition arbitrari number level built re-us recurs recurr part code ( e.g. , communic creation , local solver , inter-level data transfer , etc . ) .
approach lead excel weak scalabl result soon level-1 task mask coarser- level duti .
provid model indic choos number level coarsen ratio consecut level determin qualit scalabl limit given choic .
final , present comprehens weak scalabl analysi propos implement three- dimension ( 3d ) laplacian linear elast problem .
excel weak scalabl result obtain 458,752 core 1.8 million mpi task .
unpreced result exact domain decomposit precondition ( equip spars direct solver ) repres one order magnitud ( term scalabl limit ) improv respect best result up-to-now [ 5 ] , show tremend potenti algorithm approach propos herein .
thank result , softwar platform fempar [ 4 ] implement mlbddc algorithm high-q club ( sinc 2014 ) scalabl code juqueen ibm blue gene/q supercomput [ 11 ] .
let us summar contribut work :  novel recurs implement mlbddc method base multilevel overlap strategi time , order fulli mask coarse-grain task much larger core count two-level method .
 detail exposit effici exploit novel approach state-of-the-art supercomput , includ exploit recurs multilevel set overlap deploy task construct sub-commun .
 comprehens weak scalabl analysi propos strategi laplacian linear elast problem ibm blue gene/q supercomput , 458,752 core 1.8 million mpi task ( subdomain ) .
work structur follow .
mlbddc precondition present sect .
2 .
sect .
3 , present overlap mlbddc implement algorithm , overlap multipl level comput , elabor model determin scalabl limit given choic number level coarsen ratio consecut level .
sect .
4 provid implement key recurs creation mpi sub-commun .
sect .
5 , report comprehens set numer experi .
final , sect .
6 , draw conclus defin futur line work .
2 .
multilevel balanc domain decomposit .
section state mlbddc pre- condition .
sect .
2.1 , introduc basic notat .
sect .
2.2 devot two-level bddc algorithm .
final , mlbddc algorithm defin recurs way sect .
2.3 , reli con- 3 cept introduc sect .
2.2 two-level algorithm .
case , descript algorithm concis , refer reader [ 10,24 ] detail exposit two-level bddc method , [ 25 ] multilevel extens .
, thorough present practic implement detail domain decomposit method found [ 4 ] .
2.1 .
problem set .
let us consid bound polyhedr domain   rd = 2 , 3 quasi-uniform partit ( mesh ) t0 characterist size h0 .
usual , t0 partit tetrahedra/hexahedra = 3 triangles/quadrilater = 2 .
consid quasi-uniform partit t1 t0 nsbd1 sub-mesh , induc non-overlap domain decomposit  subdomain i1 , = 1 , .
.
.
, n sbd 1 ( characterist size h1 ) .
interfac  1 defin i1 : =  1 \  whole interfac ( skeleton ) domain decomposit 1 : = nsbd1 i=1  1 .
model problem , studi poisson problem linear elast  , arbitrari forc term boundari condit ( soon problem well-pos ) .
let us consid conform fe space v1  h1 (  ) .
denot vi1 restrict v1 i1  t1 , i.e. , local fe space .
v1 : = v11  .
.
.v nsbd1 1 global fe space function discontinu 1 .
let us also defin project e1 : v1  v1 weight averag interfac valu ( see , e.g. , [ 24 ] ) .
note t0 fe type defin v1 , wherea t1 also requir defin local discontinu space vi1 v1 , respect .
galerkin approxim problem hand ( respect v1 ) lead global linear system equat solv : a1x1 = f1 .
( 2.1 ) subdomain fe matrix correspond vi1 denot k 1 .
k1 block-diagon global sub- assembl fe matrix v1 .
( along paper , denot letter k sub-assembl matrix correspond fulli assembl one . )
analog , defin local sub-assembl right-hand side gi1 global counterpart g1 .
system matrix a1 right-hand side f1 obtain assembl k1 g1 .
non-overlap partit induc reorder dof interior interfac dof , i.e. , u1 = [ u1i , u1 ] t. also defin interior restrict oper r1iu1 : = u1i .
lead follow block structur global assembl , global sub-assembl , local matric : a1 = [ a1ii a1i a1i a1 ] , k1 = [ a1ii k1i k1i k1 ] , ki1 = [ ai1ii 1i ai1i k 1 ] , respect .
matric a1ii , a1i , a1i k1 present block-diagon structur ( amen parallel ) .
matric k1i k1i trivial extens ( zero ) a1i a1i , respect .
2.2 .
two-level bddc precondition .
sequel , state two-level bddc algorithm , describ set-up precondition applic .
input requir set-up bddc precondition ( t1 , v1 , k1 ) .
recal t1 subdomain partit , v1 global fe space , k1 global sub-assembl matrix .
construct bddc precondition requir partit degre freedom ( dof ) correspond v1 1 object , corner , edg , face .
next , associ ( ) object coars dof .
coars dof valu function corner , mean valu function edges/fac .
three common variant bddc method refer bddc ( c ) , bddc ( ce ) bddc ( cef ) , enforc continu corner coars dof , corner edg coars dof , corner , edg , face coars dof , respect .
definit object coars dof implement automat process arbitrari partit physic problem ( see [ 4 ] ) .
involv use kernel detect mechan order preserv well-posed bddc precondition ( see [ 33 ] ) .
defin coars dof , defin bddc fe space v1 subspac function v1 continu coars dof ; clear , v1  v1  v1 .
bddc precondition schwarz-typ precondition combin interior correct correct bddc space v1 ( see , e.g. , [ 10 , 31 ] ) .
defin interior correct oper p1 : = r 1i a11iir1i , involv set-up ( 1ii ) 1 , i.e. , local dirichlet problem .
bddc correct express e1k 1 1 e 1 , k1 galerkin project k1 onto v1 .
4 set-up bddc correct requir elabor .
let us consid decomposit bddc space v1 fine space v1f vector vanish coars dof k-orthogon complement v1c , denot coars space .
result , bddc fe problem decompos fine coars compon , i.e. , x1 = k 1 1 e 1r1 = x1f + x1c .
sinc fine coars space k1-orthogon definit , comput parallel .
fine space function v1f vanish coars dof ( dof involv continu among subdomain ) .
due k1-orthogon , fine compon defin x1f : = e1k 1 1f et1 , k1f galerkin project k1 onto v1f .
let us note , sinc coars dof fix ( zero ) fine correct , dof coupl subdomain , set-up k11f involv solut local neumann problem constrain valu correspond subdomain coars dof , denot ( ki1f ) 1 .
comprehens exposit implement issu regard solut constrain neumann problem refer [ 4 ] .
coars space v1c  v1 built v1c = span { 11 , 21 , .
.
.
,  ncts1 1 } ,   1  v1c coars shape function associ coars dof  , i.e. , take valu one  vanish rest coars dof .
result , comput also involv k11f ( see [ 4 ] ) .
let us note support   1 set subdomain contain  .
thus , everi subdomain comput coars space basi function relat own coars dof .
denot 1 matrix column coars shape function .
defin coars matrix k1c assembl subdomain local matric ki1c =  1 ki1 1 , = 1 , .
.
.
, n sbd 1 .
, two-level algorithm , set-up 1 1c , e.g. , assembl subdomain contribut one processor .
use fact p1a1p1 = p1 , two-level bddc precondition state compact form : m1 : = p1 + ( i1  p1a1 ) et1 ( 1a 1 1c t1 +k 1 1f ) e1 ( i1  p1a1 ) , i1 ident matrix .
2.3 .
multilevel bddc precondition .
two-level bddc algorithm involv input ( t1 , v1 , k1 ) automat generat coars dof correspond coars bddc space v1c coars matrix k1c .
easili observ bddc method suitabl multilevel general ( see [ 32 ] three-level [ 25 ] general case ) .
consid coars partit t2 t1 , obtain aggreg ( coarsen ) subdomain t1 .
everi subdomain  j 2  t2 , portion coars system matrix dispos via assembl local sub-assembl matric ki1c subdomain  1  t1 i1   j 2 , fashion fe matric .
, coars space v1c fe-lik space associ t1 , also generat correspond space discontinu function respect t2 .
therefor , readili defin bddc precondition associ coars system matrix k1c , coars space v1c , partit t2 .
sequel , express multilevel method formal way .
creat hierarchi quasi-uniform partit ( t1 , t2 , .
.
.
, tn ` 1 ) aggreg , n ` number level bddc method .
` +1 partit ` .
everi subdomain i `  ` one  j ` +1  ` +1  `   j ` +1 , denot j fat ( ` , ) .
readili use notat sect .
2.2 replac 1 ` .
input mlbddc method , requir sub-assembl subdomain matrix k1 relat t1 , global fe space v1 , hierarch partit ( t1 , t2 , .
.
.
, tn ` 1 ) .
set-up mlbddc precondition follow : ` = 1 , .
.
.
, n `  1 ,  set-up ( p ` , k 1 ` f , k ` c ,  ` , e ` ) use procedur describ sect .
2.2 replac ( t1 , v1 , k1 ) ( ` , v ` , k ` )  ` == n `  1  set-up a1n ` 1c  els  initi next level v ` +1  v ` c , k ` +1  k ` c oper set-up , posit defin mlbddc precondition recurs way follow : m1 ` = p ` + ( `  p ` ` ) et ` (  ` ` +1 ` +k 1 ` f ) e ` ( `  p ` ` ) , ` = 1 , .
.
.
, n `  1 , mn ` = 1 n ` 1c .
5 refer [ 25 ] proof follow theorem , condit number bddc- precondit system matrix .
note result [ 25 ] origin prove laplacian problem .
extend linear elast case combin two-level bound condit number bddc feti-dp method 3d linear elast ( see , e.g. , [ 21 ] ) multilevel analysi [ 25 ] .
theorem 2.1 .
condit number bddc precondit system matrix laplacian linear elast problem bound  ( )  c n ` 1 ` =1 ( 1 + log ( h ` h ` 1 ) ) 2 , bddc ( c ) bddc ( ce ) 2d , bddc ( ce ) bddc ( cef ) 3d , c > 0 constant depend hierarch partit , i.e. , number level n ` characterist size .
3 .
extrem scale parallel distributed-memori implement .
section cover detail novel approach parallel distributed-memori implement mlbddc- precondit conjug gradient ( pcg ) algorithm .
sect .
3.1 , present rational under novel approach pursu extrem scale implement algorithm .
sect .
3.2 cover build block parallel algorithm subject studi .
sect .
3.3 discuss use case illustr techniqu propos appli three-level bddc-pgc solver order reach maximum perform benefit .
final , sect .
3.4 analyz choos valu coarsen ratio govern subdomain aggreg order let techniqu propos fulli effect , includ estim number subdomain global problem split still reach goal .
3.1 .
rational under novel implement approach .
section present rational behind novel approach pursu extrem scale implement pcg- mlbddc solver .
rational built around fig .
3.1 , illustr two possibl implement approach algorithm .
implement approach illustr fig .
3.1 ( ) one typic follow exist code implement ( ml ) bddc relat dd algorithm [ 8 , 26 , 29 ] .
come natur mind reflect multilevel structur precondition .
also relat easi code due bulk-synchron structur .
howev , exploit parallel readili avail algorithm serial comput applic mlbddc hierarchi .
exampl , discuss sect .
2 , fine coarse-grid correct comput parallel due k-orthogon constraint under bddc space .
mani opportun parallel exploit implement approach .
( see sect .
3.3 full coverag opportun case three-level mlbddc-pcg solver . )
first side effect , signific loss parallel effici due idl mpi task .
due aggress coarsen method , number task level 1 much larger higher level .
thus , implement , notabl aggreg idl time rough equal number core use time time spent level higher 1 .
also negat impact energi consumpt implement .
second side effect , memori avail core respons coarsest-grid problem share among data structur correspond level .
provid alreadi limit memori per core current ( futur ) multicore-bas massiv parallel processor ( e.g. , 1gb ibm bg/q supercomput ) , limit even load per core fit memori ( thus global problem size ) . fig .
3.1 ( b ) illustr novel implement approach propos paper .
pursu full exploit parallel avail algorithm .
order reach goal , global mpi communic split mani disjoint subset mpi task , i.e. , subcommun , level mlbddc precondit hierarchi .
mpi task charg singl subdomain particular level .
besid , step algorithm re-organ ( i.e. , re-schedul ) way comput communic global critic path performed/issu soon we note distributed-memori solver coarsest-grid problem [ 16 , 26 , 29 ] mitig impact side effect , parallel overhead due idl mpi task still remain .
6 ... .. co 1 co 2 co 3 co 4 co p main mpi communic ... .. parallel ( distribut ) global communic global communic ... .. time idl idl ... .. co 1 co 2 co 3 co 4 co p 1 1st level mpi comm ... .. ... .. co 1 co 2 co p 2 2nd level mpi comm ... .. 3rd level mpi comm co 1 parallel ( distribut ) global communic global communic ... .. time ( ) ( b ) fig .
3.1 .
pictori view two possibl approach parallel distributed-memori implement mlb- ddc precondition .
( ) recurs , fully-distribut .
( b ) recurs , fully-distribut , communicator-awar , inter-level overlap .
possibl , let high degre coarse-grain overlap exploit among level .
provid mpi task level map disjoint ( special ) comput node , full memori avail per core use accommod data structur correspond level hierarchi .
target tune number level number task per level problem hand way first level duti complet absorb ( i.e. , mask ) coarser-grid duti effect inter-level overlap .
note strategi pursu [ 5 ] two-level bddc precondition remark scalabl solut 3d laplacian linear elast problem medium-s cluster ( 27k core ) .
order boost scalabl current supercomput core count ( order one million ) , work extend techniqu [ 5 ] two-level bddc method multilevel set .
3.2 .
build block .
alg .
1 present main phase parallel distributed-memori solut linear system ( 2.1 ) via mlbddc-pcg solver .
distinguish initi phase encompass line 1-2 alg .
1 , mlbddc precondition set-up , iter phase line 6 , pcg solver acceler mean mlbddc precondition .
pcg consist repeat sequenc follow four basic oper : applic precondition , spars matrix-vector product , inner product vector updat [ 27 ] .
algorithm 1 : solv a1x1 = f1 1 : set-up ( symbol stage ) alg .
2 2 : set-up ( numer stage ) alg .
3 3 : set initi solut x01 4 : x01i : = x 0 1i +a11iir1i ( f1 a1x 0 1 ) 5 : r01 : = f1 a1x01 6 : x1 : = pcg ( a1 , , r 0 1 , x 0 1 ) invok alg .
4 message-pass implement alg .
1 , matric , vector , associ comput 7 distribut among mpi task conform hierarchi non-overlap partit under- lie mlbddc precondition .
let us denot task ( ` , ) one-to-on map assign mpi task everi subdomain i `  ` , everi level ` .
vector x `  v ` distribut conform ` , i.e. , portion xi ` x ` store mpi task task ( ` , ) .
analog , matrix k ` distribut everi diagon block ki ` store mpi task task ( ` , ) .
vector v `  v ` distribut v ` , particular must continu interfac , i.e. , valu interfac dof must match among mpi task share .
result , data structur describ distributed-memori layout vector must provid addit glu inform .
refer reader [ 4 ] comprehens coverag data structur two-level bddc precondition context .
discuss straightforward appli mlbddc precondition .
alg .
2-3 collect step requir set mlbddc precondition , requir applic residu vector pcg iter shown alg .
4 .
work consid spars direct method [ 12 ] exact ( machin precis ) solut subproblem within mlbddc precondit hierarchi .
inde , reader might alreadi observ alg .
2 , 3 4 match stage involv direct solut spars linear system .
particular , precondition set-up split symbol stage alg .
2 ( gc denot graph describ sparsiti pattern matrix c ) , follow numer stage alg .
3 .
alg .
2 essenti build symbol factor graph associ local matric ` ` ii , k ` ` f , ` = 1 , .
.
.
, n `  1 , ` = 1 , .
.
.
, nsbd ` , global coarsest-grid matrix ` 1c .
part symbol analysi , fill-in reorder appli graph .
hand , alg .
3 charg numer factor matric .
spars choleski factor ( ) matric set-up , alg .
4 solv correspond linear system requir applic mlbddc precondition hierarchi mean spars forward/backward substitut .
note dirichlet pre-correct alg .
4 ( see line 2 3 ) omit first level , sinc interior residu alreadi zero due initi interior pre-correct ( see line 4 alg .
1 ) .
algorithm 2 : mmlbddc set-up ( symbol stage ) 1 : reord+symb fact ( gai ` ii ) ` 2 : identifi local coars dof ` 3 : reord+symb fact ( gki ` f ) ` 4 : gather coarse-grid dof `  ` + 1 5 : ` == n `  1 6 : build gan ` 1c ` + 1 7 : reord+symb fact ( gan ` 1c ) ` + 1 8 : els 9 : build g k j ` c ` + 1 10 : defin gk ` +1  gk ` c invok alg .
2 `  ` + 1 ` + 1 , .
.
.
, n ` 11 : end let us final describ mean label end step alg .
2 , 3 4 .
one hand , mpi task task ( ` ,  ) charg step label  `  , mpi task n ` k= ` +1 task ( k ,  ) perform label  ` + 1 , .
.
.
, n `  .
note alg .
2 , 3 , 4 assum , without loss general , ` 1c central singl mpi task last level ( serial spars direct solver use solut coarsest-grid problem ) .
howev , problem also distribut among sever mpi task ( solv mean message-pass spars direct solver one mump [ 2 ] ) .
hand , label  `  ` + 1  refer data transfer among mpi task two consecut level .
precis , among level ` mpi task , task ( ` , ) , parent level ` + 1 , i.e. , task task ( ` + 1 , j ) j = fat ( ` , ) .
3.3 .
use case three-level bddc-pcg solver .
accord sect .
3.1 , step alg .
2-4 judici re-organ order fulli exploit parallel readili avail algorithm .
tabl 3.1 depict result exercis three-level bddc- pcg solver .
( consid three-level algorithm sake simplic discuss 8 algorithm 3 : mmlbddc set-up ( numer stage ) 1 : num fact ( ai ` ii ) ` 2 : num fact ( ki ` f ) ` 3 : comput i ` ` 4 : comput ki ` c : = (  ` ) tki `  ` ` 5 : gather ki ` c `  ` + 1 6 : ` == n `  1 7 : ` 1c : = assembl ( k ` c ) ` + 1 8 : num fact ( ` 1c ) ` + 1 9 : els 10 : k j ` c : = assembl ( ki ` c ) , j = fat ( ` , ) ` + 1 11 : defin k ` +1  k ` c invok alg .
3 `  ` + 1 ` + 1 , .
.
.
, n ` 12 : end algorithm 4 : z : = m1mlbddcr 1 : ` > 1 2 : comput i ` : = ( ` ii ) 1ri ` ` 3 : comput ri `  : = r `  ai ` i  ` ` 4 : end 5 : comput ri ` : = ( e ` ) tr ` ` 6 : comput ri ` c : = (  ` ) tri ` ` 7 : gather ri ` c `  ` + 1 8 : comput si ` f : = ( k ` f ) 1ri ` ` 9 : ` == n `  1 10 : rn ` 1 = assembl ( r ` c ) ` + 1 11 : comput zn ` 1 : = 1 n ` 1 rn ` 1 ` + 1 12 : scatter zn ` 1 z ` c ` + 1 ` 13 : els 14 : r j ` c : = assembl ( ri ` c ) j = fat ( ` , ) ` + 1 15 : defin r ` +1  r ` c , z ` +1  z ` c , 16 : invok alg .
4 `  ` + 1 ` + 1 , .
.
.
, n ` 17 : scatter z j ` +1 z ` c , j = fat ( ` , ) ` + 1 ` 18 : end 19 : comput si ` c : =  ` z ` c ` 20 : comput zi ` : = e ` ( ` f + si ` c ) ` 21 : comput zi ` : =  ( ` ii ) 1ai ` iz `  ` 22 : ` > 1 23 : comput zi ` : = z ` + i ` ` 24 : end overlap potenti approach , even though implement algorithm recurs appli arbitrari number level ; see sect .
3.4 4 . )
tabl , step perform group colour region order clarifi exposit .
particular , green region encompass local comput nearest neighbor communic resid first level , blue region second level .
three green blue region separ gather scatter communic stage among first second level mpi task ( uncolor tabl ) .
blue region turn split communic stage among second third level mpi task ( color gray ) .
final , red region includ comput third level separ latter communic stage .
let us character balanc struck among time spent color region tabl 3.1 order let techniqu propos fulli effect .
character , 9 ` = 1 mpi task ` = 2 mpi task ` = 3 mpi task identifi local coars dof gather coarse-grid dof symb fact ( g k i1 ` f ) build g k i2 1c symb fact ( g i1 ` ii ) identifi local coars dof num fact ( k i1 ` f ) gather coarse-grid dof comput  i1 1 symb fact ( gki2 ` f ) build ga2c k i1 1c : = (  i1 1 ) tk i1 1  i1 1 symb fact ( gai2 ` ii ) symb fact ( ga2c ) gather k i1 1c num fact ( i1 ` ii ) k i2 1c : = assemb ( k i1 1c ) solv a1ii  0 1i = r1i r1 num fact ( k i1 ` f ) x01i : = x 0 1i + 01i comput  i2 2 r01 : = f1 a1x01 k i2 2c : = (  i2 2 ) tk i2 2  i2 2 algorithm 5 ( k  i1 ) gather ki22c num fact ( i1 ` ii ) a2c : = assemb ( k i2 2c ) num fact ( a2c ) gather r i1 1c solv k i1 ` f i1 ` f : = r i1 ` r i2 1c : = assemb ( r i1 1c ) algorithm 5 ( k  i2 ) gather r i2 2c solv k i2 ` f i2 ` f : = r i2 ` r2c : = assemb ( r i2 2c ) solv a2cz2c = r2c scatter z2c z i2 2c algorithm 6 ( k  i2 ) scatter z i2 2 z i1 1c algorithm 6 ( k  i1 ) algorithm 5 ` > 1 k ` : = ( k ` ii ) 1rk ` rk `  : = r k `  ak ` i  k ` end rk ` : = ( e k ` ) tr ` r ` c : = ( i ` ) tri ` algorithm 6 sk ` c : =  k ` z k ` c zk ` : = e k ` ( k ` f + sk ` c ) zk ` : =  ( k ` ii ) 1ak ` iz k `  ` > 1 zk ` : = z k ` + k ` end tabl 3.1 map alg .
2-4 mpi task maxim inter-level overlap three-level bddc-pcg solver .
take basi key observ deriv tabl : full effect fulfil mpi task second level issu gather scatter communic oper among first second level mpi task time first level .
delay second level mpi task reach communic stage ( respect first level mpi task ) immedi impli parallel effici loss .
situat ( later analyz factor depend get ) , step encompass green area fulli absorb ( i.e. , mask ) latenc associ coarser-grid level duti ( i.e. , blue red region tabl 3.1 ) .
reader note point scenario lead remark perform scalabl , provid step encompass green region local natur , therefor fulli parallel .
obvious necessari suffici condit achiev full effect step encompass three blue region second level take less time correspond green region .
let us discuss factor depend three green/blu region pair separ , restrict computationally-domin step :  first green/blu region pair , symbol factor graph associ dirichlet constrain neumann problem , numer factor constrain neumann problem , comput i11 first level , take least time former two symbol factor second level .
provid numer factor higher order complex symbol factor ( see sect .
3.4 ) , goal relat 10 easi achiev .
 second green/blu region pair , numer factor dirichlet problem matrix , solut linear system matrix , take least time numer factor constrain neumann dirichlet problem matric comput i22 second level .
provid two numer factor blue region per one green one , becom harder achiev goal case .
still feasibl though , e.g. , enforc suffici smaller subdomain size second level compar size one first level .
 analog , last green/blu region pair , solut constrain neumann problem first level least take time solut two dirichlet problem ( see alg .
5 6 ) , neumann constrain problem second level .
point , note blue region turn split communic stage among second third level mpi task ( color gray tabl 3.1 ) .
therefor , time spent red region becom  larg enough  , communic stage red region turn delay second level mpi task respect first level one extent start threaten full effect approach .
fact , irremedi happen point number mpi task coarsest level kept fix weak scale analysi .
order analyz effect , split discuss among overlap region , order infer coarsest level duti harm effect overlap first second level mpi task .
focus second third region sinc potenti level-2 idl first overlap green/blu area :  symbol factor level-3 coars matrix first red region becom domin , i.e. , mask symbol factor graph associ dirichlet constrain neumann problem numer factor constrain neumann problem second blue region , level-2 mpi task wait correspond gather communic .
result , first red region ( level-3 ) potenti degrad effect second green/blu overlap area .
 numer factor coarsest matrix ( charg level-3 mpi task ) mask numer factor triangular solv second third blue region , level-2 task wait correspond gather communic .
idl level-2 mpi task also occur third red region ( one spars backward/forward solv ) mask three spars backward/forward solv third blue region .
result , second third red region ( level-3 ) potenti degrad effect third green/blu overlap area .
weak scale scenario , subdomain problem size first second level fix .
hand , fix number mpi task third level , third level local problem size increas number subdomain second level .
result , maxim scalabl method , interest make subdomain coarsen first second level aggress possibl , alway keep second level ( blue ) region mask first level ( green ) one .
strategi allow achiev full effect within whole rang core count interest , possibl solut introduc addit level hierarchi amelior growth time spent red region .
stress discuss section appli minor detail mlbddc precondit hierarchi arbitrari number level .
pair intermedi level ( 1-2 , 2-3 , 3-4 , etc .
) , one deal pair region close green/blu region pair tabl 3.1 ( fact input data structur first level mpi task provid fe integr modul , instead gather previous level ) .
region intermedi level ( includ one last level ) potenti threaten full effect time spent becom  suffici larg  , sort delay-rippl effect final caus first level mpi task wast time communic stage among first second level mpi task .
case , effect coarsest-level duti green/blu region pair regardless number level .
3.4 .
estim effect coarsen ratio .
section provid comprehens discuss effect choic coarsen ratio govern subdomain aggreg among partit differ level , order overlap multilevel implement fulli effect .
11 depend number level use subdomain size everi level , turn depend coarsen ratio use definit hierarch partit .
enter upon subject , let us recal complex main stage direct solut spars linear system .
spars direct method , provid 3d problem solv , symbol factor cost csfn 4 3 , numer factor cost cnfn 2 , triangular system solv cost ctsn 4 3 , n size coeffici matrix , constant csf , cnf , cts depend particular algorithm , implement , under hardwar .
let us enter upon subject , consid partit well-balanc level , i.e. , subdomain level ( rough ) number element .
denot ` ( resp .
n ` ) local ( resp .
global ) number element level ` , p ` number subdomain level ` .
note n ` +1 = p ` , ` = n ` p ` = p ` 1 p ` = n ` n ` +1 = ( h ` +1 h ` ) .
weak scale scenario one-level algorithm , global problem size n1 number subdomain p1 level-1 duti increas way m1 = n1 p1 kept fix .
multilevel algorithm , aim keep effect precondition term iter count , requir keep bound condit number given theorem 2.1 constant .
thus , enforc local number element ` = p ` 1 p ` fix level ` = 1 , .
.
.
, n ` 2 , involv increas p ` accord .
level n ` assum map one ( least fix ) number mpi task .
given ` ( number local mesh element , denot element subdomain previous level ) , actual size local system denot  ` ` ( number local dof ) ,  ` affect type dof bddc method ` > 1 order fe space ` = 1 .
let us assum keep fix number level n ` weak scalabl analysi .
question aris : choos coarsen ratio level attain optim scalabl ?
fix hierarch partit , number mpi task overlap strategi stop effect ?
definit hierarch partit parametr desir number element per level , name m1 , .
.
.
, mn ` 1 .
order answer ( qualit ) former question , let us make two assumpt : 1 .
bulk cpu time overlap region spent differ phase spars direct solver ( symbol factor , numer factor , triangular system solv ) .
nearest-neighbor communic level 1 , communic among level ` = 2 , .
.
.
, n ` , step assum neglig .
2 .
cost phase spars direct method appli constrain neumann problem assum ident correspond one involv solut dirichlet problem .
valid first assumpt increas local problem size , mild reason load core ( 256 kb memori per core ) , due low cpu cost ( excel scalabl ) nearest-neighbor communic reduc number core level greater one .
second assumpt also reason , sinc problem almost size .
take account step three green/blu region pair tabl 3.1 , com- plexiti ( see ) , previous assumpt , easili infer step level ` + 1 fulli mask level- ` step follow over-pessimist condit hold : cnf (  ` ` ) 2 + 2cts (  ` ` ) 4 3  2cnf (  ` +1m ` +1 ) 2 + ncts (  ` +1m ` +1 ) 4 3 , ( 3.1 ) correspond second third green/blu region tabl 3.1 , n denot maximum number coars dof per subdomain .
suffici condit fulli mask coars duti level-1 ( fine ) duti , mean necessari .
comment , weak scale scenario fix number level , coarsest-grid relat step cpu time increas .
scalabl loss affect overal execut time level-1 region mask one level 2 .
consid effect coarsest level level-1 region ( see ) , get follow addit suffici ( over-pessimist ) condit fulli mask coars duti level-1 ( fine ) duti : 1m1  ( csf cnf ) 1 2 ( n ` mn ` ) 2 3 , ( 1m1 ) 4 3  ( 2m2 ) 4 3 + ( n ` mn ` ) 4 3 + ( cnf cts ) ( n ` mn ` ) 2 , ( 3.2 ) 12 correspond second third region , respect .
easili observ constraint challeng one latter , mask step higher complex .
let us consid worst-cas scenario last level central singl mpi task .
number element last level increas weak scale scenario follow : mn ` = pn ` 1 = pn ` 2 mn ` 1 = .
.
.
= p1 mn ` 1   m2 .
( 3.3 ) replac ( 3.3 ) ( 3.1 ) ( 3.2 ) , get indic maximum number core use loos parallel effici .
howev , order use formula , need provid valu ratio cts cnf csf cnf .
estim constant regress spars direct solver code comput architectur use sect .
5 3d laplacian problem five structur cubic mesh ( 4 , 096 512 , 000 fes ) , get cts cnf  400 csf cnf  500 .
larg valu expect , sinc cost per flop numer factor low , due intens use cach hierarchi mean level 3 blas , wherea triangular solut symbol factor memory-bound .
let us consid two practic exampl sect .
5 .
3d , linear fes , local problem size per subdomain m1 = 20 3 fit 1 gb memori avail per core .
  1 linear fes ,   4 bddc ( ce ) solv laplacian problem .
case , take exampl three-level bddc ( ce ) method m2 = 7 3 , condit ( 3.1 ) easili hold wherea condit ( 3.2 ) combin ( 3.3 ) lead p1 .
422 , 919 subdomain .
four level algorithm m1 = 20 3 , m2 = 43 , m3 = 33 lead p1 .
2 , 252 , 500 .
linear elast , valu  multipli factor three , last case lead p1 .
1 , 714 , 300 .
bound agreement numer result obtain sect .
5 combin , indic potenti strategi present far .
4 .
code implement detail .
section , sketch , thorough ( simplifi ) fortran95 code snippet , key hint mpi-parallel implement approach present sect .
3 .
particular , list 1 sketch initi mlbddc precondition data structur ( i.e. , type ( mlbddc ) deriv data type ) , list 2 , numer set-up stage ( see alg .
2 ) .
stress , simplic breviti , mani detail relat softwar engin practic softwar packag ( see sect .
5 ) omit , e.g. , relat algorithm code paramet , error memori handl , data encapsul .
still , expect hint use practition will implement softwar novel techniqu propos herein .
let us start subroutin list 1 .
subroutin take input argument set four mpi communic handler .
comm world communic handler includ mpi task contribut comput .
( case mpi comm world , conveni , duplic . )
comm l1 comm lgt1 two communic handler ( disjoint ) subcommun comm world includ mpi task first level hierarchi , higher level , respect .
final , intcomm l1 lgt1 handler mpi intercommun among former latter subcommun .
intercommun use precondition set-up , actual iter solut phase .
allow ( one ) first level mpi task signal ( broadcast ) higher level task whether iter process converg ( ) .
note four communic creat outsid subroutin , initi stage simul softwar , encapsul deriv data type control mpi parallel environ .
subroutin code need access object proper dispatch path follow mpi task comm world .
final , nlev , tsks per lev dummi argument refer n ` , p ` , ` = 1 , .
.
.
, n ` 1 , respect , fat map array drive subdomain aggreg among level .
particular , input root call subroutin , fat map ( ` ) =task ( ` +1 , fat ( ` , ) ) mpi task task ( ` , ) .
fat map generat preprocess phase , partit fe mesh perform , ( recurs ) partit graph subdomain .
line 24-34 list 1 , comm world split subcommun first level mpi task provid fat map ( 1 ) color , second level one , rank identifi comm l2 .
rest task contribut data transfer among first second level , therefor provid color=mpi undefin .
therefor , subcommun creat subset first second level mpi task exchang data .
( later section cover effici implement data transfer . )
prepar recurs call , second level higher level mpi task enter 13 1 recurs subroutin mlbddc_init ( comm_world , comm_l1 , comm_lgt1 , intcomm_l1_lgt1 , & 2 nlev , tsks_x_lev , fat_map , ) 3 integ , intent ( ) : : comm_world !
world mpi communic 4 integ , intent ( ) : : comm_l1 !
1st lev task mpi comm 5 integ , intent ( ) : : comm_lgt1 !
> 1st lev task mpi comm 6 integ , intent ( ) : : intcomm_l1_lgt1 !
intercomm l1 < = > lgt1 7 integ , intent ( ) : : nlev !
# level mlbddc 8 integ , intent ( ) : : tsks_x_lev ( nlev -1 ) !
# mpi task per level 9 integ , intent ( ) : : fat_map ( nlev -1 ) !
1st - > 2nd map coars fes 10 type ( mlbddc ) , intent ( ) : : !
mlbddc precondition 11 12 !
local variabl declar ( ierr , lgt1_rank , etc . )
go 13 % comm_world=comm_world 14 % comm_l1=comm_l1 15 % comm_lgt1=comm_lgt1 16 % intcomm_l1_lgt1=intcomm_l1_lgt1 17 % nlev=nlev 18 19 call mpi_comm_get_rank ( % comm_world , wrank , ierr ) 20 21 !
creat comm_world  subcommun level1 to/from level2 data transfer 22 !
* subcommun creat task comm_l2 ( l2_rank =0,1 , ... ) 23 !
* includ l2_rank task comm_l1 transfer data l2_rank 24 ( belongs_to ( % comm_l1 ) ) 25 call mpi_comm_split ( comm_world , color=fat_map ( 1 ) , key=wrank , & 26 % comm_l1_to_l2 , ierr ) 27 els ( belongs_to ( % comm_l2 ) ) 28 call mpi_comm_get_rank ( % comm_l2 , l2_rank ) 29 call mpi_comm_split ( comm_world , color=l2_rank , key=wrank , & 30 % comm_l1_to_l2 , ierr ) 31 els 32 call mpi_comm_split ( comm_world , color=mpi_undefin , key=wrank , & 33 % comm_l1_to_l2 , ierr ) 34 end 35 36 !
recurs init % p_m_c ( i.e. , mlbddc precondition coars -grid problem ) 37 ( nlev > 2 .and .
belongs_to ( comm_lgt1 ) ) 38 !
split comm_lgt1 comm_l2 comm_lgt2 39 call mpi_comm_get_rank ( % comm_lgt1 , lgt1_rank , ierr ) 40 ( lgt1_rank < tsks_x_lev ( 2 ) ) 41 call mpi_comm_split ( % comm_lgt1 , color=1 , key=lgt1_rank , % comm_l2 , ierr ) 42 % comm_lgt2 = mpi_comm_nul 43 els 44 call mpi_comm_split ( % comm_lgt1 , color=2 , key=lgt1_rank , % comm_lgt2 , ierr ) 45 % comm_l2 = mpi_comm_nul 46 end 47 48 !
creat intercomm comm_l2 < = > comm_lgt2 49 ( belongs_to ( % comm_l2 ) ) 50 call mpi_intercomm_cr ( % comm_l2 , loc_lead=0 , comm_world , & 51 & rem_lead=tsks_x_lev ( 2 ) , intcomm_l2_lgt2 , ierr ) 52 els 53 call mpi_intercomm_cr ( % comm_lgt2 , loc_lead=0 , comm_world , & 54 & rem_lead=0 , intcomm_l2_lgt2 , ierr ) 55 end 56 57 !
comm_lgt1 = > world_comm , comm_l2 = > comm_l1 , comm_lgt2 = > comm_lgt1 58 !
intcomm_l2_lgt2 = > intcomm_l1_lgt1 59 call mlbddc_init ( % comm_lgt1 , % comm_l2 , % comm_lgt2 , intcomm_l2_lgt2 , & 60 nlev -1 , tsks_x_lev ( 2 : ) , fat_map ( 2 : ) , % p_m_c ) 61 end 62 end subroutin mlbddc_init 63 64 function belongs_to ( mpi_comm ) 65 integ , intent ( ) : : mpi_comm 66 logic : : belongs_to 67 68 belongs_to = ( mpi_comm /= mpi_comm_nul ) 69 end function belongs_to list 1 simplifi fortran95 mpi recurs subroutin charg precondition initi .
line 37-61 , first level exit subroutin search addit first level duti .
former set task split comm lgt1 comm l2 comm lgt2 line 39-46 , intercommun among comm l2 comm lgt2 creat line 49-55 .
intercommun requir coarse-grid problem solv , e.g. , ( n `  1 ) -level bddc-pcg richardson 14 1 recurs subroutin mlbddc_setup_num ( , ) 2 type ( par_matrix ) , intent ( ) : : !
( distribut -memori ) matrix 3 type ( mlbddc ) , intent ( inout ) : : !
mlbddc precondition data structur 4 real ( 8 ) , allocat : : subd_elmat ( : , : ) !
subdomain contrib coars matrix 5 6 ( belongs_to ( % comm_l1 ) ) 7 call constrained_neumann_problem_setup ( , ) 8 call compute_coarse_grid_basis_vector ( , ) 9 alloc ( subd_elmat ( % nl_coars , % nl_coars ) ) 10 call compute_subd_elmat ( , , subd_elmat ) 11 call transfer_assemble_snd ( , subd_elmat ) 12 dealloc ( subd_elmat ) 13 call setup_dirichlet_problem ( , ) 14 els ( belongs_to ( % comm_l2 ) ) 15 call transfer_assemble_rcv ( ) 16 end 17 18 ( % nlev > 2 .and .
belongs_to ( comm_lgt1 ) ) 19 !
recurs set -up % p_m_c ( i.e. , mlbddc precondition coars problem ) 20 call mlbddc_setup_num ( % p_a_c , % p_m_c ) 21 els ( % nlev == 2 .and .
belongs_to ( comm_lgt1 ) ) 22 !
set -up % m_c ( serial solver ) % a_c ( serial coars -grid matrix ) 23 call serial_solver_setup_num ( % a_c , % m_c ) 24 end 25 end subroutin mlbddc_setup_num 26 27 subroutin transfer_assemble_snd ( , subd_elmat ) 28 type ( mlbddc ) , intent ( inout ) : : !
mlbddc precondition data structur 29 real ( 8 ) , intent ( ) : : subd_elmat ( % nl_coars , % nl_coars ) 30 real ( 8 ) , allocat : : buf_snd ( : ) !
messag buffer 31 !
rest local variabl declar go ( np , ierr , etc . )
32 33 call mpi_comm_get_s ( % comm_l1_to_l2 , np , ierr ) 34 35 alloc ( buf_snd ( % max_nl_coars **2 ) ) 36 call pack ( , subd_elmat , buf_snd ) !
pack subd_elmat buf_snd 37 38 !
issu ( parallel ) global collect 39 call mpi_gath ( buf_snd , % max_nl_coars **2 , mpi_double_precis , & 40 rcv_dum , int_rcv_dum , mpi_double_precis , & 41 root=np -1 , % comm_l1_to_l2 , ierr ) 42 43 dealloc ( buf_snd ) 44 end subroutin transfer_assemble_snd 45 46 subroutin transfer_assemble_rcv ( ) 47 type ( mlbddc ) , intent ( inout ) : : !
mlbddc precondition data structur 48 real ( 8 ) , allocat : : buf_snd ( : ) , buf_rcv ( : ) , all_subd_elmat ( : ) 49 !
rest local variabl declar go ( np , ierr , etc . )
50 51 call mpi_comm_get_s ( % comm_l1_to_l2 , np , ierr ) 52 53 alloc ( buf_snd ( % nl_coars **2 ) , buf_rcv ( np*m % max_nl_coars **2 ) , & 54 all_subd_elmat ( % sz_all_subd_elmat ) ) 55 56 !
issu ( parallel ) global collect 57 call mpi_gath ( buf_snd , % max_nl_coars **2 , mpi_double_precis , & 58 buf_rcv , % max_nl_coars **2 , mpi_double_precis , & 59 root=np -1 , % comm_l1_to_l2 , ierr ) 60 61 call unpack ( , buf_rcv , all_subd_elmat ) !
unpack buf_rcv all_subd_elmat 62 63 !
assembl contrib serial ( % a_c ) distribut ( % p_a_c ) coars matrix 64 ( % nlev > 2 ) 65 call par_mat_ass ( np , % p_coars , % l_coars , all_subd_elmat , % p_a_c ) 66 els !
% nlev == 2 67 call mat_ass ( np , % p_coars , % l_coars , all_subd_elmat , % a_c ) 68 end 69 70 dealloc ( buf_snd , buf_rcv , all_subd_elmat ) 71 end subroutin transfer_assemble_rcv list 2 simplifi fortran 95 mpi recurs subroutin charg precondition set-up ( numer ) .
iter solver ( instead singl applic precondition ) .
final , data structur correspond coarse-grid problem n ` 1-level bddc precondition ( i.e , % p c ) initi 15 recurs call mlbddc init line 60 .
note recurs call set four communic ( comm lgt1 , comm l2 , comm lgt2 , intcomm l2 lgt2 ) play role ( comm world , comm l1 , comm lgt1 , intcomm l1 lgt1 ) , respect , input recurs call .
subroutin charg numer set-up stage mlbddc precondition shown list 2 .
take input instanc type ( par matrix ) deriv data type , set instanc precondition data structur .
former deriv data type intern accommod distribut spars matrix , includ data describ memori layout conform non- overlap partit subdomain .
first stage precondition set-up encompass line 6- 16 , involv first second level mpi task .
particular , line 7 13 first level mpi task set local solver constrain neumann dirichlet problem , respect , comput coarse-grid basi vector line 8 .
prepar coarse-grid problem matrix assembl , first level mpi task first comput subdomain contribut coarse-grid problem store subd elmat ; see line 10 .
, first second level mpi task enter transfer assembl snd transfer assembl snd line 11 15 , respect .
mean two subroutin , second level mpi task gather subdomain contribut ( correspond ) first level mpi task .
contribut assembl line 64-68 list 2 .
note contribut assembl type ( par matrix ) instanc % p c , case intermedi level hierarchi , serial ( central ) spars matrix instanc % c , end hierarchi ( see line 65 67 , respect ) .
final stage list 2 encompass line 18-24 , recurs set , intermedi level , ( n `  1 ) -bddc precondition second higher level mpi task ( see line 20 ) , set , end hierarchi , serial solver instanc % c coarse-grid problem matrix instanc % c last level mpi task ( see line 23 ) .
implement inter-level data transfer list 2 deserv attent .
sub- communic subset ` ` + 1 level mpi task exchang data creat precondition initi .
set defin fat ( ` , ) ) togeth fat ( ` , ) ) ( master ) permit reus code implement two-level bddc coars solver solv serial singl separ mpi task [ 5 ] .
data transfer implement line 39 57 way multipl independ mpi gather oper issu simultan subcommun .
mean perform analysi tool , could confirm implement approach abl effici exploit under net- work hardwar parallel ibm bg/q supercomput .
particular , communic stage take asymptot constant time provid ` ` + 1 level mpi task scale proport ( i.e. , weak scale scenario ) .
anoth technic detail code list 2 exploit fix messag size collect , instead variabl messag size one ( mpi gatherv ) , therefor actual ( variabl size ) data sent , pack to/unpack ( pad ) messag buffer entry/exit mpi gather .
although actual code provid solut , observ practic fix message-s collect lead much better performance/scal ( despit overhead associ pad ) .
final , would like stress map tabl 3.1 three-level bddc-pcg solver ( one correspond mlbddc precondition arbitrari number level ) static code softwar , instead result recurr applic two techniqu , name recurs communicator-awar , order strateg deploy differ path mpi task resid level .
5 .
numer experi .
section , studi weak scalabl mlbddc-pcg solver code base implement techniqu present sect .
3 4 .
model problem , consid laplacian ( sect .
5.2 ) linear elast ( sect .
5.3 ) 3d pdes regular domain , discret structur ( cartesian ) fe mesh .
stress , howev , algorithm softwar design handl arbitrari geometri unstructur mesh well .
perform metric , focus number pcg iter requir converg , total comput time .
experi report section , time includ precondition set-up precondit iter solut linear system ( 2.1 ) .
5.1 .
experiment framework .
novel techniqu propos paper mlbddc- pcg solver implement fempar .
fempar , develop member lssc team 16 cimn , parallel hybrid openmp/mpi , object-ori softwar packag massiv parallel finit element ( fe ) simul multiphys problem govern pdes .
among featur , provid basic tool effici parallel distributed-memori implement substructur dd solver [ 4 ] .
parallel code fempar heavili use standard comput kernel provid ( highly-effici vendor implement ) blas lapack .
besid , proper in- terfac sever third parti librari , local dirichlet constrain neumann problem intermedi level hierarchi , global coarsest-grid problem last level , exact solv via spars direct solver .
work , particular explor hsl ma87 [ 18 ] , provid highly-effici parallel multi-thread dag-bas code implement supernod spars direct choleski solver .
nest dissect algorithm avail meti ( v5.1.0 ) [ 19 ] use fill-in reduc order hsl ma87 .
fempar releas gnu gpl v3 licens , 200k line fortran95/2003/2008 code long .
experi report section obtain either fermi , locat bologna ( itali ) cineca , juqueen , locat julich ( germani ) julich supercomput center ( jsc ) .
belong next generat ibm blue gene famili supercomput , so-cal bg/q supercomput .
particular , fermi configur 10-rack system , featur total 10,240 comput node , juqueen 28-rack one total 28,672 comput node .
node interconnect extrem low-lat five-dimension ( 5d ) torus interconnect network .
comput node equip 16-core , 4-way hardwar thread core , ibm power pc a2 processor , 16 gbyte sdram-ddr3 memori ( i.e. , 1gbyte/cor ) , run lightweight proprietari cnk linux kernel .
code compil use ibm xlf fortran compil bg/q ( v14.1 ) recommend optim flag .
custom mpich2 librari avail system use message-pass .
code link blas/lapack avail single-thread ibm essl librari bg/q ( v5.1 ) , hsl ma87 ( v2.1.1 ) .
5.2 .
3d laplacian result .
section studi weak scalabl mlbddc-pcg solver code fempar solut 3d laplacian problem unit cube  = [ 0 , 1 ]  [ 0 , 1 ]  [ 0 , 1 ] , constant forc term f = 1 , homogen dirichlet boundari condit whole boundari  .
consid global conform uniform mesh ( partit )  hexahedra trilinear fe discret ( i.e. , q1 fes ) .
3d mesh partit cubic grid p1 1 3 p1 1 3 p1 1 3 cubic subdomain , p1 total number first level subdomain .
first level subdomain size m1 1 3 m1 1 3 m1 1 3 fes .
size global fe mesh therefor equal ( m1p1 ) 1 3  ( m1p1 ) 1 3  ( m1p1 ) 1 3 .
5.2.1 .
sensit precondition robust subdomain aggreg .
section studi , three-level bddc precondition , impact coarsen ratio m2 govern aggreg ( first level subdomain second level one ) precondition robust .
measur number iter precondit iter solver take meet converg criteria .
fig .
5.1 5.2 plot number pcg iter ( y-axi ) function p1 ( x-axi ) three-level bddc ( ce ) bddc ( cef ) precondition , respect .
set initi solut vector guess x0 = 0 ( see line 3 alg .
1 ) , pcg iter stop whenev residu rk1 given iter k satisfi rk12  106r012 .
set-up also appli rest experi paper .
kept fix p3 = 1 , number subdomain first second level scale proport p1 = m2p2 , p2 = k 3 , respect , k = 2 , 3 , 4 , .
.
. , subject constraint p1+p2+p3 458,762 , i.e. , number core avail juqueen .
consid sever valu m1 = 10 3 , 203 , 303 403 size first level subdomain , valu m1 , number first level subdomain per second level subdomain vari m2 = 4 3 , 83 , 123 , 143 163 order determin sensit precondition robust m2 .
y-axi plot fig .
5.2 scale match one fig .
5.1 increas readabl .
observ fig .
5.1 5.2 , expect condit number bound ( see theorem 2.1 ) , profil plot asymptot constant number pcg iter ( final ) reach beyond valu p1 .
sensit robust three-level bddc ( ce ) precondition respect m2 observ fig .
5.1 ; much lower impact observ fig .
5.2 three-level bddc ( cef ) precondition .
general trend  small enough  17 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n p1 weak scale 3-level bddc ( ce ) solver m1=10 3 ( 1k fes/cor ) m2=4 3 m2=8 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n p1 weak scale 3-level bddc ( ce ) solver m1=20 3 ( 8k fes/cor ) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n p1 weak scale 3-level bddc ( ce ) solver m1=30 3 ( 27k fes/cor ) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n p1 weak scale 3-level bddc ( ce ) solver m1=40 3 ( 64k fes/cor ) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 fig .
5.1 .
sensit number three-level bddc ( ce ) -pcg solver iter m2 = 4 3 , 83 , 123 , 143 163 .
top bottom , left right : m1 = 10 3 , 203 , 303 403 , respect .
valu p1 , fix p1 m1 , larger valu m2 , smaller number pcg iter .
exampl , p1 =110k , m1 = 40 3 , number pcg iter 44 , 39 , 33 , 27 , m2 = 4 3 , 83 , 123 163 , respect ( see fig .
5.1 ) .
counter-intuit observ ( respect condit number bound ) explain fact two-level bddc precondition robust ( coarse-grid ) linear system get favour small second level subdomain grid small p1 larg m2 .
howev , m2 also impact fast asymptot regim reach , asymptot con- stant number pcg iter reach .
particular , fix m1 , larger valu m2 , slower take reach asymptot constant number iter , higher asymptot number iter .
justifi crossov observ among plot correspond combin m2 ( e.g. , m2 = 4 3 m2 = 8 3 top left corner fig .
5.1 ) .
case , balanc reach crossov achiev case , achiev  larg  p1 , within rang core count interest .
may fact posit ( unintend ) effect perform strategi suggest sect .
3.3 , i.e. , maxim m2 keep blue region green one tabl 3.1 .
final , compar plot fig .
5.1 one 5.2 observ three-level bddc ( cef ) , apart less sensit choic m2 , also robust three-level bddc ( ce ) precondition , take less iter converg , special larg valu m1 .
enhanc robust becom price heavier second third level coarse-grid problem .
sect .
5.2.3 , evalu extent increas still absorb mean inter-level overlap .
5.2.2 .
scalabl low load fermi .
section studi weak scalabl mlbddc-pcg solver code fermi  small  first level subdomain size , particular , m1 = 10 3 ( 1k ) 153 ( 3.4k ) fes .
valu m1 , memori consumpt per core mpi 18 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n p1 weak scale 3-level bddc ( cef ) solver m1=10 3 ( 1k fes/cor ) m2=4 3 m2=8 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n p1 weak scale 3-level bddc ( cef ) solver m1=20 3 ( 8k fes/cor ) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n p1 weak scale 3-level bddc ( cef ) solver m1=30 3 ( 27k fes/cor ) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 0 10 20 30 40 50 60 2.7k 42.8k 117.6k 175.6k 250k 343k 458k # p c g e ra tio n p1 weak scale 3-level bddc ( cef ) solver m1=40 3 ( 64k fes/cor ) m2=4 3 m2=8 3 m2=12 3 m2=14 3 m2=16 3 fig .
5.2 .
sensit number three-level bddc ( cef ) -pcg solver iter m2 = 4 3 , 83 , 123 , 143 163 .
top bottom , left right : m1 = 10 3 , 203 , 303 403 , respect .
task first level moder compar 1gb avail per core fermi , particular 19.7 29.9 mb , respect .
hand , time spent green area tabl 3.1 also moder , pose challeng full effect approach propos paper ( see sect .
3.3 ) .
fig .
5.3 ( ) ( b ) show , 64k fermi core , weak scalabl total comput time number pcg iter , respect , three-level ( solid line ) four-level ( dash line ) bddc ( ce ) ( left-hand side ) bddc ( cef ) ( right-hand side ) solver , m1 = 10 3 ( color black ) , m1 = 15 3 ( color blue ) .
consid two differ set-up three-level bddc precondition , correspond m2 = 4 3 , m2 = 6 3 , respect , singl one four-level bddc , particular , m2 = 3 3 , m3 = 3 3 .
three-level bddc , kept fix p3 = 1 , number subdomain first second level scale proport p1 = m2p2 , p2 = k 3 , respect , k = 2 , 3 , 4 , .
.
.. exampl , five point plot correspond m2 = 6 3 ( see , e.g , curv unfil squar right-hand side fig .
5.3 ( b ) ) , obtain k = 2 , 3 , 4 , 5 6 , respect .
four-level bddc , number first , second , third level subdomain scale proport p1 = m2p2 , p2 = m3p3 , p3 = k 3 , k = 2 , 3 , 4 , three point four-level bddc plot .
result fig .
5.3 clear reveal three differ scenario balanc achiev among time spent color region tabl 3.1 .
first , three-level bddc m2 = 4 3 , heavi third level reveal ( i.e. , much time spent red area tabl ) .
exampl , focus scalabl three-level bddc ( ce ) solver m1 = 10 3 left-hand side fig .
5.3 ( ) , see scalabl start ( signific ) degrad beyond p1 = 4k core ; degrad even sever three-level bddc ( cef ) solver ( see right-hand side figur ) , due heavier coarsest-grid problem .
order ( tri ) get rid , one aggress aggreg first level subdomain second level one , , consid larger valu m2 = 6 3 , lead second interest scenario fig .
5.3 .
choic m2 , reduc size coarsest- 19 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 3-lev m1=10 3 m2=4 3 3-lev m1=10 3 m2=6 3 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 weak scale mlbddc ( ce ) solver 3-lev m1=15 3 m2=4 3 3-lev m1=15 3 m2=6 3 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 4-lev m1=10 3 m2=3 3 m3=3 3 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 weak scale mlbddc ( cef ) solver 4-lev m1=20 3 m2=3 3 m3=3 3 ( ) total comput time ( sec . ) .
0 5 10 15 20 25 30 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 weak scale mlbddc ( ce ) solver 3-lev m1=10 3 m2=4 3 3-lev m1=10 3 m2=6 3 4-lev m1=10 3 m2=3 3 m3=3 3 3-lev m1=15 3 m2=4 3 3-lev m1=15 3 m2=6 3 4-lev m1=15 3 m2=3 3 m3=3 3 0 5 10 15 20 25 30 512 4k 8k 13.8k 21.9k 27k 32k 46.6k 64k p1 weak scale mlbddc ( cef ) solver 3-lev m1=10 3 m2=4 3 3-lev m1=10 3 m2=6 3 4-lev m1=10 3 m2=3 3 m3=3 3 3-lev m1=15 3 m2=4 3 3-lev m1=15 3 m2=6 3 4-lev m1=15 3 m2=3 3 m3=3 3 ( b ) number pcg iter .
fig .
5.3 .
weak scalabl total comput time ( ) number pcg iter ( b ) mlbddc ( ce ) ( left ) mlbddc ( cef ) ( right ) solver solut 3d laplacian pde m1 = 10 3 153 fes fermi .
grid problem ( therefor time spent red area tabl 3.1 ) , increas size second level subdomain .
particular , observ fig .
5.3 ( ) m1 = 10 3 , extent heavi second level reveal .
confirm ( initi ) higher comput time three-level bddc solver m2 = 6 3 compar m2 = 4 3 .
stress , although scenario asymptot constant comput time reach ( 64k core ) , lot comput resourc wast ( therefor parallel energi effici ) , first-level core wait ( second-level one ) communic stage among two level .
final ( desir ) scenario one correspond four-level bddc method , reveal balanc full effect achiev .
small p1 , comput time four-level bddc method compar three-level bddc method m2 = 4 3 ( actual smaller former due smaller m2 = 3 3 ) , case first level duti mask coarser-grid duti ( i.e. , green area tabl 3.1 domin ) .
scale p1 , four-level bddc still maintain desir balanc ( within rang core count studi ) , cut time spent red area introduct addit level hierarchi .
5.2.3 .
scalabl medium high load juqueen .
section studi weak scalabl mlbddc-pcg solver code full juqueen 28-rack ibm bg/q system , larger first level subdomain size consid sect .
5.2.2 , particular , m1 = 20 3 ( 8k ) , 253 ( 15.6k ) , 303 ( 27k ) , 403 ( 64k ) fes .
memori consumpt per first level mpi task case 80 , 146 , 233 , 651mb , respect .
fig .
5.4 ( ) ( b ) show , full juqueen system ( i.e. , 458,762 core ) , weak scalabl total comput time number pcg iter , respect , three-level ( solid line ) four-level ( dash line ) bddc ( ce ) ( left-hand side ) bddc ( cef ) ( right-hand side ) solver , m1 = 20 3 ( color black ) , 253 ( color blue ) , 303 ( color red ) , 403 ( color green ) 20 fes .
result four-level bddc report refer first two valu m1 .
consid m2 = 7 3 three-level bddc precondition , m2 = 4 3 , m3 = 3 3 four-level bddc one .
number subdomain level proport scale sect .
5.2.2 , k 6 11 , respect .
0 5 10 15 20 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 30 40 50 60 70 80 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale mlbddc ( ce ) solver 3-lev m1=40 3 m2=7 3 0 5 10 15 20 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 30 40 50 60 70 80 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale mlbddc ( cef ) solver 3-lev m1=40 3 m2=7 3 ( ) total comput time ( sec . ) .
0 10 20 30 40 50 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale mlbddc ( ce ) solver 3-lev m1=20 3 m2=7 3 4-lev m1=20 3 m2=3 3 m3=3 3 3-lev m1=25 3 m2=7 3 4-lev m1=25 3 m2=3 3 m3=3 3 3-lev m1=30 3 m2=7 3 3-lev m1=40 3 m2=7 3 0 10 20 30 40 50 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale mlbddc ( cef ) solver 3-lev m1=20 3 m2=7 3 4-lev m1=20 3 m2=3 3 m3=3 3 3-lev m1=25 3 m2=7 3 4-lev m1=25 3 m2=3 3 m3=3 3 3-lev m1=30 3 m2=7 3 3-lev m1=40 3 m2=7 3 ( b ) number pcg iter .
fig .
5.4 .
weak scalabl total comput time ( ) number pcg iter ( b ) mlbddc ( ce ) ( left ) mlbddc ( cef ) ( right ) solver solut 3d laplacian pde m1 = 20 3 , 253 , 303 403 fes juqueen .
remark scalabl observ fig .
5.4 ( ) three-level bddc-pcg solver full 28-rack ibm bg/q system , practic demonstr tremend potenti algorithm code subject studi .
valu m1 consid , time spent green region tabl 3.1  suffici larg  extent full effect alreadi obtain three-level method whole rang core count interest .
word , balanc struck first level duti complet absorb latenc associ coarser-grid level .
fulfil bddc ( ce ) , bddc ( cef ) precondition , despit addit work incur coarser-grid level introduct face constraint latter bddc space .
readili observ fig .
5.4 fact increas robust bddc ( cef ) ( i.e. , less number pcg iter fig .
5.4 ( b ) ) immedi translat lower comput time ( compar plot left right-hand side fig .
5.4 ( ) ) .
anoth observ confirm evid comput time four-level bddc precondition close three- level bddc precondition ( therefor three-level bddc method suffic keep control growth time spent coarsest-grid level ) .
5.2.4 .
rise challeng : extra concurr via overdecomposit .
section studi weak scalabl mlbddc-pcg solver code partit problem subdomain physic core involv parallel comput ( i.e. , overdecomposit problem hand ) .
core ibm bg/q supercomput requir least two instruct 21 issu per cycl differ hardwar thread order fulli fill instruct pipelin [ 17 ] ( therefor achiev peak flop perform ) .
therefor , use mpi task per physic core might possibl improv aggreg effici parallel computation. particular explor section 4 mpi tasks/cor .
( also perform experi 2 mpi tasks/cor , although result 4 mpi tasks/cor confirm higher profit hardwar thread term aggreg effici . )
howev , usag techniqu signific challeng scalabl algorithm/code/hardwar combin .
particular , 4 mpi tasks/cor impli moder amount memori 256mb/mpi task , 4-fold increas coarse-grid problem size solv level hierarchi .
challeng , howev , align current/and futur hpc trend much concurr less memori per core .
besid , physic core becom respons comput communic four differ subdomain .
order cope smaller load per core , larger coarse-grid problem , consid four-level bddc precondition cope growth time spent coarsest-grid level .
fig .
5.5 ( ) ( b ) report weak scalabl total comput time number pcg iter , respect , four-level bddc ( ce ) bddc ( cef ) solver , m1 = 10 3 ( color black ) , 203 ( color blue ) , 253 ( color red ) fes .
consid m2 = 4 3 , m3 = 3 3 , scale number first , second , third level mpi task proport p1 = m2p2 , p2 = m3p3 , p3 = k 3 , k = 2 , 3 , 4 , .
.
.
, 10 .
result number task map 4 time less core ( i.e. , 4 mpi tasks/cor ) .
therefor , largest valu k , map 1.73m first level subdomain 448.3k core .
0 1 2 3 4 5 6 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k p1 m1=10 3 m1=20 3 m1=25 3 10 15 20 25 30 35 40 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k weak scale 4-level bddc ( ce ) solver m2=4 3 , m3=3 3 # core m1=10 3 m1=20 3 m1=25 3 0 1 2 3 4 5 6 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k p1 m1=10 3 m1=20 3 m1=25 3 10 15 20 25 30 35 40 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k weak scale 4-level bddc ( cef ) solver m2=4 3 , m3=3 3 # core m1=10 3 m1=20 3 m1=25 3 ( ) total comput time ( sec . ) .
0 10 20 30 40 50 60 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k p1 weak scale 4-level bddc ( ce ) solver m2=4 3 , m3=3 3 # core m1=10 3 m1=20 3 m1=25 3 0 10 20 30 40 50 60 46.6k 216k373.2k 592.7k 884.7k 1.26m 1.73m 12.1k 56k 96.8k 153.7k 229.5k 326.8k 448.3k p1 weak scale 4-level bddc ( cef ) solver m2=4 3 , m3=3 3 # core m1=10 3 m1=20 3 m1=25 3 ( b ) number pcg iter .
fig .
5.5 .
weak scalabl total comput time ( ) number pcg iter ( b ) four-level bddc ( ce ) ( left ) bddc ( cef ) ( right ) solver solut 3d laplacian pde m1 = 10 3 , 203 253 fes juqueen 4 mpi tasks/cor .
due restrict inher ibm bg/q supercomput software/hardwar stack , done 4 mpi tasks/phys core ( i.e. , 4 subdomain handl physic core ) .
22 observ fig .
5.5 , still remark scalabl achiev approach despit 4-fold increas number subdomain .
particular , full absorpt coarse-grid duti achiev mean interlevel-overlap combin precondition m1 , except four-level bddc ( cef ) m1 = 10 3 , mild degrad scalabl achiev beyond p1 = 592.7k subdomain .
stress actual fine-tun valu m2 m3 , number level experi .
larger valu m3 ( e.g .
4 3 ) , level , could improv situat particular case .
apart confirm remark scalabl , compar , smaller test case , comput time code use 16 64 mpi tasks/nod ( number mpi tasks/level case ) , confirm approxim 50 % save aggreg effici exploit hardwar multi-thread ( i.e. , comput time 64 mpi tasks/nod approxim twice much one 16 mpi tasks/nod ) .
5.3 .
3d linear elast result .
section evalu weak scalabl mlbddc-pcg code appli vector-valu problem , name q1 fe approxim ( compress ) 3d linear elast pde ( first second lame paramet equal 1 1 10 , respect ) unit cube  = [ 0 , 1 ]  [ 0 , 1 ]  [ 0 , 1 ] , constant forc term f = 1 , homogen dirichlet boundari condit whole boundari .
experi set-up select three-level bddc precondition sect .
5.2.3 consid , except size first level subdomain , set-up m1 = 15 3 ( 3.4k ) , 203 ( 8k ) , 253 ( 15.6k ) fes .
largest subdomain size set much smaller largest one consid 3d laplacian problem sect .
5.2.3 ( i.e. , m1 = 40 3 ) .
linear elast problem vector-valu problem 3 unknown per fe mesh node .
impli , given fe mesh , size discret oper 3 time larger laplacian problem , 9 time nonzero entri .
inde , m1 = 25 3 memori consumpt per first level mpi task 713mb , compar 146mb valu m1 case 3d laplacian problem .
fig .
5.6 ( ) ( b ) report , full 28-rack ibm bg/q system , weak scalabl total comput time number pcg iter , respect , three-level bddc ( ce ) bddc ( cef ) solver , m1 = 10 3 ( color black ) , 203 ( color blue ) , 253 ( color red ) fes .
evidenc fig .
5.6 , approach pursu paper also abl obtain remark scalabl even much comput intens problem linear elast problem .
provid  suffici larg  m1 , i.e. , 20 3 253 fes bddc ( ce ) bddc ( cef ) , respect , mlbddc hierarchi equip 3 level alreadi suffici fulli overlap coarser-grid duti full rang core avail juqueen supercomput .
smaller valu m1 , ( mild ) degrad scalabl observ beyond point , e.g. , 153 , beyond p1 = 175.6k first level subdomain , would requir addit level keep perfect scalabl .
6 .
conclus futur work .
articl , present high scalabl parallel implement exact mlbddc method , i.e. , base spars direct solver local ( coarsest ) problem .
propos implement base recurs communicator-awar , order strateg deploy mpi task duti one level overlap interlevel task , base fact fine coars correct everi level mlbbdc method comput parallel .
result mpmd bulk-asynchron implement reduc synchron among core , overlap communications/comput differ level .
due recurs implement , use arbitrari number level .
implement lead close perfect weak scalabl result far ( embarrass parallel ) level-1 duti mask task higher level .
provid model help us choos effect coarsen ratio among core indic overlap strategi start loos effect .
case , main motiv work attain excel weak scalabl also reduc drastic aggreg idl , via interlevel-overlap implement .
turn improv parallel effici , energi awar , reduc time-to-solut .
detail scalabl analysi carri propos implement algo- rithm , reach whole juqueen blue gene/q , 458,752 core 1.8 million mpi task ( subdomain ) , laplacian linear elast problem .
largest scale problem 23 0 10 20 30 40 50 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 3-lev m1=15 3 m2=7 3 3-lev m1=20 3 m2=7 3 60 80 100 120 140 160 180 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale 3-lev bddc ( ce ) solver 3-lev m1=25 3 m2=7 3 0 10 20 30 40 50 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 3-lev m1=15 3 m2=7 3 3-lev m1=20 3 m2=7 3 3-lev m1=25 3 m2=7 3 60 80 100 120 140 160 180 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale 3-lev bddc ( cef ) solver ( ) 0 10 20 30 40 50 60 70 80 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale 3-lev bddc ( ce ) solver 3-lev m1=15 3 m2=7 3 3-lev m1=20 3 m2=7 3 3-lev m1=25 3 m2=7 3 0 10 20 30 40 50 60 70 80 2.7k 42.8k 117.6k 175.6k 250k 343k 458k p1 weak scale 3-lev bddc ( cef ) solver 3-lev m1=15 3 m2=7 3 3-lev m1=20 3 m2=7 3 3-lev m1=25 3 m2=7 3 ( b ) fig .
5.6 .
weak scalabl total comput time ( ) number pcg iter ( b ) three-level bddc ( ce ) ( left ) bddc ( cef ) ( right ) solver solut 3d linear elast pde m1 = 10 3 , 203 253 fes juqueen .
report far exact dd precondition .
three-level four-level algorithm ex- plore .
order stress propos implement , ( 1 ) coarsest-level problem solv one processor , i.e. , parallel exploit last level , ( 2 ) consid overdecomposit , i.e. , use partit problem subdomain physic core in- volv parallel comput , reach 1.8m subdomain .
result show close perfect weak scalabl low moder local problem size , i.e. , infra-util memori resourc per core .
use four level distribut comput coarsest problem interest thousand-fold increas number core expect near futur .
futur work includ extens framework krylov-cycl mlbddc method , order increas precondition robust keep constant number iter go level , develop inexact mlbddc ( hybrid amg-bddc ) algorithm implementa- tion .
final note implement approach also applic precondition , like bpx ( addit mg ) algorithm [ 9 ] , order improv parallel effici , time-to-solut , weak scalabl .
refer .
[ 1 ] report workshop extreme-scal solver : transit futur architectur , u.s. depart energi , 2012 .
[ 2 ] p.r .
amestoy , i. .
duff , j.-i .
l  excel , multifront parallel distribut symmetr unsymmetr solver , comput method appli mechan engin 184 ( 2000 ) , .
24 , 501520 .
[ 3 ] s. badia , a. f. martn , j. princip , enhanc balanc neumann-neumann precondit comput fluid solid mechan , intern journal numer method engin 96 ( 2013 ) , .
4 , 203230 .
[ 4 ] , implement scalabl analysi balanc domain decomposit method , archiv computa- tional method engin 20 ( 2013 ) , .
3 , 239262 .
24 [ 5 ] , high scalabl parallel implement balanc domain decomposit constraint , siam journal scientif comput ( 2014 ) , c190c218 .
[ 6 ] , overlap coarse/fin implement balanc domain decomposit inexact solver , sub- mit ( 2014 ) .
[ 7 ] a. h. baker , t. gamblin , m. schulz , u. m. yang , challeng scale algebra multigrid across modern multicor architectur , parallel distribut process symposium ( ipdp ) , 2011 ieee intern , 2011 , pp .
275 286 .
[ 8 ] s. balay , j .
brown , k. buschelman , w. d. gropp , d. kaushik , m. g. knepley , l. c. mcinn , b. f. smith , h. zhang , petsc web page , 2012. http : //www.mcs.anl.gov/petsc .
[ 9 ] j. h. brambl , j. e. pasciak , j. xu , parallel multilevel precondition , mathemat comput 55 ( 1990 ) , .
191 , 122 .
[ 10 ] s. c. brenner r. scott , mathemat theori finit element method , 3rd edit , springer , 2010 .
[ 11 ] d. brommel p. gibbon , high-q club highest scale code juqueen , innov supercomput deutschland 11 ( 2013 ) , .
2 , 106107 .
[ 12 ] t. a. davi , direct method spars linear system , vol .
2 , siam , 2006 .
[ 13 ] c. r. dohrmann , precondition substructur base constrain energi minim , siam journal scientif comput 25 ( 2003 ) , .
1 , 246258 .
[ 14 ] , approxim bddc precondition , numer linear algebra applic 14 ( 2007 ) , .
2 , 149168 ( en ) .
[ 15 ] c. farhat , k. pierson , m. lesoinn , second generat feti method applic parallel solut large-scal linear geometr non-linear structur analysi problem , comput method appli mechan engin 184 ( 2000 ) , .
24 , 333374 .
[ 16 ] v. hapla , d. horak , m. merta , use direct solver tfeti massiv parallel implement , appli parallel scientif comput , 2013 , pp .
192205 .
[ 17 ] r.a. hare , m. ohmacht , t.w .
fox , m.k .
gschwind , d.l .
satterfield , k. sugavanam , p.w .
coteus , p. heidelberg , m.a .
blumrich , r.w .
wisniewski , a. gara , g.l.-t. chiu , p.a .
boyl , n.h. chist , c. kim , ibm blue gene/q comput chip , ieee micro 32 ( 2012 ) , .
2 , 4860 .
[ 18 ] j. hogg , j. reid , j. scott , design multicor spars choleski factor use dag , siam journal scientif comput 32 ( 2010 ) , .
6 , 36273649 .
[ 19 ] g. karypi , softwar packag partit unstructur graph , partit mesh , comput fill-reduc order spars matric .
version 5.1.0 , univers minnesota , depart comput scienc engi- neer , minneapoli , mn , 2013 .
avail http : //glaros.dtc.umn.edu/gkhome/fetch/sw/metis/manual.pdf .
[ 20 ] a. klawonn o. rheinbach , high scalabl parallel domain decomposit method applic biome- chanic , zamm - journal appli mathemat mechan 90 ( 2010 ) , .
1 , 532 ( en ) .
[ 21 ] a. klawonn .
b. widlund , domain decomposit method lagrang multipli inexact solver linear elast , siam journal scientif comput 22 ( 2000 ) , .
4 , 11991219 .
[ 22 ] p. t. lin , j. n. shadid , m. sala , r. s. tuminaro , g. l. hennigan , r. j. hoekstra , perform parallel alge- braic multilevel precondition stabil finit element semiconductor devic model , journal comput physic 228 ( 2009 ) , .
17 , 62506267 .
[ 23 ] j. mandel , balanc domain decomposit , communic numer method engin 9 ( 1993 ) , .
3 , 233241 .
[ 24 ] j. mandel c. r. dohrmann , converg balanc domain decomposit constraint energi mini- mizat , numer linear algebra applic 10 ( 2003 ) , .
7 , 639659 .
[ 25 ] j. mandel , b. sousedk , c. dohrmann , multispac multilevel bddc , comput 83 ( 2008 ) , .
2 , 5585 .
[ 26 ] o. rheinbach , parallel iter substructur structur mechan , archiv comput method en- gineer 16 ( 2009 ) , .
4 , 425463 ( en ) .
[ 27 ] y. saad , iter method spars linear system , 2nd ed. , siam , 2003 .
[ 28 ] o. schenk k. gartner , fast factor pivot method spars symmetr indefinit system , electron transact numer analysi 23 ( 2006 ) , 158179 .
[ 29 ] b. sousedk , j. sstek , j. mandel , adaptive-multilevel bddc parallel implement , comput 95 ( 2013 ) , .
12 , 10871119 .
[ 30 ] k. stuben , review algebra multigrid , journal comput appli mathemat 128 ( 2001 ) , .
12 , 281 309 .
[ 31 ] a. toselli o. widlund , domain decomposit method - algorithm theori ( r. bank , r. l. graham , j. stoer , r. varga , h. yserent , ed .
) , springer-verlag , 2005 .
[ 32 ] x. tu , three-level bddc three dimens , siam journal scientif comput 29 ( 2007 ) , .
4 , 17591780 .
[ 33 ] j. sstek , m. certkova , p. burda , j. novotni , face-bas select corner 3d substructur , mathemat comput simul 82 ( 2012 ) , .
10 , 17991811 .
25 http : //www.mcs.anl.gov/petsc http : //glaros.dtc.umn.edu/gkhome/fetch/sw/metis/manual.pdf
