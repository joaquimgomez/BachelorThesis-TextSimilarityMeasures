comput linguist netherland journal 3 (2013) 217-233 submit 05/2013; publish 12/2013 neural network languag model select best translat maxim khalilov tau labs, 1011kw, amsterdam, netherland jose a.r. fonollosa centr recerca talp, universitat politecnica catalunya 08034, barcelona, spain francisco zamora-martnez dep. ciencia fsicas, matematica y la computacion universidad ceu-carden herrera 46115 alfara del patriarca (valencia), spain maria jose castro-bleda salvador espana-boquera dep. sistema informatico y computacion universitat politecnica valencia 46022 valencia, spain abstract qualiti translat produc statist machin translat (smt) system crucial depend gener abil provid statist model involv process. modern smt system us n-gram model predict element sequenc tokens, us continu space languag model (lm) base neural network (nn). contrast work nn lm estim probabl shortlist word (schwenk 2010), calcul posterior probabl out-of-shortlist word addit neuron unigram probabilities. experiment result small italian- to-english larg arabic-to-english translat task, account differ word histori length (n-gram order), nn lm scalabl small larg data improv n-gram-bas smt system. part, approach aim improv translat qualiti task lack translat data, demonstr scalabl large-vocabulari tasks. 1. introduct translat natur languag complex higher-ord activ human brain. machin translat (mt) technolog field comput linguist investig model translat texts, statist machin translat (smt), contrast automat rule-bas translat systems, translat paradigm base statist learn techniques. languag model import mt system, receiv spe- cializ attent smt community. instead, research focus 2013 maxim khalilov, jose a.r. fonollosa, francisco zamora-martnez, maria jose castro-bleda salvador espana-boquera. khalilov et al. special translat models, decod algorithms, train techniques. contrast, field natur languag processing, particularli automat speech recognit understanding, larg bodi research address specif problem languag modeling. great extent, discrep consequ noisi experiment result inconsist languag model (lm) configur translat performance. however, increas avail train data applic monolingu techniqu promis since, typically, greater data estim paramet lm, better lm performance. regardless intern configuration, smt typic take basi log-linear combin approach target languag sentenc defin combin featur functions. set normal includ target-sid lm, inform translat decod correct given sentenc fluenci translat hypothesis. paper follow continu space lm approach, coher natur evolut probabilist lms. deal better smooth challeng provid better gener unknown n-gram concentr scalabl problem crucial type lms. us continu space represent languag successfulli appli recent neural network (nn) approach languag model (xu jelinek 2004, bengio et al. 2003, castro prat 2003, arisoi et al. 2012), domain adapt (lavergn et al. 2011, park et al. 2010) speech recognit (schwenk 2007), neural network languag model (nn lm) applic state-of-the-art smt system popular. work trace studi schwenk et al., nn lm appli train target-sid lm (schwenk et al. 2006, schwenk 2010) form fully-connect multilay perceptron smooth probabl involv bilingu tupl translat model (schwenk et al. 2007). nn lm describ paper follow similar approach, differ wai probabl out-of-shortlist word estimated. detail section 3.3. inabl modern smt system accommod increas workload caus nn lms, open wai new computation expens mechan support translat process. contrast previous describ techniqu improv perform smt incorpor nn lm small train materi available, altern scenarios. first, experi small-vocabulari italian-to-english translat task demonstr nn lm potenti mt. however, interest altern field appli approach address scalabl problem especi crucial nn lm task larg train data. thus, second stage, provid translat result real-world large-vocabulari arabic-to-english task demonstr improv term final translat qualiti achiev circumvent difficulti impos complex structur natur languages. rest paper structur follows: section 2 briefli outlin n-gram-bas smt system. section 3 novel featur present paper described, i.e. nn lm train algorithm. section 4 present experiment setup obtain results, section 5 conclud articl lead discussions. 2. upc n-gram-bas smt modern smt system follow phrase-bas (koehn et al. 2007) hierarch (chiang 2007) translat approaches. study, follow altern algorithm, n-gram-bas tuple-bas smt (marino et al. 2006) prove competit state-of-the-art system recent evalu campaign (khalilov et al. 2008, lambert et al. 2007). 218 neural network languag model select best translat n-gram-bas smt deal bilingu n-grams, so-cal tuples. tupl extract word-to-word align (perform giza++1 (och nei 2000)) accord certain constraint (marino et al. 2006) compos word sourc languag zero word target one. hence, tupl induc uniqu segment pair sentences. contrast, phrase-bas system produc possibl pair phrase consistent, lead number segment possibl given pair sentences. regular phrase-bas smt consid context phrase reorder transla- tion, n-gram-bas approach condit translat decis previou translat decisions. context wai bilingu translat model seen lm, languag compos tuples. uniqu segment sentenc pairs, case n-gram-bas smt, trans- lation procedur regard stochast process maxim joint probabl p(f, e), approxim sentenc level. n-gram translat model, featur model taken consider are: (1) target lm word (train sri languag model toolkit2 (stolck 2002)); (2) word bonu model (to penal target sentenc length) (3) two-direct lexicon models. 2.1 decod optim mari decoder3 (crego et al. 2005) extend monoton distort search engin translat system. decod implement beam-search algorithm prune capabilities. featur function describ taken account decod process. given develop set set refer translations, log-linear combin weight adjust simplex optim method (nelder mead 1965) maxim score function accord combin automat evalu metrics. 2.2 extend word reorder n-gram-bas translat highli sensit differ word order sourc target languages. extend monoton distort model base automat ex- tract reorder pattern experiments. reorder pattern extract train stage cross link word alignment; step, monoton search graph extend re-ord follow pattern train set (crego marino 2007). search lattic built, decod travers graph look best translation. 2.3 rescor nn lm model integr n-gram-bas smt discrimin rescor- ing/rerank framework (compos steps), incorpor complex featur function entir translat hypothesi gener score. step, mari decod produc list m candid translat base vector weight train m basic featur (exclud orthodox n-gram lm order diminish nn lm effect). then, statist score gener translat candid rescor inform provid nn lm. modul presum add knowledg includ decod better distinguish higher lower qualiti translations. step, rescor vector train m + 1 featur provid different, better motiv choic single-best translat hypothesis. 1. code.google.com/p/giza-pp/ 2. www.speech.sri.com/projects/srilm/ 3. talp.upc.edu/talp/index.php/en/resources/tools/mari 219 code.google.com/p/giza-pp/ www.speech.sri.com/projects/srilm/ talp.upc.edu/talp/index.php/en/resources/tools/mari khalilov et al. altern wai incorpor nn lm smt us continu space lm directli decoding. decid pursu strategi result dramat increas decod time. 2.4 translat score adopt widely-us metric automat evalu mt quality: bleu score account translat qualiti evaluation, measur dis- tanc given translat set refer translat n-gram lm (a 4-gram framework study) (papineni et al. 2002). nist score sensit metric translat quality, base bleu score, weight n-gram order provid inform n-gram higher weight (doddington 2002). meteor score metric evalu mt output, calcul averag mean precis benefit recall, consid stem synonym match (more detail banerje lavi (2005)). 3. neural network languag model like encount new n-gram wit train heavi tail structur natur language. n -gram lm critic lack explicit represent depend longer n 1 preced tokens, effect rang depend significantli longer this. address problem lm smooth continu domain connectionist lm. major differ classic n-gram lm nn lm approach li distinct mechan implement smooth process. regular n-gram models, facto standard smooth algorithm modifi kneser-nei discount (chen goodman 1999), consid extens absolut discounting. method take account lower-ord model signific higher order count small zero (jame 2000, chen goodman 1999). contrast kneser-nei backing-off, interpol smooth model (for instance, jelinek-merc interpol chen-goodman model (chen goodman 1999)) us inform lower-ord model determin probabl n-gram non-zero counts. nn lm, posterior probabl interpol possibl context length n1 backing-off shorter contexts. unfortunately, gener involv greater compu- tation cost evalu train nn lm linearli depend number weights. number domin size hidden layer multipli vocabulari size. grow number requir calcul quickli overwhelm modern comput resourc make implement computation intract average-s vocabulari tasks. however, zipf law (zipf 1949) state given corpu natur languag utterances, frequenc word invers proport rank frequenc table. consequently, frequent word occur approxim twice second frequent word, occur twice fourth frequent word, on. observ explain input output nn be, practice, limit shortlist k frequent word vocabulary. choic shortlist n-gram order trade-off nn lm train time smt performance. 220 neural network languag model select best translat 3.1 model architectur nn lm statist lm follow n-gram assumpt estim lm probabl sequenc word length |w |: p(w1 . . . w|w |) |w | i=1 p(wi|win+1 . . . wi1) (1) probabl appear express estim nn. model natur fit probabilist interpret output nns: nn train classifier, output associ class estim posterior probabl defin class (bishop 1995, bengio et al. 2003). train set lm sequenc w1w2 . . . w|w | word vocabulari . order train nn predict word given histori length n 1, input word encoded. natur represent local encod follow 1-of-|| scheme4. problem relat encod task larg vocabulari (a case) huge size result nn. address problem follow bengio et al. (2003), develop distribut represent word.5 nn lm abl learn jointli distribut represent word continu space condit probabl estim equat (1). procedur allow nn lm smooth unseen sequenc words. figur 1 illustr architectur feed-forward nn estim nn lm: input compos word win+1, . . . , wi1 equat (1). example, input word wi3, wi2, wi1 4-gram. word repres local encod lj set neurons. p project layer input words, form p1, . . . , pn1 subset project units. subset project unit pj repres distribut encod input neuron lj (correspond word input posit j). weight project layer shared, is, weight local encod input lj correspond subset project unit pj input words. training, codif layer remov network pre-comput tabl size || serv distribut encoding. codif word comput follows: pj = l t j wl,p + b (2) ltj transpos vector lj , repres local codif cor- respond word win+1+j , wl,p matrix nn weight input word correspond project unit subset (share input word), b vector bias project unit subset (share subset), pj vector repres distribut encod correspond word. h denot hidden layer, computes: h = tanh(pt wp,h + c) (3) pt transpos project layer vector (concaten p1, p2, . . . , pn1), wp,h matrix nn weight project layer hidden layer, c vector hidden layer biases, tanh(.) component-wis hyperbol tangent activ function. 4. word local encod need vector || neurons, exist one-to-on map neuron words, neuron repres word activ 1, rest neuron 0s. 5. distribut representation, word map (or projected) continu space, number neuron smaller ||. 221 khalilov et al. output layer o || units, word vocabulary. o calcul follows: = ht wh,o + d (4) o = exp(a) || j=1 exp(aj) (5) wh,o matrix weight hidden layer output layer, d vector output layer biases, vector activ valu comput appli softmax normalization, aj compon j a. cross-entropi error function train experiments, ad l2 regular term weights, bias: e = d log(o) + wiw w2i 2 (6) d vector desir outputs, w union weight matrix wl,p,wp,h,wh,o, e comput error. nn estim posterior probabl word wi vocabulari given history, i.e., p(wi|win+1 . . . wi1). . . . . . . . . . 0 0 1 0 . . . 0 0 1 0 . . . 0 1 0 0 0 . . . . . . . . . . . . figur 1: architectur continu space nn lm 4-grams, hi = win+1 . . . wi1. illustr huge size nns, comput number weight largest nn lm, contain 224 neuron project layer 200 neuron hidden layer. case italian-to-english translation, nn includ vocabulari word appear twice 914,905 weights. arabic-to-english task, nn 56 neuron project layer oper 2,266,803 weights. nevertheless, standard 4-gram train corpora 20,817,600 parameters. 3.2 continu space lm experi strategi nn lm experi us italian-to-english btec corpu prelimi- nari experiments, larger effort dedic arabic-to-english nist 222 neural network languag model select best translat task. underli idea prove nn lm large-scal mt trial focus issu practic us continu space lms. consid kei paramet continu space nn lms: shortlist size defin k frequent words, word frequenc threshold impli word occur time train corpu discarded. n-gram order limit word histori n preced words. n -gram order 3 4 consid italian-to-english experiments; 4, 5 6-gram configurations, interpol high- low-ord n-grams, test translat arabic. valu set 2, 3 4 italian-to-english translation, correspond 4105, 3093, 2498 word nn lm vocabulary, respectively. translat arab english took differ approach: 10 k frequent word train vocabulary. refer tabl 1 actual vocabulari size values. italian-to-english # word english train set 4 2,498 3 3,093 2 4,105 arabic-to-english # word english train set n/a 10,000 tabl 1: number word reduc train corpora. note train set italian-to-english translat contain 10.2k uniqu words, vocabulari arabic- to-english 157.6k words. addit neuron ad input output layer account out- of-shortlist words, call osl neuron. train out-of-shortlist word replac osl identifier. test step, comput probabl out-of-shortlist word needed, activ osl neuron combin simpl standard unigram model comput out-of-shortlist words. probabl comput follows: p(wi|win+1 . . . wi1) = { owi iff wi shortlist; oosl p(wi|osl) iff otherwise, (7) owi output neuron activ relat wi, oosl osl neuron activation, p(wi|osl) standard unigram probabl comput out-of-shortlist words. finally, nn lm combin nns, size project layer (see section 3.1). defin p(wi|osl) valu follow trade-off altern strategi pre- sent compar emami mangu (2007). approach impli standard n-gram calculation, substitut p(wi|osl) p(wi|osl,win+1 . . . wi1), wai describ schwenk (2007). accord second strategy, p(wi|osl) valu set 0. ob- tain result indistinguishable, follow compromis strategy, unigram probabl model p(wi|osl). reestim weight coeffici new log-linear model nn lm, differ start point tried, best set weight result 100 bleu + 4 nist criteria. 223 khalilov et al. 3.3 differ nn lm schwenk best knowledge, studi paper attempt present nn lm differ work schwenk. arm zipf law, estim posterior probabl k frequent word vocabulari signific loss generality. posterior probabl rare word estim introduc extra neuron standard unigram model. contrast, model previous present schwenk et al. (2006) comput posterior probabl out-of-shortlist word standard n-gram model. requir estim contribut shortlist word standard n-gram model. besides, vocabulari nn lm input, codif frequent word learn (hai-son et al. 2010). 4. experi 4.1 data experi result obtain corpora, differ size train- ing materi (see brief statist tabl 2). italian-to-english btec cor- pu (takezawa et al. 2002), collect spoken dialogu data. second corpu consider 37m-word extract arabic-to-english nist6 corpu (new domain). experiments, develop test set includ ground truth refer (each refer differ translation), normal help automat evalu process error measur best references. note statist shown sourc (italian arabic) portion develop test bilingu corpora. set languag #sentenc #word vocab. size averag sent. length refer italian-to-english btec corpu train italian 24.5k 166.3k 10.2k 6.5 - train english 24.5k 155.4k 7.3k 6.1 - dev italian 489 5.2k 1.2k 6.5 7 test italian 500 6.0k 1.3k 6.9 7 arabic-to-english nist corpu (1.2m-line extraction) train arab 1.2m 37.4m 186.9k 31.2 - train english 1.2m 37.4m 157.6k 31.2 - dev arab 2,075 62.7k 10.1k 30.2 4 test arab 2,040 61.6k 9.9k 30.2 4 tabl 2: basic statist training, develop test data. 4.2 data preprocess italian-to-english system, preprocess step consist tokenization, tagging, lemma- tization, separ contract italian part, describ crego et al. (2006). arabic-to-english translation, similar approach shown habash sadat (2006). mada+tokan7 (roth et al. 2008) util disambigu tokenization. disambigu diacrit unigram statist employed. token 6. nation institut standard technolog 7. www1.ccls.columbia.edu/~cadim/mada.html 224 www1.ccls.columbia.edu/~cadim/mada.html neural network languag model select best translat d3 scheme -tagbi option split follow set clitics: w+, f+, b+, k+, l+, al+ pronomin clitics. -tagbi option produc bi po tag taggabl tokens. step n -best list extraction, paramet n set 1000, limit size list possibl translat gener mari decod output lattice. 4.3 configur provid reason comparison nn lm experiments, consid regular n-gram lm rescor wai nn lm (integr n-gram lm rescor step). configur baselin 1. altern configuration, consid inclus regular lm featur set function combin log-linear wai decod (we dec). result shown dec correspond perform standard n-gram-bas smt. secondari baselin (baselin 2 ) continu space default param- eters, describ schwenk (2010). packag avail online: univ-lemans.fr/fr/content/cslm. nn lm experiment, n -best list size set 1000. modifi kneser-nei discount chosen comput smooth n-gram lm demonstr best result term perplex final translat score (bleu) measur concaten refer translat (develop dataset). compar origin kneser-nei discount good-tur chen-goodman (uninterpol interpol versions) discount algorithm (chen goodman 1999). applic modifi kneser- nei techniqu demonstr signific improv perplex ( 12%) translat qualiti accord bleu score ( 3.9%) comparison altern smooth algorithms. order achiev good gener performance, train nn linearli combin build final nn lm. nn differ project layer size (128, 160, 192, 224 italian-to-english task, 160, 192, 224, 256 arabic-to-english task) hidden layer size (set 200 neurons). number paramet nn lm depend size layer. number taken preliminari work tasks, base literature. combina- tion differ project layer better optim size layer (which time consuming). hidden layer import effect perform model, big comput impact (schwenk 2007). automat evalu condit case-sensit includ punctuat marks. automat score calcul 7 (btec experiments) 4 (nist experiments) refer translations. 4.3.1 italian-to-english experi tabl 3 show bleu, nist, meteor score system 3- 4-gram nn lm integr compon combin smt reduc train material. baselin 1 dec system train test 4-gram target-sid lms. best score place cell fill grey. 4.3.2 arabic-to-english experi arabic-to-english experi us higher-ord nn lm italian-to- english task larger train data. differ baselin system con- sidered: baselin 1 (4), (5), (6) system emploi regular lm correspond order rescor step, 225 khalilov et al. dev test bleu nist meteor bleu nist meteor baselin 1 39.2 8.5 73.0 33.5 7.7 70.3 dec 39.4 8.6 73.2 33.6 7.7 70.9 nn lm baselin 2 42.2 8.9 74.0 34.9 7.9 71.2 nn lm 3-gram 41.7 8.8 73.4 34.3 7.8 70.2 > 4 4-gram 40.9 8.6 73.0 34.4 7.8 70.2 nn lm 3-gram 41.6 8.7 73.1 34.4 7.8 70.7 > 3 4-gram 42.0 8.8 73.8 34.4 7.8 70.4 nn lm 3-gram 41.8 8.8 73.5 34.3 7.8 70.4 > 2 4-gram 42.3 8.8 74.0 34.9 7.8 71.2 tabl 3: evalu score develop test dataset italian-to-english btec translation. dev test bleu nist meteor bleu nist meteor baselin 1 (4) 46.1 10.1 64.5 38.1 9.6 60.3 baselin 1 (5) 45.7 10.1 64.3 38.1 9.6 60.4 baselin 1 (6) 45.3 10.0 64.7 37.9 9.6 60.5 baselin 1 (4+5+6) 45.0 9.9 64.1 37.9 9.6 60.6 dec (6) 45.1 9.9 64.1 37.9 9.6 60.5 nn lm baselin 2 46.6 10.2 64.5 38.6 9.8 60.6 4-gram 46.5 10.2 64.6 38.4 9.7 60.6 5-gram 46.5 10.2 64.4 38.6 9.7 60.3 6-gram 46.6 10.2 64.4 38.5 9.7 60.3 4 + 5 + 6-gram 46.6 10.2 64.3 38.5 9.7 60.5 nn lm exclud sri lm model 4-gram 46.3 10.1 64.2 38.0 9.6 60.5 5-gram 46.5 10.3 64.2 38.3 9.6 60.4 6-gram 46.4 10.2 64.5 38.4 9.6 60.4 4 + 5 + 6-gram 46.5 10.3 64.5 38.2 9.6 60.6 tabl 4: evalu score develop test dataset arabic-to-english nist translation. baselin 1 (4+5+6) combin 4-, 5-, 6-gram n -best list, dec (6) provid decod access 6-gram standard lm addit rescoring. independ nn lm (4-, 5-, 6-grams), train network interpol high- low-ord n-gram (4+5+6-grams). isol impact nn lm translat 226 neural network languag model select best translat qualiti rescor n -best list exclud score gener regular n-gram lm (nn lm exclud sri lm models). best configur highlight aforement tables. 4.4 perplex analysi output sentenc smt built aggreg word sequenc high-scor combin probabl provid bilingu tupl translat model set featur models, includ lm. therefore, clear correl impact lm perplex assembl translation. however, perplex measur predict power lm, compar lm predict word previous unseen piec text. tabl 5 show perplex valu stand-alon lm measur merg set translat refer test corpora italian-to-english btec arabic-to-english nist tasks. languag model perplex italian-to-english btec task convent 3-gram 156 convent 4-gram 155 nn lm 3-gram, > 4 131 nn lm 4-gram, > 4 130 nn lm 3-gram, > 3 132 nn lm 4-gram, > 3 128 nn lm 3-gram, > 2 130 nn lm 4-gram, > 2 130 arabic-to-english nist task convent 4-gram 132 convent 5-gram 151 convent 6-gram 176 nn lm 4-gram 185 nn lm 5-gram 175 nn lm 6-gram 173 nn lm interpol 4, 5, 6-gram 167 tabl 5: perplex result differ languag models. architectur differ target-sid (english) nn lm italian-to-english arabic-to-english translat task li distinct algorithm not-in-the-vocabulari (unk) word processing. italian-to-english translation, perplex valu calcul basi sri lm model higher on nn lms. note neural network comput n-gram probabl subset task vocabulary, comput out-of-shortlist word probabl combin osl neuron multipli unigram model, affect perplex model. arabic-to-english task, output neural network cover 10 k frequent word only, task vocabulari 157.6 k size. perplex loss out- of-shortlist word import task. impli perplex nn lm calcul new neuron higher perplex sri lm models. 227 khalilov et al. 4.5 analysi italian-to-english result sentence-bas bleu scores8 share sentenc improv nn lm integr smt pipelin best perform integr (4-gram > 2 ) 46%. time bleu score 12% sentenc wors remain segment remain unchanged. observed, consider improv obtain nn lm italian-to-english translation. develop dataset, bleu score nn lm experi higher baselin nn lm systems. best- perform 4-gram > 2 nn lm allow gain 1.3 bleu point test set includ convent n-gram lm featur decod (dec ); gain 1.4 bleu point test dataset us regular lm rescor (baseline). aforement differ statist signific 95% confid interv 1,000 resampl bootstrap resampl method (koehn 2004). upper-bound statist signific threshold (bleu score calcul test dataset translat baselin system) li 34.0 bleu points. analysi nist score italian-to-english system show baselin result test set exceed nn lm systems. concern meteor score, 4-gram, > 4 provid better lm generalization. perform shown best statist indistinguish result shown schwenk bleu meteor scores. correl automat subject human evalu metric (fluenci adequacy) main topic area mt evaluation. report paul (2006) small btec translat tasks, fluenci correl best bleu, adequaci correl best meteor. nist metric moder correl subject human evalu metrics. take aforement observ consideration, work demonstr potenti applic nn lm smt system improv translat fluency, adequaci remain same. posit impact higher-ord n-gram clear, possibl rel short sentenc provid btec corpus. possibl issu higher-ord n-gram order slightli decreas translat quality, time, introduc noisier translations. exampl typic sentenc italian-to-english btec corpu shown figur 2. 4.6 analysi arabic-to-english result best-perform nn lm (5-gram nn lms) term bleu, 34% sentenc improv comparison baselin 2 system, 10% decreas perform rest dataset chang observed. arabic-to-english translation, bleu nist score calcul develop- ment dataset improv nn lm appli comparison perform shown baselin engines, meteor valu gener nn lm configur vari score produc integr convent n-gram lm. consid test data translat scores, differ bleu score shown best nn lm best baselin popul 0.5 bleu points, statist signific threshold task (0.5). time, result achiev rescor n -best list includ nn lms, exclud standard n-gram lms, statist distinguish baselines, nn lm systems. configur provid better bleu score correspond 5-gram lms. incorpor nn lm n-gram-bas smt allow gain 0.7 bleu point 8. gener settings, sentence-bas bleu score sens accumul natur bleu. 228 neural network languag model select best translat sourc oggi abbiamo scelta insalata ai frutti di mare insalata di patat e insalata mista. ref todai choic seafood salad potato salad wild veget salad. serv seafood salad potato salad wild veget salad today. todai salad enjoi seafood potato wild vegetables. salad seafood potato wild veget today. todai select seafood salad potato salad wild veget salad. todai seafood salad potato salad wild veget salad. todai choos seafood salad potato salad wild veget salad. baselin todai select seafood salad potato salad mix salad. 3-gram = 5 todai choos seafood salad potato salad mix salad. 4-gram = 5 todai select seafood salad potato salad mix salad. 3-gram = 3 todai choos seafood salad potato salad mix salad. 4-gram = 3 todai choos seafood salad potato salad mix salad. figur 2: exampl italian-to-english translation. italian express oggi abbiamo scelta translat baselin todai select at, nn lm system provid fluent translat todai choos from. sourc w aelnt wkalp alanba alamaratyp en wswl aleahl alardni mn dwn twdyh brnamj aw mdp zyarp h . ref emir new agenc announc arriv jordanian monarch specifi programm durat visit . emir new agenc announc arriv jordanian king , give detail program , durat visit . emir new agenc announc jordanian monarch s arriv withoutnot schedul durat stai . emir new agenc announc jordanian monarch s visit give detail purpos durat . baselin emir agenc announc king access clarifi programm durat visit .6-gram dec emir new agenc said jordanian monarch access clarifi programm durat visit . 4-gram new agenc announc jordanian monarch access clarif programm durat visit . 5-gram new agenc announc arriv jordanian monarch clarif programm durat visit . 6-gram new agenc announc jordanian monarch access clarifi programm durat visit . 4 + 5 + 6 new agenc announc arriv jordanian monarch clarif programm durat visit . figur 3: exampl arabic-to-english translation. nn lm (along dec system) manag gener correct translat jordanian monarch. 5- 4+5+6- gram model produc correct translat arab word wswl, arrival. system translat incorrectli access. 229 khalilov et al. test set dec 0.5 bleu point best baselin configurations. increas n-gram order 6 lead perform improvement, interpol 4, 5, 6-grams. obtain result ad nn lm regular nn lm discrimin rescor framework provid deliver slightli improv consist translat qualiti comparison system consid standard lms. task, schwenk perform slightli better nn lm term bleu nist score good consid meteor. figur 3 illustr example9 sentenc arabic-to-english nist. 5. discuss conclus architectur smt impli that, smaller avail train data is, wors perform translat system. paper, shown following: 1. robust nn lms, highli limit train corpus. in-domain nn lm provid significantli better gener target language, better smooth smt output, enhanc improv automat evalu translat scores. nn lm turn benefici train excerpt frequent word vocabulary. claim small translat tasks, integr nn lm improv translat fluency, adequaci remain same. empir proof claim plan near future. 2. proof claim nn lm approach scalabl modern level technolog development. demonstr techniqu nn lm set k frequent words, probabl frequent word estim us extra neuron unigram probabilities, lead minor improv translat qualiti large-vocabulari tasks. comparison exist system base continu space languag model show approach perform practic good known schwenk term bleu (corre- late fluency, small translat tasks) slightli better consid meteor (correl adequacy) (paul 2006). main disadvantag continu space lm high comput cost training. tradit n-gram lm train minut sri lm toolkit, dai estim continu space lm large-vocabulari task. possibl solut problem applic fast-train techniqu (lattic regroup util special nn librari abil parallel calculation) involv power (and expensive) comput resources. high comput cost caus re- search effort decoupl system, base n -best rescoring. recent research yield promis result effici integr nn lm decod (zamora-martnez et al. 2009, zamora-martnez et al. 2010). urgent task plan undertak increas credibl nn lm integr smt framework are: 1. experi higher train data, probabl focus distant languag pairs. 2. run human evalu campaign, base adequacy/flu scoring, confirm result automat evaluation. 9. arab exampl provid buckwalt transliter (buckwalt 1994). 230 neural network languag model select best translat 3. check applic describ mechan hierarch smt systems, like describ chiang (2005) chiang (2007). refer arisoy, e., t.n. sainath, b. kingsbury, b. ramabhadran (2012), deep neural network languag models, proceed naacl-hlt 2012 workshop: replac n-gram model? futur languag model hlt, montreal, canada, pp. 2028. banerjee, s. a. lavi (2005), meteor: automat metric mt evalu improv correl human judgments, proceed acl workshop intrins extrins evalu measur machin translat and/or summarization, pp. 6572. bengio, y., r.e. ducharme, p. vincent (2003), neural probabilist languag model, journal machin learn research 3, pp. 11371155. bishop, c.m. (1995), neural network pattern recognition, oxford univers press. buckwalter, t. (1994), issu arab orthographi morpholog analysis, proceed col- ing 2004, geneva, switzerland, pp. 3134. castro, m.j. f. prat (2003), new direct connectionist languag modeling, comput method neural modeling, springer-verlag. chen, s.f. j. goodman (1999), empir studi smooth techniqu languag mod- eling, speech languag 4 (13), pp. 359394. chiang, d. (2005), hierarch phrase-bas model statist machin translation, proceed associ comput linguist (acl) 2005, pp. 263270. chiang, d. (2007), hierarch phrase-bas translation, comput linguist 2 (33), pp. 201 228. crego, j.m., a. gispert, p. lambert, m. khalilov, m. costa-jussa, j.b. marino, r. banchs, j.a.r. fonollosa (2006), talp ngram-bas smt iwslt 2006, proceed iwslt 2006, pp. 116122. crego, j.m. j.b. marino (2007), improv statist mt coupl reorder decoding, machin translat 20(3), pp. 199215. crego, j.m., j.b. marino, a. gispert (2005), ngram-bas statist machin translat decoder, proceed interspeech05. doddington, g. (2002), automat evalu machin translat qualiti n-gram co- occurr statistics, hlt 2002 (second confer human languag technology), pp. 128132. emami, a. l. mangu (2007), empir studi neural network languag model ara- bic speech recognition, proceed ieee automat speech recognit understand workhsop (asru 2007), pp. 147152. habash, n. f. sadat (2006), arab preprocess scheme statist machin translation, proceed human languag technolog confer naacl, pp. 4952. hai-son, l., a. alluzen, g. wisniewski, f. yvon (2010), train continu space languag models: practic issues, proceed emnlp, pp. 778788. 231 khalilov et al. james, f. (2000), modifi kneser-nei smooth n-gram models, technic report, research institut advanc scienc (riacs). khalilov, m., a.h. hernandez, m.r. costa-jussa, j.m. crego, c.a. henrquez, p. lambert, j.a.r. fonollosa, j.b. marino, r. banch (2008), talp-upc ngram-bas statist ma- chine translat acl-wmt 2008, proceed acl 2008 workshop statist machin translat (wmt08), pp. 127131. koehn, p. (2004), statist signific test machin translat evaluation, proceed empir method natur languag process (emnlp) 2004, pp. 388395. koehn, p., h. hoang, a. birch, c. callison-burch, m. federico, n. bertoldi, b. cowan, w. shen, c. moran, r. zens, c. dyer, o. bojar, a. constantin, e. herbst (2007), moses: open- sourc toolkit statist machin translation, proceed associ computa- tional linguist (acl) 2007, pp. 177180. lambert, p., m.r. costa-jussa, j.m. crego, m. khalilov, j. marino, r.e. banchs, j.a.r. fonollosa, h. schwenk (2007), talp ngram-bas smt iwslt 2007, proceed intern workshop spoken languag translat (iwslt07), pp. 169174. lavergne, t., a. allauzen, h-s. le, f. yvon (2011), limsi experi domain adapt iwslt11, proceed iwslt 2011, san francisco, ca, usa, pp. 6267. marino, j.b., r.e. banchs, j.m. crego, a. gispert, p. lambert, j.a.r. fonollosa, m.r. costa-jussa (2006), n-gram base machin translation, comput linguist 32 (4), pp. 527549, acl. nelder, j.a. r. mead (1965), simplex method function minimization, organ 7, pp. 308313. och, f. h. nei (2000), improv statist align models, proc. 38th annual meet associ comput linguistics, pp. 440447. papineni, k., s. roukos, t. ward, w. zhu (2002), bleu: method automat evalua- tion machin translation, proceed 40th annual meet associ comput linguist (acl) 2002, pp. 311318. park, j., x. liu, m.j.f. gales, p.c. woodland (2010), improv neural network base languag model adaptation, proceed interspeech 2010. paul, m. (2006), overview iwslt 2006 evalu campaign, proceed iwslt06, pp. 115. roth, r., o. rambow, n. habash, m. diab, c. rudin (2008), arab morpholog tagging, diacritization, lemmat lexem model featur ranking, proceed associ comput linguist (acl), columbus, ohio. schwenk, h. (2007), continu space languag models, speech languag 21 (3), pp. 492518. schwenk, h. (2010), continuous-spac languag model statist machin translation, pragu bulletin mathemat linguist (93), pp. 137146. schwenk, h., m.r. costa-jussa, j.a.r. fonollosa (2006), continu space languag model iwslt 2006 task, proceed iwslt 2006, pp. 166173. 232 neural network languag model select best translat schwenk, h., m.r. costa-jussa, j.a.r. fonollosa (2007), smooth bilingu translation, pro- ceed empir method natur languag process (emnlp), prague, czech republic, pp. 430438. stolcke, a. (2002), srilm: extens languag model toolkit, proceed int. conf. spoken languag processing, pp. 901904. takezawa, t., e. sumita, f. sugaya, h. yamamoto, s. yamamoto (2002), broad- coverag bilingu corpu speech translat travel convers real world, pro- ceed lrec 2002, pp. 147152. xu, p. f. jelinek (2004), random forest languag modeling, proceed emnlp 2004, pp. 325332. zamora-martnez, f., m.j. castro-bleda, h. schwenk (2010), n-gram-bas machin translat enhanc neural network french-english btec-iwslt10 task, proceed seventh intern workshop spoken languag translat (iwslt), pp. 4552. zamora-martnez, f., m.j. castro-bleda, s. espana-boquera (2009), fast evalu connec- tionist languag models, cabestany, joan, francisco sandoval, alberto prieto, juan m. corchado, editors, proceed 10th intern work-confer artifici neural networks, iwann 2009, vol. 5517 lncs, springer, pp. 3340. zipf, g.k. (1949), human behavior principl least-effort, addison-wesley. 233 introduct upc n-gram-bas smt decod optim extend word reorder rescor translat score neural network languag model model architectur continu space lm experi differ nn lm schwenk' experi data data preprocess configur italian-to-english experi arabic-to-english experi perplex analysi analysi italian-to-english result analysi arabic-to-english result discuss conclus