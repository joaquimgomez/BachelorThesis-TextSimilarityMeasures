fast super-resolut dens local train invers regressor search eduardo prez-pellitero12, jordi salvador1, iban torres-xirau1, javier ruiz-hidalgo3, bodo rosenhahn2 1 technicolor r&i hannov 2 tnt lab, leibniz universitt hannov 3 imag process group, universitat politcnica catalunya abstract. regression-bas super-resolut (sr) address up- scale problem learn map function (i.e. regressor) low-resolut high-resolut manifold. local linear assumption, complex non-linear map properli model set linear regressor distribut manifold. meth- ods, test time spent search right regressor train set. paper propos novel inverse-search approach regression-bas sr. instead perform search imag dictionari regressors, search invers regressor dictionari imag patches. approxim framework appli spheric hash imag regres- sors, reduc invers search comput train function. additionally, propos improv train scheme sr linear re- gressor improv perceiv object quality. merg contribut improv speed qualiti compar state- of-the-art. 1 introduct super resolut (sr) compris reconstruct techniqu capabl ex- tend resolut discret signal limit correspond captur device. sr problem natur ill-posed, definit suit- abl prior critical. decades, proposed. originally, imag sr method base piecewis linear smooth prior (i.e. bilinear bicub interpolation, respectively), result fast interpolation-bas algorithms. tsai huang [1] show possibl reconstruct higher-resolut imag regist fuse multipl images, pioneer vast approach multi-imag sr, call reconstruction-bas sr. idea refined, others, introduct iter back-project improv registr irani peleg [2], analysi baker kanad [3] lin shum [4] show fundament limit type sr, mainli condit reg- istrat accuracy. learning-bas sr, known example-based, overcam ruben pocul texto escrito mquina ruben pocul texto escrito mquina prez-pellitero, e., salvador, j., torres-xirau, i., ruiz-hidalgo, j., & rosenhahn, b. (2015). fast super-resolut dens local train invers regressor search. d. cremers, i. reid, h. saito, & m.-h. yang (eds.), vision -- accv 2014 se - 23 (vol. 9005, pp. 346359). springer intern publishing. final public avail springer ruben pocul texto escrito mquina ruben pocul texto escrito mquina ruben pocul texto escrito mquina ruben pocul texto escrito mquina ruben pocul texto escrito mquina ruben pocul texto escrito mquina ruben pocul texto escrito mquina 2 prez-pellitero et al. aforement limit avoid necess registra- tion process build prior imag statistics. origin work freeman et al. [5] aim learn patch- feature-bas exampl produc effect magnif practic limit multi-imag sr. 1-nn (a) previou approach k-ann (b) propos approach fig. 1: overview propos inverse-search sr: (a) previou approach search 1st nearest dictionari atom imag patch. (b) propos approach search k-nearest imag patch dictionari atom. example-bas sr approach dictionari usual divid categories: intern extern dictionary-bas sr. exploit strong self-similar prior. prior learnt directli relationship imag patch differ scale input image. open work subcategori introduc glasner et al. [6], present power- ful framework fuse reconstruction-bas example-bas sr. research categori freedman fattal [7] introduc mechan high-frequ transfer base exampl small area patch, better local cross-scal self-similar prior spatial neighborhood. recent work yang et al. [8] develop idea local cross-scal self-similar prior arriv in-plac prior, i.e. best match scale locat exactli posit scale similar enough. extern dictionary-bas sr method us imag build dic- tionaries. repres wide approach base spars decomposition. main idea approach decomposit patch input imag combin spars subset entri compact dictionary. work yang et al. [9] us extern databas compos relat low high-resolut patch jointli learn compact dictionari pair. testing, imag patch decompos spars linear combin entri low-resolut (lr) dictionari weight gener high-resolut (hr) patch linear combin hr entries. dictionari train test fast super-resolut invers regressor search 3 costli l1 regular term enforc sparsity. work zeyd et al. [10] extend spars sr propos algorithm speed-up improv performance. however, bottleneck sparsiti method remain spars decomposition. recently, regression-bas sr receiv great deal attent research community. case, goal learn certain map manifold lr patch hr patches, follow manifold assumpt earlier work chang et al. [11]. map manifold assum local linear linear regressor anchor manifold piecewis linearization. method fastest state-of-the-art, search proper regressor take signific quota run time sr pipeline. paper introduc follow contributions: 1. propos train scheme notic improv qualiti linear regress sr (section 3.1) keep test complexity, i.e. increas test time. 2. formul inverse-search approach regressor dictionari k-nearest neighbor (k-nn) input imag featur (fig. 1). also, provid suitabl effici spheric hash framework exploit scheme, greatli improv speed littl qualiti cost. (section 3.2). merg contributions, improv speed qualiti fastest best-perform state-of-the-art methods, shown experiment results. 2 regression-bas sr section introduc sr problem example-bas approach tackl it, follow review recent state-of-the-art regress work timoft et al. [12] close relat work present paper. contribut paper follow section 3. 2.1 problem statement super-resolut aim upscal imag unsatisfactori pixel resolut preserv visual sharpness, formal x = (y ) s.t. x y, (1) y input image, x output upscal image, () upsam- pling oper calligraph font denot spectrum image. literatur transform usual model backward restor origin imag suffer degrad [9] 4 prez-pellitero et al. y = (b(x)), (2) b() blur filter () downsampl operator. problem usual address patch level, denot lower case (e.g. y, x). example-bas sr famili tackl super-resolut problem find meaning exampl hr counterpart known, coupl dictionari dl dh: min y dl 2 2 + p , (3) select weight element dictionari weight possibl lp-norm regular term. lp-norm select dictionary- build process depend chosen prior defin sr algorithm. 2.2 anchor neighborhood regress recent work timoft et al. [12] especi remark low- complex natur achiev order magnitud speed-up have competit qualiti result compar state-of-the-art. propos relax l1-norm regular commonli neighbor embed (ne) spars code (sc) approaches, reformul prob- lem squar (ls) l2-norm regular regression, known ridg regression. solv l1-norm constrain minim problem com- putation demanding, relax l2-norm, closed-form solut used. propos minim problem read min yf nl 2 2 + 2 , (4) nl lr neighborhood chosen solv problem yf featur extract lr patch. algebra solut = (ntl nl + i) 1ntl yf . (5) coeffici appli correspond hr neighborhood nh reconstruct hr patch, i.e. x = nh. written matrix multipl x = r yf , project matrix (i.e. regressor) calcul r = nh(n t l nl + i) 1ntl (6) comput offline, move minim problem test train time. propos us spars dictionari ds atom size, train k-svd algorithm [13]. regressor rj anchor atom dj dl, neighborhood nl equat (6) select k-nn subset dl: fast super-resolut invers regressor search 5 nlj = knn(dj , dl). (7) sr problem address find nn atom dj input patch featur yif appli associ rj it. specif case neighborhood size k = ds, gener regressor obtain neighborhood compris atom dictionari consequ requir nn search. case refer origin paper global regress (gr). 2.3 linear regress framework closest anchor point found, regress usual appli certain input featur aim recov certain compon patch. model linear regress framework gener wai x = x+r yf , (8) x coars approxim hr patch x. choic obtain x requir select prior better approxim x. work yang et al. [8] us in-plac prior first-approxim timoft et al. [12] us bicub interpol assum smooth prior. regressor train improv reconstruct coars prior sufficient. intuitively, optim performance, select featur rep- resent relat chosen approxim x. support this, [8] us input featur subtract low-pass filter in-plac exampl bicub interpolation, intuit model error in- place prior low-frequ band; [12] us gradient-bas features, repres high-frequ compon like go well-reconstruct bicub interpolation. 3 fast hashing-bas super-resolut section present super-resolut algorithm base inverse- search scheme. section divid part repres contribu- tion paper: discuss optim train stage linear super- resolut regressor introduc hashing-bas regressor select scheme. 3.1 train regression-bas sr object train given regressor r obtain certain map function lr hr patches. gener perspec- tive, lr patch form input manifoldm dimensionm hr patch form target manifold n dimens n. formally, train pair (yfi, xi) yf m xi n , like infer map :m rm n rn. 6 prez-pellitero et al. 0 200 400 600 800 1000 1200 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 neighborhood size m e n e u cl id e n d ta n ce train patch spars dictionari (a) 0 200 400 600 800 1000 1200 1400 1600 1800 29.5 30 30.5 31 31.5 32 32.5 neighborhood size p s n r ( d b ) propos train timoft train (b) fig. 2: (a) mean euclidean distanc atom neighborhood differ neighborhood sizes. (b) qualiti improv measur psnr (db) reconstruct anr [12] propos training. 1024 anchor point experiment. previous seen, recent regression-bas sr us linear regressor easili comput close form appli matrix multiplication. however, map highli complex non-linear [14]. model non-linear natur mapping, ensembl regressor {ri} trained, repres local linear parametr , assumpt manifold m n similar local geometry. analyz effect distribut regressor manifold (i.e. anchor points) import properli choos nl equat (6), conclud new train approach. work timoft et al. [12], overcomplet spars represent obtain initi lr train patch k-svd [13]. new re- duce dictionarydl anchor point manifold datapoint regress training. gr, uniqu regressor rg train element dictionary, accept higher regress error singl linear manifold. fine-tun regress re- construct propos anchor neighborhood regress (anr), us anchor point {a1, . . . , ads} dictionari point {d1, . . . , dds} build atom neighborhood k-nn spars dictionari dl. perform spars decomposit high number patch effici compress data smaller dictionary, yield atom repre- sent train dataset, i.e. manifold. reason suitabl anchor points, sub-optim neigh- borhood embedding. sub-optim necessari local condit linear assumpt like violated. l1-norm recon- struction minim impos spars dictionaries, atom dictionari close euclidean space, shown fig. 2(a). fast super-resolut invers regressor search 7 1 0 1 1 0 1 (a) 1 0 1 1 0 1 (b) 1 0 1 1 0 1 (c) 1 0 1 1 0 1 (d) fig. 3: normal degre 3 polynomi manifold illustr propos ap- proach compar [12]. (a) bidimension manifold samples. (b) manifold (blue) spars represent obtain k-svd algorithm (green) 8 atoms. (c) linear regressor (red) train neighborhood (k = 1) obtain spars dictionary, [12]. (d) linear regressor (red) obtain propos approach: neighborhood obtain sampl manifold (k = 10). observ lead propos differ approach train lin- ear regressor sr: spars represent anchor point man- ifold, form neighborhood raw manifold sampl (e.g. features, patches). fig. 2(a) how, so, closer nearest neighbor and, therefore, fulfil better local condition. additionally, higher number local independ measur avail (e.g. mean distanc 1000 neigh- bor raw-patch approach compar 40 atom neighborhood spars approach) control number k-nn selected, i.e. upper-bound dictionari size. low-dimension exampl propos train scheme fig. 3. 10 2 10 3 10 4 10 5 10 6 0 0.05 0.1 0.15 0.2 0.25 numbercofcqueri r u n n g ct im e sc (s ) parallelc(gpu) parallelc(cpu) singlecthread fig. 4: run time measur comput 6-bit hash code (6 hyper- spheres) increas number queri (in logarithm axi re- ranking) single-thread (cpu) parallel (cpu gpu) implementa- tions. fig. 2(b) comparison anr [12] train ap- proach term result imag psnr. us train dataset origin paper, neighbor embed us l2-normal 8 prez-pellitero et al. raw featur introduc k-svd algorithm. fix dictio- nari size (onli anchor point scheme) 1024. appli train scheme achiev substanti qualiti improvements, qualit quantitatively. 3.2 invers search spheric hash aim fine model map lr hr manifolds, linear regressor train better repres non-linear problem. state-of-th art regression-bas sr push for- ward comput speed regard dictionary-bas sr [10, 9], find right regressor patch consum execut time. work [12], encod time (i.e. time left subtract- ing share process time, includ bicub interpolations, patch extractions, etc.) spent task (i.e. 96% time). {rn}r1 r2 r3 r4 r5 r6 r7 0 1 0 1 1 1 0 0 1 0 1 0 0 1 ... 0 1 0 0 1 1 0 0 1 fig. 5: spheric hash appli invers search super-resolut problem. certain hash function optim featur patch statist creat set hyperspher intersect directli label hash code. train time, regressor intersect (i.e. bins) test time hash function appli patch, directli map regressor. second contribut paper novel search strategi design benefit train outcom present section 3.1, i.e. anchor point dictionari neighborhood obtain independ ahead search structure. order improv search efficiency, search structur sublinear com- plexiti built, usual form binari splits, e.g. trees, hash scheme [1518]. consid determin search partit set anchor points, element retrieve. however, small cardin set lead imprecis partit shortag sampl density. propos invers search scheme consist find k-ann (approxim nearest neighbor) patch imag anchor point, shown fig. 1. so, dens sampl (i.e. train patches) disposal, result meaning partitions. fast super-resolut invers regressor search 9 choos hash techniqu tree-bas methods. hash scheme provid low memori usag (the number split function hashing-bas structur o(log2(n)) tree-bas structur o(n), n repre- sent number clusters) highli parallelizable. binari hash techniqu aim emb high-dimension point binari codes, provid compact represent high-dimension data. vast rang applications, effici similar search, includ approxim nearest neighbor retrieval, hash code preserv rel distances. recent activ research data-depend hash function oppos hash method [17] data- independent. data-depend method intend better fit hash function data distribut [19, 18] off-lin train stage. data-depend state-of-the-art methods, select spheric hash algorithm heo et al. [16], abl defin close region rm split function. hash framework us model invers search scheme enabl benefit substanti speed- up reduc nn search appli precomput function, conveni scale parallel implementations, shown fig. 4. spheric hash differ previou approach set hyperspher defin hash function behalf previous hyperplanes. given hash function h(yf ) = (h1(yf ), . . . , hc(yf )) map point rm base 2 nc, i.e. {0, 1}c. hash function hk(yf ) indic point yf insid kth hypersphere, model purpos pivot pk rm distanc threshold (i.e. radiu hypersphere) tk r+ as: hk(yf ) = { 0 d(pk, yf ) > tk 1 d(dk, yf ) tk , (9) d(pk, yf ) denot distanc metric (e.g. euclidean distance) point rm. advantag hyperspher instead hyperplan abil defin close tighter sub-spac rm intersect hyperspheres. iter optim train process propos [16] obtain set {pk, tk}, aim balanc partit train data independ hash functions. perform mention iter hashing-funct optim set input patch featur train images, h(yf ) adapt natur imag distribut featur space. propos spheric hash search scheme symmetr fig. 5, i.e. imag anchor point label binari codes. intuit understood creat nn subspac group (we refer bins), label regressor appli hash function anchor points. relat hash code regressor train time. invers search approach return k-nn anchor point, ensur input imag patch relat regressor (i.e. patch k-nn anchor points). solut proposed: (a) us gener regressor patch k-nn 10 prez-pellitero et al. anchor point (b) us regressor closest label hash code calcu- late spheric ham distance, defin [16] dsh(a, b) = (ab) (ab) , xor bit oper bit operation. note guaranteed, rare happen patch k-nn regressor (e.g. select paramet 6 hyperspher occurs). observ signific differ performance, select (a) lowest complex solution, test (b) due. similar way, invers search assign regressor singl patch. common literatur re-rank strategi deal issu [20]. tabl 1: perform 3 4 magnif term averag psnr (db) averag execut time (s) dataset set14, kodak 2k. bicub spars [10] gr [12] anr [12] ne+l ne+nnl ne+ll propos mf psnr time psnr time psnr time psnr time psnr time psnr time psnr time psnr time set14 3 27.54 0.002 28.67 2.981 28.31 0.528 28.65 0.771 28.59 2.854 28.44 25.372 28.60 4.356 28.93 0.188 4 26.00 0.003 26.88 1.862 26.60 0.458 26.85 0.584 26.81 1.716 26.72 14.146 26.81 2.623 27.04 0.184 kodak 3 28.43 0.003 29.22 5.126 28.98 0.921 29.21 1.335 29.17 4.829 29.04 44.102 29.17 7.353 29.42 0.314 4 27.23 0.003 27.83 3.194 27.64 0.757 27.80 1.022 27.77 3.003 27.71 24.428 27.77 4.678 27.92 0.309 2k 3 31.73 0.007 32.63 27.622 32.45 4.860 32.68 7.123 32.62 26.194 32.51 242.875 32.65 40.389 32.88 1.652 4 30.28 0.006 30.97 17.225 30.81 3.968 30.99 5.344 30.94 16.363 30.87 136.058 30.96 25.967 31.04 1.578 4 result section experiment result propos method com- pare perform term qualiti execut time state-of-the- art recent methods. perform extens experi imag resolut rang 2.5kpixel 2mpixels, show perform classic litera- ture test imag addition demonstr algorithm perform current upscal scenarios. extend benchmark [12] ad set5 set14 datasets: 24 imag kodak dataset 2k, imag set 9 sharp imag obtain internet pixel resolut 1920x1080. experi run intel xeon w3690 3.47ghz code compar method obtain [12] rec- ommend parameters. method compar spars code sr zeyd et al [10], implement ls regress chang et al. [11] (ne + lle), non-neg squar (ne+nnls) method bevilaqcua et al. [21]. propos algorithm written matlab time-consum stage implement opencl emphasi optimization, fast super-resolut invers regressor search 11 tabl 2: perform 3 4 magnif term psnr (db) execut time (s) set5 dataset. set5 bicub spars [10] gr [12] anr [12] ne+l ne+nnl ne+ll propos imag mf psnr time psnr time psnr time psnr time psnr time psnr time psnr time psnr time babi 3 33.9 0.000 35.1 3.490 34.9 0.662 35.1 0.905 35.0 3.179 34.8 29.377 35.1 5.042 35.1 0.214 bird 3 32.6 0.000 34.6 1.087 33.9 0.242 34.6 0.293 34.4 1.011 34.3 9.449 34.6 1.533 34.9 0.070 butterfli 3 24.0 0.000 25.9 0.839 25.0 0.152 25.9 0.201 25.8 0.766 25.6 6.947 25.8 1.200 26.6 0.058 head 3 32.9 0.000 33.6 1.011 33.5 0.218 33.6 0.270 33.5 0.908 33.5 8.411 33.6 1.395 33.7 0.068 woman 3 28.6 0.000 30.4 0.972 29.7 0.187 30.3 0.249 30.2 0.909 29.9 8.437 30.2 1.390 30.06 0.067 averag 3 30.39 0.000 31.90 1.480 31.41 0.292 31.92 0.384 31.78 1.354 31.60 12.524 31.84 2.112 32.22 0.095 babi 4 31.8 0.000 33.1 2.136 32.8 0.525 33.0 0.652 32.9 2.033 32.8 15.535 33.0 3.128 32.9 0.256 bird 4 30.2 0.000 31.7 0.660 31.3 0.184 31.8 0.226 31.6 0.611 31.5 4.995 31.7 0.955 31.7 0.066 butterfli 4 22.1 0.000 23.6 0.536 23.1 0.138 23.5 0.165 23.4 0.456 23.3 3.882 23.4 0.730 23.7 0.052 head 4 31.6 0.000 32.2 0.582 32.1 0.135 32.3 0.212 32.2 0.567 32.1 4.587 32.2 0.882 32.3 0.061 woman 4 26.5 0.000 27.9 0.576 27.4 0.174 27.8 0.191 27.6 0.583 27.6 4.455 27.7 0.894 28.0 0.063 averag 4 28.42 0.000 29.69 0.898 29.34 0.231 29.69 0.289 29.55 0.850 29.47 6,691 29,61 1.318 29.73 0.100 run cpu platform methods. experi us k-svd spars dictionari 1024 compar methods. select bicub coars approxim x limit upscal step super-resolut (e.g. in-plac exampl meaning small magnif factors) featur zeyd et al. [10, 12] compos 1st 2nd order deriv filter compress pca truncat featur conserv 99.9% energy. us l2-norm regular linear regressor illustr equat (4). build regressor scheme propos timoft et al. [12]. 6-bit spheric hash (6 hyperspheres) chosen neighborhood 1300 k-nn. select number sphere trade-off qualiti speed, decreas number hyperspher collis regressor (i.e. regressor arriv bin) re-rank process closer exact nearest neighbor search. seen fig. 7. tabl 1 tabl 2 object result perform term psnr (db) execut time (s). measures, propos algorithm best performing. improv psnr notic magnif factor 3, reach improv 0.3 db compar second best-performer. term run time, algorithm consist speed-up dataset scales. compar gr (which fastest compar methods), speed-up rang 2 3 , addition gap qualiti reconstruction. speed-up anr rang 3 4 rest methods, run time order magnitud slower. note theoret complex gr lower method perform nn search (i.e. similar implementations, gr slightli faster). nevertheless, parallel implement effici provid author [12], reli optim matlab matrix multiplication. 12 prez-pellitero et al. fig. 6: visual qualit assess 3 magnif factor imag differ datasets. left right bottom: original, bicub inter- polation, global regressor [12], zeyd et al. [10], anr [12] propos sr. better view zoom in. fast super-resolut invers regressor search 13 2 3 4 5 6 7 8 32.82 32.84 32.86 32.88 32.9 32.92 32.94 # sphere p s n r ( d b ) (a) 2 3 4 5 6 7 8 1.6 1.65 1.7 1.75 1.8 1.85 1.9 1.95 2 # sphere ti m e ( s ) (b) fig. 7: effect number sphere select term psnr (a) time (b). fig. 6 visual qualit assess performed. method obtain natur sharp edges, strongli reduc ringing. good exampl shown butterfli image. 5 conclus paper present main contributions: improv train stage effici inverse-search approach regression-bas and, generally, dictionary-bas sr. spheric hash techniqu appli order exploit benefit inverse-search scheme. obtain qualiti improv optim train stage substanti speed-up low-complex spheric hash similar algorithm regressor selection. exhaust test perform compar method dataset pixel resolutions, differ upscal factor state-of-the-art methods. experiment result consist improv psnr run time state-of-the-art method includ benchmark, posit measures. refer 1. tsai, r., huang, t.: multipl frame imag restor registration. in: proc. advanc vision imag processing. volum 1. (1984) 317339 2. irani, m., peleg, s.: improv resolut imag registration. cvgip: graphic model imag process 53 (1991) 231239 3. baker, s., kanade, t.: limit super-resolut break them. ieee trans. pattern analysi machin intellig 24 (2002) 11671183 14 prez-pellitero et al. 4. lin, z., shum, h.y.: fundament limit reconstruction-bas superresolut algorithm local translation. ieee trans. pattern analysi machin intellig 26 (2004) 8397 5. freeman, w., jones, t., pasztor, e.: example-bas super-resolution. ieee trans. graphic applic 22 (2002) 5665 6. glasner, d., bagon, s., irani, m.: super-resolut singl image. in: proc. ieee int. confer vision. (2009) 7. freedman, g., fattal, r.: imag video upscal local self-examples. acm trans. graphic 30 (2011) 12:112:11 8. yang, j., lin, z., cohen, s.: fast imag super-resolut base in-plac ex- ampl regression. in: proc. ieee confer vision pattern recognition. (2013) 9. yang, j., wright, j., t.s., h., ma, y.: imag super-resolut spars represen- tation. ieee trans. imag process 19 (2010) 28612873 10. zeyde, r., elad, m., protter, m.: singl imag scale-up sparse- representations. in: proc. int. confer curv surfaces. (2012) 11. chang, h., yeung, d.y., xiong, y.: super-resolut neighbor embedding. (2004) 12. timofte, r., smet, v.d., goool, l.v.: anchor neighborhood regress fast example-bas super-resolution. in: proc. ieee int. confer vision. (2013) 13. aharon, m., elad, m., bruckstein, a.: k-svd: algorithm design over- complet dictionari spars representation. ieee trans. signal process 54 (2006) 14. peyr, g.: manifold model signal images. vision imag understand 113 (2009) 249260 15. breiman, l.: random forests. machin learn 45 (2001) 532 16. heo, j.p., lee, y., he, j., chang, s.f., yoon, s.e.: spheric hashing. in: proc. ieee conf. vision pattern recognition. (2012) 17. indyk, p., motwani, r.: approxim nearest neighbors: remov curs dimensionality. in: proc. thirtieth annual acm symposium theori computing. stoc 98 (1998) 604613 18. wang, j., kumar, s., chang, s.f.: semi-supervis hash scalabl imag retrieval. (2010) 19. weiss, y., torralba, a., fergus, r.: spectral hashing. (2008) 20. he, k., sun, j.: comput nearest-neighbor field propagation-assist kd- trees. in: proc. ieee conf. vision pattern recognition. (2012) 21. bevilacqua, m., roumy, a., guillemot, c., alberi-morel, m.l.: low-complex single-imag super-resolut base nonneg neighbor embedding. in: proc. british machin vision conf. (2012) 110