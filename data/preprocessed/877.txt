improv svm classif imbalanc dataset introduc new bia haydemar nunez central univers venezuela, venezuela lui gonzalez-abril universidad sevilla, espana cecilio angulo technic univers catalonia, spain 22, 2017 abstract support vector machin (svm) learn imbalanc datasets, learn machines, show poor perform minor class svm design induc model base overal error. improv perform kind problems, low-cost post-process strategi propos base calcul new bia adjust function learn svm. propos bia consid proport size class order improv perform minor class. solut avoid introduc tune new parameters, mod- ifi standard optim problem svm training. experiment result 34 datasets, differ imbal de- gree, propos method actual improv classifi- cation imbalanc datasets, standard error measur base sensit g-means. furthermore, perform compar well-known cost-sensit smote schemes, with- ad complex comput costs. keywords: support vector machine, post-processing, bias, cost- sensit strategy, smote 1 1. introduct major problem face classif learn algorithm imbal class datasets. appear ex- ampl classes, remain classes. domain situat aris medic diagnosis, text classification, fraud detect credit card usage, detect commun network intrusion, others. usual repres target classi- ficat task, scenario import obtain model ex- hibit high predict perform minor class. however, standard learn algorithm tend produc hypothesi have good perform major class, construct classif model base error train set, independ repres balanc classes. solv problem, mechan exist allow algo- rithm show good perform minor class. effect, strategi proposed, re-balanc dataset sam- pling techniques, construct classifi account cost error differ classes, combin (ensemble) result sev- eral classifi train differ data distribut (he garcia 2009; lopez, fernandez, garca, palade, herrera 2013; sun, wong, kamel 2009). case svm, learn mechan interest option deal imbalanc datasets, svm build classif model base subset train instanc (cristianini shawe- taylor 2000; vapnik 1999). however, like machin learn techniques, svm minim error dataset gener models, bias major class imbal severe. enhanc perform svm problem imbalanc classes, solut proposed. gen- eral application, like sampl techniqu re-balanc dataset pre- process stage; other, specific, consid svm particular featur like base cost-sensit learn (batuwita palad 2013). re- search paper suggest post-process stage order reduc bia major class classifi learn svm (he garcia 2009). 2 follow research line, strategi svm base cal- culat new bia threshold proposed. new bia consid class proport dataset allow tune origin function learn svm improv perform minor class. pro- pose solut introduc new parameters, modifi origin optim problem svm training. paper organ follows: section 2 briefli introduc svm learn mechan provid overview strategi improv perform kind problems. section 3, propos post- process procedur determin new bia detailed. section 4, present experi perform verifi applic proposal, analysi results, comparison perform new approach cost-sensit scheme. finally, conclus research presented. 2. svm imbalanc dataset svm base statist learn theori appli suc- cessfulli classif regress problem differ domain (cris- tianini shawe-taylor 2000; oneto, ridella, anguita 2016; vapnik 1999). hypothesi space learn machin hyperplan (linear decis surfaces). train look decis function maximum margin separ classes. thus, binari clas- sific task set train data z = {(x1, y1), . . . , (xn , yn)}, xi x <m, yi y = {+1,1}, decis function f(x) = w xb, optim hyperplan determin follows, min w,b 1 2 w2 + c n i=1 s. t. { yi(w xi b) + 1, 0, = 1 . . . n (1) w vector hyperplan defin orientation, b bia determin position. slack variabl measur error instanc violat constraint yi(w xi b) 1. user-defin paramet c determin trade-off maxim 3 margin minim error, i.e. higher valu c, svm focus minim errors. dual form, optim problem solv as, max i< n i=1 1 2 n i,k=1 iyikyk xi xk s.t. 0 c, = 1 . . . n n i=1 iyi = 0, (2) lead follow decis function, f(x) = sign ( n i=1 iyi xi x b ) . (3) construct nonlinear decis boundaries, input vector pro- ject inner product space higher dimens basi set nonlinear functions. new space optim hyperplan determined. theori kernel satisfi mercer theorem, oper perform directli input space xi xj = k(xi,xj). then, decis function formul as, f(x) = sign ( n i=1 iyik(xi,xj) b ) . (4) train vectors, associ weight greater zero (3) (4). element lie decis margin known support vector (sv). unsign valu f(x) measur distanc exampl x hyperplane, sign determin class label (posit negative). moder imbalanc datasets, empir result that, un- like machin learn algorithms, svm produc good hypothesi modif (akbani, kwek, japkowicz 2004; imam, ting, kamruzzaman 2006; wu chang 2005). explan phenomenon svm us set support vector construct clas- sific models, neg instanc far decis border taken account svm affect them, 4 numer ones. however, svm overcom problem imbal- anc data distribut imbalanced. cases, observ hyperplan separ learn svm close minor class, result low perform gener exampl class, comparison major class (batuwita palad 2013; ghodsi 2010; liu, an, huang 2006, wu chang 2005). 2.1 strategi svm imbalanc dataset strategi propos improv perform svm imbalanc datasets. describ introduc section accord moment appli learn process. 2.1.1 pre-process strategi base re-sampl techniqu balanc dataset. wai over-sampl data minor class; hence, new instanc creat order increas proport dataset. contrast, under-sampl seek reduc size major class remov subset data. general-purpos procedures, target particular machin learn technique. commonli smote, emploi k-nearest neighbor techniqu over-sampl minor class (chawla, bowyer, hall, kegelmey 2002; vilarino, spyridonos, vitria, radeva 2005). strategi appli cluster algorithm sub-sampl major class (li, yu, bi huang 2014; yu, debenham, jan, simoff 2006; zhou, ha, wang 2010). strategi svm seek increas minor class consid margin area class (castro, carvalho, braga 2009). work base us svm obtain posit support vectors, over-sampl data (hernandez- santiago, cervantes, lopez-chau, garca-lamont; wang 2008). featur exploit build under-sampl algorithm (tang, zhang, chawla, krass 2009; wang 2014), svm build new dataset compos inform neg support vector posit data. solut us sampl method 5 ensembl (kang cho 2006; liu et al. 2006; waske, benediktsson, sveinsso 2009; yang, zhang, zhou, zomaya 2011; sukhanov, merentitis, debes, hahn zoubir 2015). furthermore, propos seek over-sampl train activ learn (ertekin 2013). 2.1.2 train strategi strategi includ propos modifi standard op- timiz problem svm train order incorpor inform relat proport class dataset. approach cost- sensit learn incorpor learn problem inform relat penalti associ wrong predict class. case svm, cost inform type error introduc formul learn problem, regular- izat parameters, c+ c, associ error posit neg class, respect (ver-opoulos, campbell, cristianini 1999; cohen, hilario, sax, hugonnet, geissbuhl 2006), min w,b 1 2 w2 + c+ i|yi=+1 + c i|yi=1 s. t. { yi(w xi b) + 1, 0, = 1 . . . n (5) work ad new restrict slack variabl i, order control margin separ class (he ghodsi 2010; yang, wang, yang, yu 2008). differ approach present batuwita palad (2010), regular paramet c used, inform cost error incorpor alloc differ weight variabl i. propos solut combin cost-sensit learn techniqu (akbani et al. 2004; muscat, mahfouf, zughrat, yang, thornton, khondabiand, sortano 2014; wang japkowicz, 2010; zieba, tomczak, lubicz, swiatek 2014). propos modifi kernel matrix accord observ imbal distribut data, kba algorithm (wu chang 2005). ramrez allend (2012) method propos train one-class svm, fit class, aggreg decis nest manner boundari improved. finally, 6 he, wu, silva, zhao, qian (2015), model-bas approach integrat- ing cost-sensit learn gaussian mixtur model imbalanc classif problem proposed. 2.1.3 post-process strategi general, approach orient either, modifi weight vector w function decis determin new bias, order adjust decis boundari learn svm provid good margin separ posit class. example, z-svm method propos imam et al. (2006), determin valu new paramet z, solv ad optim problem. optim paramet weight contribut support vector minor class vector w decis function obtain training. li, hu, hirasawa (2008), bia decis function modifi calcul offset averag unsign valu gener f(x) support vectors. similar strategi shanahan roma (2003), new offset calcul appli beta-gamma algorithm. studi suggest re-interpret output svm. example, fuzzi decis function appli li et al. (2008), paramet estim observ distribut dataset. wang zheng (2008), decis process incorpor post-process module, construct base method inform theori defin new bia classification. 3. novel post-process strategi base bia strategi propos improv perform svm imbal- anc dataset gener requir tune new paramet sampl rate number k select neighbors. method computa- tional expens consid construct classifi (such method base ensembles), base iter algorithms, mod- ifi kernel matrix (kba) sampl techniqu requir over-train steps. cost-sensit approaches, standard svm optim problem modifi cost error class 7 known. moreover, produc over-fit model (wang japkowicz 2010). hand, empir shown hyperplan learn svm presenc imbalanc dataset approxim orient ideal hyperplan (he ghodsi 2010; liu et al. 2006; wu chang 2005). reduc gener minor class associ bia b, posit instanc lie far ideal limit, i.e., svm learn boundari close class. studies, present sun, lim, liu (2009) domain text classification, suggest increas research strategi determin new threshold svm decis function. modif base distribut class dataset, directli affect standard svm training. follow research line, novel post-process strategi base calcul new bia propos paper. proport class dataset considered, adjust function learn svm order improv perform minor class. propos solut involv tune new parameters. fur- thermore, requir modifi standard optim problem train svm, addit step re-training. proposal, base develop present gonzalez-abril, angulo, velasco, ortega (2008), modifies, training, separ margin hyperplan major class order achiev better gener perform data minor class. new bia calcul follow (nunez, gonzalez-abril, angulo 2011). let z = {(x1, y1), . . . , (xn , yn)} train set, xi x <m, yi y = {+1,1}. also, let z1 z2 dataset belong posit class (+) neg (-), respectively. standard formul bias, linearli separ case indic bia obtain as, bs = + 2 (6) (b = bstandard) maximum valu hyperplan bia appli set neg instanc z2, minimum valu hyperplan bia appli entri minor set z1, 8 is, = max xkz2 n i=1 ik(xi,xk), = min xkz1 n i=1 ik(xi,xk). (7) let indic bia b chosen b = , instanc posit class correctli labeled. furthermore, smallest valu ensur 100% correct classif train vector (gonzalez- abril, nunez, angulo, velasco 2014). definit bs (6) extend take account proport class dataset. hence, n1 n2 number pattern class (+) (-) respectively, new proport bia bp defined, bp = n1 +n2 n1 +n2 . (8) hence, imbalanc problems, n1 n2, new bia decis limit neg class, increas margin separ posit class. moreover, maximum minimum valu hyperplan bia reach support vectors, consid point calcul , = max xksv2 n i=1 ik(xi,xk), = min xksv1 n i=1 ik(xi,xk) (9) sv1 sv2 set support vector class (+) (-), respectively. new decis function simpli express follows, f(x) = sign ( n i=1 iyik(xi,xj) b ) . (10) furthermore, svm decis function gener support vector (the inform instanc classif task), addit modif propos exists: consid number support vector posit neg classes, nsv1 nsv2, respectively, valu n1 n2. hence, second bia bp1 proposed, bp1 = nsv1 +nsv2 nsv1 +nsv2 . (11) 9 let relationship bias n1 n2. case, optim problem usual provid number support vector nsv1 < nsv2 and, defin r = n2/n1 r = nsv2/nsv1, 1 r r (thi fact check section 4). hence, results, | bp| = 1 1 +r | | , | bp1| = 1 1 + r | | , | bs| = 1 2 | | , (12) 0 | bp| | bp1| | bs| (13) is, bs farther bp1, turn, farther awai bp. thus, check decis function move awai zone posit samples, increasing, later demonstrated, accuraci class. therefore, new bias hyperplan learn svm obtain better classif perform posit class, consid proport classes: greater imbalance, greater margin separ minor class. 4. experiment result analysi perform post-process strategi propos test 34 dataset uci repositori (frank asuncion 2010). characterist dataset shown tabl 1. label (+) assign class shown brackets, label (-) remain data. perform classifi obtain new bias measur sensit geometr mean (g-means) (he garcia, 2009). sensit measur posit accuracy, indic exampl minor class correctli classified; g-mean evalu perform term sensit specif (neg accuracy) follows, g-mean = sensit specif (14) 10 tabl 1: uci dataset experimentation. dataset order extrem moder imbalance. dataset number % dataset number % instanc posit posit instanc posit posit winequ white (3) 4868 20 0.41 user model (1) 258 24 9.30 abalon (19) 4177 32 0.77 sat (4) 4435 415 9.36 winequ red (8) 1593 18 1.18 satimag (4) 6435 626 9.70 page-block (5) 5473 115 2.10 euthyroid 2000 238 11.90 yeast (7) 1483 35 2.36 glass (7) 214 29 13.55 thyroid (1) 3772 93 2.47 segment (1) 2310 330 14.29 nursei (3) 12960 328 2.53 hepat 129 24 18.60 fault (5) 1941 55 2.83 column 310 60 19.35 winequ white (4) 4864 163 3.35 cmc (2) 1473 333 22.61 yeast (5) 1483 51 3.44 dna 2000 464 23.20 mun (3) 8124 292 3.59 vehicl (1) 846 199 23.52 letter (a) 20000 789 3.95 transfus 748 178 23.80 car (3) 1728 69 3.99 haberman 306 81 26.50 derma (2) 358 21 5.87 german 1000 300 30.00 ecoli (5) 336 20 5.95 waveform (0) 5000 1657 33.00 balance(2) 625 49 7.24 pima 768 268 34.00 gtc 2126 176 8.28 tictac (2) 958 332 34.66 sensit allow posit class classifi g-mean show balanc accuraci posit neg- ativ classes. also, accuraci included, standard metric. svm training, usual rbf kernel used, matlab bioinformat toolbox processing. valu (rbf width) c (regular term) obtain explor two-dimension grid: = {20, 21, . . . , 26}, c = {20, 21, . . . , 210} best valu accuraci classifi (svm, cost-sensit svm smote svm) used. averag valu accuracy, g-mean sensit shown tabl 2, dataset, ten-fold cross-valid like empir exper- iment repeat procedur 10 time order ensur good statist behavior. result statement established: work imbalanc datasets, evalu metric like g-mean sensit measur classifi perform independ data distribution, elect correct kind problems. example, svm accuraci valu 0.99 abalon dataset. however, complet fail classifi posit class, re- flect valu g-means. 11 tabl 2: averag valu accuracy, g-mean sensit dataset, ten-fold cross-valid (100 replications). dataset accuraci g-mean sensit svm bp bp1 svm bp bp1 svm bp bp1 wineq white (3) .996 .968 .969 .120 .449 .449 .038 .235 .235 abalon (19) .992 .491 .619 .000 .625 .552 .000 .828 .592 wineq red (8) .989 .506 .813 .000 .603 .296 .000 .835 .280 page-block (5) .984 .761 .892 .534 .858 .803 .296 .974 .821 yeast (7) .978 .649 .961 .300 .749 .831 .143 .891 .723 thyroid (1) .992 .928 .992 .851 .95 .886 .733 .976 .794 nursei (3) .993 .981 .984 .878 .983 .602 .774 .986 .37 fault (5) .955 .938 .951 .323 .594 .432 .166 .417 .247 wineq white 4) .963 .764 .921 .336 .621 .568 .128 .513 .353 yeast (5) .965 .464 .953 .000 .628 .623 .000 .907 .453 mun (3) .954 .953 .954 .572 .903 .587 .349 .858 .366 letter (a) .998 .994 .998 .975 .995 .994 .951 .996 .991 car (3) .960 .973 .968 .000 .941 .595 .000 .911 .39 derma (2) .968 .951 .957 .923 .944 .935 .864 .936 .906 ecoli (5) .986 .968 .974 .851 .950 .902 .775 .945 .855 balance(2) .921 .774 .845 .749 .830 .851 .623 .909 .867 gtc .980 .882 .975 .912 .920 .950 .804 .970 .924 user model (1) .979 .981 .985 .885 .968 .971 .791 .956 .956 sat (4) .948 .868 .943 .787 .894 .837 .634 .929 .727 satimag (4) .945 .911 .942 .811 .889 .83 .678 .865 .716 euthyroid .907 .761 .889 .703 .809 .809 .517 .887 .727 glass (7) .951 .879 .953 .855 .898 .868 .768 .935 .792 segment (1) .996 .993 .996 .988 .994 .991 .977 .995 .984 hepat .852 .721 .741 .643 .732 .744 .562 .855 .84 column .867 .872 .868 .758 .873 .714 .645 .883 .57 cmc(2) .759 .630 .714 .527 .618 .594 .320 .607 .452 dna .967 .949 .957 .948 .957 .96 .914 .972 .967 vehicl (1) .986 .983 .985 .982 .985 .985 .975 .988 .986 transfus .779 .748 .778 .540 .614 .557 .327 .462 .350 haberman .719 .634 .661 .464 .617 .609 .273 .606 .512 german .762 .623 .708 .667 .660 .695 .517 .804 .680 waveform (0) .897 .873 .895 .877 .884 .863 .824 .921 .783 pima diabet .755 .742 .753 .672 .725 .676 .518 .687 .530 tictac (2) .983 .997 .997 .974 .996 .997 .950 .995 .998 12 datasets, despit imbalance, origin svm reason model (e.g. ecoli); cases, fail (abalone, winequality, yeast). new bias improv perform standard svm dataset respect g-mean sensit metrics. furthermore, perform sensit metric bp bia better emploi bp1, dataset tic-tac dataset. fact tic-tac dataset uniqu 34 dataset r r true. compar perform post-process strategi report literature, both, smote cost-sensit scheme train svm list uci datasets. comparison bp bias. matlab bioinformat toolbox provid cost- sensit scheme valu c+ c (5) calcul c as: c+ = c n 2n1 , c = c n 2n2 . (15) worth note above, c = c +n1+c n2 n1+n2 , is, similar formula bia bp chang c + c , respectively. result obtain evalu metrics, ten-fold cross-valid structure, shown tabl 3. moreover, comparison proport support vector learn decis function relat number train data offer scheme tabl 4. ratio measur complex svm classifier. therefore, conclud both, smote cost sen- sitiv approach provid decis function complex hence, aforementioned, produc over-fit models. order measur similar result schemes, friedman test appli (demser 2006). non-parametr test detect signific differ multipl classifiers. obtain p-valu accuracy, g-mean sensibl 0.1076, 0.0150 0.0083, respectively. results, confid level fix 5%, conclud following: accuraci measure, long p-valu friedman test 0.1076, conclud signific evid equival methods. 13 tabl 3: comparison novel method vs cost-sensit smote approaches. averag valu accuracy, g-mean sensit dataset, ten-fold cross-validation. dataset accuraci g-mean sensit bp cost smote bp cost smote bp cost smote wineq white (3) .968 .950 .963 .449 .147 .497 .235 .050 .328 abalon (19) .491 .739 .584 .625 .682 .605 .828 .648 .646 wineq red (8) .506 .787 .789 .603 .653 .588 .835 .650 .458 page-block (5) .761 .932 .808 .858 .795 .704 .974 .690 .619 yeast (7) .649 .896 .902 .749 .875 .876 .891 .860 .853 thyroid (1) .928 .975 .989 .950 .957 .885 .976 .934 .795 nursei (3) .982 .982 .955 .983 .991 .977 .986 1.00 1.00 fault (5) .938 .945 .697 .594 .353 .627 .417 .178 .579 wineq white (4) .764 .839 .897 .621 .657 .627 .513 .515 .441 yeast (5) .464 .867 .863 .628 .843 .871 .911 .834 .775 mun (3) .953 .962 .964 .903 .951 .981 .858 .939 .998 letter (a) .994 .997 .998 .995 .997 .995 .996 .995 .991 car (3) .973 .899 .903 .941 .946 .948 .911 1.00 1.00 derma (2) .951 .931 .904 .944 .935 .917 .936 .944 .939 ecoli (5) .968 .943 .945 .950 .976 .959 .945 1.00 .978 balance(2) .774 .584 .813 .830 .781 .801 .909 .684 .796 gtc .882 .972 .887 .920 .941 .894 .970 .907 .904 user model (1) .981 .950 .949 .968 .933 .926 .956 .961 .904 sat (4) .868 .919 .921 .894 .892 .863 .929 .861 .799 satimag (4) .911 .934 .919 .889 .847 .863 .865 .749 .803 euthyroid .761 .897 .887 .809 .826 .883 .887 .782 .881 glass (7) .879 .896 .945 .898 .905 .897 .935 .863 .842 segment (1) .993 .995 .996 .994 .994 .991 .995 .992 .985 hepat .721 .817 .695 .732 .740 .589 .855 .720 .440 column .872 .864 .866 .873 .874 .887 .883 .898 .926 cmc(2) .630 .668 .737 .618 .643 .595 .607 .556 .435 dna .949 .971 .965 .957 .959 .955 .972 .941 .937 vehicl (1) .983 .983 .964 .985 .983 .972 .988 .985 .988 transfus .748 .698 .522 .614 .633 .548 .462 .546 .615 haberman .634 .721 .642 .617 .485 .572 .606 .359 .490 german .623 .728 .714 .660 .691 .657 .804 .629 .557 waveform (0) .873 .884 .853 .884 .888 .856 .921 .901 .866 pima diabet .742 .733 .634 .725 .725 .649 .687 .694 .723 tictac (2) .997 .983 .984 .996 .983 .982 .995 .987 .978 respect g-mean metric, p-valu 0.0150, fried- man test detect signific differences. furthermore, test indi- cate signific differ post-process strategi cost-sensit svm methods. nevertheless, evi- 14 tabl 4: proport support vector relat number train data. dataset number support vector dataset number support vector bp cost smote bp cost smote wineq white (3) 1.79 7.93 16.52 user model (1) 12.02 17.83 6.78 abalon (19) 6.08 55.99 52.78 sat (4) 15.04 19.07 27.10 wineq red (8) 6.28 36.66 38.32 satimag (4) 9.09 10.29 18.64 page-block (5) 8.31 18.19 59.13 euthyroid 22.00 38.00 31.25 yeast (7) 4.18 27.91 28.79 glass (7) 17.76 19.16 13.55 thyroid (1) 2.49 11.24 9.92 segment (1) 4.07 4.89 48.72 nursei (3) 3.36 5.14 11.39 hepat 32.56 47.29 38.37 fault (5) 6.08 6.65 54.53 column 25.80 26.77 29.51 wineq white (4) 11.47 30.26 31.37 cmc(2) 44.26 64.09 41.98 yeast (5) 7.35 55.02 28.66 dna 37.35 39.85 18.05 mun (3) 5.87 52.90 5.85 vehicl (1) 9.69 9.34 22.87 letter (a) 1.36 2.18 1.27 transfus 42.65 57.62 58.82 car (3) 8.22 17.48 17.53 haberman 52.28 65.03 38.23 derma (2) 14.80 25.42 30.31 german 45.70 50.60 36.25 ecoli (5) 7.74 31.25 19.05 waveform (0) 22.84 24.12 43.88 balanc (2) 17.44 53.92 21.28 pima diabet 48.95 52.60 55.86 gtc 7.38 8.98 67.38 tictac (2) 35.59 36.01 18.47 denc classifi smote significantli different. respect sensit measure, p-valu 0.0083, is, friedman test detect signific differences. furthermore, test indic signific differ post- process strategi methods. therefore, conclud post-process strategi best strategi order maxim sensit metrics. 5. conclus futur work experiment result dataset differ degre imbalance, conclud svm perform significantli improv new bia consid proport classes. import benefit propos approach standard optim problem associ svm modified. new paramet tuned, comput cost practic insignificant. compar strategi cost-sensit 15 smote approaches, bia modif approach achiev superior perform term sensitivity, classif function far complex term number support vectors. futur work, theoret framework studi movement bia workspac accord definit developed. refer akbani, r., kwek, s., japkowicz, n. (2004), appli support vector machin imbalanc datasets, proceed 15th european confer machin learn ecml2004, pp. 3950. batuwita, r. palade, v. (2010), fsvm-cil: fuzzi support vector machin class imbal learning, ieee transact fuzzi sys- tems, 18, 558571. batuwita, r. palade, v. (2013), class imbal learn method support vector machines, imbalanc learning: foundations, al- gorithms, applications, pp. 83-99, berlin, germany: john wilei & sons. castro, c. l., carvalho, m. a., braga, a. p. (2009), improv algorithm svm classif imbalanc data sets, proceed- ing 11th intern confer enginn applic neural network eann 2009, pp. 108118. chawla, n. v., bowyer, k. w., hall, l. o., kegelmeyer, w. p. (2002), smote: synthet minor over-sampl technique, journal artifici intellig research, 16, 321357. cohen, g., hilario, m., sax, h., hugonnet, s., geissbuhler, a. (2006), learn imbalanc data surveil nosocomi infection, artifici intellig medicine, 37, 718. cristianini, n. shawe-taylor, j. (2000), introduct support vector machin kernel-bas learn methods, new york, ny: cambridg univers press, 1st edition. demser, j. (2006), statist comparison classifi multipl data sets, journal machin lean research, 7, 130. ertekin, s. (2013), adapt oversampl imbalanc data classifi- cation, inform scienc systems, lectur note electr engineering, 264, 261269. 16 frank, a. asuncion, a. (2010), uci machin learn repository, uni- versiti california, school inform science. irvine. archive.ics.uci.edu/ml gonzalez-abril, l., nunez, h., angulo, c., velasco, f. (2014), gsvm: svm handl imbalanc accuraci class bi-classif problems, appli soft computing, 17, 23-31. gonzalez-abril, l., angulo, c., velasco, f., ortega, j. a. (2008), note bia svm multiclassification, ieee trans- action neural networks, 19(4), 723725. he, h. garcia, e. a. (2009), learn imbalanc data, ieee transact knowledg data engineering, 21(9), 12631284. he, h. ghodsi, a. (2010), rare class classif support vector machine, proceed 20th intern confer pattern recog- nition, icpr10, pp. 548551. hernandez-santiago, j., cervantes, j., chau, a. l., garcia- lamont, f. (2012), enhanc perform svm skew data set excit support vectors, proceed 13th ibero-american confer artifici intellig iberamia 2012, pp. 101110. imam, t., ting, k. m., kamruzzaman, j. (2006), z-svm: svm improv classif imbalanc data, proceed 19th aus- tralian confer artifici intellig aus-ai 2006, pp. 264273. kang, p. cho, s. (2006), eu svms: ensembl under-sampl svm data imbal problems, lectur note science, 4232, 837846. li, b., hu, j., hirasawa, k. (2008), improv support vector ma- chine soft decision-mak boundary, proceed 26th iast intern confer artifici intellig applic aia08, pp. 4045. li, p., yu, x., bi, t. t., huang, j. l. (2014), imbalanc data svm classif method base cluster boundari sampl dt-knn pruning, intern journal signal processing, imag process pattern recognition, 7(2), 61-68. liu, y., an, a., huang, x. (2006), boost predict accuraci imbalanc dataset svm ensembles, proceed 10th pacific- asia confer knowledg discoveri data mine pakdd 2006, pp. 107118. 17 lopez, v., fernandez, a., garcia, s., palade, v., herrera, f. (2013), insight classif imbalanc data: empir- ical result current trend data intrins characteristics, inform sciences, 250, 113141. muscat, r., mahfouf, m., zughrat, a., yang, y. y., thornton, s., khondabi, a. v., sortanos, s. (2014), hierarch fuzzi support vector machin (svm) rail data classification, proceed 19th ifac world congress, pp. 1065210657. nguyen, h. m., cooper, e. w., kamei, k. (2011), borderlin over- sampl imbalanc data classification, intern journal knowl- edg engin soft data paradigms, 3, 421. nunez, h., gonzalez-abril, l., angulo, c. (2011), post-process strategi svm learn unbalanc data, proceed 19th european symposium artifici neural network esann2011, pp. 195 200. oneto, l., ridella, s., anguita, d. (2016). tikhonov, ivanov morozov regularizationfor support vector machin learning, machin learning, 3, 103136. ramirez, f. allende, h. (2012), dual support vector domain de- scription imbalanc classification, artifici neural network machin learn icann 2012, lectur note science, 7552, 710717. shanahan, j. g. roma, n. (2003), improv svm text classif perform threshold adjustment, lectur note science, 2837, pp. 361372. sukhanov, s., merentitis, a., debes, c., hahn, j., zoubir, a. (2015), bootstrap-bas svm aggreg class imbal problems, pro- ceed 23rd european signal process confer eusipco 2015, pp 155169. sun, a., lim, e.-p., liu, y. (2009), strategi imbalanc text classif svm: compar study, decis support sys- tems, 48, 191201. sun, y., wong, a. c., kamel, m. s. (2009), classif imbal- anc data: review, intern journal pattern recognit artifici intelligence, 23, 687719. tang, y., zhang, y.-q., chawla, n. v., krasser, s. (2009), svm 18 model highli imbalanc classification, ieee transact sys- tems, man cyberneticspart b, 39, 281288. vapnik, v. n. (1999), natur statist learn theori (inform scienc statistics), new york, ny: springer. veropoulos, k., campbell, c., cristianini, n. (1999), control- ling sensit support vector machines, proceed 16th intern joint confer artifici intellig ijcai 1999, pp. 5560. vilarino, f., spyridonos, p., vitria, j., radeva, p. (2005), ex- periment svm stratifi sampl imbalanc problem: detect intestin contractions, proceed 3rd intern confer advanc pattern recognit icapr 2005, vol. 2, pp. 783 791. wang, b. x. japkowicz, n. (2010), boost support vector machin imbalanc data sets, knowledg inform systems, 25, 120. wang, h. zheng, h. (2008), improv support vector machin classif imbalanc biolog datasets, proceed 4th intern confer intellig comput icic 2008, pp. 6370. wang, h.-y. (2008), combin approach smote biased-svm imbalanc datasets, proceed intern joint confer neural network ijcnn 2008, pp. 228231. wang, q. (2014), hybrid sampl svm approach imbalanc data classification, abstract appli analysis, articl id 972786. waske, b., benediktsson, j. a., sveinsson, j. r. (2009), classi- fy remot sens data support vector machin imbalanc train data, proceed 8th intern workshop multipl classifi system mcs09, pp. 375384. wu, g. chang, e. y. (2005), kba: kernel boundari align con- sider imbalanc data distribution, ieee transact knowledg data engineering, 17, 786795. yang, c.-y., wang, j., yang, j.-s., yu, g.-d. (2008), imbalanc svm learn margin compensation, proceed 5th intern symposium neural networks: advanc neural network isnn08, pp. 636644. yang, p., zhang, z., zhou, b. b., zomaya, a. y. (2011), sampl subset optim classifi imbalanc biolog data, pro- 19 ceed 15th pacific-asia confer advanc knowledg discoveri data mine pakdd 2011, vol. 2, pp. 333344. yu, t., debenham, j., jan, t., simoff, s. (2006), combin vector quantiz support vector machin imbalanc datasets, artifici intellig theori practice. ifip 19th world congress, vol. 217, chapter 9, pp. 8188. zhou, b., ha, m., wang, c. (2010), improv algorithm un- balanc data svm, advanc intellig soft computing, fuzzi inform engineering, 78, pp. 549-555. zieba, m., tomczak, j. m., lubicz, m., swiatek, j. (2014), boost svm extract rule imbalanc data applic predic- tion post-op life expect lung cancer patients, appli soft computing, 14(part a), 99-108. 20