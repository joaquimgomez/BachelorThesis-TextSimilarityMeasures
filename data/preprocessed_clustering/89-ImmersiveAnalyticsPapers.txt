right, mr. demille, im readi closeup: ad mean user action video immers analyt andrea batch* nikla elmqvist colleg inform studi univers maryland, colleg park, md, usa abstract us machin learn vision classifi human behavior grown large, well-established, interdisci- plinari area research, area somewhat overlook intersect vision tool evalu user behavior virtual reality, particularli context immers analyt visualization. draw literatur pattern recogni- tion, vision, machin learn compos simple, compar resource-cheap pipelin camera-bas extract featur profession analyst user session exist vr visual system, imaxes. result high accuraci predict self-report featur users, survei respons user experi immers interfac somewhat ambigu vari base features. keywords: visualization, visual analytics, ubiquit analytics, evaluation, video analytics, machin learning, deep learning. index terms: human-cent computingvisualizationvisu- aliz techniques; human-cent computingvisu visual design evalu method 1 introduct full-bodi interact common virtual realiti (vr) in- terfac present rich opportun us camera sensor data inform human-comput interact (hci) research understand particip experiences. research in- terest intersect machin learn techniqu vr environ grow develop increasingli af- fordabl consum vr hardwar coincid dramat im- provement predict accuraci neural network architectur machin learning. evidenc format confer as, example, ieee intern confer artifici intellig & virtual realiti (aivr). constitut appropri us vision ma- chine learn vr visual problem? argu good area applic classifi user evalu experience. specifically, applic augment ambigu find improv scalabl costli process qualit evalu user sessions. vision us vision hci research shift awai video input intract media format, cheap captur expens analyz evalu studi hci visualization, us video data reveal behavior dataset time-cost cheap scalabl analysi larg user populations. scenario, visual hci research simpli add video record setup, turn onboard camera test mobil wearabl *e-mail: e-mail: device, collect addit inform user physic behavior action particip user study. video footag easili quickli analyz off-the-shelf models, discuss here, result time sequenc action synchron rest studi telemetrics, summari action performed. action includ gross motor skills, movement head, arms, legs, fine motor skills, hand fingers, facial expressions. biometr inform pulse, pupil dilation, respir rate, etc, deduc appropri models. metric complement task perform data collect user study. end, demonstr specif applic vision machin learn low-cost low-resourc camera-bas track human behavior video footag pre-train neural network random forest models. us prototyp ad hoc machin learn pipelin extract particip behavior video footag user studi involv analyt profession u.s. depart commerc interact data virtual reality. argu approach like potenti fed interact enabl react user behavior extract live video footage. 2 relat work goal better us video data evalu user behavior, consid work defin track user interactions. 2.1 persona hci: user classif technique? good caus argu hci visual communi- ti mind context peopl actual us thing build [8, 38]. conduct appli wild studies, however, run risk encount issu replic plagu domain [21]. common approach understand factor influenc user need infor- mation visual involv construct personasrepresent archetyp typic user daili live [38]. gen- eral involv qualit ethnograph method research tracks, records, interpret user daili activ collabor participant, reach share understand- ing user thought process interview activ- iti [38, 76]. altern persona-bas approach exist event interface, mous activ [3], develop data-driven persona [90] character type users. access large-scale, user-gener dataset plat- form crowdsourc experi amazon mechan turk [45] creation type persona man- ageabl larger scales. similarly, data-driven method hci increasingli classifi interact [53, 63, 68]. 2.2 action classification: action user behavior? hci commun scratch surfac artifici neural network (anns), includ recurr convolut neu- ral network (rnn cnns), evalu user behavior. author' version articl publish ieee vi mlui workshop. chang version publish prior publication. task identifi gestur [61, 71, 82] gaze [65, 91], classify- ing user emot facial express [34, 63, 81], detect characterist user, gender [83], construct implement neural network architectur lightli explor hci-rel discourse. visual commun contribut toolkit method evalu user video, logs, transcripts, qualit data [18], user gestur analysi [40, 42]. vision (cv) machin learn (ml) com- munities, however, decad worth literatur evalu observ behavior method action classi- ficat [51, 66], motion path predict [58], ey track [46], gestur detect [64]. posit paper [14] studi [35] advoc closer relationship hci machin intellig communities, current bodi literatur subject surprisingli sparse. train neural network identifi individu emot express [27, 55, 89, 92] accur predict basketbal player good bad [10], littl work identifi user feel implement posit negative, task perform good bad? 2.3 intersect visual ml visual contribut domain cv ml tensorboard, visual tool embed wildli popular tensorflow [1], method algorithm [25] recent nuanc techniqu (e.g., dataflow graph model structur [87]) system (e.g., activi [41], deploi facebook assist model train subset discovery). said, develop techniqu aid cv/ml researchers, closer increasingli refer explain ai (xai) [12, 32] evok sens trust lai person [72]. conversely, work cv ml commun augment inform visual interact sensemak slightli sparser [26]. notabl except claim computa- tional model percept long propos mean infer biolog vision [69], rare occas successfulli appli enhanc optim inform visual neural network [44, 70]. recent work present semant model visual larg imag dataset [88] hand, visual model classifi semant dataset [67] other, poetic illustr mutual benefit deepen relationship close relat set topics. former, cnn caption image, caption determin layout visual model semant associ concepts, present novel pipelin handl visual problem (graph layout base conceptu similarity) cv natur languag process (nlp) methods. latter, interact vi- sualiz system, conceptvector, implement support user build lexicon relat concepts; lexicon improv recommend visual system, present novel support nlp modeling, address ml issu semant lexicon creation. studi draw heavili overlap combin topic visu- aliz ml present solut relev fields: ml solut visual problem, visual solut ml problem, respectively. case cv/ml techniqu improv visual repres model train infer 3d scatter point [6, 77]. general-purpos graphics, neural network predict incomplet region 2d imag [56]. broader interfac design, latenc reduct area touch upon, ripe explor [36]. 2.4 immers analyt neural network ubiquit [24] immers analyt [17] recently- establish sub-domain visual analysi research, visualiza- tion techniqu [23] evalu method [5, 22] augment realiti (ar) vr emerg grow frequency. direc- tion research valid recent work indic inform recal task improv work vr [47]. strong argument ar, vr, mix realiti (mr) implement well-suit dnn architec- ture evalu contextual-awar adapt design rel tradit implementations. first, environ heavili reliant camera sensors, collect wealth input data evaluation. second, wide gestur movement user environ fit fairli satur domain cv research action classif [15, 31, 51, 66]. model detect hand foot gestur smartphon [57], example, trigger event inform evalu ar visual mobil devices. concept implement multimod interfaces, an- form immers environment, new [75]. like hu- mans, neural network architectur construct interpret multimod stimuli [62, 92]. semant interpret chemo- sensori stimulitast smelli open problem, sound increasingli well-explored. srivastava et al. [79] us audio data infer semant mean video data; similarly, deepear [50], artifici neural network train infer user surround mobil devices, soundnet train identifi context video combin video audio in- put [4]. schissler et al. [74] extend concept acoust audio analysi present visual-acoust displai base aural signal processing. star trek holodeck popular metaphor immers storytel [16, 37]; argu impli immersion, learn adapt implement user. metaphor conjur potenti scientif engin us [60] educ [20]. metaphor captur public eye, unreason one, given emerg immers analyt recent evid benefit [17, 47]. present, best path end creat adaptive, immers analyt environmenti make stride combin visual techniqu immer- sive environ cv ml techniques. 2.5 dataset pre-train weight network discuss far larg semi-supervised: depend semant label creat user large-scal platform (e.g., youtub 8m dataset [2]) research them- selv (e.g., summ dataset [33]). movement un- supervis learn result creation architectures, techniqu [9], dataset identifi atom action (e.g., ava dataset [31]) simpli predict futur frame unlabel video data [80]. said, train weight deep neural network (dnn)an ann featur larg number hidden layers, approach taken contemporari cv researchi typic computation costly, albeit costli human label interact researcher. immedi future, us pre-train network creat semant label evalu user interact easi point entri (e.g., kinet human ac- tion video dataset [15]).the major benefit approach requir research embark challeng computation costli journei construct train dnn. words, labor-cheap easi implement, compar comprehens large-scal qualit evalua- tion studies, compar approach construct train on cv pipelin end end. author' version articl publish ieee vi mlui workshop. chang version publish prior publication. a. 0% 17% 17% 33% 100% 83% 83% 67% abl examin object close abl survei scope environ easili examin specif detail environ inform gain examin object multipl viewpoint 100 50 0 50 100 percentag user' examin (no prior vr) b. 0% 0% 0% 17% 100% 100% 100% 83% abl examin object close abl survei scope environ easili examin specif detail environ inform gain examin object multipl viewpoint 100 50 0 50 100 percentag user' examin (prior vr) 0% 17% 17% 33% 100% 83% 83% 67% control mechan distract virtual object easi manipul virtual filter control easi us displai control devic inobtrus 100 50 0 50 100 percentag user' perceiv qualiti control mechan (no prior vr) 0% 17% 33% 50% 100% 83% 67% 50% control mechan distract virtual object easi manipul virtual filter control easi us displai control devic inobtrus 100 50 0 50 100 percentag user' perceiv qualiti control mechan (prior vr) c. 0% 0% 0% 25% 100% 100% 100% 75% abl examin object close abl survei scope environ easili examin specif detail environ inform gain examin object multipl viewpoint 100 50 0 50 100 percentag user' examin (littl gaming) d. 0% 50% 50% 50% 100% 50% 50% 50% abl examin object close abl survei scope environ easili examin specif detail environ inform gain examin object multipl viewpoint 100 50 0 50 100 percentag user' examin (regular gaming) 0% 25% 25% 25% 100% 75% 75% 75% control mechan distract virtual object easi manipul virtual filter control easi us displai control devic inobtrus 100 50 0 50 100 percentag user' perceiv qualiti control mechan (littl gaming) 0% 0% 0% 50% 100% 100% 100% 50% control mechan distract virtual object easi manipul virtual filter control easi us displai control devic inobtrus 100 50 0 50 100 percentag user' perceiv qualiti control mechan (regular gaming) figur 1: rate user prior vr us (a) versu prior vr us (b), user prior vr us regularli plai game (c) versu user vr experi regularli plai game (d). 3 studi method purpos studi evalu us immers analyt environ imax [22] domain experts, extend implementation.1 data collect particip emploi data scientists, econom analysts, economist u.s. bureau econom analysi (bea), author embed employee. 3.1 procedur experiment design studi stages. form stage, includ pilot user session sampl drawn target popul in-the-wild user session open-end self-direct follow brief tutorial. second sum- mativ stage. summ stage, user session involv explor phase user ask freeli ex- plore data environment, present phase user ask prepar present narr featur data researcher. mixed-method studi conduct small offic approxim 10 10 feet (3 3 meters). comput equip- ment person equip nvidia geforc gtx 1060 (6gb) gpu, intel xeon e5-2620 v3 (2.40ghz) cpu, 16gb ram, run microsoft window 10. imax applic [22] built uniti 5.6.5f1, instal local aforement pc. rig equip 1these find drawn posthoc analysi data collect mixed-method studi ieee infovi 2019 [7]. however, includ result deriv vision machin learn method paper. thus, data present uniqu paper previous published. however, method discuss necess similar infovi 2019 paper. htc vive vr system, includ head-mount displai (hmd) base stations. video stream user interact captur raspberri pi zero 8mp pi camera mo- tioneyeos. camera serv motion-activ webcams, mount differ posit room. raspberri pi posit chest height user start position, posit corner room near vive base stations. addit evalu video teleme- try data conduct pc equip evga geforc gtx 1080 sc (8gb) gpu, intel core i7-7700 cpu (3.60ghz, 4 cores), 24gb ram, run window 10. immedi user sessions, particip ask complet survei question relat pre-regist pre- dictions2 user sens presence, feel control mechanisms, engag task ask them, aspect session observ self-report [7]. predictions, follow weak mix result base evid survei responses: a3.3 particip report fatigu physic navig interaction. motivation: us gross bodi motor control navig virtual environ interact object yield signific exert fatigu participants. a4 particip encount signific navig interac- tion hurdl lack vr expertise. motivation: particip pool specif vr training, challeng 3d navig interact concerns. 2 author' version articl publish ieee vi mlui workshop. chang version publish prior publication. h sword fight train dog drink beer smoke count monei write whistl bend exercis arm rais lungesquat stretch arm stretch leg tai chi yoga hug read book read newspap text toss coin fly kite beatbox plai bagpip plai cello plai clarinetplai flute plai record plai saxophon plai trombon plai trumpet plai ukulel plai violin record music iron plaster sand floor contact juggl danc ballet robot danc tango danc blast sand extinguish plai paintbal sprai paint sprai archeri throw ball aggress anim handl bat hit consumptioncraft deskwork emot exercis food prepar friendli game leisur hand gestur jump locomot make music manual labor perform person care present sprai throw figur 2: rel sum predict probabl semant label assign video segments: consid repres compar frequenc semant label video segment users. a4.1 particip 3d game experi hinder lack vr training. motivation: 3d game experi help peopl interact efficiently. anticip user report fatigu navig interact (a3.3) prompting, occur; telemetri log indic increas user activ time went on, measur evalu user fatigue. contrari prediction, survei indic relationship prior vr usag user-report hurdl navig environ (a4), 3d game substanti posit effect (a4.1) respect user percept control mechan (figur 1). result hypothesi a4.1 mixed, regular gamer report examin object difficult, report control mechan slightli higher qualiti particip regularli game. fatigu explicitli measur study, proxi measur particip experienc nausea given 10-second window 2 minut decid voluntarili stop session. result hypothes ambigu mixed, such, present opportun exploit video data miss pieces. furthermore, user session split activ (explor present), predict conting assumpt user behav differ- ent wai explor stage rel behav present stage. assumpt call question confirm techniqu describ section 3.2. 3.2 data analysi pre-train weight 3d convolut neural network (3dcnn) appli kinet human action video dataset [15] classifi segment video captur user consent video recorded. primari rational pre-train weight labor-cheap easi implement, note section 2.5; kinet 3dcnn weights, specifically, chosen rel popular accessibility. 3dcnn assign semant action class label 10-second window video data video captur user interact environment. window assign 400 action class labels, label predict probabl score; aggreg probabl score (figur 2) view pseudo-frequ action observ study. check method ways: valid check valu check. measur action class validity, captur video data camera set record user differ point view. compar similar assign label find rank-bias overlap (rbo) tempor segment [86]. demonstr valu ad appli technique, 3dcnn output random forest input featur predict featur user (e.g., regularli plai game engag athlet sport activ involv mov- ing faster run speed), experi (whether vr before), type activ engag session (explor versu presentation). train random forest [13] predict characterist particip relev activitiesnamely, prior experi vr, regularli plai game free time, engag strenuou sport athlet activities. author' version articl publish ieee vi mlui workshop. chang version publish prior publication. 0.00 0.05 0.10 0.15 0.20 u se r. 0 1 0.00 0.05 0.10 0.15 0.20 u se r. 0 2 0.00 0.05 0.10 0.15 0.20 u se r. 0 3 0.00 0.05 0.10 0.15 0.20 u se r. 0 4 0.00 0.05 0.10 0.15 0.20 u se r. 0 5 0.00 0.05 0.10 0.15 0.20 u se r. 0 6 0.00 0.05 0.10 0.15 0.20 u se r. 0 7 0.00 0.05 0.10 0.15 0.20 u se r. 0 8 0.00 0.05 0.10 0.15 0.20 u se r. 0 9 0.00 0.05 0.10 0.15 0.20 u se r. 1 0 0.00 0.05 0.10 0.15 0.20 u se r. 1 1 0.00 0.05 0.10 0.15 0.20 u se r. 1 2 figur 3: probability-weight rank-bias overlap (rbo) distanc scores: lower rbo distanc score indic better match rank list semant action label probabl assign ten-second window time. thought indic 85% match action assign video camera differ angl room. 4 result video data input pre-train kinet 3dcnn [15], predict semant label ten-second chunk video (figur 2). prediction-probability-weight rank se- mantic label result pre-train 3dcnn yield strong match camera (figur 3), indic network reason consist job predict user actions. ran- dom forest model abl predict particip engag fast-mot sport activ 97.35% accuracy, like- lihood experienc form vr sick 93.63% accuracy, user prior vr experi 92.74% accuracy, game habit 91.93% accuraci (tabl 1). rel high predict accuraci prior vr experi support predict (a4) user vr experi characterist differ experi user vr experience. predict accuraci regular game activ support notion tradit game habit influenc user vr experi analyt setting, albeit nu- anc fashion predict (a4.1), given particip survei respons ran counter hypothesis. evalu tempor featur user session, appli model similar predict featur user com- binat video data collect form studi video collect explor phase summ studies. differ model primarili omiss time start featur independ variabl model predict tempor featur session, featur share determinist relationship depend variable. hope proxi measur user fatigu extracted: i.e. user nearli readi stop doing. user explor stage summ studi form studi control end activ engag in. assumpt user fatigu plai role user decis readi exit environ end current activ fatigu appear minut window decis event, method abl author' version articl publish ieee vi mlui workshop. chang version publish prior publication. model out-of-bag error % hold-out error % cross-valid error % cm[0] cm[1] cm[2] cm[3] cm[4] fmaa* ranger 6.43 5.48 6.25 randomforest 6.41 6.65 cm [no/0] 1131 105 cm [yes/1] 75 1629 game ranger 8.91 7.3 7.86 randomforest 1.83 8.07 cm [least/0] 648 9 3 28 6 cm [1] 16 722 5 6 38 cm [2] 36 11 258 6 9 cm [3] 33 14 0 508 11 cm [4/most] 9 3 0 4 557 prior vr ranger 5.0 4.75 6.65 randomforest 5.41 7.26 cm [no/0] 1456 63 cm [yes/1] 88 1333 vr sick ranger 5.78 4.63 6.37 randomforest 6.10 6.37 cm [no/0] 1937 44 cm [yes/1] 130 916 phase ranger 28.09 24.25 24.51 randomforest 24.90 23.73 cm [explore/0] 801 182 cm [present/1] 258 966 quit 2 min ranger 10.34 10.16 11.00 randomforest 11.45 11.20 cm [no/0] 2501 1 cm [yes/1] 288 24 * user particip fast-mov athlet activ (fmaa) leisur time. tabl 1: predict error confus matric random forest. detect approach fatigue. result yield 88.8% accuracy, 92.3% moment 2-minut window misclassifi outsid (i.e., classifi prox- imiti quit event). result bolster neg find predict (a3.3) user report fatigue, abl empir evidence. rel approach pick user fatigue, model perform slightli better predict user explor present phase study, phase (76.27% accuracy; 81.7% user prepar present correctli classified, 73.5% explor user correctli classified, video user engag activ classifi correctli 43.2% time). indic random forest classif 3dcnn output perform level significantli better chanc predict user engag type behavior entir novel environ experi activities. general, however, approach perform tempor segment video data pick relev user traits, prior vr experience. 5 conclus discuss techniqu evalu user behavior, achiev goal implement robust method test hypothes possibl vision approach. find rbo camera captur user activ differ angl reason low, contribut addit valid pre-train 3dcnn deriv semant label predictions. method robust, semant label random forest input, produc result (figur 4 5) convei semant signific lai reader: behavior look like beekeep slacklin (or doesnt) import predict vr user prior vr experi (figur 4). gestur look like extinguish reliabl indic user regular gamer user anticip experienc vr sick (figur 4). result specif implement user studi design, action look like laugh throw ax import predict user explor data present narr onlooker. hold semant context implement particular, gestur destroi object imax throw it, vive control regist look somewhat like small axe: indic destruct object import indic kind activ user engag in. similarly, vr user speech present look like laughter face obscur vr headset. goal essai urg hci research shift direct set standard model cv and/or ml user evalu visual architecture. argument practic implic well-suit vr implementations, promis applic desktop (e.g., mobil devic augment realiti implementations). author' version articl publish ieee vi mlui workshop. chang version publish prior publication. sprai bunge jump plai trombon slacklin bee keep 0 20 40 60 variabl import (mean decreas gini index) 3 d c n n l b e l random forest results: prior vr experi second start read book auction answer question plai cricket 0 20 40 60 variabl import (mean decreas gini index) 3 d c n n l b e l random forest results: fast sport plai cricket dive cliff extinguish scuba dive whistl 0 20 40 60 80 variabl import (mean decreas gini index) 3 d c n n l b e l random forest results: regular game cano kayak blow leav feed fish extinguish shovel snow 0 50 100 150 variabl import (mean decreas gini index) 3 d c n n l b e l random forest results: vr sick figur 4: rel import semant label predict particip involv fast-mov sports, time spent gaming, prior vr experience, user predisposit vr sickness. model includ time, improv fit context user activ character rel time session, given featur user unrel time. eat spaghetti tai chi exercis arm snowkit rais sprai paint stretch arm blast sand extinguish read book 0 2 4 6 variabl import (mean decreas gini index) 3 d c n n l b e l random forest results: explor capoeira butterfli stroke clean window plai frisbe fold paper headbut disc golf throw ax laugh 0 10 20 30 variabl import (mean decreas gini index) 3 d c n n l b e l random forest results: activ phase figur 5: rel import semant label predict particip engag explor vs. presentation, minut quitting. model includ time, depend variabl directli relat time session. 5.1 limit studi exploit vision machin learn techniqu inform ambigu illumin pattern observ user sessions. analysi paper conduct extens data collect work accept ieee infovi 2019 public tvcg [7]; scope work limit deeper explor hypothes left partli answer convent means. such, step analyz possibl intervent improv user experience; remain open area futur research. 5.2 futur context-aware, adapt environ concept context-awar comput environ nearli old hci [73]. typic manifest model us sensor telemetri inform predetermin se- mantic infer user environment, idea neural network aid achiev end new [11, 48, 54]. fact, work implement ann architectur rgb camera input face detect context hci emerg later year concept context-awar comput [39]. contemporari work queri predict [78] combin context- awar information-seek techniqu neural network seen success end, gener under-explor area research. visual community, direc- tion typic visual assist cv ml work (e.g., conceptvector [67]), certainli univers true [88]. simpl context detect cv ml communities, however, problem space seen achiev past year [19, 28, 43, 85]. interfac truli context-aware, argu capabl semant evalu user behavior environ- ment real time. hardwar capabl consumer- readi electronics, includ mobil devices, matur technolog year old [29]. foundat ar, mobil devic commonli depend non-visu sensor (e.g., compass, gps, accelerometer) comput cost perfor- mance, mobil ar implement rgb camera cv techniqu decad [59]. interfac us- ing special sensor cnn architectur capabl real-tim gestur interpret year [52]. semant segment video imag task evalu old [69] increasingli optim [49] area cv research. contemporari architectur video analysi incorpor long short term memori (lstm) autoencod [80]. gener adver- sarial network (gans), propos goodfellow et al. [30] successfulli implement vondrick et al. [84], increasingli state art us neural network creat imagery. train semant gener models, however, computation intens real-tim consum electronics. here, again, us pre-train model shine immedi steptheir potenti applic user studi evalu seen step stone embed evalu process iter implement follow initi results. construct- ing architectur chang event subset model output valu base evalu findings, example, visual interfac design step context-aware. construct architectur updat train model (not necessar- ili real-time), visual interfac design step adaptive. author' version articl publish ieee vi mlui workshop. chang version publish prior publication. refer [1] m. abadi, p. barham, j. chen, z. chen, a. davis, j. dean, m. devin, s. ghemawat, g. irving, m. isard, m. kudlur, j. levenberg, r. monga, s. moore, d. g. murray, b. steiner, p. tucker, v. vasudevan, p. warden, m. wicke, y. yu, x. zheng. tensorflow: large-scal machin learning. proceed usenix confer oper system design implementation, pp. 265283. usenix association, berkeley, ca, usa, 2016. [2] s. abu-el-haija, n. kothari, j. lee, p. natsev, g. toderici, b. varadara- jan, s. vijayanarasimhan. youtube-8m: large-scal video clas- sific benchmark. arxiv preprint arxiv:1609.08675, 2016. [3] a. a. e. ahm i. traore. new biometr technolog base mous dynamics. ieee transact depend secur computing, 4(3):165179, 2007. doi: 10.1109/tdsc.2007.70207 [4] y. aytar, c. vondrick, a. torralba. soundnet: learn sound rep- resent unlabel video. advanc neural inform process systems, pp. 892900, 2016. [5] b. bach, r. sicat, j. beyer, m. cordeil, h. pfister. hologram hand: effect interact explor 3d visual immers tangibl augment reality? ieee transact visual graphics, 24(1):457467, 2018. doi: 10. 1109/tvcg.2017.2745941 [6] j. barhak a. fischer. parameter reconstruct 3d scatter point base neural network pde techniques. ieee transact visual graphics, 7(1):1 16, 2001. doi: 10.1109/2945.910817 [7] a. batch, a. cunningham, m. cordeil, n. elmqvist, t. dwyer, b. h. thomas, k. marriott. spoon: evalu performance, space use, presenc expert domain user immers analyt- ics. ieee transact visual graphics, appear, 2020. [8] a. batch n. elmqvist. interact visual gap initi exploratori data analysis. ieee transact visual graphics, 24(1):278287, 2018. doi: 10.1109/tvcg.2017. 2743990 [9] a. batch, k. lee, h. t. maddali, n. elmqvist. gestur action discoveri evalu virtual environ semi-supervis segment telemetri records. proceed ieee interna- tional confer artifici intellig virtual realiti (aivr), pp. 110, dec 2018. doi: 10.1109/aivr.2018.00009 [10] g. bertasius, h. s. park, s. x. yu, j. shi. baller? basketbal perform assess first-person videos. proceed ieee intern confer vision, pp. 21962204, 2017. doi: 10.1109/iccv.2017.239 [11] c. biancalana, f. gasparetti, a. micarelli, a. miola, g. sansonetti. context-awar movi recommend base signal process machin learning. proceed challeng context-awar movi recommendation, pp. 510. acm, new york, ny, usa, 2011. doi: 10.1145/2096112.2096114 [12] m. bojarski, p. yeres, a. choromanska, k. choromanski, b. firner, l. jackel, u. muller. explain deep neural network train end-to-end learn steer car. arxiv preprint arxiv:1704.07911, 2017. [13] l. breiman. random forests. machin learning, 45(1):532, 2001. doi: 10.1023/a:1010933404324 [14] e. cambria, g.-b. huang, l. l. c. kasun, h. zhou, c. m. vong, j. lin, j. yin, z. cai, q. liu, k. li, v. c. m. leung, l. feng, y. s. ong, m. h. lim, a. akusok, a. lendasse, f. corona, r. nian, y. miche, p. gastaldo, r. zunino, s. decherchi, x. yang, k. mao, b. s. oh, j. jeon, k. a. toh, a. b. j. teoh, j. kim, h. yu, y. chen, j. liu. extrem learn machin [trend & controversies]. ieee intellig systems, 28(6):3059, 2013. doi: 10.1109/mis.2013.140 [15] j. carreira a. zisserman. quo vadis, action recognition? new model kinet dataset. proceed ieee confer vision pattern recognition, pp. 47244733, 2017. doi: 10.1109/cvpr.2017.502 [16] m. cavazza, j.-l. lugrin, d. pizzi, f. charles. madam bovari holodeck: immers interact storytelling. proceed acm intern confer multimedia, pp. 651660. acm, new york, ny, usa, 2007. doi: 10.1145/1291233.1291387 [17] t. chandler, m. cordeil, t. czauderna, t. dwyer, j. glowacki, c. goncu, m. klapperstueck, k. klein, f. schreiber, e. wilson. immers analytics. proceed intern symposium big data visual analytics, pp. 18, sep 2015. doi: 10.1109/bdva. 2015.7314296 [18] s. chandrasegaran, s. k. badam, l. kisselburgh, k. peppler, n. elmqvist, k. ramani. vizscribe: visual analyt approach understand design behavior. intern journal human- studies, 100:66 80, 2017. doi: 10.1016/j.ijhcs.2016.12. 007 [19] q. chen, z. song, j. dong, z. huang, y. hua, s. yan. contex- tualiz object detect classification. ieee transact pattern analysi machin intelligence, 37(1):1327, 2015. doi: 10. 1109/tpami.2014.2343217 [20] t. c. chen, c. f. chiu, a. klimenko, t. k. shih. holodeck like edutain game wearabl devic motion sensors. proceed intern confer ubi-media computing, pp. 242247, 2015. doi: 10.1109/umedia.2015.7297462 [21] e. coiera, e. ammenwerth, a. georgiou, f. magrabi. health informat replic crisis? journal american medic informat association, p. ocy028, 2018. doi: 10.1093/jamia/ocy028 [22] m. cordeil, a. cunningham, t. dwyer, b. h. thomas, k. marriott. imaxes: immers ax embodi afford interact mul- tivari data visualisation. proceed acm symposium user interfac softwar technology, pp. 7183. acm, new york, ny, usa, 2017. doi: 10.1145/3126594.3126613 [23] m. cordeil, t. dwyer, k. klein, b. laha, k. marriott, b. h. thomas. immers collabor analysi network connectivity: cave-styl head-mount display? ieee transact visual- izat graphics, 23(1):441450, 2017. [24] n. elmqvist p. irani. ubiquit analytics: interact big data anywhere, anytime. ieee computer, 46(4):8689, 2013. doi: 10. 1109/mc.2013.147 [25] a. endert, m. s. hossain, n. ramakrishnan, c. north, p. fiaux, c. andrews. human loop: new direct visual analytics. journal intellig inform systems, 43(3):411435, dec 2014. doi: 10.1007/s10844-014-0304-9 [26] a. endert, w. ribarsky, c. turkay, b. w. wong, i. nabney, i. d. blanco, f. rossi. state art integr machin learn visual analytics. graphic forum, 36(8):458486, 2017. doi: 10.1111/cgf.13092 [27] c. fabian benitez-quiroz, r. srinivasan, a. m. martinez. emo- tionet: accurate, real-tim algorithm automat annot million facial express wild. proceed ieee confer vision pattern recognition, pp. 55625570, 2016. doi: 10.1109/cvpr.2016.600 [28] r. girshick, j. donahue, t. darrell, j. malik. rich featur hier- archi accur object detect semant segmentation. proceed ieee confer vision pattern recognit (cvpr), pp. 580587, 2014. doi: 10.1109/cvpr.2014.81 [29] v. gokhale, j. jin, a. dundar, b. martini, e. culurciello. 240 g-ops/ mobil coprocessor deep neural networks. proceed ieee confer vision pattern recognit workshops, pp. 696701, 2014. doi: 10.1109/cvprw.2014.106 [30] i. goodfellow, j. pouget-abadie, m. mirza, b. xu, d. warde-farley, s. ozair, a. courville, y. bengio. gener adversari nets. proceed advanc neural inform process systems, pp. 26722680, 2014. [31] c. gu, c. sun, s. vijayanarasimhan, c. pantofaru, d. a. ross, g. toderici, y. li, s. ricco, r. sukthankar, c. schmid, j. malik. ava: video dataset spatio-tempor local atom visual actions. arxiv preprint arxiv:1705.08421, 2017. [32] d. gunning. explain artifici intellig (xai). defens ad- vanc research project agenc (darpa), 2017. [33] m. gygli, h. grabner, h. riemenschneider, l. van gool. creat summari user videos. proceed ieee european confer vision, pp. 505520, 2014. doi: 10.1007/978 -3-319-10584-0 33 [34] k. harezlak, p. kasprowski, m. stasch. idiosyncrat repeat author' version articl publish ieee vi mlui workshop. chang version publish prior publication. calibr error ey tracker calibration. proceed intern confer human interactions, pp. 95100, 2014. doi: 10.1109/hsi.2014.6860455 [35] s. heng d. yunfeng. research cooper control human- interact tool high recognit rate base neural network. proceed ieee intern confer virtual realiti visualization, pp. 350354, 2014. doi: 10.1109/ icvrv.2014.6 [36] n. henze, m. funk, a. s. shirazi. software-reduc touchscreen latency. proceed intern confer human- interact mobil devic services, pp. 434441. acm, new york, ny, usa, 2016. doi: 10.1145/2935334.2935381 [37] r. hill, j. gratch, w. l. johnson, c. kyriakakis, c. labore, r. lind- heim, s. marsella, d. miraglia, b. moore, j. morie, j. rickel, m. thiebaux, l. tuch, r. whitney, j. douglas, w. swartout. holodeck: integr graphics, sound, charact story. proceed intern confer autonom agents, pp. 409416. acm, new york, ny, usa, 2001. doi: 10.1145/375735. 376390 [38] k. holtzblatt h. beyer. contextu design: evolved. synthe- si lectur human-cent informatics. morgan & claypool publishers, 2014. [39] m. hunk a. waibel. face locat track human- interaction. proceed asilomar confer signals, system computers, vol. 2, pp. 12771281, 1994. doi: 10. 1109/acssc.1994.471664 [40] s. jang, n. elmqvist, k. ramani. motionflow: visual abstract aggreg sequenti pattern human motion track data. ieee transact visual graphics, 22(1):21 30, 2016. doi: 10.1109/tvcg.2015.2468292 [41] m. kahng, p. y. andrews, a. kalro, d. h. p. chau. activis: visual explor industry-scal deep neural network models. ieee transact visual graphics, 24(1):8897, 2018. doi: 10.1109/tvcg.2017.2744718 [42] c. kerdvibulvech h. saito. vision-bas detect guitar play- er fingertip markers. proceed intern confer graphics, imag visualis (cgiv), pp. 419428, 2007. doi: 10.1109/cgiv.2007.88 [43] d. kim, c. park, j. oh, s. lee, h. yu. convolut matrix factor- izat document context-awar recommendation. proceed acm confer recommend systems, pp. 233240. acm, new york, ny, usa, 2016. doi: 10.1145/2959100.2959165 [44] y. kim a. varshney. saliency-guid enhanc volum vi- sualization. ieee transact visual graph- ics, 12(5):925932, 2006. doi: 10.1109/tvcg.2006.174 [45] a. kittur, e. h. chi, b. suh. crowdsourc user studi mechan turk. proceed acm confer human factor comput systems, pp. 453456, 2008. doi: 10.1145/ 1357054.1357127 [46] k. krafka, a. khosla, p. kellnhofer, h. kannan, s. bhandarkar, w. ma- tusik, a. torralba. ey track everyone. proceed ieee confer vision pattern recognition, pp. 21762184, 2016. doi: 10.1109/cvpr.2016.239 [47] e. krokos, c. plaisant, a. varshney. spatial mnemon vir- tual reality. proceed 2018 10th intern confer autom engineering, pp. 2730, 2018. [48] m. krstic m. bjelica. context-awar person program guid base neural network. ieee transact consum electronics, 58(4):13011306, 2012. doi: 10.1109/tce.2012.6414999 [49] a. kundu, v. vineet, v. koltun. featur space optim semant video segmentation. proceed ieee confer vision pattern recognition, pp. 31683175, 2016. doi: 10.1109/cvpr.2016.345 [50] n. d. lane, p. georgiev, l. qendro. deepear: robust smartphon audio sens unconstrain acoust environ deep learn- ing. proceed acm intern joint confer pervas ubiquit computing, pp. 283294. acm, new york, ny, usa, 2015. doi: 10.1145/2750858.2804262 [51] i. laptev, m. marszalek, c. schmid, b. rozenfeld. learn realist human action movies. proceed ieee confer vision pattern recognition, pp. 18, 2008. doi: 10.1109/cvpr.2008.4587756 [52] j. h. lee, t. delbruck, m. pfeiffer, p. k. park, c.-w. shin, h. ryu, b. c. kang. real-tim gestur interfac base event-driven process stereo silicon retinas. ieee transact neural network learn systems, 25(12):22502263, 2014. doi: 10. 1109/tnnls.2014.2308551 [53] h. li, c. ye, a. p. sample. idsense: human object interact detect base passiv uhf rfid. proceed acm confer human factor comput systems, pp. 25552564. acm, new york, ny, usa, 2015. doi: 10.1145/2702123. 2702178 [54] t. lin, c. wang, p.-c. lin. neural-network-bas context-awar handoff algorithm multimedia computing. acm transact multimedia computing, communications, applications, 4(3):17:1 17:23, 2008. doi: 10.1145/1386109.1386110 [55] m. liu, s. shan, r. wang, x. chen. learn expressionlet spatio-tempor manifold dynam facial express recognition. proceed ieee confer vision pattern recognition, pp. 17491756, 2014. doi: 10.1109/cvpr.2014.226 [56] p. liu, j. p. lewis, t. rhee. low-rank matrix complet reconstruct incomplet render images. ieee transact visu- aliz graphics, pp. 11, 2018. doi: 10.1109/tvcg. 2017.2722414 [57] z. lv, a. halawani, s. feng, h. li, s. u. rehman. multimod hand foot gestur interact handheld devices. acm transac- tion multimedia computing, communications, applications, 11(1s):10:110:19, 2014. doi: 10.1145/2645860 [58] w.-c. ma, d.-a. huang, n. lee, k. m. kitani. forecast inter- activ dynam pedestrian fictiti play. proceed ieee confer vision pattern recognition, pp. 46364644, 2017. doi: 10.1109/cvpr.2017.493 [59] s. mann. humanist computing: wearcomp new framework applic intellig signal processing. proceed ieee, 86(11):21232151, 1998. doi: 10.1109/5.726784 [60] s. marks, j. e. estevez, a. m. connor. holodeck: fulli immers virtual realiti visualis scientif engineer- ing data. proceed intern confer imag vision comput new zealand, pp. 4247. acm, new york, ny, usa, 2014. doi: 10.1145/2683405.2683424 [61] g. marqu k. basterretxea. effici algorithm accelerometer- base wearabl hand gestur recognit systems. proceed ieee intern confer embed ubiquit computing, pp. 132139, 2015. doi: 10.1109/euc.2015.25 [62] h. p. martnez g. n. yannakakis. deep multimod fusion: com- bine discret event continu signals. proceed intern confer multimod interaction, pp. 3441. acm, new york, ny, usa, 2014. doi: 10.1145/2663204.2663236 [63] h. meng, n. bianchi-berthouze, y. deng, j. cheng, j. p. cos- mas. time-delai neural network continu emot dimens predict facial express sequences. ieee transact cybernetics, 46(4):916929, 2016. doi: 10.1109/tcyb.2015.2418092 [64] p. molchanov, x. yang, s. gupta, k. kim, s. tyree, j. kautz. onlin detect classif dynam hand gestur recurr 3d convolut neural networks. proceed ieee confer vision pattern recognition, pp. 42074215, 2016. doi: 10.1109/cvpr.2016.456 [65] s. s. mukherje n. m. robertson. deep head pose: gaze-direct estim multimod video. ieee transact multimedia, 17(11):20942107, 2015. doi: 10.1109/tmm.2015.2482819 [66] j. c. niebl l. fei-fei. hierarch model shape appear human action classification. proceed ieee confer vision pattern recognition, pp. 18, 2007. doi: 10.1109/cvpr.2007.383132 [67] d. park, s. kim, j. lee, j. choo, n. diakopoulos, n. elmqvist. conceptvector: text visual analyt interact lexicon build- ing word embedding. ieee transact visual graphics, 24(1):361370, 2018. [68] k. park g. lee. fingmag: finger identif method smart- watch. extend abstract acm confer human author' version articl publish ieee vi mlui workshop. chang version publish prior publication. factor comput systems, pp. lbw2216:1lbw2216:6. acm, new york, ny, usa, 2019. doi: 10.1145/3290607.3312982 [69] r. j. peter l. itti. bottom-up: incorpor task- depend influenc comput model spatial attention. proceed ieee confer vision pattern recognition, pp. 18, 2007. doi: 10.1109/cvpr.2007.383337 [70] d. pineo c. ware. data visual optim computa- tional model perception. ieee transact visual graphics, 18(2):309320, 2012. doi: 10.1109/tvcg.2011. 52 [71] ramakant, n.-e.-k. shaik, l. veerapalli. sign languag recognit fusion 5dt data glove camera base information. proceed ieee intern advanc comput conference, pp. 639643, 2015. doi: 10.1109/iadcc.2015.7154785 [72] m. t. ribeiro, s. singh, c. guestrin. trust you?: explain predict classifier. proceed acm intern confer knowledg discoveri data mining, pp. 11351144. acm, new york, ny, usa, 2016. doi: 10. 1145/2939672.2939778 [73] b. schilit, n. adams, r. want. context-awar comput applica- tions. proceed workshop mobil comput system applications, pp. 8590, 1994. doi: 10.1109/wmcsa.1994.16 [74] c. schissler, c. loftin, d. manocha. acoust classif optim multi-mod render real-world scenes. ieee transact visual graphics, 24(3):1246 1259, 2018. doi: 10.1109/tvcg.2017.2666150 [75] r. sharma, v. i. pavlovic, t. s. huang. multimod human- interface. proceed ieee, 86(5):853869, 1998. doi: 10.1109/5.664275 [76] k. shilton. intervention: foreground operational- iz ethic technolog design. k. d. pimple, ed., emerg- ing pervas inform commun technologies: ethic challenges, opportun safeguards, pp. 177192. springer, dordrecht, 2014. [77] z. shu, s. xin, x. xu, l. liu, l. kavan. detect 3d point multipl featur stack auto-encoder. ieee transact visual graphics, pp. 11, 2018. doi: 10.1109/tvcg.2018.2848628 [78] a. sordoni, y. bengio, h. vahabi, c. lioma, j. grue simonsen, j.- y. nie. hierarch recurr encoder-decod gener context- awar queri suggestion. proceed acm intern confer inform knowledg management, pp. 553562. acm, new york, ny, usa, 2015. doi: 10.1145/2806416.2806493 [79] m. srivastava a. agarwal. classif emot speech implicit features. proceed intern confer industri inform systems, pp. 16, 2014. doi: 10.1109/ iciinfs.2014.7036518 [80] n. srivastava, e. mansimov, r. salakhudinov. unsupervis learn video represent lstms. proceed intern confer machin learning, pp. 843852, 2015. [81] p. suja, k. v. kumar, s. tripathi. dynam facial emot recog- nition 4d video sequences. proceed intern confer contemporari computing, pp. 348353, 2015. doi: 10. 1109/ic3.2015.7346705 [82] j. tompson, m. stein, y. lecun, k. perlin. real-tim continu pose recoveri human hand convolut networks. acm transact graphics, 33(5):169:1169:10, 2014. doi: 10.1145/ 2629500 [83] j. van wolfshaar, m. f. karaaba, m. a. wiering. deep con- volut neural network support vector machin gender recognition. proceed ieee symposium seri com- putat intelligence, pp. 188195, 2015. doi: 10.1109/ssci.2015. 37 [84] c. vondrick, h. pirsiavash, a. torralba. gener video scene dynamics. proceed advanc neural inform process systems, pp. 613621, 2016. [85] t.-h. vu, a. osokin, i. laptev. context-awar cnn person head detection. proceed ieee intern confer vision, pp. 28932901, 2015. doi: 10.1109/iccv.2015. 331 [86] w. webber, a. moffat, j. zobel. similar measur indefinit rankings. acm transact inform systems, 28(4):20:1 20:38, nov. 2010. doi: 10.1145/1852102.1852106 [87] k. wongsuphasawat, d. smilkov, j. wexler, j. wilson, d. mane, d. fritz, d. krishnan, f. b. viegas, m. wattenberg. visualiz- ing dataflow graph deep learn model tensorflow. ieee transact visual graphics, 24(1):112, 2018. doi: 10.1109/tvcg.2017.2744878 [88] x. xie, x. cai, j. zhou, n. cao, y. wu. semantic-bas method visual larg imag collections. ieee transact visual- izat graphics, pp. 11, 2018. doi: 10.1109/tvcg. 2018.2835485 [89] j. yi, x. mao, l. chen, y. xue, a. compare. facial express recognit consid individu differ facial structur texture. iet vision, 8(5):429440, 2014. doi: 10.1049/iet -cvi.2013.0171 [90] x. zhang, h.-f. brown, a. shankar. data-driven personas: con- struct archetyp user clickstream user telemetry. proceed acm confer human factor comput systems, pp. 53505359. acm, new york, ny, usa, 2016. doi: 10. 1145/2858036.2858523 [91] x. zhang, y. sugano, m. fritz, a. bulling. appearance-bas gaze estim wild. proceed ieee confer vision pattern recognition, pp. 45114520, 2015. doi: 10.1109/cvpr.2015.7299081 [92] z. zhang, j. m. girard, y. wu, x. zhang, p. liu, u. ciftci, s. canavan, m. reale, a. horowitz, h. yang, j. f. cohn, q. ji, l. yin. multi- modal spontan emot corpu human behavior analysis. proceed ieee confer vision pattern recognition, pp. 34383446, 2016. doi: 10.1109/cvpr.2016.374 author' version articl publish ieee vi mlui workshop. chang version publish prior publication. introduct relat work persona hci: user classif technique? action classification: action user behavior? intersect visual ml immers analyt neural network dataset pre-train weight studi method procedur experiment design data analysi result conclus discuss limit futur context-aware, adapt environ