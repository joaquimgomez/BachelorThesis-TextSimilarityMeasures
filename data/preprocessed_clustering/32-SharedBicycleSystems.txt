1 cluster cluster explan neural network jacob kauffmann, malt esders, gregoir montavon, wojciech samek, klaus-robert muller abstracta wealth algorithm develop extract natur cluster structur data. identifi structur desir sufficient: want understand data point assign given cluster. cluster algorithm offer systemat answer simpl question. propos new framework can, time, explain cluster assign term input featur comprehens manner. base novel theoret insight cluster model rewritten neural networksor neuralized.predict obtain network quickli accur attribut input features. showcas demonstr abil method assess qualiti learn cluster extract novel insight analyz data representations. index termsunsupervis learning, k-mean clustering, neural networks, neuralization, explain machin learn f 1 introduct cluster success unsupervis learn model reflect intrins heterogen common data gener- ation process [1], [2], [3], [4]. natur cluster structur observ varieti context gene express [5] ecosystem composit [6] textual data [7]. method accur identifi cluster structur object sustain research past decad [8]. basic techniqu k-mean [9] extend oper kernel featur space [10], [11], represent built deep neural network [12], [13], [14]. paper, bring new ingredi clustering: systemat human-interpret explan cluster assignments. end, leverag recent suc- cess interpret decis supervis machin learn models, shed light decis complex deep neural network classifi [15], [16], [17], [18], [19]. interpret cluster desper needed, consid- er main motiv perform cluster place knowledg discovery. espe- cialli high-dimension featur space, cluster knowledg discoveri provid prototyp data point cluster. prototypes, however, reveal featur prototypical. instead, j. kauffmann berlin institut technolog (tu berlin), 10587 berlin, germany. m. esder berlin institut technolog (tu berlin), 10587 berlin, germany. g. montavon berlin institut technolog (tu berlin), 10587 berlin, germany. e-mail: w. samek fraunhof heinrich hertz institute, 10587 berlin, germany. k.-r. muller berlin institut technolog (tu berlin), 10587 berlin, germany; depart brain cognit engi- neering, korea university, seoul 136-713, korea; max planck institut fur informatik, 66123 saarbrucken, germany. e-mail: klaus- (correspond authors: gregoir montavon, klaus-robert muller) like let cluster model explain cluster assign made. best knowledge, work attempt systemat comprehens obtain explanations. specifically, propos framework systemat explain cluster assign term input variables. framework draw novel theoret insight gener k-mean cluster model rewritten function equival neural network standard detection/pool layers. backbon guid explan process. technically, suggest appli follow steps: first, cluster model neural rewrit function equival neural network. cluster assign form output propag backward neural network lrp-type procedur [16] input variabl reached. outcom shown heatmap, highlight input variabl explain respect cluster memberships. result neuralization-propag procedur (or short, neon), test number showcas dataset cluster models. time, neon extract us insight cluster assignments. exper- iment demonstr practic valu two- step approach compar potenti simpler one-step approach neuralization. stress pro- pose method requir chang retrain cluster model. prove us futur shed new light exist cluster-bas typolog e.g. comput biologi [20], [21] consum data [22], research practition start us increasingli support scientif reason decisions. 1.1 relat work far, research explan method over- whelmingli focus case supervis learning. ar x iv :1 90 6. 07 63 3v 1 [ cs .l g ] 1 8 ju n 20 19 2 particular, method exist systemat explain cluster assignments. previou work fall follow categories: explain classif decis multipl direct taken explain nonlinear supervis models: method base gradient [23], [24], [25], local perturb [15], [26], surrog func- tion [17] propos explain gener machin learn predictions. method exploit layer structur machin learn model design layer-wis propag procedur [16], [27], [28], [29], [30] produc accur explan low comput cost. work focus explain classifiers, recent work sought extend principl type model kernel one-class svm [31], lstm recurr neural network [32]. valid cluster model type valid metric intern ones, example, base compact separ clus- ter [33]. propos look cluster stabil resampl perturb [34], [35]. metric assess cluster qualiti implicit choic cluster dis- tanc metric. second type valid techniqu us extern sourc (e.g. ground truth data labels) measur cluster puriti (e.g. absenc exampl differ- ent label cluster) [36]. extern valid assum avail label data, specif label match- ing true cluster structure. work includ user interfac develop better navig cluster structures, motiv e.g. applic biologi [37], [38], construct cluster prototyp visual deep cluster model imag data [14]. work help guid process cluster data produc us visualizations, answer question data point assign given cluster. 2 framework explain cluster propos new framework address (so far unsolved) problem explain cluster assignments. framework base concept neuralization- propag (neon) introduc later detail. work, develop neon broad class k-mean cluster model simpl [9] complex [11], [12]. extens neon success cluster paradigms, e.g. dbscan [39], affin propag [40], hierarch cluster [41], left futur work. k-mean algorithm partit data simpl convex region repres prototyp each. data point assign cluster nearest prototype. complex variant k-mean comput cluster input space directly, nonlinear potenti high-dimension featur space. induc kernel [11] map activ deep neural network [12]. nonlinear extens refer kernel k-mean deep k-mean provid flexibl implement complex non- convex cluster structur occur real world. on, assum train standard / kernel / deep k-mean model. feed data cluster model, data point get assign cluster. like explain cluster assign term input variables. neon approach appli consecut follow steps: step 1 neural cluster model: retrain- ing, convert k-mean model (whose cluster assign given score fc) multilay neural network. network produc exactli output k-mean model, structur suitabl propag predict fc input variables. step 2 propag cluster predictions: appli construct neural network layer-wis relev prop- agat (lrp) procedur [16] allow propag clus- ter assign backward arriv relev input features. order produc meaning explanations, propag mechan deriv deep taylor decomposit framework [28]. here, briefli motiv neon two-step approach compar hypothet one-step approach appli structure-agnost method (e.g. [17], [23], [25]) k-mean output directly. two-step approach follow advantages: first, convert k-mean neural network give access propagation-bas expla- nation methods. shown deliv accur explan low comput cost [42]. second, neural network support potenti detail explanations, e.g. let propag quantiti flow specif subset neuron [30]. procedur summar fig. 1 develop section 3 4. fc > 0 k-mean cluster x1 x2 . . . xd hk .. . fc r1 r2 . . . rd rk .. . fc 1: neural cluster model 2: propag cluster predict fig. 1. overview neon two-step approach cluster explanation. cluster model transform neural network. then, output neural network explain term input featur mean revers propag procedure. 3 neural cluster model neural network typic consist sequenc de- tection pool layers. layer plai similar role 3 simpl cell complex cell describ neuro- scienc [43], execut organ restor organ automata theori [44]. detect layer type ak = ( j ajwjk), detect function (e.g. rectifi identity). learn paramet wk form dis- crimin direct input space. pool layer parameterless reduc neuron pool re- gion singl neuron pool oper ak = p((aj)j). object recognit tasks, typic max-pool sum-pool operations. certain task anomali detect instead us min- pool [31]. goal map cluster assign model standard layer structures. start basic soft-assign model extend cover standard / kernel / deep k-means. 3.1 neural cluster assign model cluster assign model map data point membership probabl score soft-assign functions. common us exponenti function: p (c |x) = exp( oc(x)) k exp( ok(x)) (1) ok(x) measur outlier input x cluster k (cf. section 3.2). paramet stiff hyperparameter, inverse-temperatur neural network literatur [45] fuzzi context cluster [46], [47]. , eq. (1) approach indic func- tion nearest cluster hard clustering. soft cluster assign train time better account cluster membership uncertainty, test time produc smoother transit clusters. quantiti suitabl repres evid cluster membership logit function: fc(x) = log ( p (c |x) 1 p (c |x) ) (2) particular, posit evid cluster membership (i.e. p (c |x) = 0.5), neutral score fc(x) = 0. conversely, overwhelm evid cluster membership, fc(x) allow grow larg value, cluster probabl satur 1.0. similar argument favor log-quant context one-class svm [31]. proposit 1. logit quantifi cluster membership written soft min-pool layer fc(x) = min k 6=c {ok(x) oc(x)}, (3) defin min{} = 1 log exp(()). proof given supplement (appendix a). neural logit function lend follow- ing interpret cluster assignment: data point x member cluster c outlier cluster inferior outlier compet cluster k 6= c. words, member cluster c hypothes membership cluster rejected. 3.2 neural standard k-mean k-mean algorithm find set centroid (k)k mini- mize object function min ik ik xi k2, (4) ik indic data point member cluster k. model cluster outlier k-mean ok(x) = x k2 squar distanc centroid. inject mea- sure outlier eq. (3) give two-lay detection- pool neural network standard k-mean hk = w > k x+ bk (layer 1) fc = min k 6=c {hk} (layer 2) layer linear detect layer param- eter wk = 2 (c k) bk = k2 c2, second layer min-pool proposit 1. architectur illustr fig. 2 (left). 3.3 neural kernel k-mean kernel cluster method describ [11] perform k-mean featur space (x) induc kernel, i.e. (x),(x) = k(x,x). approach variant spectral cluster [48], [49], omit normal dimens reduct step. kernel k-mean opti- mizat problem written as: min ik ik (xi) k2 (5) (k)k set centroid featur space. following, restrict discuss case gaussian kernel k(x x2) = exp( x x2). explicit featur map kernel, k-mean problem rewritten kernel form. let k = z 1 k jck (xj) unit-norm centroid cluster k, zk = ( j,jck k(xjxj 2))0.5. norm data cluster featur space constant, distanc minim origin k- mean formul replac maxim dot product (xi),k. specifically, problem eq. (5) rewritten as: max ik ikik(xi) ik(x) = (x),k = z1k jck k(x xj 2) dot product data centroid, interpret parzen window estim measur inlier x respect cluster k. like 4 compet contributor cluster contributor cluster competitor cluster standard k-mean kernel k-mean deep k-mean competitor lin ea r m -p oo l m ax -p oo l lin ea r m -p oo l m -p oo l po ol nv lin ea r m -p oo l po ol nv... fig. 2. illustr neural k-mean models. compos success detect pool layers. [31], outlier function obtain inlier- ness function applic invers kernel function. specifically, defin develop outlier function as: ok(x) = k1(ik(x)) = 1 log ( z1k jck exp( x xj 2) ) = min jck { x xj2 1 logz1k } (6) min{} revers log-sum-exp pool function introduc proposit 1. words, point x outlier cluster k distant point cluster. inject outlier function ok(x) soft-assign model eq. (1), result cluster model interpret featur space: proposit 2. soft-assign model base outlier score relat power-bas assign model kernel featur space. p (c |x) = exp( oc(x)) k exp( ok(x)) = ic(x) / k ik(x) / . (7) proof given supplement (appendix b). assign similar propos [50], [51]. focu structur sequenc comput distanc logit neural network. compos outlier eq. (6) logit computa- tion (eq. (3)) give four-lay neural network: kernel k-mean (naive) djk = x xj2 + bk (layer 1) ok = min jck {djk} (layer 2) hk = ok oc (layer 3) fc = min k 6=c {hk} (layer 4) bk = 1 logz1k . unlik standard k-means, layer linear anymore, consequently, neu- ron layer longer detect specif input space directions. however, sequenc comput reorgan deliv desir neural network structure: kernel k-mean (improved) aijk = w > ijx+ bijk (layer 1) zjk = max icc {aijk} (layer 2) hk = min jck {zjk} (layer 3) fc = min k 6=c {hk} (layer 4) linear layer paramet wij = 2 (xi xj) bijk = xj2 xi2 + bk bc, follow cascad pool layers. appendix c supplement exactly. better structur model shown graphic fig. 2 (middle). compar naiv architecture, number op- erat increas drastically: size layer quadrat number exampl cluster, linear naiv architecture. reduc comput cost tradeoff possibl report literature, e.g. reduc set [52], [53], replac n data point smaller set support vector obtain vector quantiz [54]. return, gain comput structur close resembl obtain standard k-means, higher number neuron allow detail analysi cluster assignments. 3.4 neural deep k-mean consid deep neural network abstract sequenc layer-wis map (x) = l 1(x). 5 like section 3.3, appli k-mean featur space: min ik ik (x) k2 (8) unlik kernel case, featur map (x) given explicitli learn backpropag pro- duce desir cluster structure. defin outlier function: ok(x) = (x) k2 inject eq. (3) let rewrit model neural network compos l layer comput deep represent follow layer obtain cluster scores: deep k-mean = l 1(x) (layer 1 . . . l) hk = w > k a+ b (layer l+ 1) fc = min k 6=c {hk} (layer l+ 2) wk = 2 (c k) b = k2 c2. layer type standard k-means, receiv neuron activ input. overal architectur illustr fig. 2 (right). 4 propag cluster predict come main objective: explain cluster assign term input features. problem explan studi extens context supervis learn (e.g. classif regression). particular, layer-wis relev propag (lrp) [16] shown stabli quickli explain predict broad rang classifi varieti applic (e.g. [55], [56], [57], [58]). requir lrp model structur neural network. step extend lrp standard / kernel / deep k-means, perform sec- tion 3 prerequisit step neuralization. specifically, lrp explain evid cluster membership, appear output unsupervis neural networks. lrp procedur illustr simpl neural network fig. 3. start layer rc set fc, neural network output. quantiti rc propag layer layer, input. let j k indic neuron consecut layers, prop- agat procedur design neuron j rjk = rk rj = k rjk. extension, iri = j rj = k rk = fc, i.e. outcom propag procedur sum-decomposit fc input variabl (cf. [27], [29], [59] techniqu similar conserv mechanism). deep taylor decomposit (dtd) [28] develop wai determin redistribut rule set layer. dtd view task redistribut identifi term taylor expans rk express function lower-lay activ (aj)j . term defin messag rjk sent lower- layer neurons. let = (aj)j vector lower-lay ex pl io n pu t o ut pu t xi aj ak fc rc rkrjri fig. 3. illustr lrp techniqu explanation. neural network output propag neural network mean local redistribut rule input variabl reached. activ neuron k directli connected. first- order taylor expans rk(a) refer point written as: rk(a) = rk(a) + j [rk(a)]j (aj aj) +o(aa>) element sum linear term serv messag rjk. however, function rk(a) complex, find good refer point difficult, unclear first-ord expans suffici model rk accurately. therefore, second idea deep taylor decomposit us place rk, relev model rk easier analyze. following, choos rk = pk (ak + k), affin function neuron activation, constant paramet pk > 0 k 0 set wai rk(a) = rk(a). appli deep taylor decomposit need consid aspects: (1) root point chosen first-ord term taylor expans ap- proxim function relev domain. (2) redistribut step result score (rj)j model affin function, i.e. rj = pj (aj +j) pj > 0 j 0. second criterion allow propag procedur pursu layer below, continu input layer. 4.1 propag pool layer consid propag soft-min-pool layer ak = min j {aj}. type layer comput top- layer logit, pool member compet cluster kernel-bas clustering. relev model given rk(a) = pk ( min j {aj}+ k ) observ function rk linear parameter- iz line {a t 1, t r}. take root point line, taylor expans give first-ord terms: rjk = exp(aj) j exp(aj) rk (9) min-take-most redistribut scheme. observ j rjk = rk, impli redistribut con- servative. appli top-layer, propag 6 rule redistribut relev mainli nearest compet clusters. appli layer kernel clustering, propag rule redistribut mainli activ data point compet clusters. soft-max- pool oper second layer kernel clustering, similar result follow, particular, max-take-most redistribut scheme, neg sign eq. (9) replac posit signs. proposit 3. redistribut relev pool layer local approxim linear function neuron activation, i.e. rj = pj (aj+j) pj 0 j 0. approxim increasingli better (proof appendix d supplement). result ensur score rj redistribut lower layers. 4.2 propag linear layer second type layer occur studi cluster model linear layer type ak = w>k + bk. relev model layer given by: rk(a) = pk (w>k a+ bk + k) taylor expans root point give rk(a) = j pk wjk (aj aj) note absenc second- higher-ord term linear relev model. here, vast choic choos root point. simplest strategi view bia input neuron 0, i.e. a0 = 1, w0k = bk + k, choos root point = 0. case, z-rule [28]: rj = k ajwjk 0,j ajwjk rk (10) observ relev written rj = k ajwjkpk = ajpj pj = k wjkpk, have pk lo- calli approxim constant impli pj local approxim constant, condit set deep taylor decomposit abl continu propag layer below. note redistribut procedur approxim conserv here, relev redistribut neuron a0. practice, root point = 0 far data turn produc neg score (rj)j . nearer root point propos [28] lead z+ zb-rule contribut ajwjk eq. (10) replac ajw + jk xiwij liw+ij hiw ij respectively, ()+ = max(0, ), () = min(0, ), li xi hi. modifi rule appli layer activations, pixel layer respectively. 5 experi section 3 4 describ detail step neuralization-propag (neon) approach. following, test method showcases: sec- tion 5.1 test neon perform mnist data standard / kernel / deep k-mean models. section 5.2 show 20newsgroup data neon produc better assess cluster qualiti compar con- vention puriti metric. section 5.3 demonstr propos method data analysi techniqu extract insight multipl layer vgg-16 imag classifier. 5.1 cluster explan mnist data experi test neon mnist handwrit- digit dataset. cluster structur given class labels. goal section learn cluster structure, studi standard, kernel, deep k- mean abl repres clusters, neon explain cluster assign pixel-wise. specifically, solv optim problem eqs. (4), (5), (8), assign ik fix ground truth. standard k-mean model built simpli averag- ing data point class. kernel k-mean reduc comput build set 10 support vector class, obtain standard k-means. deep k-means, fix layer centroid constant values, learn cluster structur backpropag k-mean error three-lay fulli connect network. choos nonlinear modifi relu function max(0, x) 0.75 max(0, x 1), second term encourag agglomer data compact clusters. architecture, stiff paramet chosen highest soft-assign probabl averag 0.9 (we lower 0.8 standard k-mean account higher rigid model). kernel paramet chosen self-similar score rep- resent 90% total similar score support vectors. figur 4-a show visual model k- mean cluster (here, t-sne visual cluster logits), classif accuracy. unsurprisingly, nonlinear ad kernel deep k-mean allow build cluster structur better match class label (accuraci 0.9 vs. 0.63 standard k-means). figur 4-b show neon pixel-wis explan cluster assign model digit mnist test set. red color indic pixel rele- vant cluster membership. irrelev pixel shown white, pixel appear contradictori shown blue. pixel-wis explan similar models, small differ observed. neon appli standard k-mean produc heatmap vari inten- siti fulli align digits. effect attribut rigid standard k-mean fulli express class structure. kernel k-mean deep k-mean abl repres class structur accurately, neon explan consequ close uniformli align input digit. example, digit 3, cluster assign perform base pixel area left extrem digit. pixel relev cluster membership fill turn digit 3 8. figur 4-c show function neon inherit explicit relianc neural network structure: relev flow neural network isol highlight distinct aspect expla- nation. here, isol relev flow neuron repres compet clusters. detail analysi shed light individu contribu- tion overal cluster assignment: explan digit 0 compos vertic bar differenti competitor 1, top-level stroke eman 7 kernel (acc: 0.94) deep (acc: 0.96) standard (acc: 0.63) k-mean cluster b cluster explan neon c detail neon explan 1 3 5 7 9 0 2 4 6 8 1 3 5 7 9 0 2 4 6 8 fig. 4. cluster model explain neon (in experiment, cluster hard-cod repres digit classes). right panel, detail explan obtain dissoci relev flow competitor branch neural network. competitor 3, singular pattern competitors. second exampl identifi competitor 5 import support cluster assign 3, highlight top-part digit expla- nation. competitor receiv relev cluster 2 8. principle, detail explan obtain (e.g. term support vector neuron activations) depend choic architecture. benchmark evalu current establish method ex- plain cluster membership, construct simpl base- line method similar explain classifiers: sa(x, fc) = (fc(x))2 gi(x, fc) = fc(x) x sr(x,x, fc) = (x x)2/x x2 fc(x) ig(x,x, fc) = x =x fc() d. construct baselin reli neural net- work structur identifi section 3 describ one-step structure-agnost approaches. baseline, sensit analysi (sa), base local evalu gradient [23]. second baseline, gradient input (gi) integr gradient input data [60]. like neon, baselin run o(forward pass). fourth baselin us optim refer point x = arg min x 2 subject f() 0. optim step compu- tation expens reveal us global inform function. baselin sr understood build surrog linear model {x, x} follow decomposit prediction. seen variant lime [17]. baselin ig comput path integr f segment x x seen special case integr gradient [25]. figur 5-a show exampl explan produc differ techniques. gi tend produc spuriou neg evid explanation. sa align digit, imbal wai score spatial distributed. sr ig similar neon sparser. figur 5-b evalu accuraci explan f x (f)2 sr(...) ig(...)neon accur fast detail * standard kernel deep f x (f)2 sr(...) ig(...)neon ba c cl te r ev id en ce f (x ) * kernel k-means, requir support vector reduct heuristic. standard kernel deep 0 2 8 18 32 51 73 100 percentag flip f x (f)2 neon sr(...) ig(...) 0 2 8 18 32 51 73 100 percentag flip f x (f)2 neon sr(...) ig(...) 0 2 8 18 32 51 73 100 percentag flip f x (f)2 neon sr(...) ig(...) fig. 5. benchmark evalu neon. tabl left show explan obtain techniqu summari strength weaknesses. accuraci explan assess quantit pixel flip (cf. plot right). lower pixel-flip curve, accur explanation. note pixel-flip tend overst perform sr(...) ig(...) share root point x. plot contain inform cluster evid shown rel neon. 8 produc neon compet approach pixel-flip [42] analysis. pixel-flip sequenti flip pixel relevant. flip oper- ation us consist replac featur x root point x above. flip procedure, monitor quickli function valu f(x) repres cluster evid drops. faster drops, better explanation. unlik origin method [42], order featur comput pixel data point data point individually. variant better reflect abil explan techniqu assign relev proport actual evid example. observ neon systemat better sa gi, inferior sr ig. note baselin advantag share root point x evalu procedure. figur 5-c give summar tabular comparison be- tween differ explan techniques. sa gi comput quickli explan lack accuraci demonstr pixel-flip evaluation. sr ig baselin accur significantli expens compute. overall, benchmark experi verifi neon provid solut far unsolv problem explain cluster assignments, wai accurate, computation efficient, potenti high level detail. 5.2 explain kernel cluster newsgroup data follow experi show neon produc rich nuanc assess cluster qual- iti goe convent metric cluster purity. consid experi 20newsgroup dataset [61] contain messag 20 public mail lists, record year 1996. headers, footer quot remov messages. extract consecu- tive letter length 3 longer token t document d project gener purpos word-vector space (t) w [62]. stop word removed. hereafter, document removed. document embed aggreg vector space take mean word vector (x = 1d td (t)). document vector reduc 50 dimens pca improv cluster runtime. t-sne embed preprocess data color- code true label (i.e. mail list messag posted) given fig. 6 (top right). cluster algorithm, object assign nearbi point cluster. consid kernel k-mean model. bandwidth paramet gaussian kernel chosen 50% similar score fall b#data point #cluster c nearest neighbors. initi kernel cluster ground truth label train kernel k-mean model em-styl procedur (see appendix e sup- plement details), cluster assign converg local optimum final assign given fig. 6 (bottom middle). observ cluster separ initialization. focu assess qualiti learn clusters. standard cluster puriti metric give score 45% test set. score, conclud algorithm learn bad clusters. instead, neon aim expos user given document relev membership certain cluster. here, explan term dimens input vector x easili interpret human a. document word-vector space keyboard mous appl fruit = || () b. network view redistribut av er ag e lin ea r m ax -p oo l m -p oo l m -p oo l() () () () c. function view redistribut keyboard mous appl fruit true label cluster assign f. rec.motorcycl i'm sure older bikes, yamaha virago 535 spec'd seat height 27.6 in. honda shadow 27.2 in. g. misc.forsal sale: thule car rack 2 bike holder accessories. come nissan pathfind bracket bui appropri on car cheap. look $100.00 everything. live bethesda area. thank interest. d. talk.politics.gun capit offense, warrant arrest warrant, search warrant. words, evid illeg arms, suggest judg sign licens search illeg evidence. e. sci.crypt salient differ number 5th amend relat suprem court opinions. court limit 5th amend protect "testimonial" evidence, oppos physic evidence. question hing crypto kei consid "testimonial" evidence. suppos argument way, obvious hope consid testimonial. fig. 6. applic neon cluster newsgroup data. left: depict cluster assign explan process. right: true label t-sne embedding. bottom: newsgroup text word relev cluster membership highlighted. grai word vocabulary. 9 word document embed usual abstract. interpret word-level explan achieved, observ map word docu- ment (an averag word vectors) layer neural kernel k-means, linear. thus, combin singl big linear layer (cf. figur 6-b) take input word distinctly. redistribut phase, appli z-rule (eq. (10)) big layer, lead attribut cluster evid individu word dimension. score pool word dimensions, lead singl relev score rt individu word t (cf. figur 6- c). explan render highlight text. messag fig. 6-d e assign cluster post differ newsgroups. here, neon highlight document term evidence. close relat term like court, warrant, illeg testi- monial highlight well. fact evid messag relat word present constitut explan justif messag assign cluster. second example, consid messag figur 6-f g, post differ groups, assign cluster. messag f discuss specif motorcycles, whilst messag g sale offer bike hold car rack. relev term brand term bike car. part like rack seat provid evid cluster membership. again, word select hint meaning similar messages, justifi assign messag cluster. overall, showcas experiment, minim cluster object led low puriti score. accord common valid procedures, constitut reason rejection. instead, cluster mem- bership explan produc neon pinpoint user meaning cluster membership decis speak favor learn cluster structure. 5.3 analyz deep represent final experi demonstr neon appli cluster assessment, particular, better understand represent layer deep neural network. problem receiv grow attent recent year [63], [64], [65]. perform experi pretrain vgg16 convolut network [66]. feed imag time network, lead spatial activ featur map layer. collect activ max-pool layer, build dataset, spatial locat layer correspond data point. rescal data point unit norm, strong dispers lead singular clusters. this, appli k-mean k = 8 clusters. neon, obtain relev valu assign data point 8 clusters. contrari experi previous dis- cuss text, comput relev data point time pool relevance. focu analysi posit cluster evidence, propag relev data point fc(a) 0. relev propag backward network, result heatmap pixel contribut cluster. complex featur increas layer deep neural network, expect repres neon explan well. featur extract vgg-16 consist 31 layers, altern 3 3 convolut layers, relu layer total 2 2 max-pool layers. layer pick experi layer 17, 24 31. layer 17 max-pool layer, layer 24 fourth max-pool layer layer 31 fifth. cluster explan shown fig. 7 artifici spiral image, citi streetcar image, well-known dog plai poker images, titl poker game cassiu marcellu coolidge, 1894. imag resolut 448 448, twice resolut imagenet images. order datapoint cluster upper layer network. propag relev layer vgg-16, hybrid z/z+-rule contribut set aj (wjk + w+jk) set 0.25 layer 1-17 0.1 layer 18 above. higher valu lower layer make explan noisy. artifici spiral image, cluster layer 17 map edg certain angl orient color (black white) edg type (black-to-white, white-to-black). interestingly, strictli vertic strictli horizont edg fall cluster high angl specificity, edg angl fall broader clusters. build cluster layer 24, color edg inform prominent. cluster select angl curvature. spiral imag heatmap layer 31 like im- ages. inform contain spiral pictur simpl artificial, neural network extract complex featur higher layer extract cluster meaningless. citi streetcar image, cluster layer 17 map color edg similar angles, similarli artifici spiral image. note circl traffic sign right corner split seper clusters, differ diagon edges, cluster vertic horizont edges. cluster encod sky texture, cluster bushes/pl textur cluster street texture. layer 31, recept field neuron span quarter imag (recept field size 212 212, imag size 448 448). cluster level are, expected, abstract captur real-world objects. cluster exclus rep- resent streetcar, cluster repres traffic sign traffic lights. interestingly, cluster specif repres tini street light. poker game image, similar cluster street car image. layer 17, cluster repres horizont vertic edges. additionally, cluster specif shade green textur background image. layer 31, cluster form high level concepts. cluster big lamp image, cluster paint upper right, cluster repres dogs. note 10 la ye r 17 la ye r 31 la ye r 1 7 la ye r 2 4 la ye r 17 la ye r 31 artifici spiral citi streetcar "poker game" (coolidge, 1894) vertic edg specif diagon edg select curvatur street car traffic lights, traffic sign green textur dog horizont edg horizont edg vertic edg horizont edg sky textur vertic edg lamp green background fur textur street light build diagon edg diagon edg street textur top-left curvatur paint tabl clutter build textur fig. 7. neon analysi imag repres differ layer deep neural network (pretrain vgg16). k-mean cluster k = 8 perform layers. column show pixel-contribut clusters. cluster layer 31 littl heat overall. code specif object highli separ clusters, result high soft- assign probabilities. accordingly, result littl relev propagated. also, littl relev propagated, get dilut part image, lead fainter heatmap. neon-bas analysi deep network sens vgg-16 represent select images. imag (streetcar poker) help understand vgg-16 repres progress disentangl natur concept outdoor/indoor scenes. in- stead, imag (spiral) us interest specif properti neural network, e.g. model curvatur layer, demonstr flexibl analysis. 6 conclus propos gener framework explain clus- ter assignments. method converts, retraining, cluster model function equival neural network compos detect pool layers. convers step call neural en- abl cluster assign effici attribut input variabl mean revers propag procedure. quantit evalu show explan method capabl identifi cluster-relev input fea- ture precis systemat manner. extract insight pattern varieti data involv text, natu- ral images, representations. neural network structur elicit allow detail target explanations. overall, method propos complement standard cluster valid techniqu provid rich interpret feedback natur cluster built. furthermore, pair well- function cluster algorithm, provid us tool data analysi complex data distribut summar finit clusters, ex- pose human interpret manner. acknowledg work support german ministri edu- cation research berlin big data centr (01is14013a) berlin center machin learn (01is18037i), 11 german research foundat (dfg) math+: berlin mathemat research center (exc 2046/1, project-id: 390685689). work partli support institut inform & commun technolog plan & evalu (iitp) grant fund korea govern (no. 2017-0-00451, no. 2017-0-01779). refer [1] a. k. jain, m. n. murty, p. j. flynn, data clustering: review, acm comput. surv., vol. 31, no. 3, pp. 264323, 1999. [2] r. xu d. c. w. ii, survei cluster algorithms, ieee trans. neural networks, vol. 16, no. 3, pp. 645678, 2005. [3] a. k. jain r. c. dubes, algorithm cluster data. upper saddl river, nj, usa: prentice-hall, inc., 1988. [4] t. hastie, r. tibshirani, j. friedman, element statist learning. springer new york, 2009. [5] d. jiang, c. tang, a. zhang, cluster analysi gene express data: survey, ieee trans. knowl. data eng., vol. 16, no. 11, pp. 13701386, 2004. [6] h. celik j. gore, cluster commun structur replic ecosystem follow long-term bacteri evolut experiment, natur communications, vol. 5, no. 1, aug. 2014. [7] d. mekala, v. gupta, b. paranjape, h. karnick, scdv : spars composit document vector soft cluster distribut representations, proceed 2017 confer empir method natur languag processing, 2017, pp. 659 669. [8] a. k. jain, data clustering: 50 year k-means, pattern recognit letters, vol. 31, no. 8, pp. 651666, 2010. [9] j. macqueen, method classif analysi multivari observations, proceed fifth berkelei sym- posium mathemat statist probability, volum 1: statistics. berkeley, calif.: univers california press, 1967, pp. 281297. [10] j. shi j. malik, normal cut imag segmentation, ieee trans. pattern anal. mach. intell., vol. 22, no. 8, pp. 888905, 2000. [11] i. s. dhillon, y. guan, b. kulis, kernel k-means: spectral cluster normal cuts, proceed tenth acm sigkdd intern confer knowledg discoveri data mining, 2004, pp. 551556. [12] j. xie, r. b. girshick, a. farhadi, unsupervis deep embed- ding cluster analysis, proceed 33nd intern confer machin learning, 2016, pp. 478487. [13] j. r. hershey, z. chen, j. l. roux, s. watanabe, deep clustering: discrimin embed segment sep- aration, ieee intern confer acoustics, speech signal processing, 2016, pp. 3135. [14] m. caron, p. bojanowski, a. joulin, m. douze, deep cluster- ing unsupervis learn visual features, 15th european confer vision, 2018, pp. 139156. [15] m. d. zeiler r. fergus, visual understand convolut networks, 13th european confer vision, 2014, pp. 818833. [16] s. bach, a. binder, g. montavon, f. klauschen, k.-r. muller, w. samek, pixel-wis explan non-linear classifi de- cision layer-wis relev propagation, plo one, vol. 10, no. 7, p. e0130140, 2015. [17] m. t. ribeiro, s. singh, c. guestrin, trust you?: explain predict classifier, proceed 22nd acm sigkdd intern confer knowledg discoveri data mining, 2016, pp. 11351144. [18] r. r. selvaraju, m. cogswell, a. das, r. vedantam, d. parikh, d. batra, grad-cam: visual explan deep network gradient-bas localization, ieee intern confer vision, 2017, pp. 618626. [19] m. alber, s. lapuschkin, p. seegerer, m. hagele, k. t. schutt, g. montavon, w. samek, k. muller, s. dahne, p. kindermans, innvestig neural networks! journal machin learn re- search, vol. 20, pp. 18, 2019. [20] s. tavazoie, j. d. hughes, m. j. campbell, r. j. cho, g. m. church, systemat determin genet network architec- ture, natur genetics, vol. 22, no. 3, pp. 281285, jul. 1999. [21] g. ciriello, m. l. miller, b. a. aksoy, y. senbabaoglu, n. schultz, c. sander, emerg landscap oncogen signatur human cancers, natur genetics, vol. 45, no. 10, pp. 1127 1133, sep. 2013. [22] a. k. kau, y. e. tang, s. ghose, typolog onlin shop- pers, journal consum marketing, vol. 20, no. 2, pp. 139156, apr. 2003. [23] j. m. zurada, a. malinowski, i. cloete, sensit analysi minim input data dimens feedforward neural network, ieee intern symposium circuit systems, 1994, pp. 447450. [24] d. baehrens, t. schroeter, s. harmeling, m. kawanabe, k. hansen, k. muller, explain individu classif deci- sions, journal machin learn research, vol. 11, pp. 18031831, 2010. [25] m. sundararajan, a. taly, q. yan, axiomat attribut deep networks, proceed 34th intern confer machin learning, 2017, pp. 33193328. [26] s. m. lundberg s. lee, unifi approach interpret model predictions, advanc neural inform process system 30, 2017, pp. 47684777. [27] w. landecker, m. d. thomure, l. m. a. bettencourt, m. mitchell, g. t. kenyon, s. p. brumby, interpret individu classifi- cation hierarch networks, ieee symposium computa- tional intellig data mining, 2013, pp. 3238. [28] g. montavon, s. lapuschkin, a. binder, w. samek, k. muller, explain nonlinear classif decis deep taylor decomposition, pattern recognition, vol. 65, pp. 211222, 2017. [29] a. shrikumar, p. greenside, a. kundaje, learn import featur propag activ differences, proceed- ing 34th intern confer machin learning, 2017. [30] g. montavon, w. samek, k. muller, method interpret understand deep neural networks, digit signal process- ing, vol. 73, pp. 115, 2018. [31] j. kauffmann, k. muller, g. montavon, explain anomalies: deep taylor decomposit one-class models, corr, vol. abs/1805.06230, 2018. [32] l. arras, g. montavon, k. muller, w. samek, explain recurr neural network predict sentiment analysis, proceed 8th workshop comput approach subjectivity, sentiment social media analysis, 2017, pp. 159168. [33] m. halkidi, y. batistakis, m. vazirgiannis, cluster valid techniques, j. intell. inf. syst., vol. 17, no. 2-3, pp. 107 145, 2001. [34] t. lange, v. roth, m. l. braun, j. m. buhmann, stability- base valid cluster solutions, neural computation, vol. 16, no. 6, pp. 12991323, 2004. [35] m. meila, tell cluster (approximately) correct convex relaxations, advanc neural inform process system 31, 2018, pp. 74187429. [36] c. d. manning, p. raghavan, h. schutze, introduct inform retrieval. cambridg univers press, 2008. [37] t. metsalu j. vilo, clustvis: web tool visual cluster multivari data princip compon analysi heatmap, nucleic acid research, vol. 43, no. w1, pp. w566 w570, 2015. [38] m. kern, a. lex, n. gehlenborg, c. r. johnson, interact visual explor refin cluster assignments, bmc bioinformatics, vol. 18, no. 1, sep. 2017. [39] m. ester, h. kriegel, j. sander, x. xu, density-bas algorithm discov cluster larg spatial databas noise, proceed second intern confer knowledg discoveri data mining, 1996, pp. 226231. [40] d. dueck b. j. frey, non-metr affin propag unsupervis imag categorization, ieee 11th intern confer vision, 2007, pp. 18. [41] j. h. ward, hierarch group optim object func- tion, journal american statist association, vol. 58, no. 301, pp. 236244, mar. 1963. [42] w. samek, a. binder, g. montavon, s. lapuschkin, k.-r. muller, evalu visual deep neural net- work learned, ieee transact neural network learn systems, vol. 28, no. 11, pp. 26602673, 2017. [43] d. h. hubel t. n. wiesel, recept fields, binocular inter- action function architectur cat visual cortex, journal physiology, vol. 160, no. 1, pp. 106154, jan. 1962. 12 [44] j. von neumann, probabilist logic synthesi reliabl organ unreli components, automata studies, vol. 34, pp. 4398, 1956. [45] t. hofmann j. m. buhmann, pairwis data cluster determinist annealing, ieee trans. pattern anal. mach. intell., vol. 19, no. 1, pp. 114, 1997. [46] j. c. bezdek, pattern recognit fuzzi object function algorithms. norwell, ma, usa: kluwer academ publishers, 1981. [47] z.-d. wu, w.-x. xie, j.-p. yu, fuzzi c-mean cluster al- gorithm base kernel method, proceed fifth intern confer comput intellig multimedia applications, 2003, pp. 4954. [48] m. meila j. shi, learn segment random walks, advanc neural inform process system 13, 2000, pp. 873879. [49] a. y. ng, m. i. jordan, y. weiss, spectral clustering: analysi algorithm, advanc neural inform process system 14, 2001, pp. 849856. [50] y. kanzawa, maxim model bezdek-lik spheric fuzzi c-mean clustering, ieee intern confer fuzzi systems, 2014, pp. 24822488. [51] , kernel maxim model bezdek-lik spheric fuzzi c-mean clustering, 11th intern confer model decis artifici intelligence, 2014, pp. 108121. [52] b. scholkopf, s. mika, c. j. c. burges, p. knirsch, k. muller, g. ratsch, a. j. smola, input space versu featur space kernel-bas methods, ieee trans. neural networks, vol. 10, no. 5, pp. 10001017, 1999. [53] b. scholkopf a. j. smola, learn kernels: support vector machines, regularization, optimization, beyond. mit press, 2002. [54] r. zhang a. i. rudnicky, larg scale cluster scheme kernel k-means, 16th intern confer pattern recognition, 2002, pp. 289292. [55] s. lapuschkin, a. binder, k.-r. muller, w. samek, under- stand compar deep neural network ag gender classification, proceed ieee intern confer vision workshops, 2017, pp. 16291638. [56] y. ding, y. liu, h. luan, m. sun, visual under- stand neural machin translation, proceed 55th annual meet associ comput linguistics, 2017, pp. 11501159. [57] f. horst, s. lapuschkin, w. samek, k.-r. muller, w. i. schollhorn, explain uniqu natur individu gait pat- tern deep learning, scientif reports, vol. 9, no. 1, feb. 2019. [58] l. perotin, r. serizel, e. vincent, a. guerin, crnn-base multipl doa estim acoust intens featur ambison recordings, j. sel. topic signal processing, vol. 13, no. 1, pp. 2233, 2019. [59] j. zhang, s. a. bargal, z. lin, j. brandt, x. shen, s. sclaroff, top-down neural attent excit backprop, intern journal vision, vol. 126, no. 10, pp. 10841102, 2018. [60] m. ancona, e. ceolini, c. oztireli, m. gross, better understand gradient-bas attribut method deep neural networks, 6th intern confer learn repre- sentations, 2018. [61] t. joachims, probabilist analysi rocchio algorithm tfidf text categorization, proceed fourteenth intern confer machin learning, 1997, pp. 143151. [62] t. mikolov, k. chen, g. corrado, j. dean, effici estima- tion word represent vector space, 1st intern confer learn representations, 2013. [63] a. nguyen, a. dosovitskiy, j. yosinski, t. brox, j. clune, synthes prefer input neuron neural network deep gener networks, advanc neural inform process system 29, 2016, pp. 33873395. [64] b. zhou, d. bau, a. oliva, a. torralba, interpret deep visual represent network dissection, ieee transact pattern analysi machin intelligence, pp. 11, 2018. [65] s. lapuschkin, s. waldchen, a. binder, g. montavon, w. samek, k.-r. muller, unmask clever han predictor assess- ing machin learn, natur communications, vol. 10, p. 1096, 2019. [66] k. simonyan a. zisserman, deep convolut net- work large-scal imag recognition, 3rd intern con- ferenc learn representations, 2015. 1 cluster cluster explan neural network (supplementari material) jacob kauffmann, malt esders, gregoir montavon, wojciech samek, klaus-robert muller document contain supplementari materi support result experi main paper. appen- dice ac contain proof justif non-trivi step taken section 3 neural k-mean models. appendix d provid theoret justif treatment min-pool layer section 4. appendix e describ modifi train procedur pro- duce kernel k-mean model section 5. appendix neural soft cluster assign prove proposit 1 main paper, express logit cluster assign probabl neural network type min-pool differ outlier scores. proof. soft cluster assign model given p (c | x) = exp( oc(x)) k exp( ok(x)) . (1) consid logit probabl score logit(c | x) = log ( p (c | x) 1 p (c | x) ) (2) describ evid cluster membership. like express quantiti neural network. insert (1) (2) gives: logit(c | x) = log ( exp(oc(x)) k exp(ok(x)) 1 exp(oc(x)) k exp(ok(x)) ) = log exp( oc(x)) k 6=c exp( ok(x)) = log 1 k 6=c exp( (ok(x) oc(x))) = log k 6=c exp( (ok(x) oc(x))) = min k 6=c {ok(x) oc(x)} underli min-pool structur cluster assign logit appear explicitly. appendix b connect power cluster assign appendix prove proposit 2 main paper state kernel k-mean propos soft-min clus- ter assign outlier score defin oc(x) = 1 log ic(x) express power-bas softmax assign measur inlier ic(x). proof. result follow directli properti ab = exp(b log(a)) > 0 b r: p (c | x) = exp( oc(x)) k exp( ok(x)) = exp( log ic(x)) k exp( log ik(x)) = ic(x) / k ik(x) / power-bas soft-assign model. appendix c improv neural kernel k-mean appendix, function equival naiv improv variant neural kernel k-mean model describ section 3.3. first, min {} oper commut w.r.t. addit scalars: min j {aj}+ c = [ 1 log j exp( aj) ] + c = 1 log j exp( (aj + c)) = min j {aj + c} allow high level point view hold hard- soft-min pools: differ minima equal minimax differences, min j (aj)min (bi) = min j (max (aj bi)). exploit fact multipl times, deriv follow- ing reformul logit kernel clustering: fc = min k 6=c {ok oc} = min k 6=c { min jck {dj} min icc {di} } = min k 6=c { min jck { max icc {dj di} }} . finally, defin aij := dj di complet derivation. ar x iv :1 90 6. 07 63 3v 1 [ cs .l g ] 1 8 ju n 20 19 2 appendix d redistribut min-pool layer appendix prove proposit 3 main paper. redistribut relev soft min-pool layer local approxim linear input activa- tions. that, pj asymptot approach (hard-min) indic function. proof. rewrit relev function input aj pool layer rk(a) as: rk rj = exp(aj) j exp(aj) pk pj (aj + min j {aj aj} j ) relev rj local approxi- mate linear function aj j = 1, . . . ,m. this, identifi cases. case 1: aj smallest input margin second smallest input, bound pj rewriting: pj = exp(aj) j exp(aj) = 1 1 + j 6=j exp((aj aj )) (1 + (m 1) e)1 pj bound (1+(m1) e)1 pj 1, converg 1 . similarly, bound j rewriting: j = min j {aj aj} = 1 log [ 1 + j 6=j exp((aj aj )) ] 1 log(1 + (m 1) e) 1 log(1 + (m 1) e) j 0 converg 0 . asymptot case, relev rj activ aj express term quantiti lower layers. case 2: aj fail smallest input margin , term pj bound 0 pj (1+e)1, converg 0 . then, product pjj bound pj1 log(1 + e) pjj 0, converg 0 . therefore, inputs, linear re- distribut relev hold stiff paramet grow large. appendix e modifi train kernel k-mean here, train procedur kernel k- mean model section 5.2. kernel k-mean issu kernel bandwidth small: local densiti point x0 domin gaussian bump k(x0, ) object local optima possibl cluster assignment. smooth train procedure, modifi standard expectation-maxim algorithm, minim instead distanc nearest centroid featur space bump k(x0, ) remov comput distanc featur space. learn procedur summar follow step [1]: 1) initi random assign inform start point, e.g. standard k-mean ground truth label assignments. 2) comput normal leave-one-out centroid data point x` cluster k = 1, . . . ,k : (`) k = (`) k jck\{`} (xj) normal perform kernel expansion: (`) k = ( j,jck\{`}k(xj ,xj) ) 1 2 3) assign data point x` cluster smallest distanc featur space. 4) reiter step 2 convergence. note procedur perform kernel expansions. map comput explicitly. kernel matrix comput once. leave- one-out trick make train robust bad local optima. train finished, final normal centroid comput set cluster member logit comput neural network equival section 3. refer [1] x.-l. meng d. b. rubin, maximum likelihood estim ecm algorithm: gener framework, biometrika, vol. 80, no. 2, pp. 267278, 1993.