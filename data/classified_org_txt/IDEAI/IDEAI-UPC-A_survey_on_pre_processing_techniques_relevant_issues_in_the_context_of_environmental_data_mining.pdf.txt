





















































A survey on pre-processing techniques: Relevant issues in the context of environmental data mining


AU
TH

O
R 

 C
O
PY

AI Communications 29 (2016) 627–663 627
DOI 10.3233/AIC-160710
IOS Press

A survey on pre-processing techniques:
Relevant issues in the context of
environmental data mining

Karina Gibert a,∗, Miquel Sànchez–Marrè b and Joaquín Izquierdo c
a Knowledge Engineering and Machine Learning Group, Department of Statistics and Operation Research,
Universitat Politècnica de Catalunya-BarcelonaTech, Barcelona, Catalonia, Spain
b Knowledge Engineering and Machine Learning Group, Computer Science Department, Universitat Politècnica
de Catalunya-BarcelonaTech, Barcelona, Catalonia, Spain
c Fluing-IMM Universitat Politècnica de València, Valencia, Spain

Abstract. One of the important issues related with all types of data analysis, either statistical data analysis, machine learning,
data mining, data science or whatever form of data-driven modeling, is data quality. The more complex the reality to be analyzed
is, the higher the risk of getting low quality data. Unfortunately real data often contain noise, uncertainty, errors, redundancies or
even irrelevant information. Useless models will be obtained when built over incorrect or incomplete data. As a consequence, the
quality of decisions made over these models, also depends on data quality. This is why pre-processing is one of the most critical
steps of data analysis in any of its forms. However, pre-processing has not been properly systematized yet, and little research is
focused on this. In this paper a survey on most popular pre-processing steps required in environmental data analysis is presented,
together with a proposal to systematize it. Rather than providing technical details on specific pre-processing techniques, the
paper focus on providing general ideas to a non-expert user, who, after reading them, can decide which one is the more suitable
technique required to solve his/her problem.

Keywords: Pre-processing, data quality, data mining, knowledge discovery from databases, multidisciplinary approach,
environmental systems

1. Introduction

Environmental systems (ESs) typically contain
many interrelated components and processes, which
may be biological, physical, geological, climatic,
chemical, or social. Whenever we attempt to ana-
lyze ESs and related problems, we are immediately
confronted with complexity stemming from various
sources. Thus, there is a great need for data analysis,
modeling of ESs and development of decision support
systems in order to improve the understanding of ESs
behavior and the management of these complex sys-
tems (specially under abnormal situations). As stated
in [92], the special features of environmental processes
demand a careful approach to improve the analysis
to be better known, modeled and consequently better
managed or controlled.

*Corresponding author. E-mail: karina.gibert@upc.edu.

Knowledge Discovery of Data (KDD) [46,231] ap-
peared in 1989 [69] referring to high level applications
including particular methods of Data Mining (DM,
see Fig. 1), oriented to extract useful and understand-
able knowledge from complex data. Thus, KDD is
a specifically appealing umbrella to analyze environ-
mental data. Providing highly useful information from
data bases is per se very important, although KDD
commonly is a preparatory activity for an environ-
mental software system development. Also, the KDD
approach facilitates the integration of different knowl-
edge sources and fields of expertise and the involve-
ment of end-user (domain expert) criteria and stake-
holders’ points of view in algorithm design and result
interpretation, which bridges the gap between data and
real effective decision-making levels. Thus, in the last
years an increasing interest to apply DM to environ-
mental data has been observed.

Fayyad’s proposal marked the beginning of a new
paradigm in KDD research, considering prior and pos-

0921-7126/16/$35.00 © 2016 – IOS Press and the authors. All rights reserved

mailto:karina.gibert@upc.edu


AU
TH

O
R 

 C
O
PY

628 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

Fig. 1. Outline of the knowledge discovery from data process as originally defined by Fayyad [69].

terior analysis as much important as the application of
DM techniques themselves [69]:

Most previous work on KDD has focused on [. . . ]
DM step. However, the other steps are of consid-
erable importance for the successful application of
KDD in practice.

In fact, both prior and posterior analyses require
great effort when dealing with real applications [85–
87]. Prior analysis is critical, mainly owing to two rea-
sons:

• Real data sets tend to be imperfect, contain errors,
outliers, missing data, extra noise. Tools either for
detecting or correcting them are required.

• Application of a certain data mining technique
may require specific conditions for the data set
(only binary variables, centered data, normality,
only qualitative variables, etc.). In this case, tools
for verifying that those conditions hold, or eventu-
ally transform data in the appropriate way to meet
these conditions, are required.

In environmental data, measurement errors (from
automatic or manual monitoring), uncertainty, impre-
cision, multi-scalarity, non-linearities, non-stacionari-
ty, non-normality, heterogeneity, etc., are frequent.
Also, redundant variables, irrelevant, or even contra-
dictory, are found. Systematic data pre-processing in-
cluding objective exploration, visualization and data
transformation is particularly critical for:

• Better understanding the data set;
• Detecting imperfections in the data and manag-

ing them in the proper way to guarantee a correct
analysis;

• Correctly preparing data for the selected DM
technique/s, if the required assumptions do not
hold;

• The correctness of the DM itself, which critically
depends on the quality of the data and wrong or
poor pre-processing may lead to incorrect results.

Also, particular efforts in post-processing the results
provided by a DM technique are important in this con-
text, in order to make these results directly understand-
able by an environmental scientist, who has to make
real decisions upon them, ensuring a real impact to
the environmental system behavior [35,49,82,83,87].
In fact, it can be said that the quality of the deci-
sions will depend, not only on the quality of the data
mining results themselves, but also on the capacity of
the system to communicate the relevant results to the
decision-maker as understandably as possible, and, in
a first level, on the data quality itself (Fig. 2) [190].

Indeed, data cleaning [165], also known as data
preparation [190] or pre-processing [65], are often
time consuming and difficult; mainly because the ap-
proaches taken should be tailored to each specific ap-
plication, and human interaction is required. In fact,
once pre-processing is finished and data is ready for the
analysis, the application of DM algorithms becomes
quick (requiring some parameter settings in the appro-
priate software), and can be often automated. In fact, a
small proportion of the time devoted to the whole KDD
process is spent in the DM step. In real applications,
the time devoted to pre-processing is rarely below 70%
of the time-life for the whole KDD process [128,195].
Anyway, a serious pre-processing is essential for ob-
taining good quality results, and useful new knowl-

Fig. 2. Propagation of lacks of quality: from data to decisions.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 629

edge from it. Data miners should become conscious of
the importance of performing very careful and rigor-
ous pre-processing, and allocate sufficient time to this
activity accordingly.

Most part of the research in Data Mining has been
basically focused on providing better data mining al-
gorithms, but few works specifically focus on both pre
and post-processing, despite both are recognized as
critical steps for successful KDD processes by the sci-
entific community. In this work we focus on the very
first step of the process, related to pre-processing in
environmental applications. A clear methodology for
tackling pre-processing within the KDD process has
not been well-established yet. This paper does not in-
tend to be exhaustive, but to provide environmental
scientists with tools for addressing the most common
problems arisen in pre-processing as well as to pro-
vide a specific methodological proposal. Authors ex-
pect this paper can contribute to make pre-processing
fully available to both data miners and environmental
scientists and to help them to better pre-process their
data, getting better KDD results, and as a consequence,
better knowledge from their environmental systems.

This paper tries to provide a reference material of
what to do in practice, although work is still required to
provide systematic decision criteria in pre-processing.

1.1. Structure of the paper

The structure of the paper is the following: Sec-
tion 2 identifies the main pre-processing issues. Sec-
tion 3 provides a brief introduction on getting the origi-
nal data. Section 4 discusses about how domain knowl-
edge is used to filter objects and select a first set of
variables to build the first working data matrix. Sec-
tion 5 gives a basic introduction on visualization tools
useful for pre-processing. Section 6 deals with out-
liers. Section 7 is on error detection. Section 8 on miss-
ing data. Section 9 on relevance and redundancy de-
tection and dimensionality reduction. Section 10 intro-
duces most used transformations in pre-processing for
the data mining purpose. Section 11 discusses on some
interesting scenarios where creation of new variables
is useful for data mining. Section 12 contains the con-
clusions and future lines.

2. The main pre-processing issues

As previously stated, pre-processing or data clean-
ing is a fundamental aspect, too often neglected. Min-

ing data with imperfections can have dramatic conse-
quences on the results of the analysis. Detecting and
properly treating them is a very complex task which re-
quires a lot of expertise and is highly time-consuming.

Classically, statistics provided a wide set of possibil-
ities to pre-process data. The global process of trans-
forming a raw dataset to a correct one ready for anal-
ysis is called data pre-processing or data cleaning or
data preparation and makes an intensive use of basic
descriptive statistics and basic univariate and bivari-
ate graphical representations of data. Sometimes some
multivariate descriptive techniques (Principal Compo-
nent Analysis) are also useful to detect high-order
outliers or some non-linearities. Also, inductive tech-
niques, coming from Artificial Intelligence, are useful
in some tasks, like determining variables’ relevance.
Visualization techniques, more or less sophisticated,
play a crucial role in the detection of many important
issues for pre-processing.

No well-established pre-processing methodologies
have been formalized yet, but some clear guidelines
can be provided. Pre-processing ranges from the sim-
plest descriptive techniques to the more sophisticated
data analysis methods, depending on the nature of data
and the goals of the analysis itself. The authors be-
lief that most of the operations performed in a pre-
processing step can be reduced to a preliminary step
for entering the data and four main families of tech-
niques, which are required, or not, depending on the
original data formats received for the analysis:

• Introducing data in the pre-processing tool:
oriented to read the data from the software as
well as metadata supported by the software to
create the correct context for a proper data pre-
processing.

• Visualization techniques: oriented to show char-
acteristics of data that will require attention be-
fore the analysis.

• Detection techniques: Those oriented to detect
imperfections in datasets or to verify the accom-
plishment of required assumptions for a particu-
lar analysis. These are often associated with some
kind of diagnoses on data, and consequently, to
some decisions related to pre-processing:

∗ Outlier detection
∗ Data errors detection
∗ Missing data detection
∗ Relevance or redundancy detection, feature

weighting
∗ Independence assessment



AU
TH

O
R 

 C
O
PY

630 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

∗ Detection of influent observations
∗ Normality assessment
∗ Linearity assessment

• Transforming techniques: Those oriented to per-
form transformations in the dataset in order to
correct the imperfections detected before, or to
achieve the technical conditions to apply a certain
analysis technique.

∗ Determining the active set of data matrix rows
and columns (expert-based objects and vari-
ables selection, including filtering)

∗ Outlier treatment
∗ Error data treatment
∗ Missing data treatment
∗ Treatment of relevance and redundancy and di-

mensionality reduction techniques

∗ Instance selection and resampling
∗ Feature selection
∗ Factorial methods

∗ Transforming variables
∗ Homogenization
∗ Differences and ratios
∗ Compositional data
∗ Functional transforms (inverse, logarithmic,

quadratic, Box–Cox, etc., transformations)

∗ Recodification
∗ Discretization
∗ Centering, standardization and normaliza-

tion
∗ Fuzzyfication
∗ Imbalanced datasets

∗ Creation of new variables
∗ Aggregation
∗ Feature extraction
∗ Building indicators
∗ Multivalued variables

When data is geospatial, additional smoothing or
noise reduction operations can be required. This issue
is out of the scope of this paper and will not be specifi-
cally treated (see [71] for an overview). In some cases,
images are pre-processed to extract relevant features
that can be placed in standard data matrices, eventually
georeferenced, for further joint mining [72].

Based on our experience in mining real data sets
from more than 20 years, we would say that the main
flow of this process can be synthesized in Fig. 3.

In the following sections we provide information for
all the topics stated above. For some of them detection
and treatment are strongly related and both aspects are
treated in the same section, as is the case of outliers
treatment and detection.

Fig. 3. A proposal for pre-processing process.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 631

3. Building the original data matrix

As said before, many different sources of informa-
tion can be involved in the observation of an ES. In the
recent years, data coming from smart sensors or im-
ages are quite usual. Today, the web is still not a com-
mon information source for ES analysis, but it might
come in the near future as a relevant provider of both
structured and unstructured data.

This means that when an ES is analyzed, different
data sources might be combined and some hard pre-
processing might be required even before getting the
very first original data matrix suitable for analysis.

Thus, when information sources are images, and a
data mining approach is to be adopted, some feature
extraction methods [171] need to be applied to trans-
form the image into a vector of indicators identify-
ing the image itself, which can be properly represented
in a classical two-dimensional data matrix. For exam-
ple, in a land use application, satellite images are of-
ten used, and they might be synthesized in a vector of
indicators like existence of river or not in the image,
surface of the urban area, surface of forest, surface
of crop area, etc. Feature extraction processes often
are domain-dependent and require enough contextual
knowledge to determine which are the relevant features
to be extracted from the image, according also to the
goals of the problem.

Also, data might come from smart sensors, which
are on-line, monitoring several parameters of an ES,
like a Waste Water Treatment Plant (WWTP) for ex-
ample. In that case, data corresponds to a certain in-
stant time, and normally each observation has a time
stamp, expressed in one of the variables of the data ma-
trix. For instance, the data matrix can include biochem-
ical parameters of the WWTP along time (like concen-
tration of organic matter, suspended solids, etc.), pro-
viding, for example, daily means. In most applications,
each parameter (or variable) will represent a column
of the data matrix and the timestamp will be an addi-
tional variable. Sometimes, some pre-processing is re-
quired to integrate the information of several sensors
in a single indicator (like mean water level of a biore-
actor for example, measured by several sensors in dif-
ferent points) and the imprecision or uncertainty asso-
ciated to the measurements must be properly treated at
pre-processing level, in particular the noise associated
to the signals [170]. Regarding imprecision, georadar,
or GPS systems, for example, provide a region where
the target can be located with a certain probability, but
are not able to provide exact positions of the target in-

stances. SCADA provides the minimum and maximum
values of a signal in a certain interval. Strictly, despite
being able to measure a magnitude, its exact value will
be never known; it is only known that the measurement
is somewhere in a certain range, bounded by the pre-
cision of the measurement instrument or measurement
procedure itself [224]. In applications with these kinds
of inputs, it makes sense to include the uncertainty as-
sociated to the measure itself in the representation of
the values. Fuzzy sets [236] or rough sets [180] are
among the most commonly used representation mod-
els for these situations, but require specific data min-
ing methods accepting this kind of fuzzy or rough vari-
ables. In the first case, linguistic labels are used to rep-
resent the uncertainty associated to the values of the
variables for every observation [220]. Fuzzy datasets
can directly come from a monitoring system, like radar
data, which intrinsically provides areas of localization.
Also, original crisp data can be artificially fuzzified for
specific analysis, taking into account the precision of
the measurement instruments and using specific fuzzi-
fication algorithms (see [20,68,119,126,139,175]). In
this paper, we will provide pre-processing methods ori-
ented to crisp representation of data, which is the most
used for simplicity in current real applications. How-
ever, the reader has to keep in mind the possibility
of dealing with these uncertainties naturally associated
with raw data, by using specific uncertainty formalisms
which are less known from a practical point of view.

Even though the web data is not very popular yet
in the current environmental applications, the web pro-
vides documents and pages that in some near future
might become relevant for the environmental data min-
ing. In that case, text mining methods need to be
used to extract relevant characteristics of these doc-
uments that might also be represented in a classical
two-dimensional data matrix, and can be used as or-
dinary inputs of classical data mining methods. There
is a good chance to discover new findings and conclu-
sions using modern methods of text mining supported
by ontologies and semantic patterns. A survey of these
methods is given in [51]. Text mining technology plays
an important role in detecting epidemics (epidemic in-
telligence) based on event alerting from unstructured
web pages and social networks [47]. A sample of the
interest of environmental authorities in the monitor-
ing of social networks for environmental purposes, or
emergency management is the challenge on predic-
tion of timing and intensity of seasonal climate events
like hurricanes or earthquakes, among others [74,166].
Also, sentiment analysis is currently providing a sig-



AU
TH

O
R 

 C
O
PY

632 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

nificant added value by analyzing publications either in
social networks or web pages [178,225]. Some public
resources are available to process this kind of textual
information and obtain new variables, like the intensity
of the earthquake [14].

Thus, building the original data matrix means to
make data fusion [52,65] with all information com-
ing from the different sources, and eventually merge
several independent data matrices into a single one
containing all available information for each data row
[195]. Finally, one of the current problems in envi-
ronmental data representation is the lack of a stan-
dard environmental data format, despite some propos-
als and efforts of standardization made. Environmen-
tal data sharing is classically based on sharing data
format, measurement methods, analysis methods and
instrumentations. On the other hand, the standardiza-
tion of the environmental data aims at sharing the data
structure in order to share information among vari-
ous bodies. Since environmental data possess coordi-
nates on the globe, it can be treated as a form of ge-
ographical information. Then data structuring defini-
tion work is done by the ISO/TC211 [116,117]. If this
is applied, then environmental data may be standard-
ized in the same way as other geographical informa-
tion. The ISO/TC211 considers that geographical in-
formation comes from geographical feature data and
meta-data [115].

One of the most promising directions has been the
use of XML files to encode environmental data. XML
is becoming an increasingly common format for data
representation in data mining domains due to its ex-
pressiveness, flexibility, and cross-platform nature. An
example is Extended Environments Markup Language
(EEML), which is a protocol for sharing sensor data
between remote responsive environments, both phys-
ical and virtual, initially aimed for construction pur-
poses [63].

As an example, in the analysis of river’s retention
[155], one could get different databases. One contain-
ing morphologic characteristics of the river, built by
experts; another containing biochemical characteristics
of the rivers at different timestamps, provided by a bio-
chemical laboratory; another containing information
on microbiological diversity, provided by a set of bi-
ologists, and based on microscope observation of sam-
ples; another with some satellite pictures to learn land
uses around the river; and another with some sensor
data about flows along time. The very first step of pre-
processing is to properly transform all this informa-
tion into a single data matrix where rivers are in the

rows and all flow, land use, morphologic, biochemical
and microbiological, characteristics are expressed as
columns in the data matrix with values at the different
timestamps (one row per timestamp).

This single data matrix is the one used as original
data matrix to pass to the next step of pre-processing.

3.1. Structure of original data matrix

In the classical data mining applications, the origi-
nal data matrix is a two-dimensional table containing
n rows representing the set I of n observations (also
named individuals or instances) and K columns, which
are the variables used to describe the observations. In
cell (i, k), i = 1 : n, k = 1 : K , the value of variable
X_k observed for object i is registered, xik . Usually,
these data matrices can be represented in plain text files
or CSV formats. Variables are in columns, and can be
numerical (like pressures, concentrations of pollutants,
depths, etc.), ordinal (level of risk of fire in a forest), bi-
nary (presence of certain microorganism in water, dan-
gerous levels of a certain pollutant in air, etc.) or qual-
itative with more than two modalities (color of algae,
type or eruption in a volcano, etc.).

Along this paper, the terminology used to refer to
both the variables and observations of the usual two-
dimensional data matrix changes according to the most
usual term used in the concrete field of study provid-
ing the different pre-processing techniques (i.e., statis-
tics, machine learning, etc.). Therefore, in the text, we
refer to a variable with the following words: feature,
attribute, property or data column. Likewise, we refer
to an observation with the following words: instance,
example, object or data row.

Eventually, when observations are taken along the
time, one of the variables is the timestamp, which can
be expressed in any date format, and requires attention
to ensure it is properly interpreted by the software.

Also, when observations are linked to a specific lo-
cation, some variables giving geographical information
may appear, like Cs radioactivity measures in points of
a surrounding area of a nuclear plant. They could refer
to 2D or 3D spatial information. In 2D situations, each
geographical position is determined by a pair of val-
ues, like latitude and longitude values, a pair of UTM
coordinates, etc. [31]. In the case of 3D, in addition to
the pair of coordinates, an additional elevation or deep
value is provided. This data is relevant for territorial
graphical models where geographical areas are identi-
fied with the same coordinates. Then, data can be visu-
alized over digital maps, by using this information as



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 633

pointers to particular map areas, to visualize or induce
some spatial predictive model.

In this paper two special cases of subsets of related
variables that require special pre-processing are also
considered: multivalued qualitative variables and com-
positional data, both common in ESs.

Multivalued qualitative variables are qualitative
variables that can have several values simultaneously.
This is the case, for example, of the water access tech-
nologies available in cities. Imagine a data matrix con-
taining one city per row. The qualitative variable water
access technology can take a set of possible values (for
example, gravity water service, pump water services,
well water services and call water services) [123,208].
As the same city can have different districts with dif-
ferent access technologies, this variable is taking a list
of values (technologies) in each city. Formally, it corre-
sponds to the case where the corresponding cell of the
data matrix is a vector of modalities. As this would not
be manageable in a classical data mining software, it
is usual to represent each of the modalities as a binary
variable and report a Boolean for each city, by indicat-
ing whereas the city has households with gravity water
services or not, pump water services or not and so on.
Thus, for a multivalued variable with s modalities, a
set of s binary variables will be used, which is different
from dummy variables, as here, more than one can be
true simultaneously.

Finally, compositional data are sets of s strictly pos-
itive real variables with constant sum Z > 0 [181].
Z is generally 100 (percentage) or 1 (proportion). This
implies the existence of a linear relationship among the
s variables, with implications in the results of the data
mining process, particularly when methods assuming
independence of variables are used. This is the case, for
example, of geochemical composition of rocks, com-
position of sediments in rivers, sources of pollution in
a lake, land use composition, waste/wastewater com-
position, among others [5].

3.2. Introducing the data into the pre-processing tool

It is always important to verify that the data matrix
read by the software really contain the expected rows
and columns, with proper contents. Here, format issues
can be responsible of reading numeric columns as al-
phanumeric, or to merge several columns in one and so
on. It is essential to detect these issues at the very first
step of the data pre-processing. As an example, the for-
mat of a csv file generated by Excel if it has Spanish
language activated, uses “,” as decimal indicator and

“;” as column separator instead of the standard “.” and
“,”. This means that importing a csv file generated by
a Spanish version of Excel in pre-processing software
expecting an English csv file will not work.

Data should always incorporate extra information
describing the variables, like source of information,
type of the variable, unit of measurement, degree of
uncertainty, etc. This is the meta-data [157]. It is cru-
cial for correct interpretation of results and also for
proper data pre-processing and right data mining tech-
nique choice. For example, a dataset containing a col-
umn entitled Age with values 10, 20, 30, 40,. . . can
be wrongly interpreted as a numerical variable, but it
can also be the encoding for something like Children,
Young, Adult. . . In this case, the values of the variable
are not numbers, in a mathematical sense, but simple
codes of a qualitative variable that do not admit, for in-
stance, standard deviation computation. Moreover, this
can be the current age of the individual, but also the age
at which he was diagnosed for a certain illness. Only
the proper additional meta-data makes it possible to be
sure that data is properly understood, and that a proper
treatment will be provided.

Unfortunately, most software tools do not support
meta-data, or cannot process it, and it is too frequent to
get datasets with incomplete meta-data, that can propa-
gate wrong assumptions (like assuming the same mea-
surement units in the whole sample for a pollutant
concentration, where different laboratories have been
involved and results are expressed in different concen-
tration units, according to each laboratory). Meta-data
must be collected and properly understood for a proper
data analysis and interpretation. Also, it must be trans-
ferred to the pre-processing software tool at the high-
est possible level. For example, nowadays, most of the
software tools admit some declaration of qualitative
variables that prevent for certain wrong manipulations,
as wrong interpretation of numerical codes, wrong
computation of standard deviations, or even wrong in-
troduction into classical linear regression models by
mistake. Of course, this is software-depending and it is
the responsibility of the analyst to manage each vari-
able in the proper way, with or without the support of
the software.

4. Determining the working data matrix

Once all available information is expressed in the
original data matrix, the data really used for the analy-
sis is going to be determined. We name this step build-
ing the working data matrix, and basically consists in
answering two main questions:



AU
TH

O
R 

 C
O
PY

634 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

• Which data matrix rows (instances) are going to
be kept for the analysis?

• Which data matrix columns (variables) are going
to be considered in the analysis?

4.1. Determining the data matrix rows

This is basically related with defining the target
population. Consider a dataset with information of a
WWTP for the whole year. Whereas we are interested
in modeling the plant in the summer period, it is use-
less to consider the rows corresponding to winter days.
Filtering is mainly devoted to selection of subsamples
from the main data matrix, with several purposes:

• Restricting the scope of the analysis to a local sub-
domain: only summer measurements, as said be-
fore; also, given 5 years of air quality measure-
ments, reduce the analysis to those days where
emission of pollutants are higher than some legal
threshold to identify emission sources.

• Eliminating observations coming from other do-
mains not targeted in the analysis. As an example,
eliminate pieces of land in the mountain, wrongly
included in a research regarding agricultural land-
uses in a valley.

Thus, filters will be required to select the rows rele-
vant for the analysis. Formally, filters are Boolean ex-
pressions defined over individuals that will evaluate to
TRUE when the row must be kept or FALSE when
must be excluded for the analysis.

In medicine, defining the target population for a
clinical trial is a very sensitive task and the experts
commonly talk about defining inclusion/exclusion cri-
teria that determine the target population. Inclusion/
exclusion criteria is a term non-commonly used in en-
vironmental systems, whereas it can be perfectly intro-
duced into the picture, as this is exactly the mission of
filters: to clearly express the conditions that must be
held by the rows kept for the analysis, and those held
by the rows excluded for the analysis.

When different filters are managed together, the
main caution required is to be sure, at every step of
the analysis, that the results only apply to the exact
subpopulation targeted. Making an extensive use of
intermittent filters can produce confusions and cross-
results, which are extremely difficult to discover. If
many local analyses are required to complete the entire
analysis, sometimes it is better to run the filters once
at the beginning of the study and physically produce
the required subsamples in separate databases. Then,

local analyses can be performed directly on the correct
data matrix thus decreasing the risk of errors. If this
methodology is adopted, subsamples must be rebuilt
every time a change happens in the master data matrix.

The target population defined for the analysis is
framing the conclusions of the mined data and it is ab-
solutely relevant to report it explicitly in order to make
clear the scope of the conclusions and to which extent
they can be generalized.

4.2. Determining the data matrix columns

The second issue is to determine the subset of
columns of the data matrix to be considered for the
analysis. This is related to the goals of the analysis and
the kind of questions that have to be answered with
the analysis, or the decisions to be made afterwards.
Datasets may contain irrelevant variables regarding to
these goals, questions or decisions as well as redun-
dant variables [86]. As previously stated, the quality
of discovered knowledge usually depends on the qual-
ity of the data used. Also, the success of some learn-
ing schemes, in their attempts to construct models of
data, often relies on the identification of a small set of
highly predictive variables. The inclusion of irrelevant,
redundant and noisy variables in the model can result
in poor predictive performance and increased compu-
tation. This is the reason why a previous work for de-
termining the variables to be kept is required.

At this stage of the analysis an expert-guided vari-
able selection process is to be conducted. This means
that, according to the goals of the analysis, the existing
knowledge on the targeted Environmental System and
the meaning of the available variables, a first selection
is performed. This first step is interactive with the ex-
perts. For example, given a database with information
about water infrastructures in a set of villages [123],
with an interest in improving drinking water access to
households, all variables describing the status of the
sewer network infrastructures can be dismissed.

In case of doubt, it is preferable to be maximal-
ist and keep a variable that will be disregarded later
along the analysis, than to find during the data min-
ing step that some variables initially dismissed become
required. The pre-processing can imply several trans-
formations on the data matrix (like selection of some
rows) that may complicate the addition of other vari-
ables in an intermediate point of the modeling pro-
cess. Re-processing a certain data mining model with-
out some variable has a much lower computational cost
than re-pre-processing the whole database.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 635

4.3. Practicum

Most of the commercial packages or data manage-
ment products provide very sophisticated tools to per-
form filters over data, as well as to select subsets
of variables from the data matrix, most of them us-
ing Boolean expressions (or logical rules) to specify
the inclusion-exclusion criteria. Nowadays, it is quite
usual that the system permits to activate and inactivate
several filters intermittently.

Sometimes, when data has to be retrieved from a
big information system, the original data matrix is the
whole reference data warehouse, and the filter is ex-
pressed in an SQL query, already providing the work-
ing data matrix with the required rows and columns to
be considered in the analysis.

When data has to be acquired from scratch, deter-
mining both the inclusion/exclusion criteria and the
subset of relevant variables is the first step of the data
acquisition process. Then, other disciplines become
relevant: sample theory will help in observational stud-
ies, when measurements only require system observa-
tion. Experimental design theory will help when spe-
cific control conditions of the ES must be artificially
created to make the measurements. This issues are out
of the scope of the paper and regard the data acquisi-
tion step, but will determine in which form data comes
to the study, how the working data matrix is built and
which pre-processing tasks are required to get it. Of
course, selecting data from a pre-existing data matrix,
a pre-existing data warehouse or requiring to perform
the experimental or observational measurements from
scratch implies very different costs. It is then relevant
to consider which data is already available in previous
data bases at the beginning of the study.

5. Data visualization

Visualization is a powerful strategy for leveraging
the visual orientation of sighted human beings. Sighted
humans are extraordinarily good at recognizing visual
patterns, trends and anomalies; these skills are valuable
at all stages of the KDD [158,232]. Many of the pre-
processing detection issues, like outlier detection, nor-
mality assessment or linearities, randomness of miss-
ing values or multimodalities, can be assessed through
visualization techniques [201].

Graphs commonly used for classical exploratory
visualization, like boxplots, histograms, time series
plots or scatter plots perform poorly considering the
great number of variables involved in ESs datasets,
along with their complex interrelations or spatio-

temporal references. Thus, more sophisticated visual-
ization methods are required, as for example:

• Distributional plots,
• Three, four, and five dimensional plots (color and

symbols may be used to represent the higher di-
mensions),

• Density plots,
• Dimension scaling, like log scales,
• Rotary frames,
• Animation and interactive graphs,
• Geo-referenced visualizations and maps.
Most data mining packages, such as Weka [100],

TABLEAU [105], QlikTech [159], include visualiza-
tion tools, while more advanced features are provided
with wide-spread tools such as Matlab, a dedicated
data language such as IDL [66] or the CommonGIS
tool [9]. Dedicated interactive visualization tools are
also available, such as GGobi [215] or GODIVA2 [28].
Visual representations are extremely effective at pre-
processing stage to identify the basic pre-processing is-
sues to deal with. For example, some data errors or ex-
istence of outliers or structural missing data might be
visible on certain visualizations.

Also, some KDD methods, mainly those coming
from classical statistics, usually involve some techni-
cal assumptions that must held on the target domain
to guarantee the validity of the model. This is the
case of linear regression, for example, in which in-
dependent observations, non-correlated variables, con-
ditional normality of the response variable and linear
relationship between the response variable and the ex-
planatory variables are required. Some of these proper-
ties can be assessed on data just before the analysis and
can help to decide, for instance, whether the linear re-
gression is the proper approach for the target model or
other alternatives like artificial neural nets can perform
better on our data. Part of these properties can be as-
sessed using visualization tools, like assessing normal-
ity by means of histograms or Q–Q plots, or the nor-
mal probability plot and the Henri line [217]; or Scatter
plots can help to see independence or linearities.

This is not exclusive for statistical methods. Induc-
tive machine learning methods, like Decision Trees for
example, require balanced data sets to provide reliable
results, or certain clustering methods require constant
data density. These kinds of issues are easily observ-
able over some charts or scatterplots and become ex-
tremely difficult to evaluate in a numerical way.

Although visual assessment often is very efficient
in these cases, when possible, tests can be introduced



AU
TH

O
R 

 C
O
PY

636 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

in automatic repetitive KDD procedures, since they do
not require human interaction to decide: the χ2-tests
for independence, or the Shapiro–Wilks for normality
[41], the assessment of multivariate normality it is not
so easy, but Mardia test can be used [197].

However, these tests usually have their own techni-
cal assumptions in turn and it is important to ensure
these last hold to guarantee the reliability of the signif-
icance test results

Thus, by making previous visualization of data, one
can easily point out which kind of pre-processing oper-
ations are required either related to intrinsic data qual-
ity, or to make available the application of some partic-
ular Data Mining method.

6. Outliers and influential observations: Detection
and treatment

Outliers are instances with very extreme values in
one or more variables [17]. Some data mining meth-
ods are robust to the presence of outliers, like hierar-
chical clustering methods, while others, like regression
or density estimation methods, can provide highly dis-
turbed results when outliers are present in the dataset.
For non-robust methods, it is crucial to detect outliers
and to properly treat them in order to get a model that
fits well the reality and not too much distorted by the
outlier itself.

6.1. Dimensionality of outliers

Something often ignored is that outliers have a cer-
tain dimensionality. This means that sometimes an ob-
servation appears as an outlier regarding a certain sin-
gle variable, but sometimes it appears as outlier in a
subspace of higher dimensionality. To clarify, a two-
dimensional example is shown in Fig. 4. The same sit-
uation can also appear in higher dimensional spaces,
although we are not able to visualize in a clear way yet.
In Fig. 4, a scatter plot for COD (Chemical Oxygen
Demand) and BOD (Biochemical Oxygen Demand) of
a WWTP is shown. In one day, the WWTP inflow has
a value of COD concentration equal to 1279 mg/l, and
BOD concentration equals 198 mg/l. None of these
values are particularly extreme (see the marginal his-
tograms) [88]. This means that this day is not con-
sidered as an outlier, neither regarding the COD, nor
the BOD by themselves. However, looking at the plot,
things change. In the bivariate space formed by BOD
and COD the point (1279, 198) becomes an outlier. The

Fig. 4. Dimensionality of outliers.

relationship between the two values is the rarity. Thus,
individuals appear as outliers or not, according to the
subset of variables considered together.

Outliers become dangerous only when they perform
as influential observations in the analysis. An influen-
tial observation is an observation that strongly deter-
mines the results of a certain analysis. In the top-right
corner of Fig. 4, a second outlier is much far from the
general data cloud than the one marked by the circle.
However, the former aligns better with the general as-
sociation between BOD and COD. Thus the one with
the circle in Fig. 4 is more influent. Influent observa-
tions are well-known in the context of regression anal-
ysis, but they can also distort other non-robust models.
This means that they may spoil modeling. When the
outlier is far from other observations, but follows the
same model, it is not dangerous.

6.2. Causes of outliers

Outliers might be produced by different reasons.
Understanding them helps to diagnose and deciding
the proper treatment:

• Error: COD = 1279 mg/l is not compatible with
low BOD and low suspended solids.

• Informative point: a single point from a missing
part of the population (a single observation refer-
ring a sensor failure, etc.).

• Member of another population.
• Intrinsic extreme value: i.e. the case of COD =

1579 mg/l with BOD = 987 mg/l.
• Missing code: 99999 usually appears as an out-

lier for the univariate distribution of the variables
(Fig. 5).



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 637

Fig. 5. Pattern of missing codified as numerical code.

6.3. Outlier detection

Using domain-related additional information pro-
vided by the experts, like the normal range of values of
the variables, supervised outlier detection can be con-
ducted [1]. When this information is totally, or par-
tially, available [2], it is convenient to take advantage
of it. In front of a lack of this domain-related informa-
tion, unsupervised outlier detection must be used.

Graphical techniques were once the most common
method for unsupervised outlier detection, but in-
creases in database sizes and dimensionality have led
to a variety of automated techniques [209]. The use
of standard deviations is possible when considering
variables following symmetric distribution; but outliers
may also take the form of unusual combinations of
two or more variables, as said before, and then, they
only manifest in the proper dimensionality subspace.
This means that graphical representation will not be
useful over more than three dimensions. In that case,
hierarchical clustering, where outliers form singletons
[146] or other methods [32] might help. Also, some
specific statistical tests exist [23] but they are only im-
plemented in some software tools, and usually assume
some specific univariate distributions.

In any case, the data point should be analyzed as a
whole to understand the nature of the outlier and mul-
tivariate approach is required. See [42] for a survey in
the field. Determining whereas an outlier is perform-
ing as an influent observation can be quantified by the
Cook’s D coefficient [48] and it is important to deter-
mine if the observation belongs to the target population
or not.

6.4. Outlier treatment

The treatment will depend on the nature of the out-
lier. As said before, the outliers become dangerous
when they perform as influent observations. If not, they
do not need to be treated. The influence of outliers can

dramatically affect the results or certain methods, and
should feature the choice of tools used throughout the
rest of the process. See [165] for an interesting discus-
sion on the dangers of simply eliminating rows with
outliers:

In 1985 British scientists reported a hole in the
ozone layer of the Earth’s atmosphere over the
South Pole. [. . . ] The British report was at first
disregarded, since it was based on ground instru-
ments looking up. More comprehensive observa-
tions from satellite instruments looking down had
shown nothing unusual. Then, examination of the
satellite data revealed that the South Pole ozone
readings were so low that the computer software
[. . . ] had automatically suppressed these values as
erroneous outliers! Readings dating back to 1979
were reanalyzed and showed a large and growing
hole in the ozone layer [. . . ] suppressing an out-
lier without investigating it can keep valuable out
of sight.

When the outlier is a mistake, the best option, if pos-
sible, is to come back to the original observation and
correct it. When it is no more available, then the ob-
servation must be substituted by a missing code and
conveniently treated.

When an outlier belongs to a segment of population
that has not been properly represented in the data sam-
ple, it is convenient to enlarge the DB with more obser-
vations in the same subpopulation. If this is no possi-
ble, then the scope of the analysis should be restricted
to the subpopulations sufficiently observed. For exam-
ple, in the analysis of biodiversity in a river, some
days may correspond to a scenario in which forbid-
den industrial discharge occurred, with consequences
in biodiversity. As this is, fortunately, infrequent, only
few rows of data matrix are involved. Getting a global
model for biodiversity requires enlarging the data sam-
ple to more days with industrial discharges. Otherwise,
the study must be restricted to normal days. Another
example is to design the analysis on air pollutants in
a certain urban area involving the past five years and,
provided that measurements for the 1st and 2nd year
are too scarce in some cities to get a reliable analysis, a
restriction to the last three years would be convenient.

When the outlier is a member of another population,
included in the data sample by mistake, it is convenient
to treat it apart, and properly report in an explicit way
that the general model is obtained without considering
it.

When the outlier is the natural skew of the distribu-
tion it often will not perform as influent observation



AU
TH

O
R 

 C
O
PY

638 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

and will follow the general model (Fig. 4). Then, it is
convenient to keep it for the analysis.

When the outlier is a numerical code used for rep-
resenting missing data (i.e. 999999), it is convenient
to substitute it by the missing code and proceed with
missing data treatment (imputation).

It is of utmost importance to report any decision
made over outliers in a clear and explicit way that per-
mits the end user to properly evaluate if something is
wrong with the data or outlier treatment decisions must
be revisited.

7. Error detection and treatment

Corrupt data can come from sensor failure, data
transmission, improper data entry, etc. Often, it pro-
duces values out of range [65].

Corrupt data can also be identified by evaluating
consistency rules over the data, representing well-
known relationships among the variables. As an exam-
ple, in Fig. 4, the mistake in BOD = 1279 can be iden-
tified by applying specific domain knowledge: In fact,
BOD and COD are correlated and it is not possible that
one is high and the other one is low simultaneously. To
see which of both is wrong, other variables help, like
suspended solids, also positively correlated with them.

To identify corrupt data, specific domain knowledge
is required together with intensive interaction with the
expert.

When an error is detected, they are basically two
ways of treating it:

• Correction: This means to come back to the origi-
nal data source and try to retrieve the correct data.
This is the preferable option, when available.

• Missing production: When correction is not avail-
able, the data must be substituted by a missing
value and conveniently imputed.

A special task in this topic is terminology normaliza-
tion for qualitative variables. More and more, qualita-
tive variables play a bigger role in data mining. It often
happens that a same modality is expressed with differ-
ent strings, which are not properly recognized by the
software as equal, thus significantly altering the data
mining results. For example, data coming from differ-
ent countries can have modalities expressed in differ-
ent languages (a qualitative variable indicating risk of
fire can have Alt, Alto, High, Elevé, all of them mean-
ing high in different languages; they will be treated as

different in the analysis if they are not properly pre-
processed).

In a more specific situation, as common Thesaurus
are not extensively used yet to fill-in qualitative vari-
ables, slight differences in the words spelling might
have the same consequences (accents, blank char-
acters, capital letters, special characters like dashes,
slashes or points are the biggest enemies of qual-
itative variables’ treatment). As an example, in the
dataset containing the measurements from Hipparcos
satellite, one of the variables is the Spectrum of the
measured star [84]. The file contains measurements
over 87475 stars, and the descriptive analysis says that
they are 3655 different Spectrum values. Some of the
stars show a Spectrum type “M2 :”, while other show
“M2 : ” or even “M2:”, which in fact are the same,
but codified in different strings, and the software do
not identify as equal. Also “M3 J ” and “M3 J ”. The
difference is subtle, only some blank spaces distin-
guishing them, but the effect over data mining results
is important. Also “A0p (Si)”, or “A0p Si” and also
“A5 IV–V” or “A5IV/V”. After standardizing the labels
of all modalities of the variable, the number of modal-
ities reduces to some hundreds. So, very careful edit-
ing is required to normalize terminology before start-
ing the analysis.

Any decision made for data error handling must be
transparently reported.

8. Missing data

Often, a number of cells are missing from the data
matrix. In fact, the existence of missing data is not neg-
ligible in real applications. A missing data may occur
by many different reasons and may have different na-
tures. First, missing data must be detected. After, di-
agnosed. Accordingly, proper treatment can be deter-
mined.

8.1. Types of missing data

Most of the references found in the literature clas-
sify missing data into missing completely at random,
missing at random and missing not at random [8,147].
However, the origin of the missing data is analyzed in
depth, so that the user can get criteria to understand.
From the application point of view there are basically
two main families of missing data: random (completely
or not) and non-random, and it is crucial to distinguish
between them.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 639

• Random missing data (include missing complete-
ly at random and missing at random, usually re-
ferred in the literature) are randomly produced
and do not follow any particular pattern. This
means that, even if unobserved, it is correct to as-
sume that they follow the same distribution of ob-
served data. Thus, observed data can provide use-
ful information about them. They might appear by
chance:

∗ an employee is copying a set of manual mea-
surements from paper to computer and inciden-
tally skips one of them

∗ a sensor is temporarily loosing connection with
the server and one reading is lost

∗ a forced missing value often is also random. It
results of detecting a wrong observation. If it
cannot be properly corrected, then it has to be
transformed in a missing.

• Non-random missing values are produced by
identifiable causes; normally they come from a
specific subpopulation; and there are differences
in their behavior with respect to the observed data.
In this case, observed data is not providing useful
information about them. They might appear from
many different reasons:

∗ Because it is deliberately hidden: Too high
concentration of nitrates in the outflow of a
wastewater treatment plant in a concrete day
can be deliberately substituted by a missing to
avoid a fine. Some industries do not provide
emissions of certain pollutants to avoid extra
taxes.

∗ Because data is explicitly not provided: In sur-
veys on water consumption carried out by wa-
ter companies, some users do not explicitly
declare sensible variables to prevent eventual
requalification of the household profile, often
with consequences in the bill. Some of these
variables are the existence of garden or pool on
the property, the use of devices saving water,
the number of occupants of the dwelling or the
level of family income.

∗ Because it corresponds to a special value. Pres-
ence of filamentous bacteria in a WWTP plant
might indicate a further bulking and problems
with the normal operation of the plant, provid-
ing effluent with too high concentration of or-
ganic matter, over the permitted limits.

∗ It is not possible to obtain the measurement, by
some reasons:

∗ Because the technology is not available:
in the case of Hurricane IKE, occurred in
Cuba in 2008 [24], anemometers in Pinar
del Río (western Cuba) ceased to measure
wind speed over 250 km/h, since they sim-
ply broke, thus producing missing mea-
surements after the moment in which wind
reached this speed. In a volcano eruption,
sensors are used to measure the toxics emit-
ted by the eruption, but no measures are
available in the middle of the crater, because
the heat fudges the sensors. Also, water age
in potable water distribution networks, rele-
vant to predict biofilm development [194]. It
is no available under conditions far from the
steady state, since it depends on water qual-
ity and some consumption parameters that
cannot be determined under conditions far
from the steady state;

∗ Because of a lack of privileges to get it: mea-
surements of air quality in a grid of points of
a certain mountain region including a mili-
tary area will produce missing values in the
points falling inside military zone.

∗ Lost: The measurement has been taken cor-
rectly, and lost. This can happen when digi-
tal data bases corrupt after time, and past data
disappears; in migration of software versions,
sometimes this might also occur. Another ex-
ample is the study of migration of birds, in
which GPS trackers are used to mark the migra-
tory birds. Some of the measures can be lost,
just when visibility from the satellite is tem-
porarily lost [27,226].

∗ Structural: data unavailable for a certain part
of the population: As an example, the variable
form of the beak of a mammal, in a database
on animal species leaving in a certain ecosys-
tem, like a natural park or a forest. Number of
legs of a fish is another example. These miss-
ing values often appear as a consequence of a
wrong data encoding policy at data collection
step. Meaningful values (like lackOfBeak) are
a better option to represent structural missing
values instead of missing values, this requiring
a proper design of the data collection process.

8.2. Missing data representation

It is important to get precise information about how
the missing data are represented in data matrix, as part
of the meta-data information.



AU
TH

O
R 

 C
O
PY

640 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

Sometimes, missing cells are marked as a *, ?, NA
(Not Available), blank space or other special character
or special numeric code such as 99999.

Blank space can be problematic when imputing the
data matrix in some specific software, as it might hap-
pen that the software does not detect the blank and
shifts values of the row one position to the right.

Numerical codes assigned to missing data like
99999 can produce severe mistakes in calculations if
not properly treated, as they can be wrongly incorpo-
rated in the analysis.

Missing code automatically recognized by the soft-
ware (“*” in Minitab, “?” in Weka, “NA” in R package,
etc.) can also be problematic, if the user is not aware
of the implicit assumptions that the software is making
over data, and missing treatment.

8.3. Detection

For missing data represented by means of the miss-
ing code properly recognized by the pre-processing
software tool (“*”, “?”, “NA”, etc.), simple basic de-
scriptive statistics of the variables will directly report
it.

When they are represented as other kind of numeri-
cal codes, basic visualization of the variables (in a his-
togram, for example) will show a missing data pattern,
as a very extreme value of the variable with certain
prevalence. In Fig. 5, the concentration of suspended
solids at the inflow of a WWTP is shown. The variable
is measured in mg/l and 9999 is a non-possible value
for the variable, indeed representing the missing value
code. Understanding if this is a real extreme value of
the variable or a value out of range, used as a numerical
code for the missing value, requires interaction with
the expert and some domain knowledge.

8.4. Diagnoses

The main issue is to identify whereas the missing
data of a certain variable are random or not.

When a variable contains a block of missing data, it
is useful to check together with the expert if they might
be structural missing data that corresponds to partic-
ular conditions (as said before, mammals do not have
beak).

In the general case, the Little’s test [147] can pro-
vide a technical outcome about the randomness of the
missing data. It is a statistical test based on checking
differences in the multivariate distribution of data be-
tween the set of observation with a certain missing data
pattern and the others. When differences are big, miss-
ings are non-random. The R package implements it.
See [182,186] for details on tests on missing data.

8.5. Treatment

Nowadays, most of the data mining softwares claim
that they can deal with missing values. Apparently, this
seems to mean that one can forget about pre-processing
them. However, one needs to know well which kind
of missing data treatment is going to be automatically
performed by the software in each different data min-
ing method and properly evaluate whether this treat-
ment fits well the data and goals of the analysis or not.
In most cases, the pre-processing software tool is using
the listwise deletion strategy or a complete case analy-
sis strategy, both meaning that rows of data matrix con-
taining missing data are excluded from the analysis.
Removing rows with missing cells from a dataset may
cause serious problems if the missing values are not
randomly distributed [127]. As this decision is trans-
parently taken by the software, when the missing data
are non-random, this means, in practice, that the anal-
ysis is performed without a certain subpopulation. For
example, in the database with animals leaving on a
certain ecosystem, all registers describing a mammal
will be ignored in the analysis, just because they have
a missing in the form of the beak variable. The first
consequence is that the resulting description of life in
the ecosystem obtained is implicitly ignoring all things
happening with mammals. As this decision is transpar-
ently taken by the software, the risks to assume that
results describe well the complete ecosystem is very
high, with potential dramatic consequences. So, using
the default missing data treatment provided by the pre-
processing tool software directly impacts on the scope
of the analysis results, and restricts the conclusions to
the smaller part of the population really analyzed by
the software. In some cases, particularly when a big
number of variables are considered, this produces soft-
ware errors: it may happen that none of the data rows
contains values in none of the remaining variables, or
that some of them become constant, and then the soft-
ware will intend to analyze an empty dataset or one
with null variance. Also, this might produce loss of sig-
nificant information (imagine for example, excluding
from the analysis data row that lacks one out of 100
variables).

Also, some systems assume substitution by the vari-
able global mean. This permits to take advantage of the
whole data rows, and it has the property of keeping the
variable means invariant after imputation. However,
this reduces the variance of the variable, having conse-
quences in any inference operation and distorting the
relationship with other variables, with effects in coeffi-
cients like covariances, and biases in inducted models.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 641

Thus, leaving the missing data treatment to the soft-
ware requires a good knowledge of what is the soft-
ware doing, and evaluating if it is correct for the partic-
ular case analyzed. Otherwise, this kind of automatic
treatment can seriously damage the analysis in a trans-
parent way.

A better option is imputation. Imputation [204] is a
complex process for converting missing data into use-
ful data using estimation techniques.

For qualitative variables, sometimes it is useful to
use a specific modality to encode missings, like Un-
kownValue. This is treated as an ordinary modality by
the software pre-processing tool and the variable can
be entered in the models. Impact of the qualitative
missing in modeling could be then better evaluated,
and decisions at the data mining step made accord-
ingly.

Structural missing data: the experts might provide
rules to manually impute them, i.e. substitute form of
the beak of all mammals by lackOfBeak or 0 legs for
fishes.

For temporal data, interpolation might be a good op-
tion. When data with different granularities are repre-
sented in a data matrix, the slower (or higher level)
variables (measured less frequently) will produce lots
of false missing data. They can be interpolated be-
tween useful data in case of temporal data, or prop-
erly propagated to the lower abstraction levels in case
of spatial data, by taking advantage of some implicit
assumptions, like constant behavior inside the interval
(whether spatial or temporal), this being the reason for
saving measurements at certain intervals (or granular-
ity levels). As an example [88], consider the levels of
BOD, measured in a WWTP, once a week, while pH
in the bioreactor is measured daily. Since BOD evolves
very slowly and require a laboratory test, they are mea-
sured less frequently. If the dataset contains one daily
register, this situation is producing 6 weekly missings
for BOD when, in fact, it can be assumed that it be-
haves constantly from week to week. Monotonous in-
terpolation or constant imputation will be used accord-
ing to the knowledge on the variables provided by the
expert. For example, in water resources management,
different levels of aggregation regarding a water con-
sumption model cannot be disconnected between them
and from the global information available on the re-
sources [62,107]. This coexistence of different granu-
larities in both time and space, in geospatial phenom-
ena, characteristic in ESs, is a reason why learning
global models for environmental phenomena is so dif-
ficult [16].

In a most precise approach, model-based imputation
builds a predictive model for each variable to be im-
puted. Many different predictive models has been used
to this purpose, like regression [216], local least square
imputation [129], Support Vector Machines [109]. In
[12] a decision tree has been used to impute the type of
cooking habits of a household, and an artificial neural
net to impute the PM10 emissions of their traditional
stoves. However, model-based imputation is criticized
by many authors for being time consuming [96].

A more sophisticated proposal consists in perform-
ing the missing imputation by preserving the relation-
ships among variables in the entire dataset. This is
the case of Maximum Likelihood Estimation (MLE
method) and related methods [147,222], which per-
forms well, with good theoretical properties, provided
that a sound statistical model is available for the target
problem, which is not always the case, especially in
ESs. Also, building a MLE from scratch at every single
application is often difficult and too time-consuming.
The EM algorithm is also well founded, and involves
bootstrapping to find standard errors for imputation,
but requires high expertise to be properly used.

Multiple imputation [7,15,113] uses a distributional
model for every variable and estimates several com-
plete imputed data matrices by repeatedly making ran-
dom trials from the reference distributional model.
Then, all the imputed data matrices are mined and a
consensus model is built, combining the results ob-
tained for each of the matrices. It requires realistic sta-
tistical assumptions for the variables, only specialized
software performs it, and it is no trivial to get the con-
sensus final model.

In [59,206] surveys of missing imputation data can
be found, some of them with a very practical ap-
proach [95]. The most important aspect is to avoid
false assumptions when considering imputation meth-
ods, which may have a significant effect on the results.
All the methods have pros and cons, and the choice
must be made with care.

The most sophisticated a method is, the more precise
are the imputations provided, but the more expertise
is required to apply properly, the most time consum-
ing is. Considering that missing imputation is a spe-
cific part of the pre-processing step, which in turn is
only the first step of the KDD process, excessive time
consuming methods are not the best option in real ap-
plications, where often answers are required in a very
limited time.

In the following, some intermediate solutions from
the context of data mining that try to provide accept-
able imputation in short time are mentioned.



AU
TH

O
R 

 C
O
PY

642 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

Distance-based methods can be used. The most ba-
sic approach in this line is using the k-NN [18]. The
idea is to use distances to identify the closest instance
in the data base to the one containing the missing, and
to use the value of the nearest neighbor for imputation.
This can be extended to consider the mean value of
the first k nearest neighbors as imputation value. The
method is quick and performs well, provided that the
variables used for the k-NN are complete, i.e. do not
contain missing values. Otherwise, distances that can
deal with missing values in turn are required, and stan-
dard softwares do not implement them. Other distance-
based works are [192].

The MICE method [15] solves in an iterative pro-
cess the existence of missings in the other variables.
Each variable is imputed based on a regression model
built with the remaining variables. All missing data of
regressors need to be previously imputed. They are ini-
tially imputed by the mean, and all variables imputed
iteratively until convergence.

The MIMMI method [81] is based on performing a
previous clustering with a subset of complete variables
and to impute missing values of the remaining vari-
ables with the conditional means of discovered clus-
ters. Such an approach may even help to identify errors
in the dataset (when some observation is placed in a
wrong cluster and the reasons are investigated).

Another method proposed in [13] was to use qual-
itative indicators (i.e. by assigning quality labels in-
stead of predicting values) for substituting missing or
erroneous values. Similar work has been done for sub-
stituting missing or erroneous values with fuzzy sets,
probability distributions or confidence intervals.

Sometimes, the best imputation method depends on
the particular data mining method to be applied after-
wards [67,153].

It is important to impute missing data before using
the corresponding variable to create other variables or
indicators, in order not to propagate the missings to the
new variables.

It is of outmost importance to report the missing im-
putation method used explicitly for allowing the end
user to evaluate if the missing imputation decisions are
acceptable or not for each specific application.

9. Relevance or redundancy detection and
dimensionality reduction

In Section 4 some strategies for an expert-based se-
lection of rows and columns from the original data ma-

trix are presented. In this section, methods for a se-
lection of both rows and columns from a more techni-
cal perspective are presented. These methods can only
be applied after errors in database have been corrected
and outliers and missing values have been properly
treated. The main aim is to detect irrelevant rows or
columns from the working data matrix or those redun-
dant that inefficiently increase the dimensionality of
the dataset without providing any additional informa-
tion. Building the models without those irrelevant or
redundant observations or variables is expected to not
significantly decrease the model quality.

9.1. Instance selection

Sometimes, redundancy of instances can produce
unnecessary big size of the dataset. It also can produce
imbalanced datasets, which can disturb the data mining
process.

Detection and treatment of redundant instances is
relevant in the pre-processing step to guarantee proper
performance of the data mining. The aim of the in-
stance selection techniques [120,148,196] is to reduce
the number of examples of the dataset with a minimum
loss of information (predictive accuracy, clustering ac-
curacy, etc.); thus retaining the relevant instances and
removing the redundant ones, which probably are not
useful for modeling.

Instance selection techniques obtain a subset S, S ⊆
T , of the original training dataset T ⊆ I, such that
S does not contain redundant or superfluous instances,
and the accuracy in the models mined (usually predic-
tive or clustering models) is nearly equivalent using S
or T as the training set.

Instance selection techniques can be organized ac-
cording to the processing way of how S is computed:

• Incremental methods: start with an initial empty
new set of instances (S = ∅), and proceed by in-
corporating only the relevant instances.

• Decremental methods: start with S = T , and pro-
ceed by removing the redundant instances until
getting the final set S.

• Mixed methods: use a mixed incremental or
decremental way of the selection process. In some
step of the algorithm, they proceed incrementally
by adding new instances or decrementally by re-
moving instances, and at a further step, they pro-
ceed in the opposite way.

According to the bias dimension, like in feature se-
lection methods, the instance selection methods can be
divided into filter methods and wrapper methods.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 643

9.1.1. Filter instance selection methods
These methods use several selection criteria, but

none is based on the feedback of a mined model from
the data (classifier, clustering, etc.). There are some
special filtering methods, sometimes noted as exhaus-
tive methods, which mainly use the size or some per-
centage of instances to make the selection process:

• Random methods: take randomly just the percent-
age of instances that the system is able to process
(sampling).

• Supervised random methods: take randomly the
same percentage of instances within each class
group (stratified sampling).

Random filters are repeatedly used as the first step of
iterations in resampling methods [94], like Bootstrap,
Jackknife or Monte Carlo methods [233] and ensem-
ble methods [26,58]. Resampling methods are nowa-
days becoming very popular in the context of big data,
since they belong to a divide and conquer approach
that makes dealing with huge datasets possible. Also,
resampling techniques, and in consequence, repeated
random filters can be involved in some validation pro-
cedures as cross-validation [131].

Among the filter methods in the literature, we can
find methods based on k-d Trees [168], where a bi-
nary tree is constructed. Some authors [37,211] have
proposed the idea of using clustering for the selec-
tion process, taking the centers (or nearby) of the clus-
ters as the selected instances. Some methods follow-
ing those ideas are the GCM (Generalized-Modified
Chang Algorithm) method [164], the NSB (Nearest
Sub-class Classifier) method [221], the CLU (Clus-
tering) instance selection method [154], and the OSC
(Object Selection by Clustering) [176]. Other authors
suggested the assignment of weights to the instances
and selecting only the relevant instances like the WP
(Weighting Prototypes) method [179] and the PSR
(Prototype Selection by Relevance) method [177].

Other filter methods are based on other semantic cri-
teria, based on some properties about the instances.
Instances are defined to be border instances or in-
ner instances. A border instance for a class C is the
nearest neighbor of an instance from another class
C′. Some filter methods based on selecting border in-
stances are the POP (Pattern by Ordered Projections)
method [199], which discards inner instances and se-
lects some border instances, and POC-NN (Pair Oppo-
site Class-NearestNeighbor) method [193], which se-
lects border instances.

9.1.2. Wrapper instance selection
The selection criterion is based on the accuracy ob-

tained by a model mined with the considered instances.
Usually, those instances not contributing to the accu-
racy of the model are the candidates to be discarded
from the training set.

Most of the wrapper methods proposed in the lit-
erature are based on the k-NN classifier [78]. This is
the case of Condensed Nearest Neighbor (CNN) [106],
one of the oldest, the SNN (Selective Nearest Neighbor
rule) method [200], and the Generalized Condensed
Nearest Neighbor rule (GCNN) [45]. Other early in-
stance selection methods focused on discarding noisy
instances in the training set, like the Edited Nearest
Neighbor method [229] and some variations like the all
k-NN method [218] and the Multiedit method [57]. The
IB2 and IB3 (Instance Based) methods were proposed
by [4], which are incremental methods. Other methods
related to k-NN are those proposed by [230]: DROP1,
DROP2, DROP5 (Decremental Reduction Optimiza-
tion Procedure), which are based on the concept of as-
sociates. The associates of an instance i are those in-
stances such that i is one of their k nearest neighbors.
Another method related to the associates concept was
the Iterative Case Filtering algorithm (ICF) proposed
by [33]. In addition, evolutionary algorithms have been
used for instance selection [25,38,56,79,140]. In [77]
it was proposed a memetic algorithm, combining evo-
lutionary algorithms and local search in the evolutive
process, and [75] proposed the Clonal selection Algo-
rithm (CSA), which is based on the artificial immune
system approach. Finally, some authors proposed the
taboo search for the selection of the instances [40,237].

A special wrapper method is the Windowing method
[124,191], which uses a percentage of random in-
stances to make the initial selection process, followed
by feedback guidance; the selected data is mined (un-
der a classifier/predictive model) and the inducted
model is applied on the remaining data. Those which
have not been correctly predicted are added to the se-
lected dataset and the process is repeated. It could save
up to 20% of the data.

9.2. Relevance of variables and dimensionality
reduction

A major problem in data mining is to find out which
are the relevant variables or features to be taken into
account. When experts are available in a particular do-
main or application, experts could give their advice.
When there is no expertise available, or the selection



AU
TH

O
R 

 C
O
PY

644 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

provided by experts wants to be technically refined,
some automatic methods should be used.

Feature weighting methods quantify the relevance of
the variables. Feature selection identifies the subset of
relevant variables. Factorial methods build a smaller
set of new synthetic variables conserving most infor-
mation from the working data matrix.

When the number of variables is too high to deal
with in a reasonable way, which is not unusual in data
mining context, a data reduction method can be ap-
plied. Either Factorial methods or Feature Weighting
or Feature Selection are suitable possibilities in these
cases.

Next subsections give details.

9.2.1. Feature weighting
Feature weighting [3,172] techniques provide a

ranking (weight) of the attributes (or variables) accord-
ing to their degree of relevance.

The degree of relevance of a variable Xk is ex-
pressed by means of a weight (wk), usually in the inter-
val [0, 1]. They assign high weights to more important
variables, and low weights to irrelevant or redundant
variables. This way, it is possible to decide which the
important variables in a dataset are. These techniques
are useful only when the importance of the variables
for the dataset can be taken into account in the analysis.
For instance, feature weight assignment is frequently
used to denote the relevance of attributes in similarity
or distance-based methods, like clustering or some in-
ductive classification rule methods, allowing to empha-
size the relevant variables in the distance/similarities.

One of the problems in the feature weighting meth-
ods is how to decide when a set of weights is better
than another. They must be evaluated in terms of the
performance of a task:

• In supervised domains, a classification task could
measure the accuracy of the label predictions for
unlabeled instances.

• Unsupervised weighting methods assign weights
to variables without any knowledge about class
labels, so this task is presumed more difficult
[53,111]. In fact, they use alternative measures
like significant changes in similarity or distances
to evaluate the goodness of a set of weights.

In [228] a conceptual framework for the classifica-
tion of weight assignment methods is presented. This
framework consists of five dimensions: Bias feedback,
preset, Weight space continuous, binary, Representa-
tion Given, Transformed, Generality Global, Local and
Knowledge Poor, Intensive. Most important dimen-

sions are the Bias, the Weight Space and the General-
ity.

The Bias dimension refers to whether the weight
learning bias is guided by feedback from the perfor-
mance algorithm (i.e. the classifier), or whether it is in-
stead, a preset bias (i.e. maximize intra-class similar-
ity and minimize inter-class similarity) that does not
incorporate performance feedback.

Bias is useful to separate those algorithms that use
feedback from those that do not use it. The first ones
are known as wrappers [133]. The second ones are
named as filters. Presumably, the wrappers have an ad-
vantage. Their search for attribute weight assignment is
guided by how well those assignments perform. Thus,
there should be no mismatch between the biases of the
weighting and performance algorithms.

The Weight Space dimension distinguishes feature
weighting from feature selection algorithms. The lat-
ter are a proper subset of feature weighting algorithms
because they only employ binary weights (i.e. 0 or 1),
meaning that the attribute is either deleted or retained.
Although weight assignment improve accuracy in clas-
sification and retrieval tasks, feature selection is vital
to reduce the dimensionality in learning tasks, elimi-
nating irrelevant attributes. In general, feature weight-
ing is more appropriate for tasks where features vary in
their relevance, but such methods search larger spaces
of weight assignments. Feature selection algorithms
perform better when the features are either highly cor-
related with the class label or completely irrelevant.

Feature weighting algorithms can also be distin-
guished by their Generality. While most algorithms
learn settings for a single set of weights that are em-
ployed globally (i.e., over the entire instance space),
other algorithms assume different weights among local
regions of the instance space.

• The assumption of global weighting methods that
attribute relevance is invariant over the whole set
of instances I is constraining, and often inappro-
priate. Other algorithms assume that the relevance
of attributes is not necessarily the same in the
whole domain.

• Two types of local weighting schemes are pop-
ular. The first assigns a different weight to each
qualitative value of the attribute. Although this
allows feature relevance to vary over the values
of the feature, it still constrains weights to be
identical for all instances with the same qual-
itative feature value. The second local weight-
ing scheme removes these constraints by allow-
ing feature weights to vary as a function of the
instance and their belonging to a class.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 645

In recent years, many researchers are focusing on su-
pervised feature weighting, i.e. where a response vari-
able is available and relevance towards it is evaluated:

• Some research in filter global methods has been
done like the Mutual Information technique (MI)
[112,228], the QM2 method by [162], and [50]
about their introduced Cross-Category Feature
importance (CCF) method. On the other hand, fil-
ter local weighting methods have been proposed
in the literature such as the value difference metric
(VDM) of [212], the Class Distribution Weight-
ing method (CDW) [110], and the Per-Category
Feature importance criterion (PCF) [50].

• Some wrapper methods are: the RELIEF-F
method [130,137] and the DIET algorithm [134],
and some approaches based on the use of Genetic
Algorithms [93,114].

However, less work has been done in the field on un-
supervised feature weighting, based on the sensitivity
of instance similarities to that variable. Unsupervised
scenario is clearly the required one when facing a new
unknown database, which we want to mine to discover
new knowledge, and usually no reference classification
is available. In the literature, one of the few works is
[207] on a Gradient Descent technique (GD) and fea-
ture selection approach on unsupervised entropy-based
method [53]. In [61] a feature weighting method for
supervised learning is presented. Derived from it, in
[173] two new unsupervised feature weighting meth-
ods were proposed (UEB-1 and UEB-2) with promis-
ing results.

Empirical works [228] and theoretical ones [143],
suggest that the learning complexity is exponential re-
garding the number of irrelevant variables. Therefore,
the failures in the data mining process could be related
to a similarity model, and in particular, with an incor-
rect weight assignment methodology. A comprehen-
sive review of feature weighting can be found in [172].

Indeed, the use of pure feature weighting is quite
marginal in real applications. It is more frequent to use
them as a previous step of feature selection by choos-
ing a relevance threshold to keep a variable for the
analysis.

9.2.2. Feature selection
Feature selection is a specialization of Feature

Weighting, where all the weights get binary values: 0
or 1. If the weight of a variable is 0, that means it is
discarded, if the weight is 1, the variable is used for
the analysis. For a survey of common feature selection
techniques, see [29,43,149,163,205].

Feature selectors are algorithms that attempt to iden-
tify and remove as much irrelevant and redundant
information as possible prior to learning or knowl-
edge discovery. It is important to remark that most
of the feature selection methods perform with respect
to a certain response variable that has to be modeled,
although some research has been done in the non-
supervised scenario [61,161,173,174]. Feature selec-
tion can result in enhanced performance, a reduced hy-
pothesis search space, and, in some cases, reduced stor-
age requirement. Usually, analyzing the feature sub-
set selection provides better results than analyzing the
complete set of variables [103].

Feature selection can be performed by applying a
cutting threshold over the results of a feature weight-
ing process. However, automated techniques for iden-
tifying and removing unhelpful or redundant variables
are extensively used. Some can use statistical crite-
ria to rank the variables [34,102], others are mutual-
information criteria based [151,184] or more general
concepts like instance consistency [10,11,54].

Feature selection methods usually take one of the
three forms [103]:

• Filter Feature Selection Methods: Direct exam-
ination of the relevance of candidate variables,
independently of the machine learning model to
be used for the data mining, usually a predictive
method. Usually we can distinguish two types of
filter methods [21,39,55,135,144,203]:

∗ Feature ranking methods: they rank features by
a metric and eliminates all features bellow an
adequate score. Often these methods do not
consider potential interactions among features.

∗ Subset selection methods: they search for the
optimal subset of features. Some of them can
take into account the interaction among fea-
tures.

∗ Mixed approaches: they can sequentially in-
terleave some feature ranking operations with
some subset selection operations, to try to cap-
ture all the benefits from both approaches.

• Wrapper Feature Selection methods: Searching
the best combination of variables in terms of
model performance and feedback. They utilize the
ML model of interest as a black box to score sub-
sets of feature according to their predictive power
[185]:

∗ Brute-force methods: they explore all the pos-
sible subsets of combinations of features and
get the optimal one [132,169].



AU
TH

O
R 

 C
O
PY

646 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

∗ Forward methods: starting with an empty set
of features, they add one feature at each step
until getting the optimal subset of features
[188,198].

∗ Backward methods: starting from a set contain-
ing all the features, they discard one feature at
each step until getting the optimal set of fea-
tures [167,210,213].

∗ Random methods: they can try several subsets
of features in a random way, trying to avoid to
be trapped in local optima (i.e. anytime algo-
rithms) [6,125,150,189,214,235].

• An intermediate type of methods are the embed-
ded methods: they perform feature selection in
the process of training of the ML technique and
are usually specific, and included within the given
ML techniques [60], such as in decision trees or
classification problems [142] and artificial neu-
ral networks techniques [160]. Some works give
common frames for different modeling methods
[152].

Most recent works tackle the problem of computa-
tional cost in big data scenarios [122,238]. As an exam-
ple, in [194] up to 15 variables associated with biofilm
development in water distribution systems were avail-
able, including sampling and incubation methods and
physical, hydraulic and physico-chemical characteris-
tics of water. However, a random wrapper suggested an
adequate model using only eight of those variables.

9.2.3. Factorial methods and related
Reducing the original set of variables of a dataset to

a smaller set of equivalent variables is an interest topic
of Multivariate Statistics from the beginning of XXth
century [183]. The main goal of factorial methods is to
substitute the original list of variables by a subset of
factors that keep the same information as the original
dataset. The approach is radically different from the
one followed in feature selection. Here, an algebraic
metaphor (classical in statistics), in which each row
of the data matrix is associated with a K-dimensional
point in a vector space (K being the number of vari-
ables), is used to find subspaces of smaller dimension
where the projection of the original data cloud con-
serves the adjacency relationships among both vari-
ables and individuals. These methods are mainly based
in matrix rotations and diagonalizations and several
techniques are available according to the kind of infor-
mation contained in the dataset. The Principal Com-
ponents Analysis (PCA) [99] is one of the best known
techniques of this group and it is suitable for datasets

with only numerical variables. Each principal compo-
nent is a linear combination of the original variables,
and the aim is to work with a reduced set of these, such
that the loss of information is not relevant. Thus, PCA
is suitable for synthesizing an original set of numeri-
cal variables into a small number of fictitious variables
conserving as much information as possible from the
original dataset. Equivalent techniques are available
for qualitative data, like simple or multiple correspon-
dence analysis [145]. A general formulation including
all factorial techniques as particular cases is the canon-
ical analysis, in which other more general methods are
embraced, like non-linear multivariate analysis [136]
and methods based on oblique rotations [197]. In the
context of finding a relevant set of descriptors from im-
ages, non-linear generalizations of PCA based on arti-
ficial neural networks and deep learning methods are
used [108].

In these methods, the information retained in each of
the factors is quantified and the number of significant
factors to be retained can be evaluated. However, it has
to be taken into account that, in most cases, interpreta-
tion of the new variables (or factors) may not be clear.
If the factors are later used to build data mining models
with them, implications over understandability of the
final results might appear.

An appropriate alternative for the case when the in-
terpretation of the significant factors is difficult is to
identify the subset of original variables having higher
contribution to that factors, and use this subset of orig-
inal variables in the data mining. This is an intermedi-
ate alternative producing a subset of original variables
bigger than the set of significant factors, but keeping
the data mining in the original space, thus maintaining
the interpretability of the obtained models.

10. Transformations

Sometimes transformation of variables may be re-
quired. We can distinguish three main reasons: one re-
lated to data quality, the second oriented to achieve the
technical assumptions required by some specific data
mining method, and the third one regarding data inter-
pretability.

10.1. Transform to improve data quality

These are mandatory transformations and required,
because when skipped, wrong data mining results will
be obtained. Most of the times they are difficult to



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 647

Fig. 6. Daily load of NH4 in a WWTP (up: row data; down: pre-pro-
cessed data).

identify and require good knowledge of the variables’
meaning and sometimes details about how they were
measured. Three basic scenarios are considered here:

• Homogenization of instances: sometimes, espe-
cially when the original data matrix is the result of
the fusion of different data sources, it may happen
that the measurement units of some groups of ob-
servations differ. It seems obvious that all obser-
vations must be transformed to a single measure-
ment unit. But unless a complete meta-data set for
the original data matrix arrives, this information
might be masked with dramatic consequences for
the further data mining.
As an example, a dataset contains rows with
daily information about a WWTP for a com-
plete year. Among them the NH4 load in the
inflow (Fig. 6(up)). However, when looking at
the metainformation, it is reported that data from
March, April and May is reported in Kg/day,
whereas it is in g/day for the rest. As all days are
properly transformed to g/day one gets a complete
different image (Fig. 6(down)).

• Homogenization of columns: Let us suppose a
dataset on National Parks with a National Park in

each row and a variable indicating the number of
members of a certain protected species, like lynx
pardinus. If the size of the parks is highly variable
all along the database, the number of specimens
per park is not directly comparable among parks
and the variable needs to be transformed in a den-
sity of specimens per surface like specimens/Km2

of the park.
• Elimination of individual variability: This is typi-

cally required in pre-post analysis scenarios, that
is, when repeated measurements are taken before
and after some intervention in the domain, or the
occurrence of some event. As an example, con-
sider measuring air quality before and after using
a certain pesticide on some pieces of land. Among
others, the data base will contain a couple of
columns with the concentration of organometal-
lic components in the atmosphere before (X0) and
after (X1) pesticide distribution. If a piece of land
is in a place with high toxic components in the
air, probably it will register also high values of
X1. The effect of distributing the pesticide in the
air quality in each piece of land, can be evalu-
ated as the difference in the two concentrations
(X = X1 − X0). Using X for the data mining in-
stead of both X0 and X1, as two independent vari-
ables, the individual characteristics of each piece
of land are eliminated. Thus, for paired observa-
tions, differences have to be built before the data
mining. For the case of variables with very small
values, ratios can behave better than differences,
because too many values close to 0 might give nu-
merical instability.

• Compositional data treatment (introduced in Sec-
tion 3.1): Let us suppose a dataset on rivers in-
cluding information about the sediments. Suppose
these are classified in boulders, cobbles, pebbles,
gravel, sand, silt and organic mud. The dataset
has a subset of 7 variables containing the propor-
tion of that material found in the sediments. And
the total of those 7 columns is always 1, for all
the rivers; this establishes a linear relation among
the 7 variables. These are compositional data, and
cannot enter as independent variables in the data
mining models. In [181] a theoretical explana-
tion is given. The log-ratio transformation usu-
ally works well, particularly for those data min-
ing method requiring normality, like classical re-
gression models. Thus, given a subset of s com-
positional variables, it is enough to choose any of
them as a referential variable Xj (j = 1 : s),



AU
TH

O
R 

 C
O
PY

648 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

build X′
k

= log(Xk/Xj ), for (k = 1 : s) and
use X′

k
in modeling. In [36] a nice introduction

to the compositional data treatment is given and
alternative transformations which make sense in
some specific situations are discussed. Note that
sometimes, the compositional data are hidden in
the dataset. This means that the components ap-
pear in non-contiguous columns; sometimes they
are presented as concentrations or absolute num-
bers and they are not directly expressed as pro-
portions. For example, in a microscopic observa-
tion of a water sample, it is usual to account for
the number of specimens of different families of
microorganisms. Thus, the dataset contains some
columns with counts of individuals for different
families, which need to be transformed into pro-
portions with respect to the total number of spec-
imens counted and treated as compositional data.

10.2. Transformations required to increase
interpretability

These are convenient transformations done before
the data mining step, oriented to better interpret the
resulting models. However, the data mining models
would not be incorrect if the transformation would be
skipped. In this group, several operations appear: re-
coding, discretization, functional univariate transfor-
mations.

Consider as an example a dataset about car models
in a study about air pollution caused by traffic, and sup-
pose some variables with technical characteristics of
the car. Among them the X = miles per gallon. Build-
ing a new variable Y = 1/X, the consumption of the
car is obtained, much easy to interpret by itself, but
also linearly related with pollutant emissions in a more
natural way. Even if it is not strictly required, making
these kinds of transformations may linearize the rela-
tionships with the response variable.

In a second scenario, consider qualitative vari-
ables with a large number of modalities; they can
be grouped according to expert knowledge (recod-
ing). As an example, consider a variable contain-
ing the type of dominant microorganism in a mi-
crobiological sample of wastewater. Several hundreds
of species can be identified in microscopical analy-
ses. Thus, we are here in front of a qualitative vari-
able with hundreds of modalities. Nevertheless, this
information performs as noise when introduced in
the data mining process, because the variable con-
tains too many modalities, each with low preva-

lence. In these cases it seems reasonable to group the
species by Genera. In [22,80], 24 Genera related with
WWTP are mentioned (Pseudomonas, Achromobacter,
Bacillus, Alcaligenes, Flavobacterium, Arthrobacter,
Zooglea, Acinetobacter, Citromonas, Bacillus, Nitro-
somonas, Nitrobacter, Nitrospirillum, Vorticella, Aspi-
cidica, Paramecium, Nocardia, Microthrix, Spaerotilus
fungi, snails, Phosphate accumulating organisms, al-
gae (lagoons), Viruses, yeast, pathogens (include
Salmonella, Giardia, etc.)). Even more, those can be
grouped according to their role in the treatment pro-
cess. Also according to [22]: Floc forming, Nitrifying,
Predators, Nuisance bacteria and eukaryotes, Spe-
cialty populations, other. In that case, a qualitative
variable with only 6 modalities is obtained, much easy
to work with. The kind of recoding and the granular-
ity to which recoding is required depend on the goals
of the analysis. The authors dissuade the reader for re-
coding operations as a general principle, since this is
changing the structure of the original dataset space, by
collapsing sets of values in a single one, and relevant
information can be missed. However, when the qualita-
tive variable has so much detail that perspective is lost,
this kind of recodifications oriented to increase inter-
pretability and provide structure can work well.

Another situation quite common in pre-processing
is discretization of numerical variables. Here, the au-
thors keep a similar position as for recodifications:
better avoid. However, sometimes, interpretability in-
creases by dealing with the qualitative version of the
variable. Discretization may work, provided that the
cutpoints are non-arbitrary. As an example, a vari-
able accounting for the level of radioactivity can
be discretized according to the associated risks for
humans: <1000 mSv, innocuous; [1000, 2000) mSv,
Minor Nausea, Headache; [3000, 6000) mSv, illness
(Mod Vomiting, loose of Hair, diarrheas, Mild
Headache, Mod Fever, Cognitive Impairment); [6000,
8000) mSv, severity (Vomiting, Mod Headache, High
Fever, Cognitive Impairment, Hemorrhage, Infection);
[8000, 20,000) mSv, lethal in some weeks (Severe
Vomiting, Severe Headache, Severe Fever, Incapaci-
tated); [20,000, 30,000) mSV, lethal in some hours;
>30,000 mSv, lethal with immediate effect. In most
cases a discretization operation transforms a numerical
variable in an ordinal one, but sometimes, the result
of a discretization may produce a binary variable, or
also a qualitative one. For example, grouping all abnor-
mal values of a sensor (either high or low) in a single
modality, to send and alarm or not, makes sense in in-
telligent decision support systems, or control systems
and produces a binary variable as a result.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 649

Most of the software tools provide automatic meth-
ods to discretize numerical variables (by equal width
bins, or equal frequency groups, given the number of
desired groups) [195]. However, despite this transform
can increase model performance, the meaning of the
resulting beans often becomes difficult to understand
and the data mining loses interpretability. In fact, in
real applications it is quite common to globally dis-
cretize any numerical attribute before applying learn-
ing algorithms to datasets, since a number of them
cannot handle numerical variables directly. Some data
mining techniques increase performance with prior
discretization, like decision trees [73]. However, the
reader must know that discretization is changing the in-
ner structure of the problem and the results provided by
the discretized data can be significantly different from
those provided by the original data, even if the inter-
vals are well justified and consensual. This is the case
of [91], where concentration of hormones in blood are
discretized to Low, Normal and High to identify pro-
files of thyroids dysfunctions and it is shown that the
models obtained with the original quantitative concen-
trations are better, even if the discretization is based on
WHO reference values.

Noise is often a critical issue, and especially with
environmental data, some bias may exist that can be
removed with a filter [65]. Transformations should al-
ways be justified and documented, and the biases that
may be introduced noted.

It is strongly recommended to keep the interpretabil-
ity of transformed variables.

10.3. Transformations required to fit technical
assumptions of data mining methods

Some data mining methods involve technical as-
sumptions that must be hold to guarantee the va-
lidity of the results. For example, linear regression
or ANOVA require normality, homo-chedasticity, and
non-colinearity. Also, inductive machine learning clas-
sifiers perform badly in front of imbalanced datasets.

Thus, in front of non-normal, correlated or imbal-
anced datasets, transformations are apparently manda-
tory previous to the data mining step.

The vision of the authors is to better change an-
other data mining method well adapted to the nature
of the data, rather than changing data to adapt the
method [89,90]. This is mainly because, in the context
of data mining, this may lead to good models, with un-
certain usefulness, made of non-understandable trans-
formed variables. Nowadays, there are so many data

mining methods, that we presume this technical trans-
formations could be complete dispensable in the pre-
processing. As an example, it is enough to change lin-
ear regression by ANN to avoid normality, linearity
and homochedasticity assumptions and get better pre-
dictions for non-normal, non-linear or heterochedas-
tic data. However, we provide here details on the most
popular technical transformations, just for the cases in
which they make really sense to use them.

Centering: Some methods, like PCA gain proper-
ties when operating over centered data. This means
to have a dataset with a center of gravity in the ori-
gin of coordinates. Each variable Xk is transformed
in X′

k
= Xk − x̄k , being x̄k the arithmetic mean of

Xk . Centering guarantees that x̄k = 0. Performing
PCA with centered data enables to interpret the angles
formed by the vectors projecting the variables over the
factorial planes as a visualization of their linear corre-
lation, such that 90° means uncorrelated variables, and
0° means linearly associated variables.

Standardizing and normalizing: This is often used
to avoid variables of different magnitudes coexisting
in the same data matrix, which can bias the anal-
ysis, and also invalidate comparisons among them.
Also, it is used to meet technical requirements of
some data mining methods that need the variables to
be properly scaled to avoid numerical ill conditions
(like some distance-based methods) [195]. All these
operations move the original variable to the interval
[−1, 1]. In the classical standardization transforma-
tion named Z-score standardization in [104], the vari-
able is centered and scaled to have null mean and
variance equal to 1. The standardization transforma-
tion is X′

k
= Xk−x̄k

sk
, x̄k being the arithmetic mean

of Xk and sk the standard deviation. With this trans-
formation X′

k
always has mean equal to 0 and vari-

ance equal to 1. Other normalization transformations
can also be used to move to [−1, 1] intervals, but they
do not guarantee variance equal to 1. Normalization
based on the range of the variable [101], also referred
by some authors as min-max normalization [104,195]:
X′

k
= Xk−min(Xk)max(Xk)−min(Xk) ; if it is generalized to the trans-

formation X′
k

= a + (Xk−min(Xk))(b−a)max(Xk)−min(Xk) , then X
′
k

moves
to the interval [a, b] [101]. Decimal scaling normaliza-
tion [104], where X′

k
= Xk10r , where r is the smaller in-

teger such that max(|Xk|) < 1; this is what happens
when measurement expressed in m are transformed to
Km, in this case with r = 3. In [195], a transformation
to achieve the normal distribution of the new variable is
proposed, based on first ranking observations and sec-
ond substituting original values of the variable by the



AU
TH

O
R 

 C
O
PY

650 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

z-score corresponding to the Z distribution percentile
equal to the rank value.

Logarithmic transformation: Useful to achieve ho-
mochedasticity. When the variable has values too close
to 0, it is important to previously translate it horizon-
tally to avoid numerical instability.

Quadratic transformation: catching quadratic rela-
tionships with the response variable. They are useful
to linearize the quadratic terms in regression models
for example. The same concept extends to high order
transforms Y = Xr , r > 1.

In [30], the Box–Cox transformation is formulated
as a family of transforms depending on a parameter r

in the form X′
k

= X
r
k
−1
r

, r > 0, and X′
k

= log(Xk),
r = 0. For r = 1 it simply makes a horizontal
translation of the variable. In statistical modeling they
are often used to eliminate skewness and normalize
asymmetric distributions. See [141] for details. As it
can be seen, the purpose of Box–Cox transforms are
strictly technical and the interpretation of X′

k
is usually

missed.
The rank transformation [195] is based on sorting

the observations of the variable and enumerating them.
The transformed variable accounts for the relative po-
sition of the object in this list and becomes an ordi-
nal variable. This is useful when the ordering among
individuals is important, but the distance between the
individuals in consecutive positions has no particular
meaning. For example, when evaluating enterprises to
get the contract to manage a WWTP in the next five
years, and some evaluation is obtained from the candi-
dates, the differences in the final score are not impor-
tant, but the ranking is. Also, for distance-based data
mining methods, ordinal variables (like risk of eruption
in a volcano: low, moderate, high, sure) can be first re-
codified to ordinal encoding (1, 2, 3, 4) and the result-
ing encoded variable ranked and used in the distance-
based methods (like clustering) instead of the ordinal
one, for better exploiting the ordinal structure of the
variable in the data mining process.

As said before, sometimes data is fuzzified to bet-
ter include uncertainty intrinsic from data into the data
mining model [76]. This means to transform a crisp
value in a fuzzy number and requires specific fuzzy al-
gorithms to work with them. This process is particu-
larly natural when data coming from certain technolog-
ical devices provide uncertain punctual measurements
together with some evaluation of the uncertainty itself,
like variability, error bounds, occurrence regions, and
so on. Radar data, GPS data or SCADA data are par-
ticularly suitable for this kind of treatment [224].

10.3.1. Transformations associated to imbalanced
datasets

As said before, some data mining methods require
balanced datasets. However, imbalanced sets appear in
many real applications. They appear where the num-
ber of instances of one subpopulation (or class, usu-
ally the general class representing the usual scenario)
is much bigger than the number of instances of some
other classes (usually abnormal classes), which fre-
quently are the most informative and valuable ones.
These often are the object of interest in the specific
problem. Among others, imbalanced datasets typically
appear in the identification of anomalies in certain sys-
tems designed to work under steady conditions. Some
examples are: Water Distribution Systems [118], the
detection of oil spills from satellite images [138], the
identification of power distribution fault causes [234]
and the prediction of pre-term births [97]. This issue is
growing in importance since it appears more and more
in most real domains, especially in systems where data
from the usual scenario are abundant while data from
abnormal ones are scarce.

Most classical inductive machine learning algo-
rithms generally perform poorly on imbalanced data-
sets, because they are designed to minimize the global
error rate [121], thus biasing toward the majority class.
Classification algorithms tend to classify almost all in-
stances within the majority class, poorly classifying the
minority class. Using normal and abnormal data in a
water distribution system as they are produced, without
discrimination, produces a tolerance to failure unable
to ensure capacity redundancy, and network operation
under the failure of one component cannot be ensured
[156].

To deal with imbalanced datasets several approaches
are suitable:

• Pre-processing: trying to balance data by over-
sampling the minority classes, under-sampling
the majority one or a combination of both [19,
44,98]. Resampling for building several balanced
subsets of data and using ensemble methods poli-
cies are also used in these cases, to gain robust-
ness. Also, introduction of synthetic data for ab-
normal situations, simulated on the basis of the
real one is also used by some authors to balance
the dataset and proceed normally. As an exam-
ple, in [118] a complex hybrid model, which com-
bines a calibrated classical model of a Water Sup-
ply System and a neuro-fuzzy technique, is used
to identify leaks and other anomalies in the sys-
tem. Since data on abnormal situations are scarce,



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 651

the calibrated classical model of the hydraulic net-
work is used to simulate abnormal data, thus pro-
ducing a dataset with enough fuzzy examples to
correctly train the artificial neural network. How-
ever, in [223] it is shown that over-sampling the
abnormal class might produce an unacceptable
rate of false positives that might carefully be ana-
lyzed.

• Solutions at the algorithmic level: modifying the
cost per class [187], adjusting the probability es-
timation by establishing a bias towards the minor-
ity classes [227], etc.

In [70], different pre-processing mechanisms are
used in conjunction with a Fuzzy Rule Classification
System to deal with imbalanced datasets.

10.3.2. Practicum
In the context of data mining, avoidance of unnec-

essary transformations is recommended, especially if
the transformation decreases interpretability. For ex-
ample, Y = log(stream_flow), can produce a variable
Y compatible with the normal distribution and suitable
to enter in a regression model; however, the logarithm
of a flow is something difficult to understand. It is
non-recommended to strongly transform data with the
unique purpose of getting a better fitting of the model.
Indeed, hard transformations can provide high quality
models from a technical point of view. However, these
are often models built over variables with difficult in-
terpretation. Then, usability of the model will decrease.

If transformations are definitely required, some bias
is often introduced into the results. Thus, it is conve-
nient to minimize arbitrariness of the transformation as
much as possible (in recoding Age, it is always difficult
to justify why the modality Adult encodes ages from 18
to 65 or from 20 to 70), and this implies that the goals
of the analysis must also be taken into account. For
arithmetic transformations, imputation of missing data
before building the transformed variables is strongly
recommended in order not to propagate the missing
values to the transformed variable, since any operation
with a missing value produces a missing.

Finally, instead of strongly transform the data till it
fits on the technical assumptions of the preferred data
mining technique (in spite of losing interpretability, or
introducing arbitrariness), a better practice is to select
a data mining technique fitting well on the target phe-
nomenon and the type and structure of available data
[83]. This means that, when data is far from normal
distribution, it is better to use ANNs rather than classi-
cal multivariate regression, or that an alternative to ID3

method should be used to find groups over data if it is
numeric, like clustering, instead of forcing discretiza-
tion.

11. Creating new variables

Finally, in KDD it is quite usual to build new
variables by combining some others included in the
dataset. Here, expert knowhow and domain knowledge
is usually the guide. Often new variables are nonlin-
ear combinations of the measured variables, thus pro-
viding new elements for the modeling step that can
significantly increase model performances. When the
purpose of these transformations is to make explicit
some of the decisional variables that the user really has
in mind for decision-making, this is a good practice,
bringing significantly added value to the dataset, and
opening the way to better data mining models. There
are different scenarios:

Creation of aggregates. Like totals. As an example,
consider the concentrations of several pollutants in at-
mosphere, like CO2, nitrogen, arsenic, selenium, an-
timony in atmosphere, it makes sense to build a new
variable with total concentration of heavy metals, by
adding concentrations of arsenic, selenium and anti-
mony. This might increase interpretability of the data
mining models obtained. However, care is required, as
this decreases the granularity of information, by col-
lapsing different profiles of air quality; whereas deci-
sions associated with exceeding of arsenic should be
different of exceeding of cadmium, using a global vari-
able for all heavy metals together is masking the pos-
sibility to understand what is happening in detail. The
same happens if all pollutants found in the outflow of a
WWTP are added together in a variable concentration
of pollutants [88]. In that case, a high level of this vari-
able do not permit to understand if the plant has abnor-
mal operations in the settler (producing too high sus-
pended solids concentrations) or in the bioreactor (too
high organic matter concentrations).

Another operation that merits attention in pre-pro-
cessing is what we can group under the term knowhow-
based feature extraction, where new variables will ap-
pear on the database by combining several original
variables in specific ways indicated by the experts. As
said before, explicit creation of new variables that ap-
proach the decisional criteria used by the experts might
provide a powerful set of variables that improve per-
formance of data mining models. This practice often
increases also usability, as providing models in terms
of the reasoning variables used by experts, and ap-



AU
TH

O
R 

 C
O
PY

652 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

proaches better the understanding of the expert and
their trust on the model. As an example, consider a
study on water quality in rivers, where several param-
eters of the water are measured, among them concen-
trations on suspended solids, organic matter, biochemi-
cal organic matter, ammonium, nitrates, phosphor, also
conductivity, temperature, flow, etc. Experts tend to
evaluate the water quality based on some standard-
ized indexes that can be computed and added to the
dataset as new variables. As an example, the ISQA
(used in Spain) is a Simplified Index for Water Quality
is ISQA = T ∗ (COD + SS + DO + C) where T is a
function of the temperature of the water (T = 1 if the
temperature t � 20°C, T = 1 − 0.0125(t − 20) oth-
erwise), COD a function of the chemical oxygen de-
mand (COD = 30 − cod if cod � 10 mg/l, COD = 0
if cod > 60 mg/l, COD = 21 − (0.35 − cod) oth-
erwise), SS a function of the suspended solids (SS =
25 − (0.15 ∗ sst), if sst � 100 mg/l, SS = 0 if
sst > 250 mg/l, SS = 17− (0.07∗sst), otherwise), DO
a function of the dissolved oxygen (DO = 2.5 ∗ do,
if do < 10 mg/l, DO = 25 otherwise), C a func-
tion of the conductivity (C = 15.4 ∗ (3.6 − log c), if
c � 4000 μS/cm, D = 0 otherwise) [219] and, as it
can be seen, ISQA is a non-linear relationship among
these parameters. Introducing it as an additional vari-
able in data mining modeling can improve the perfor-
mance and interpretability of the model. Another ex-
ample is to build the atmospheric clearness index as
the ratio between two other variables that can be di-
rectly measured: the solar radiation and ground level to
extra-terrestrial solar radiation.

Creation of binary indicators: evaluating to TRUE
or FALSE according to a certain Boolean condition ex-
pressed over some subset of observed variables. For
example, a variable Abnormal operation of the biore-
actor in a WWTP, that will evaluate to TRUE if the ef-
fluent of the plant has concentrations of organic matter
over the thresholds permitted by the law or there was
bulking or foaming in the bioreactor. Or lethal radia-
tion that will evaluate to TRUE if the radiation is lethal
in some weeks or lethal in some hours or lethal with
immediate effect.

Another type of operation is related to creation of
new variables by split or concatenation of the original
ones:

• Splitting a variable: It creates a vector of variables
Xk by decomposing a variable Y in parts. As an
example, the EWC [64] is an international Euro-
pean code for encoding waste, which has a format
that reflects the taxonomy in behind. Thus, the se-

mantics of the several groups of digits of the code
correspond to the categories of the waste from
a more general classification to a more specific.
As an example, code 050105 has the following
meaning: first two digits 05 indicates a group of
sources generating similar kinds of waste, in this
case Wastes from petroleum refining, natural gas
purification and pyrolytic treatment of coal. Third
and fourth digits indicate a more specific sector
inside the main source. In the example 01 means
wastes from petroleum refining. Finally, the last
two digits give the specific waste. The code 05
corresponds to oil spills. Thus, the code could be
transformed in three new variables regarding the
three levels of waste categories and one of them
used in the data mining modeling, according to
the level of granularity appropriate for the goals
of the analysis. A date can be as well decom-
posed in three new columns with day, month and
year, which can be used for different purposes,
like filtering all weekend days (day equal to Satur-
day or Sunday) to build specific pollution models
in big cities for weekends, or entering the month
into data mining modeling as a seasonal term.
Of course all these operations are strongly linked
with a good knowledge of the semantics of the
variables and are absolutely domain-dependent.
Dedicated parsers are usually required to this pur-
pose.

• Creating a new variable Y by concatenation of
two (or more) pre-existent variables Xk , k ∈ S,
S ⊆ (1 : K). When Xk are qualitative, Y is in
the space of the Cartesian product of all Xk used.
From a practical point of view, this corresponds to
creating a crossing variable. This is the case, for
example, of having a dataset with volcanic erup-
tions in the rows and, among others, a set of 15
binary variables indicating whereas the eruption
generated or not different kinds of products (liq-
uid lava, glassy lava, lava bombs, chunk of lava,
rubbly lava, lobes of lava, viscous magma, frag-
mented rocks, scoria, tephra, ash, stem, pyroclas-
tic density current, gas). Whereas the 15 variables
become concatenated, a variable indicating the
whole list of products generated in the eruption
will produce values like LiquidLavaLavaBombs-
FragmentedRocksScoriaAshGas, just liquidLava,
or viscousMagmaTephraAshPiroclasticDensity-
Current and the resulting variable can perform
better in classifying the type of eruption than the
15 binary variables. If the Xk are represented



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 653

by numerical codes of d digits or less, an effi-
cient form of obtaining Y is by the transformation
Y = ∏k∈{1:card(S)} X(sk) × dk .

Special attention is required for multivalued vari-
ables, introduced in Section 3.1. From the informa-
tional point of view, the s binary variables, repre-
senting the modalities of the multivalued variable, are
providing information of a single aspect of the object
described in each data row. An example is the variable
technologies available in a city to provide access to
water expressed by means of s = 3 binary variables in
[12]. However, if the multivalued variable is introduced
in the data mining analysis as a set of s independent
binary variables, this artificially increases the dimen-
sionality of the data matrix by dedicating s dimensions
to a single variable. This often biases the analysis. In
the example, it is like if the water access technologies
were more important in the description of the cities
than other variables, like population density or type of
climate, both explicable in a single column each. To
avoid this excess of weight of the multivalued variable,
some possibilities are available: one is to build the con-
catenation of all binary components, as explained be-
fore. The new variable, in the example, will contain
the list of combinations of water access technologies
available in each city. This enables also to analyze the
kind of combinations really used or not and some anal-
ysis on the interactions between water access technolo-
gies. Also, some expert-based feature extraction can be
done to substitute the multivalued variable by a vector
of more powerful variables describing it. In the exam-
ple, a possibility is building the number of water ac-
cess technologies available in the city, and, for exam-
ple, an indicator for the existence of rudimentary sys-
tems or most modern ones, etc. This clearly depends on
the goal of the analysis and must be carefully designed
using specific domain-knowledge.

In Section 9.2.3 the factorial methods have been pre-
sented as a method to reduce data dimensionality. The
retained factors are in fact new variables, which corre-
spond to (usually linear) combinations of the original
variables and can be used instead of them in the data
mining. It is important to ensure that the factors can
be properly interpreted, to get meaningful data mining
models.

Finally, when serial measurements are obtained,
from sensors for example, it is dangerous to introduce
the series itself in the analysis as it is, for the same rea-
sons discussed in the multivalued variables case. Imag-
ine a dataset describing some pieces of land, with their

surface, characteristics of the soil, climate, proxim-
ity to the see, type of culture, agricultural technology
used (including type of fertilizers, pesticides, etc.), ob-
tained production, etc., and including a set of 365 vari-
ables with the daily mean temperatures along a com-
plete year, another with the minimum daily temper-
atures, and another with the maximum temperatures.
Building data mining models to analyze the relation-
ship between production and the other factors, cannot
include directly the 365 temperature variables as they
are. In this case, expert-based feature extraction can be
used to substitute the temperature series to a subset of
meaningful variables describing the series. For exam-
ple, mean temperature in the period of growing the veg-
etable, max temperature in the same period, min tem-
perature in the same period, some synthetic descrip-
tors or indicators, as for example, was the temperature
below 0 degrees in between November and February?.
In [202] it is shown that reducing the series to a sin-
gle mean significantly decreases the quality of the data
mining models.

12. Conclusions and challenges for pre-processing
in environmental data mining

Environmental processes have some intrinsic com-
plexities that make the data analysis difficult and often
the classical data analysis methods do not perform well
in environmental applications. KDD is a good sup-
port to analyze environmental domains, defining a new
paradigm where, apart from the data mining step itself,
pre-processing of data and post-processing of results
are included in the methodology itself, and the prior
expert knowledge is also considered to discover new
useful results.

The main focus of this paper is to provide a general
overview of the pre-processing techniques regarding
its use within Environmental Systems providing guide-
lines to the end user from a practical perspective.

Few works are found on pre-processing in the liter-
ature, but none of them is specific for environmental
data, and none provides methodological recommenda-
tions for the pre-processing process as a whole.

This paper proposes a general pre-processing meth-
odology. Usually, the pre-processing tasks are a mix-
ture of different techniques that have not been ana-
lyzed and structured. Most data scientists use some
pre-processing techniques taken from an unstructured
pool of techniques, but without a reasonable and right
ordering of the pre-processing steps. Sometimes, for-



AU
TH

O
R 

 C
O
PY

654 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

getting to check some data issues gives problems dur-
ing the data mining step and requires costly backtrack-
ing to the pre-processing step thus forcing to rebuild
the whole analysis several times.

In this paper, the most relevant steps involved in
pre-processing with environmental data are identified
and reviewed, and a general framework and methodol-
ogy are proposed for dealing with the pre-processing
from an applied perspective in the Data Mining pro-
cess, considering that pre-processing is highly related
with data quality issues and it is of paramount im-
portance to avoid propagation of errors within the
data mining models and consequent decision-making
based on them. As it can be seen, the pre-processing
step is strongly multidisciplinary, and requires inte-
grated tools from many different origins, including
basic Statistics, multivariate analysis, machine learn-
ing, information management, visualization and GIS,
among others.

The most relevant aspects of each issue in pre-
processing have been reviewed and structured in a way
that the reader can effectively use them to make deci-
sions on how to pre-process a real dataset.

As a result of the state-of-the-art review performed
in this paper, and the efforts in conceptualization of the
different sections, like the one about outliers or miss-
ing data, some guidelines for pre-processing tasks are
synthetized here, as a first approach to provide a global
guide to perform correct KDD processes as a whole.

The main contribution of this paper is to pro-
vide a deep analysis of the steps composing the pre-
processing task. It can be said that, in general, most of
the pre-processing techniques are devoted to: visual-
ization of the data, detection of imperfections or verifi-
cation of the accomplishment of some properties of the
dataset, transformation of data for correcting imperfec-
tions or achieving certain properties for better analy-
sis. This include a number of different operations that
make sense in different scenarios suitable in environ-
mental data mining, which often are related with the
nature of data itself, but also with the goals of the anal-
ysis and the related domain knowledge.

Finally, as a last contribution of the paper, the enor-
mous effort in reviewing the work done in the litera-
ture in pre-processing, allowed to identify the hot is-
sues and challenging aspects in and of the interdisci-
plinary field of environmental DM in coming years.
The achievement of some specific goals, non-well es-
tablished yet, at the level of both end-users and devel-
opers or researchers would increase the benefits of a
proper pre-processing process and improve the quality
of the further data mining modeling step.

At the level of end-users the following issues are im-
portant:

• Get acquainted with available methods and tech-
nical assumptions of pre-processing methods,
even before data collection, if possible.

• Try to clarify the relevant questions to be an-
swered by the KDD process to orient an adequate
Data Mining. Do not lose perspective of final use-
fulness.

• Consider all data available which might be rel-
evant for the KDD goal, independently of their
original format (images, text, etc.) and use data
fusion to get the initial data matrix. Try to min-
imize the missing data collected by properly
designing the structural missing data manage-
ment. Consider the collection of qualitative vari-
ables without internal tedious numerical encod-
ings when possible.

• Collaboration between the expert and the data
miner becomes crucial in most of the decisions
made during the pre-processing step.

• Be careful and rigorous on meta-data collection. It
is important to store it in an available format and
to clearly understand it for proper analysis deci-
sions, and proper data and results interpretation.

• Reserve a relevant time in your projects for data
pre-processing and devote the required time to it,
by performing it carefully enough to guarantee
the quality of the data mining models results.

• Carefully verify that missing data is properly im-
puted in the pre-processing tool, and transfer to
the system as much meta-information as possible.

• Select relevant data for the working data matrix
before starting the pre-processing analysis itself.
To this purpose, KDD goals, and priori specific
domain knowledge has to be taken into account.
In case of doubt, it is preferable to keep a variable
and make decisions at modeling step.

• Use visualization to decide which pre-processing
aspects need to be treated.

• Try to change your data as little as possible; make
the minimum pre-processing operations required
to improve data quality and guarantee better in-
terpretability of the final data mining results. Try
to avoid, as much as possible, transformations
that generate variables without a meaning that can
make data mining models understandable. Instead
of adapting data to the data mining method tech-
nical requirements, try to find a method that fits
well with the intrinsic structure of your dataset.



AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 655

• However, spend all required time in terminology
standardization.

• Pay special attention to imbalanced datasets, com-
positional data, and multivalued variables. Con-
sider the available possibilities to deal with data
noise and uncertainty.

• Whenever possible, include in the dataset syn-
thetic variables approaching the decisional indi-
cators used by experts to reason.

• Use both instance selection and/or feature selec-
tion to detect redundant or irrelevant variables or
instances, reducing the dimension of the dataset
by keeping the quality of the data mined models.

• When appropriate, use feature weighting to esti-
mate the relevance of the variables of the data ma-
trix, and make further decisions accordingly.

• Try resampling to check stability of results and
to process too big datasets with a particular lim-
ited method. Use the classical statistical sampling
principles, traditionally used to get samples from
real populations, to sample over a big dataset.

• Ensure that conclusions are well framed into the
subpopulations determined by the pre-processing
step really analyzed.

• Always report in a transparent way all pre-pro-
cessing assumptions done, and all pre-processing
decisions made, particularly those related to elim-
ination of some rows or variables, or modifica-
tions owing to error correction.

At the level of researchers, the following issues
merit serious attention in the near future, in order to ad-
vance the state of the art in pre-processing in the chal-
lenging directions that will make it easier, more pow-
erful and reliable in real applications:

• The development of meta-data management sup-
port tools that permit to standardize the meta-
data representation models, and the way they can
be used in the pre-processing is of main inter-
est, since there is a lack of standards and all the
responsibility relies nowadays to the end user,
this increasing the risks of disregarding impor-
tant details. Also, to develop mechanisms to in-
troduce these standards into the data mining sys-
tem is relevant, since currently only basic issues
like declaring qualitative variables is available in
a reduced number of data mining software tools.
This will permit automation of some decisions,
increasing efficiency of the pre-processing step.

• Elaborate protocols to enhance data sharing and
reuse, particularly oriented to build data matrices

to be mined, by combining different data sources
from different formats. In particular, the potential
of web/text mining needs to be further explored.

• There is a great need of developing methods for
data fusion able to deal simultaneously with data
coming from different sources, with different na-
tures, scales, granularities and formats. Particular
attention on getting data from smart sensors, im-
ages or web documents is suggested.

• Particular attention is required to explore the chal-
lenges for properly pre-processing incremental
data and also the specific considerations required
for pre-processing in big data.

• More research is still required to better under-
stand the complexity of pre-processing tasks in
the context of KDD processes. The methodolog-
ical proposal presented in this paper is a first
attempt to systematize the pre-processing pro-
cess, which can be used to guide the flow of
pre-processing operation in data mining software
tools. However, additional research is required to
stabilize a complete and exhaustive methodology
for pre-processing in environmental systems.

• As it has been extensively illustrated along the pa-
per, it is very relevant to take into account the ex-
pertise of the end-user as well as the specific do-
main knowledge to guide many decisions and op-
erations required in the pre-processing step. For
this reason, a methodology that considers the in-
volvement of both prior domain knowledge and
interaction with the expert is essential for the
progress in the pre-processing field.

• As many Data Mining softwares provide GUIs
to design the workflow for a complete KDD pro-
cess, collect KDD experiences in a workflow li-
brary and develop Data Mining strategies to mine
the workflows themselves for improving the pre-
processing, data mining and post-processing rec-
ommenders under an evidence-based approach
are much needed.

• Development of standard procedures (bench-
marks) for experimental testing and validation of
new pre-processing tools.

• There is a lack of tools for explicit representation
and handling of discovered knowledge for greater
understandability. Development of tools to bridge
the gap between modeling and effective decision-
making [82,85–87] can be enormously useful at
all levels of data mining models, to guarantee the
effective impact of the data mining step at an en-
vironmental decision-making level. As in ESs it is



AU
TH

O
R 

 C
O
PY

656 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

usual to meet decision-makers with no technical
skills, development of this kind of tools is more
needed than in other more technical fields.

• Finally, there is a need of moving towards the de-
velopment of integrated KDD-systems, with suf-
ficient intelligence to pursue integral approaches,
covering the whole KDD process, from problem
formulation to discovered knowledge production,
including interpretation of results. This requires
KDD systems including data fusion, meta-data
management, data cleaning and the whole range
of pre-processing techniques, taking into account
prior expert knowledge, data mining method rec-
ommendation, and post-processing, and involving
the expert in both the problem formulation, pref-
erences on kind of results and interpretation.

Regarding the support provided to pre-processing
by the various available KDD software packages, we
can say that, although they often include some avail-
able tools to perform some pre-processing steps, cur-
rent packages seem still to be far from being such a
complete intelligent system that could support both a
well systematized pre-processing and an integral KDD,
as stated above. The complete pre-processing process
is normally designed from scratch for every applica-
tion and only lists of unconnected techniques for pre-
processing are provided in most softwares. We strongly
believe that efforts should be made in the near future in
this direction to provide a general framework for pre-
processing the data in a reliable way, and any advance
on the state of the art in this direction will have ben-
efits not only in the quality of results in data mining
projects, but also in the time required for them.

References

[1] C.C. Aggarwal, Supervised Outlier Detection, Arfken and
Weber, 2012.

[2] C.C. Aggarwal, Outlier analysis, in: Data Mining, Springer,
2015, pp. 237–263.

[3] D.W. Aha, Feature weighting for lazy learning algorithms,
in: Feature Extraction, Construction and Selection, Springer,
1998, pp. 13–32. doi:10.1007/978-1-4615-5725-8_2.

[4] D.W. Aha, D. Kibler and M.K. Albert, Instance-based learn-
ing algorithms, Machine Learning 6(1) (1991), 37–66.

[5] J. Aitchison, in: Principles of Compositional Data Analysis,
Lecture Notes – Monograph Series, 1994, pp. 73–81.

[6] A. Alexandridis, P. Patrinos, H. Sarimveis and G. Tsekouras,
A two-stage evolutionary algorithm for variable selection in
the development of RBF neural network models, Chemomet-
rics and Intelligent Laboratory Systems 75(2) (2005), 149–
162. doi:10.1016/j.chemolab.2004.06.004.

[7] P.D. Allison, Multiple imputation for missing data: A cau-
tionary tale, 1999.

[8] P.D. Allison, Missing Data, Vol. 136, Sage Publications,
2001.

[9] G. Andrienko and A. Andrienko, Research on visual analysis
of spatio-temporal data at fraunhofer ais: An overview of his-
tory and functionality of commongis, in: Proceedings of the
Knowledge-Based Services for the Public Services Sympo-
sium, Workshop III: Knowledge Discovery for Environmental
Management, 2004, pp. 26–31.

[10] A. Arauzo-Azofra, J.L. Aznarte and J.M. Benítez, Em-
pirical study of feature selection methods based on indi-
vidual feature evaluation for classification problems, Ex-
pert Systems with Applications 38(7) (2011), 8170–8177.
doi:10.1016/j.eswa.2010.12.160.

[11] A. Arauzo-Azofra, J.M. Benitez and J.L. Castro, Consis-
tency measures for feature selection, Journal of Intelligent
Information Systems 30(3) (2008), 273–292. doi:10.1007/
s10844-007-0037-0.

[12] I. Arregui, A. Balaguer et al., Learning on the relationships
between respiratory disease and the use of traditional stoves
in Bangladesh households, in: Procs IEMSs’2016, Vol. 3,
2016.

[13] I.N. Athanasiadis, V.G. Kaburlasos, P.A. Mitkas and
V. Petridis, Applying machine learning techniques on air
quality data for real-time decision support, in: First Interna-
tional NAISO Symposium on Information Technologies in En-
vironmental Engineering (ITEE-2003), Gdansk, Poland, Cite-
seer, 2003.

[14] J. Atserias et al., Syntactic and semantic services in an open-
source NLP library, in: Procs LREC, Vol. 6, 2006.

[15] M.J. Azur, E.A. Stuart, C. Frangakis and P.J. Leaf, Multiple
imputation by chained equations: What is it and how does it
work?, International Journal of Methods in Psychiatric Re-
search 20(1) (2011), 40–49. doi:10.1002/mpr.329.

[16] A. Bargiela and W. Pedrycz, Granular Computing: An In-
troduction, Vol. 717, Springer Science & Business, Media,
2012.

[17] V. Barnett, V. Barnett and T. Lewis, Outliers in Statistical
Data, Wiley, 1978.

[18] G.E. Batista, M.C. Monard et al., A study of k-nearest neigh-
bour as an imputation method, HIS 87(251–260) (2002), 48.

[19] G.E. Batista, R.C. Prati and M.C. Monard, A study of the
behavior of several methods for balancing machine learn-
ing training data, ACM Sigkdd Explorations Newsletter 6(1)
(2004), 20–29. doi:10.1145/1007730.1007735.

[20] B. Bazartseren, G. Hildebrandt and K.-P. Holz, Short-term
water level prediction using neural networks and neuro-
fuzzy approach, Neurocomputing 55(3) (2003), 439–450.
doi:10.1016/S0925-2312(03)00388-6.

[21] R. Bekkerman, R. El-Yaniv, N. Tishby and Y. Winter, Distri-
butional word clusters vs. words for text categorization, Jour-
nal of Machine Learning Research 3 (2003), 1183–1208.

[22] O.B. Akpor and M. Muchie, Bioremediation of polluted
wastewater influent: Phosphorus and nitrogen removal, Sci-
entific Research and Essays 5(21) (2010), 3222–3230.

[23] D.A. Belsley, E. Kuh and R.E. Welsch, Regression Diagnos-
tics: Identifying Influential Data and Sources of Collinearity,
Vol. 571, John Wiley & Sons, 2005.

[24] R. Berg, Hurricane ike (al092008) 1–14 September 2008, Na-
tional Hurricane Center Tropical Cyclone Rep, 2009.

http://dx.doi.org/10.1007/978-1-4615-5725-8_2
http://dx.doi.org/10.1016/j.chemolab.2004.06.004
http://dx.doi.org/10.1016/j.eswa.2010.12.160
http://dx.doi.org/10.1007/s10844-007-0037-0
http://dx.doi.org/10.1007/s10844-007-0037-0
http://dx.doi.org/10.1002/mpr.329
http://dx.doi.org/10.1145/1007730.1007735
http://dx.doi.org/10.1016/S0925-2312(03)00388-6


AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 657

[25] J.C. Bezdek and L.I. Kuncheva, Nearest prototype classi-
fier designs: An experimental study, International Journal of
Intelligent Systems 16(12) (2001), 1445–1473. doi:10.1002/
int.1068.

[26] A. Bifet, G. Holmes, B. Pfahringer, R. Kirkby and
R. Gavaldà, New ensemble methods for evolving data
streams, in: Proceedings of the 15th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining,
ACM, 2009, pp. 139–148. doi:10.1145/1557019.1557041.

[27] R. Bischof, L.E. Loe, E.L. Meisingset, B. Zimmermann,
B. Van Moorter and A. Mysterud, A migratory northern un-
gulate in the pursuit of spring: Jumping or surfing the green
wave?, The American Naturalist 180(4) (2012), 407–424.
doi:10.1086/667590.

[28] J.D. Blower, K. Haines, A. Santokhee and C.L. Liu, Godiva2:
Interactive visualization of environmental data on the web,
Philosophical Transactions of the Royal Society of London A:
Mathematical, Physical and Engineering Sciences 367(1890)
(2009), 1035–1039. doi:10.1098/rsta.2008.0180.

[29] V. Bolón-Canedo, N. Sánchez-Maroño and A. Alonso-
Betanzos, A review of feature selection methods on synthetic
data, Knowledge and Information Systems 34(3) (2013), 483–
519. doi:10.1007/s10115-012-0487-8.

[30] G.E. Box and D.R. Cox, An analysis of transformations, Jour-
nal of the Royal Statistical Society, Series B (Methodological)
26(2) (1964), 211–252.

[31] G. Bretana, Admiralty Manual of Navigation, Vol. 1, 1987.
[32] M.M. Breunig, H.-P. Kriegel, R.T. Ng and J. Sander, Lof:

Identifying density-based local outliers, in: ACM Sigmod
Record, Vol. 29, ACM, 2000, pp. 93–104.

[33] H. Brighton and C. Mellish, Advances in instance selec-
tion for instance-based learning algorithms, Data Mining and
Knowledge Discovery 6(2) (2002), 153–172. doi:10.1023/
A:1014043630878.

[34] G. Brown, A. Pocock, M.-J. Zhao and M. Luján, Conditional
likelihood maximisation: A unifying framework for informa-
tion theoretic feature selection, Journal of Machine Learning
Research 13 (2012), 27–66.

[35] I. Bruha and A. Famili, Postprocessing in machine learning
and data mining, ACM SIGKDD Explorations Newsletter 2(2)
(2000), 110–114. doi:10.1145/380995.381059.

[36] A. Butler and C. Glasbey, A latent Gaussian model for com-
positional data with zeros, Journal of the Royal Statistical
Society: Series C (Applied Statistics) 57(5) (2008), 505–520.
doi:10.1111/j.1467-9876.2008.00627.x.

[37] Y. Caises, A. González, E. Leyva and R. Pérez, Scis: Com-
bining instance selection methods to increase their effective-
ness over a wide range of domains, in: International Confer-
ence on Intelligent Data Engineering and Automated Learn-
ing, Springer, 2009, pp. 17–24.

[38] J.R. Cano, F. Herrera and M. Lozano, Using evolu-
tionary algorithms as instance selection for data reduc-
tion in KDD: An experimental study, IEEE Transac-
tions on Evolutionary Computation 7(6) (2003), 561–575.
doi:10.1109/TEVC.2003.819265.

[39] R. Caruana and V.R. de Sa, Benefiting from the variables that
variable selection discards, Journal of Machine Learning Re-
search 3 (2003), 1245–1264.

[40] V. Cerveron and F.J. Ferri, Another move toward the mini-
mum consistent subset: A tabu search approach to the con-
densed nearest neighbor rule, IEEE Transactions on Systems,

Man, and Cybernetics, Part B (Cybernetics) 31(3) (2001),
408–413. doi:10.1109/3477.931531.

[41] J.M. Chambers, Graphical Methods for Data Analysis,
Wadsworth, 1983.

[42] V. Chandola, A. Banerjee and V. Kumar, Anomaly detection:
A survey, ACM Computing Surveys (CSUR) 41(3) (2009), 15.
doi:10.1145/1541880.1541882.

[43] G. Chandrashekar and F. Sahin, A survey on feature selection
methods, Computers & Electrical Engineering 40(1) (2014),
16–28. doi:10.1016/j.compeleceng.2013.11.024.

[44] N.V. Chawla, K.W. Bowyer, L.O. Hall and W.P. Kegelmeyer,
Smote: Synthetic minority over-sampling technique, Journal
of Artificial Intelligence Research 16 (2002), 321–357.

[45] C.-H. Chou, B.-H. Kuo and F. Chang, The general-
ized condensed nearest neighbor rule as a data reduc-
tion method, in: 18th International Conference on Pattern
Recognition (ICPR’06), Vol. 2, IEEE, 2006, pp. 556–559.
doi:10.1109/ICPR.2006.1119.

[46] K. Cios, W. Pedrycz, R.W. Swiniarski and L.A. Kurgan, Data
Mining: A Knowledge Discovery Approach, Springer, 2007.

[47] N. Collier, Uncovering text mining: A survey of current work
on web-based epidemic intelligence, Global Public Health
7(7) (2012), 731–749. doi:10.1080/17441692.2012.699975.

[48] R.D. Cook, Influential observations in linear regression, Jour-
nal of the American Statistical Association 74(365) (1979),
169–174. doi:10.1080/01621459.1979.10481634.

[49] P. Cortez and M.J. Embrechts, Using sensitivity analysis and
visualization techniques to open black box data mining mod-
els, Information Sciences 225 (2013), 1–17. doi:10.1016/
j.ins.2012.10.039.

[50] R.H. Creecy, B.M. Masand, S.J. Smith and D.L. Waltz, Trad-
ing MIPS and memory for knowledge engineering, Com-
munications of the ACM 35(8) (1992), 48–64. doi:10.1145/
135226.135228.

[51] H.-J. Dai, C.-H. Huang, J.Y.-W. Lin, P.-H. Chou, R.T.-H. Tsai
and W.-L. Hsu, A survey of state of the art biomedical
text mining techniques for semantic analysis, in: Sensor
Networks, Ubiquitous and Trustworthy Computing, 2008.
SUTC’08. IEEE International Conference on, IEEE, 2008,
pp. 410–417. doi:10.1109/SUTC.2008.86.

[52] S.K. Das, High-Level Data Fusion, Artech House, 2008.

[53] M. Dash and H. Liu, Handling large unsupervised data via
dimensionality reduction, in: 1999 ACM SIGMOD Workshop
on Research Issues in Data Mining and Knowledge Discov-
ery, 1999.

[54] M. Dash and H. Liu, Consistency-based search in fea-
ture selection, Artificial Intelligence 151(1) (2003), 155–176.
doi:10.1016/S0004-3702(03)00079-1.

[55] J.L. Davidson and J. Jalan, Feature selection for steganal-
ysis using the mahalanobis distance, in: IS&T/SPIE Elec-
tronic Imaging, International Society for Optics and Photon-
ics, 2010, pp. 754104.

[56] J. Derrac, S. García and F. Herrera, A survey on evolu-
tionary instance selection and generation, Int’l J. Applied
Metaheuristic Computing 1(1) (2010), 60–92. doi:10.4018/
jamc.2010102604.

[57] P.A. Devijver and J. Kittler, On the edited nearest neighbor
rule, in: Proc. 5th Int. Conf. on Pattern Recognition, 1980,
pp. 72–80.

http://dx.doi.org/10.1002/int.1068
http://dx.doi.org/10.1002/int.1068
http://dx.doi.org/10.1145/1557019.1557041
http://dx.doi.org/10.1086/667590
http://dx.doi.org/10.1098/rsta.2008.0180
http://dx.doi.org/10.1007/s10115-012-0487-8
http://dx.doi.org/10.1023/A:1014043630878
http://dx.doi.org/10.1023/A:1014043630878
http://dx.doi.org/10.1145/380995.381059
http://dx.doi.org/10.1111/j.1467-9876.2008.00627.x
http://dx.doi.org/10.1109/TEVC.2003.819265
http://dx.doi.org/10.1109/3477.931531
http://dx.doi.org/10.1145/1541880.1541882
http://dx.doi.org/10.1016/j.compeleceng.2013.11.024
http://dx.doi.org/10.1109/ICPR.2006.1119
http://dx.doi.org/10.1080/17441692.2012.699975
http://dx.doi.org/10.1080/01621459.1979.10481634
http://dx.doi.org/10.1016/j.ins.2012.10.039
http://dx.doi.org/10.1016/j.ins.2012.10.039
http://dx.doi.org/10.1145/135226.135228
http://dx.doi.org/10.1145/135226.135228
http://dx.doi.org/10.1109/SUTC.2008.86
http://dx.doi.org/10.1016/S0004-3702(03)00079-1
http://dx.doi.org/10.4018/jamc.2010102604
http://dx.doi.org/10.4018/jamc.2010102604


AU
TH

O
R 

 C
O
PY

658 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

[58] T.G. Dietterich, Ensemble methods in machine learning,
in: International Workshop on Multiple Classifier Systems,
Springer, 2000, pp. 1–15. doi:10.1007/3-540-45014-9_1.

[59] Y. Dong and C.-Y.J. Peng, Principled missing data meth-
ods for researchers, SpringerPlus 2(1) (2013), 1. doi:10.1186/
2193-1801-2-1.

[60] B. Duval, J.-K. Hao and J.C. Hernandez Hernandez,
A memetic algorithm for gene selection and molecular classi-
fication of cancer, in: Proceedings of the 11th Annual Confer-
ence on Genetic and Evolutionary Computation, ACM, 2009,
pp. 201–208.

[61] J.G. Dy and C.E. Brodley, Feature selection for unsupervised
learning, Journal of Machine Learning Research 5 (2004),
845–889.

[62] M. Edwards, N. Ferrand, F. Goreaud and S. Huet, The rele-
vance of aggregating a water consumption model cannot be
disconnected from the choice of information available on the
resource, Simulation Modelling Practice and Theory 13(4)
(2005), 287–307. doi:10.1016/j.simpat.2004.11.008.

[63] EEML, http://www.eeml.org, 2008.
[64] EPA, European Waste Catalogue and Hazardous Waste List,

European Environmental Protection Agency, 2002.
[65] F. Famili, W.-M. Shen, R. Weber and E. Simoudis, Data pre-

processing and intelligent data analysis, International Jour-
nal on Intelligent Data Analysis 1(1) (1997).

[66] D.W. Fanning, IDL Programming Techniques, Fanning soft-
ware consulting, 2000.

[67] A. Farhangfar, L. Kurgan and J. Dy, Impact of imputation
of missing values on classification error for discrete data,
Pattern Recognition 41(12) (2008), 3692–3705. doi:10.1016/
j.patcog.2008.05.019.

[68] R.M. Faye, S. Sawadogo, C. Lishou and F. Mora-Camino,
Long-term fuzzy management of water resource systems, Ap-
plied Mathematics and Computation 137(2) (2003), 459–475.
doi:10.1016/S0096-3003(02)00151-0.

[69] U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth and R. Uthu-
rusamy, Advances in Knowledge Discovery and Data Mining,
Vol. 21, AAAI Press, Menlo Park, 1996.

[70] A. Fernández, S. García, M.J. del Jesus and F. Herrera,
A study of the behaviour of linguistic fuzzy rule based clas-
sification systems in the framework of imbalanced data-
sets, Fuzzy Sets and Systems 159(18) (2008), 2378–2398.
doi:10.1016/j.fss.2007.12.023.

[71] W. Förstner, Image preprocessing for feature extraction
in digital intensity, color and range images, in: Geomatic
Method for the Analysis of Data in the Earth Sciences,
Springer, 2000, pp. 165–189. doi:10.1007/3-540-45597-
3_4.

[72] P.G. Foschi, D. Kolippakkam, H. Liu and A. Mandvikar, Fea-
ture extraction for image mining, in: Multimedia Information
Systems, 2002, pp. 103–109.

[73] E. Frank and I.H. Witten, Making better use of global dis-
cretization, in: Proc. of the Sixteenth International Confer-
ence on Machine Learning, 1999.

[74] H. Gao, G. Barbier, R. Goolsby and D. Zeng, Harnessing the
crowdsourcing power of social media for disaster relief, Tech-
nical report, DTIC Document, 2011.

[75] U. Garain, Prototype reduction using an artificial immune
model, Pattern Analysis and Applications 11(3–4) (2008),
353–363. doi:10.1007/s10044-008-0106-1.

[76] M. Garcia, E. López, V. Kumar and A. Valls, A multicri-
teria fuzzy decision system to sort contaminated soils, in:
International Conference on Modeling Decisions for Artifi-
cial Intelligence, Springer, 2006, pp. 105–116. doi:10.1007/
11681960_12.

[77] S. García, J.R. Cano and F. Herrera, A memetic algorithm
for evolutionary prototype selection: A scaling up approach,
Pattern Recognition 41(8) (2008), 2693–2709. doi:10.1016/
j.patcog.2008.02.006.

[78] S. Garcia, J. Derrac, J. Cano and F. Herrera, Prototype selec-
tion for nearest neighbor classification: Taxonomy and em-
pirical study, IEEE Transactions on Pattern Analysis and
Machine Intelligence 34(3) (2012), 417–435. doi:10.1109/
TPAMI.2011.142.

[79] N. García-Pedrajas, Evolutionary computation for training set
selection, Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery 1(6) (2011), 512–523.

[80] M.H. Gerardi, Wastewater Bacteria, Vol. 5, John Wiley &
Sons, 2006.

[81] K. Gibert, Mixed intelligent-multivariate missing imputa-
tion, International Journal of Computer Mathematics 91(1)
(2014), 85–96. doi:10.1080/00207160.2013.783209.

[82] K. Gibert and D. Conti, atlp: A color-based model of uncer-
tainty to evaluate the risk of decisions based on prototypes,
AI Communications 28(1) (2015), 113–126.

[83] K. Gibert and D. Conti, On the understanding of pro-
files by means of post-processing techniques: An applica-
tion to financial assets, International Journal of Computer
Mathematics 93(5) (2016), 807–820. doi:10.1080/00207160.
2014.898065.

[84] K. Gibert and U. Cortés, Clustering based on rules and
knowledge discovery in ill-structured domains, Revista Com-
putación y Sistemas 1(4) (1998), 213–227.

[85] K. Gibert, A. García-Rudolph and G. Rodríguez-Silva, The
role of KDD support-interpretation tools in the conceptual-
ization of medical profiles: An application to neurorehabilita-
tion, Acta Informatica Medica 16(4) (2008), 178.

[86] K. Gibert, J. Izquierdo, G. Holmes, I. Athanasiadis, J. Co-
mas and M. Sànchez-Marrè, On the role of pre and post-
processing in environmental data mining, in: Proceedings of
IEMSs 2008 International Congress on Environmental Mod-
eling and Software, iEMSs, 2008, pp. 1937–1958.

[87] K. Gibert, G. Rodríguez-Silva and R. Annicchiarico, Post-
processing: Bridging the gap between modelling and ef-
fective decision-support. The profile assessment grid in hu-
man behaviour, Mathematical and Computer Modelling 57(7)
(2013), 1633–1639. doi:10.1016/j.mcm.2011.10.046.

[88] K. Gibert, G. Rodríguez-Silva and I. Rodríguez-Roda,
Knowledge discovery with clustering based on rules by states:
A water treatment application, Environmental Modelling &
Software 25(6) (2010), 712–723. doi:10.1016/j.envsoft.2009.
11.004.

[89] K. Gibert and M. Sànchez-Marrè, Improving ontological
knowledge with reinforcement in recommending the data
mining method for real problems, in: Procs of CAEPIA 2015
(TAMIDA), CEDI, 2015, pp. 769–778.

[90] K. Gibert, M. Sènchez-Marrè and V. Codina, Choosing the
right data mining technique: Classification of methods and
intelligent recommendation, in: Proceedings of IEMSs 2010
International Congress on Environmental Modeling and Soft-
ware, iEMSs, 2010, pp. 2448–2453.

http://dx.doi.org/10.1007/3-540-45014-9_1
http://dx.doi.org/10.1186/2193-1801-2-1
http://dx.doi.org/10.1186/2193-1801-2-1
http://dx.doi.org/10.1016/j.simpat.2004.11.008
http://www.eeml.org
http://dx.doi.org/10.1016/j.patcog.2008.05.019
http://dx.doi.org/10.1016/j.patcog.2008.05.019
http://dx.doi.org/10.1016/S0096-3003(02)00151-0
http://dx.doi.org/10.1016/j.fss.2007.12.023
http://dx.doi.org/10.1007/3-540-45597-3_4
http://dx.doi.org/10.1007/3-540-45597-3_4
http://dx.doi.org/10.1007/s10044-008-0106-1
http://dx.doi.org/10.1007/11681960_12
http://dx.doi.org/10.1007/11681960_12
http://dx.doi.org/10.1016/j.patcog.2008.02.006
http://dx.doi.org/10.1016/j.patcog.2008.02.006
http://dx.doi.org/10.1109/TPAMI.2011.142
http://dx.doi.org/10.1109/TPAMI.2011.142
http://dx.doi.org/10.1080/00207160.2013.783209
http://dx.doi.org/10.1080/00207160.2014.898065
http://dx.doi.org/10.1080/00207160.2014.898065
http://dx.doi.org/10.1016/j.mcm.2011.10.046
http://dx.doi.org/10.1016/j.envsoft.2009.11.004
http://dx.doi.org/10.1016/j.envsoft.2009.11.004


AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 659

[91] K. Gibert and Z. Sonicki, Classification based on rules and
medical research, Journal of Applied Stochastic Models and
Data Analysis (JAMSDA) 15(3) (1999), 319–324.

[92] K. Gibert, J. Spate, M. Sànchez-Marrè, I.N. Athanasiadis and
J. Comas, Chapter twelve data mining for environmental sys-
tems, Developments in Integrated Environmental Assessment
3 (2008), 205–228. doi:10.1016/S1574-101X(08)00612-1.

[93] E. Golobardes, X. Llora, J.M. Garrell, D. Vernet and
J. Bacardit, Genetic classifier system as a heuristic weight-
ing method for a case-based classifier system, Butlletı de
l’Associació Catalana d’Intel. ligencia Artificial 22 (2000),
132–141.

[94] P.I. Good and P. Good, Resampling Methods: A Practical
Guide to Data Analysis, Springer Science & Business, Media,
2013.

[95] J.W. Graham, Missing data analysis: Making it work in the
real world, Annual Review of Psychology 60 (2009), 549–576.
doi:10.1146/annurev.psych.58.110405.085530.

[96] J.W. Graham, P.E. Cumsille and E. Elek-Fisk, Methods for
handling missing data, in: Handbook of Psychology, 2003.

[97] J.W. Grzymala-Busse, L.K. Goodwin and X. Zhang, In-
creasing sensitivity of preterm birth by changing rule
strengths, Pattern Recognition Letters 24(6) (2003), 903–910.
doi:10.1016/S0167-8655(02)00202-7.

[98] H. Guo and H.L. Viktor, Learning from imbalanced data
sets with boosting and data generation: The databoost-im ap-
proach, ACM SIGKDD Explorations Newsletter 6(1) (2004),
30–39. doi:10.1145/1007730.1007736.

[99] J.F. Hair, Multivariate Data Analysis, Kennesaw State Uni-
versity, 2009.

[100] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann
and I.H. Witten, The WEKA data mining software: An up-
date, ACM SIGKDD Explorations Newsletter 11(1) (2009),
10–18. doi:10.1145/1656274.1656278.

[101] M. Hall, I. Witten and E. Frank, Data Mining: Practical Ma-
chine Learning Tools and Techniques, Kaufmann, Burlington,
2011.

[102] M.A. Hall, Correlation-based feature selection for discrete
and numeric class machine learning, in: Proceedings of the
Seventeenth International Conference on Machine Learning,
Morgan Kaufmann Publishers Inc., 2000, pp. 359–366.

[103] M.A. Hall and L.A. Smith, Practical Feature Subset Selection
for Machine Learning, Springer, 1998.

[104] J. Han and M. Kamber, Data Mining: Concepts and Tech-
niques, Morgan Kaufmann, 2006.

[105] P. Hanrahan, Tableau software white paper-visual thinking
for business intelligence, in: Tableau Software, Seattle, WA,
2003.

[106] P. Hart, The condensed nearest neighbor rule, in: IEEE Trans.
Inform. Theory (Corresp.), Vol. IT-14, 1968, pp. 515–516.

[107] M. Herrera, J. Izquierdo, R. Pérez-García and I. Montalvo,
Multi-agent adaptive boosting on semi-supervised water sup-
ply clusters, Advances in Engineering Software 50 (2012),
131–136. doi:10.1016/j.advengsoft.2012.02.005.

[108] G.E. Hinton and R.R. Salakhutdinov, Reducing the dimen-
sionality of data with neural networks, Science 313(5786)
(2006), 504–507. doi:10.1126/science.1127647.

[109] F. Honghai, C. Guoshun, Y. Cheng, Y. Bingru and C. Yumei,
A SVM regression based approach to filling in missing val-
ues, in: International Conference on Knowledge-Based and

Intelligent Information and Engineering Systems, Springer,
2005, pp. 581–587. doi:10.1007/11553939_83.

[110] N. Howe and C. Cardie, Examining locally varying weights
for nearest neighbor algorithms, in: International Conference
on Case-Based Reasoning, Springer, 1997, pp. 455–466.

[111] N. Howe and C. Cardie, Feature subset selection and order
identification for unsupervised learning, in: Proceedings of
17th International Conference on Machine Learning, Morgan
Kaufmann, 2000.

[112] M.Y. Huh, Incremental subset selection for complex data, in:
Proceedings, COMPSTAT2006, Rome, Italy, 2006.

[113] L. Ingsrisawang and D. Potawee, Multiple imputation for
missing data in repeated measurements using MCMC and
copulas, in: Proceedings of the International MultiConfer-
ence of Engineers and Computer Scientists (IMECs), Hong
Kong, 2012.

[114] N. Ishii and Y. Wang, Learning feature weights for similarity
using genetic algorithms, in: Intelligence and Systems, 1998.
Proceedings, IEEE International Joint Symposia on, IEEE,
1998, pp. 27–33.

[115] T. Ishiwata, M. Muroi, T. Harada, H. Nakajima and I. Oga-
sawara, Establishing an environmental data platform for pro-
moting coastal zone environmental management, The Inter-
national Archives of the Photogrammetry, Remote Sensing
and Spatial Information Sciences XXXVII (2008), 25–30.

[116] ISO/TC211, Iso19115 Geographic Information – Metadata,
2003.

[117] ISO/TC211, Iso19136 geographic information – Geomatics,
2007.

[118] J. Izquierdo, P. López, F. Martínez and R. Pérez, Fault detec-
tion in water supply systems using hybrid (theory and data-
driven) modelling, Mathematical and Computer Modelling
46(3) (2007), 341–350. doi:10.1016/j.mcm.2006.11.013.

[119] J. Izquierdo, R. Pérez, P. López and P. Iglesias, Neural iden-
tification of fuzzy anomalies in pressurized water systems,
in: Proceedings of the 3rd Biennial Meeting of the Interna-
tional Environmental Modeling and Software Society, iEMSs,
Burlington, VT, USA, 2006.

[120] N. Jankowski and M. Grochowski, Comparison of instances
selection algorithms. I. Algorithms survey, in: International
Conference on Artificial Intelligence and Soft Computing,
Springer, 2004, pp. 598–603.

[121] N. Japkowicz and S. Stephen, The class imbalance problem:
A systematic study, Intelligent Data Analysis 6(5) (2002),
429–449.

[122] K. Javed, H.A. Babri and M. Saeed, Feature selection based
on class-dependent densities for high-dimensional binary
data, IEEE Transactions on Knowledge and Data Engineer-
ing 24(3) (2012), 465–477. doi:10.1109/TKDE.2010.263.

[123] A. Jiménez and A. Pérez-Foguet, Improving water access in-
dicators in developing countries: A proposal using water point
mapping methodology, Water Science and Technology: Water
Supply 8(3) (2008), 279–287.

[124] G.H. John and P. Langley, Static versus dynamic sampling for
data mining, in: KDD, Vol. 96, 1996, pp. 367–370.

[125] D. Jouan-Rimbaud, D.-L. Massart, R. Leardi and O.E. De No-
ord, Genetic algorithms as a tool for wavelength selection in
multivariate calibration, Analytical Chemistry 67(23) (1995),
4295–4301. doi:10.1021/ac00119a015.

[126] C.-F. Juang, Temporal problems solved by dynamic fuzzy
network based on genetic algorithm with variable-length

http://dx.doi.org/10.1016/S1574-101X(08)00612-1
http://dx.doi.org/10.1146/annurev.psych.58.110405.085530
http://dx.doi.org/10.1016/S0167-8655(02)00202-7
http://dx.doi.org/10.1145/1007730.1007736
http://dx.doi.org/10.1145/1656274.1656278
http://dx.doi.org/10.1016/j.advengsoft.2012.02.005
http://dx.doi.org/10.1126/science.1127647
http://dx.doi.org/10.1007/11553939_83
http://dx.doi.org/10.1016/j.mcm.2006.11.013
http://dx.doi.org/10.1109/TKDE.2010.263
http://dx.doi.org/10.1021/ac00119a015


AU
TH

O
R 

 C
O
PY

660 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

chromosomes, Fuzzy Sets and Systems 142(2) (2004), 199–
219. doi:10.1016/S0165-0114(03)00119-2.

[127] M. Juhola and J. Laurikkala, Missing values: How many can
they be to preserve classification reliability?, Artificial Intel-
ligence Review 40(3) (2013), 231–245. doi:10.1007/s10462-
011-9282-2.

[128] KDNuggets, Data preparation, Website, 2003.
[129] H. Kim, G.H. Golub and H. Park, Missing value estima-

tion for DNA microarray gene expression data: Local least
squares imputation, Bioinformatics 21(2) (2005), 187–198.
doi:10.1093/bioinformatics/bth499.

[130] K. Kira and L.A. Rendell, A practical approach to feature se-
lection, in: Proceedings of the Ninth International Workshop
on Machine Learning, 1992, pp. 249–256.

[131] R. Kohavi et al., A study of cross-validation and bootstrap for
accuracy estimation and model selection, Ijcai 14(2) (1995),
1137–1145.

[132] R. Kohavi and G.H. John, Wrappers for feature sub-
set selection, Artificial Intelligence 97(1) (1997), 273–324.
doi:10.1016/S0004-3702(97)00043-X.

[133] R. Kohavi and G.H. John, The wrapper approach, in: Fea-
ture Extraction, Construction and Selection, Springer, 1998,
pp. 33–50. doi:10.1007/978-1-4615-5725-8_3.

[134] R. Kohavi, P. Langley and Y. Yun, The utility of feature
weighting in nearest-neighbor algorithms, in: Proceedings of
the Ninth European Conference on Machine Learning, 1997,
pp. 85–92.

[135] D. Koller and M. Sahami, Toward optimal feature selec-
tion, in: 13th International Conference on Machine Learning,
1995.

[136] S. Konishi, Introduction to Multivariate Analysis: Linear and
Nonlinear Modeling, CRC Press, 2014.

[137] I. Kononenko, Estimating attributes: Analysis and extensions
of relief, in: European Conference on Machine Learning,
Springer, 1994, pp. 171–182.

[138] M. Kubat, R.C. Holte and S. Matwin, Machine learning
for the detection of oil spills in satellite radar images,
Machine Learning 30(2–3) (1998), 195–215. doi:10.1023/
A:1007452223027.

[139] L. Kuncheva, J. Wrench, L.C. Jain and A. Al-Zaidan, A fuzzy
model of heavy metal loadings in Liverpool bay, Envi-
ronmental Modelling & Software 15(2) (2000), 161–167.
doi:10.1016/S1364-8152(99)00031-6.

[140] L.I. Kuncheva, Fitness functions in editing k-nn reference
set by genetic algorithms, Pattern Recognition 30(6) (1997),
1041–1049. doi:10.1016/S0031-3203(96)00134-3.

[141] M.H. Kutner, C.J. Nachtsheim, J. Neter, W. Li et al., Ap-
plied Linear Statistical Models, Vol. 103, McGraw-Hill, Ir-
win, New York, 2005.

[142] N. Kwak and C.-H. Choi, Input feature selection for clas-
sification problems, IEEE Transactions on Neural Networks
13(1) (2002), 143–159. doi:10.1109/72.977291.

[143] P. Langley and W. Iba, Average-case analysis of a nearest
neighbor algorithm, in: IJCAI, Citeseer, 1993, pp. 889–894.

[144] C. Lazar, J. Taminau, S. Meganck, D. Steenhoff, A. Co-
letta, C. Molter, V. de Schaetzen, R. Duque, H. Bersini and
A. Nowe, A survey on filter techniques for feature selection
in gene expression microarray analysis, IEEE/ACM Transac-
tions on Computational Biology and Bioinformatics (TCBB)
9(4) (2012), 1106–1119. doi:10.1109/TCBB.2012.33.

[145] L. Lebart, Correspondence analysis, in: Data Science, Classi-
fication, and Related Methods: Proceedings of the Fifth Con-
ference of the International Federation of Classification Soci-
eties (IFCS-96), Kobe, Japan, March 27–30, 1996, Springer
Science & Business, Media, 2013, p. 423.

[146] K. Leung and C. Leckie, Unsupervised anomaly detection in
network intrusion detection using clusters, in: Proceedings
of the Twenty-Eighth Australasian Conference on Computer
Science, Vol. 38, Australian Computer Society, Inc., 2005,
pp. 333–342.

[147] R.J. Little and D.B. Rubin, Statistical Analysis with Missing
Data, John Wiley & Sons, 2014.

[148] H. Liu and H. Motoda, Data Reduction Via Instance Selec-
tion. In Instance Selection and Construction for Data Mining,
Springer, 2001, pp. 3–20.

[149] H. Liu and H. Motoda, Computational Methods of Feature
Selection, CRC Press, 2007.

[150] H. Liu and H. Motoda, Feature Selection for Knowledge Dis-
covery and Data Mining, Vol. 454, Springer Science & Busi-
ness, Media, 2012.

[151] H. Liu, J. Sun, L. Liu and H. Zhang, Feature selection
with dynamic mutual information, Pattern Recognition 42(7)
(2009), 1330–1339. doi:10.1016/j.patcog.2008.10.028.

[152] H. Liu and L. Yu, Toward integrating feature selection al-
gorithms for classification and clustering, IEEE Transactions
on Knowledge and Data Engineering 17(4) (2005), 491–502.
doi:10.1109/TKDE.2005.66.

[153] J. Luengo, S. García and F. Herrera, On the choice of
the best imputation methods for missing values consider-
ing three groups of classification methods, Knowledge and
Information Systems 32(1) (2012), 77–108. doi:10.1007/
s10115-011-0424-2.

[154] A. Lumini and L. Nanni, A clustering method for auto-
matic biometric template selection, Pattern Recognition 39(3)
(2006), 495–497. doi:10.1016/j.patcog.2005.11.004.

[155] P. Marmonier, G. Archambaud, N. Belaidi, N. Bougon,
P. Breil, E. Chauvet, C. Claret, J. Cornut, T. Datry, M.-
J. Dole-Olivier et al., The role of organisms in hyporheic pro-
cesses: Gaps in current knowledge, needs for future research
and applications, Annales de Limnologie – International
Journal of Limnology 48(3) (2012), 253–266. doi:10.1051/
limn/2012009.

[156] J.B. Martínez-Rodríguez, I. Montalvo, J. Izquierdo and
R. Pérez-García, Reliability and tolerance comparison in wa-
ter supply networks, Water Resources Management 25(5)
(2011), 1437–1448. doi:10.1007/s11269-010-9753-2.

[157] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz and T. Eu-
ler, Yale: Rapid prototyping for complex data mining tasks,
in: Proceedings of the 12th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, ACM,
2006, pp. 935–940. doi:10.1145/1150402.1150531.

[158] H.J. Miller and J. Han, Geographic Data Mining and Knowl-
edge Discovery, CRC Press, 2009.

[159] M. Minelli, M. Chambers and A. Dhiraj, Big Data, Big An-
alytics: Emerging Business Intelligence and Analytic Trends
for Today’s Businesses, John Wiley & Sons, 2012.

[160] T.M. Mitchell, Generalization as search, Artificial Intelli-
gence 18(2) (1982), 203–226. doi:10.1016/0004-3702(82)
90040-6.

[161] P. Mitra, C. Murthy and S.K. Pal, Unsupervised feature selec-
tion using feature similarity, in: IEEE Transactions on Pattern

http://dx.doi.org/10.1016/S0165-0114(03)00119-2
http://dx.doi.org/10.1007/s10462-011-9282-2
http://dx.doi.org/10.1007/s10462-011-9282-2
http://dx.doi.org/10.1093/bioinformatics/bth499
http://dx.doi.org/10.1016/S0004-3702(97)00043-X
http://dx.doi.org/10.1007/978-1-4615-5725-8_3
http://dx.doi.org/10.1023/A:1007452223027
http://dx.doi.org/10.1023/A:1007452223027
http://dx.doi.org/10.1016/S1364-8152(99)00031-6
http://dx.doi.org/10.1016/S0031-3203(96)00134-3
http://dx.doi.org/10.1109/72.977291
http://dx.doi.org/10.1109/TCBB.2012.33
http://dx.doi.org/10.1016/j.patcog.2008.10.028
http://dx.doi.org/10.1109/TKDE.2005.66
http://dx.doi.org/10.1007/s10115-011-0424-2
http://dx.doi.org/10.1007/s10115-011-0424-2
http://dx.doi.org/10.1016/j.patcog.2005.11.004
http://dx.doi.org/10.1051/limn/2012009
http://dx.doi.org/10.1051/limn/2012009
http://dx.doi.org/10.1007/s11269-010-9753-2
http://dx.doi.org/10.1145/1150402.1150531
http://dx.doi.org/10.1016/0004-3702(82)90040-6
http://dx.doi.org/10.1016/0004-3702(82)90040-6


AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 661

Analysis and Machine Intelligence, Vol. 24, 2002, pp. 301–
312.

[162] T. Mohri and H. Tanaka, An optimal weighting criterion of
case indexing for both numeric and symbolic attributes, in:
AAAI-94 Workshop Program: Case-Based Reasoning, Work-
ing Notes, 1994, pp. 123–127.

[163] L.C. Molina, L. Belanche and À. Nebot, Feature selection
algorithms: A survey and experimental evaluation, in: Data
Mining, ICDM 2003. Proceedings. 2002 IEEE International
Conference on, IEEE, 2002, pp. 306–313.

[164] R.A. Mollineda, F.J. Ferri and E. Vidal, An efficient proto-
type merging strategy for the condensed 1-nn rule through
class-conditional hierarchical clustering, Pattern Recogni-
tion 35(12) (2002), 2771–2782. doi:10.1016/S0031-3203(01)
00208-4.

[165] D.S. Moore, G.P. McCabe and M.J. Evans, Introduction to the
Practice of Statistics Minitab Manual and Minitab Version
14, WH Freeman & Co., 2005.

[166] A. Murakami and T. Nasukawa, Tweeting about the tsunami?:
Mining Twitter for information on the Tohoku earthquake and
tsunami, in: Proceedings of the 21st International Conference
on World Wide Web, ACM, 2012, pp. 709–710.

[167] S. Nakariyakul and D.P. Casasent, An improvement on float-
ing search algorithms for feature subset selection, Pattern
Recognition 42(9) (2009), 1932–1940. doi:10.1016/j.patcog.
2008.11.018.

[168] B.L. Narayan, C. Murthy and S.K. Pal, Maxdiff kd-trees for
data condensation, Pattern Recognition Letters 27(3) (2006),
187–200. doi:10.1016/j.patrec.2005.08.015.

[169] P.M. Narendra and K. Fukunaga, A branch and bound al-
gorithm for feature subset selection, IEEE Transactions on
Computers 100(9) (1977), 917–922. doi:10.1109/TC.1977.
1674939.

[170] D.F. Nettleton, A. Orriols-Puig and A. Fornells, A study of
the effect of different types of noise on the precision of super-
vised learning techniques, Artificial Intelligence Review 33(4)
(2010), 275–306. doi:10.1007/s10462-010-9156-z.

[171] M. Nixon, Feature Extraction & Image Processing, Aca-
demic Press, 2008.

[172] H. Núñez, Feature weighting in plain case-based reasoning,
PhD thesis, Universitat Politècnica de Catalunya, 2004.

[173] H. Núñez and M. Sànchez-Marrè, Instance-based learning
techniques of unsupervised feature weighting do not perform
so badly!, in: ECAI, Vol. 16, 2004, pp. 102–106.

[174] H. Núñez, M. Sànchez-Marrè and U. Cortés, Improving sim-
ilarity assessment with entropy-based local weighting, in: In-
ternational Conference on Case-Based Reasoning, Springer,
2003, pp. 377–391.

[175] S.-K. Oh and W. Pedrycz, Self-organizing polynomial neu-
ral networks based on polynomial and fuzzy polynomial neu-
rons: Analysis and design, Fuzzy Sets and Systems 142(2)
(2004), 163–198. doi:10.1016/S0165-0114(03)00307-5.

[176] J.A. Olvera-López, J.A. Carrasco-Ochoa and J.F. Martínez-
Trinidad, Object selection based on clustering and border ob-
jects, in: Computer Recognition Systems 2, Springer, 2007,
pp. 27–34. doi:10.1007/978-3-540-75175-5_4.

[177] J.A. Olvera-López, J.A. Carrasco-Ochoa and J.F. Martínez-
Trinidad, Prototype selection via prototype relevance, in:
Iberoamerican Congress on Pattern Recognition, Springer,
2008, pp. 153–160.

[178] B. Pang and L. Lee, Opinion mining and sentiment analy-
sis, Foundations and Trends in Information Retrieval 2(1–2)
(2008), 1–135. doi:10.1561/1500000011.

[179] R. Paredes and E. Vidal, Weighting prototypes. A new editing
approach, in: Proceedings of the International Conference on
Pattern Recognition ICPR, Vol. 2, 2000, pp. 25–28.

[180] Z. Pawlak, Rough Sets: Theoretical Aspects of Reasoning
About Data, Vol. 9, Springer Science & Business, Media,
2012.

[181] V. Pawlowsky-Glahn and A. Buccianti, Compositional Data
Analysis: Theory and Applications, John Wiley & Sons,
2011.

[182] J. Pearl and K. Mohan, Recoverability and testability of miss-
ing data: Introduction and summary of results, Available at
SSRN 2343873, 2013.

[183] K. Pearson, Liii. on lines and planes of closest fit to systems
of points in space, The London, Edinburgh, and Dublin Philo-
sophical Magazine and Journal of Science 2(11) (1901), 559–
572. doi:10.1080/14786440109462720.

[184] H. Peng, F. Long and C. Ding, Feature selection based on mu-
tual information criteria of max-dependency, max-relevance,
and min-redundancy, IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence 27(8) (2005), 1226–1238.
doi:10.1109/TPAMI.2005.159.

[185] T.M. Phuong, Z. Lin and R.B. Altman, Choosing SNPS us-
ing feature selection, Journal of Bioinformatics and Com-
putational Biology 4(02) (2006), 241–257. doi:10.1142/
S0219720006001941.

[186] R.F. Potthoff, G.E. Tudor, K.S. Pieper and V. Hasselblad, Can
one assess whether missing data are missing at random in
medical studies?, Statistical Methods in Medical Research
15(3) (2006), 213–234. doi:10.1191/0962280206sm448oa.

[187] F. Provost and T. Fawcett, Robust classification for impre-
cise environments, Machine Learning 42(3) (2001), 203–231.
doi:10.1023/A:1007601015854.

[188] P. Pudil, J. Novovičová and J. Kittler, Floating search meth-
ods in feature selection, Pattern Recognition Letters 15(11)
(1994), 1119–1125. doi:10.1016/0167-8655(94)90127-9.

[189] W.F. Punch III, E.D. Goodman, M. Pei, L. Chia-Shun,
P.D. Hovland and R.J. Enbody, Further research on feature se-
lection and classification using genetic algorithms, in: ICGA,
1993, pp. 557–564.

[190] D. Pyle, Data Preparation for Data Mining, Vol. 1, Morgan
Kaufmann, 1999.

[191] J.R. Quinlan, C4.5: Programs for Machine Learning, Else-
vier, 2014.

[192] M.G. Rahman and M.Z. Islam, Fimus: A framework for
imputing missing values using co-appearance, correlation
and similarity analysis, Knowledge-Based Systems 56 (2014),
311–327. doi:10.1016/j.knosys.2013.12.005.

[193] T. Raicharoen and C. Lursinsap, A divide-and-conquer ap-
proach to the pairwise opposite class-nearest neighbor (poc-
nn) algorithm, Pattern Recognition Letters 26(10) (2005),
1554–1567. doi:10.1016/j.patrec.2005.01.003.

[194] E. Ramos-Martinez, A.M. Herrera Fernandez, J. Izquierdo
and R. Perez-Garcia, A multi-disciplinary procedure to ascer-
tain biofilm formation in drinking water pipes, in: Interna-
tional Congress on Environmental Modelling and Software,
iEMSs, 2016.

[195] M. Refaat, Data Preparation for Data Mining Using SAS,
Morgan Kaufmann, 2010.

http://dx.doi.org/10.1016/S0031-3203(01)00208-4
http://dx.doi.org/10.1016/S0031-3203(01)00208-4
http://dx.doi.org/10.1016/j.patcog.2008.11.018
http://dx.doi.org/10.1016/j.patcog.2008.11.018
http://dx.doi.org/10.1016/j.patrec.2005.08.015
http://dx.doi.org/10.1109/TC.1977.1674939
http://dx.doi.org/10.1109/TC.1977.1674939
http://dx.doi.org/10.1007/s10462-010-9156-z
http://dx.doi.org/10.1016/S0165-0114(03)00307-5
http://dx.doi.org/10.1007/978-3-540-75175-5_4
http://dx.doi.org/10.1561/1500000011
http://dx.doi.org/10.1080/14786440109462720
http://dx.doi.org/10.1109/TPAMI.2005.159
http://dx.doi.org/10.1142/S0219720006001941
http://dx.doi.org/10.1142/S0219720006001941
http://dx.doi.org/10.1191/0962280206sm448oa
http://dx.doi.org/10.1023/A:1007601015854
http://dx.doi.org/10.1016/0167-8655(94)90127-9
http://dx.doi.org/10.1016/j.knosys.2013.12.005
http://dx.doi.org/10.1016/j.patrec.2005.01.003


AU
TH

O
R 

 C
O
PY

662 K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining

[196] T. Reinartz, A unifying view on instance selection, Data
Mining and Knowledge Discovery 6(2) (2002), 191–210.
doi:10.1023/A:1014047731786.

[197] A.C. Rencher, Methods of Multivariate Analysis, Vol. 492,
John Wiley & Sons, 2003.

[198] J. Reunanen, Overfitting in making comparisons between
variable selection methods, Journal of Machine Learning Re-
search 3 (2003), 1371–1382.

[199] J.C. Riquelme, J.S. Aguilar-Ruiz and M. Toro, Finding repre-
sentative patterns with ordered projections, Pattern Recogni-
tion 36(4) (2003), 1009–1018. doi:10.1016/S0031-3203(02)
00119-X.

[200] G. Ritter, H. Woodruff, S. Lowry and T. Isenhour, An al-
gorithm for a selective nearest neighbor decision rule, IEEE
Transactions on Information Theory 21(6) (1975), 665–669.
doi:10.1109/TIT.1975.1055464.

[201] J.C. Roberts, State of the art: Coordinated & multiple views in
exploratory visualization, in: Coordinated and Multiple Views
in Exploratory Visualization, 2007. CMV’07. Fifth Interna-
tional Conference on, IEEE, 2007, pp. 61–71.

[202] J. Rodas, K. Gibert and J.E. Rojo, KDSM methodology for
knowledge discovery from ill-structured domains presenting
very short and repeated serial measures with blocking factor,
in: Topics in Artificial Intelligence, Springer, 2002, pp. 228–
238. doi:10.1007/3-540-36079-4_20.

[203] G. Roffo, S. Melzi and M. Cristani, Infinite feature selec-
tion, in: Proceedings of the IEEE International Conference
on Computer Vision, 2015, pp. 4202–4210.

[204] D.B. Rubin, Inference and missing data, Biometrika 63(3)
(1976), 581–592. doi:10.1093/biomet/63.3.581.

[205] Y. Saeys, I. Inza and P. Larrañaga, A review of feature se-
lection techniques in bioinformatics, Bioinformatics 23(19)
(2007), 2507–2517. doi:10.1093/bioinformatics/btm344.

[206] J.L. Schafer and J.W. Graham, Missing data: Our view of
the state of the art, Psychological Methods 7(2) (2002), 147.
doi:10.1037/1082-989X.7.2.147.

[207] S.C. Shiu, D.S. Yeung, C.H. Sun and X.Z. Wang, Transfer-
ring case knowledge to adaptation knowledge: An approach
for case-base maintenance, Computational Intelligence 17(2)
(2001), 295–314. doi:10.1111/0824-7935.00146.

[208] SIASAR, http://siasar.org/reportes/resumen_regional_
sistema_distrito/resumen_regional_sistema_distrito.php,
2016.

[209] K. Singh and S. Upadhyaya, Outlier detection: Applications
and techniques, International Journal of Computer Science
Issues 9(1) (2012), 307–323.

[210] P. Somol, P. Pudil, J. Novovičová and P. Paclık, Adaptive
floating search methods in feature selection, Pattern Recogni-
tion Letters 20(11) (1999), 1157–1163. doi:10.1016/S0167-
8655(99)00083-5.

[211] B. Spillmann, M. Neuhaus, H. Bunke, E. Pekalska and
R.P. Duin, Transforming strings to vector spaces using pro-
totype selection, in: Joint IAPR International Workshops
on Statistical Techniques in Pattern Recognition (SPR)
and Structural and Syntactic Pattern Recognition (SSPR),
Springer, 2006, pp. 287–296.

[212] C. Stanfill and D. Waltz, Toward memory-based reason-
ing, Communications of the ACM 29(12) (1986), 1213–1228.
doi:10.1145/7902.7906.

[213] S.D. Stearns, On selecting features for pattern classifiers, in:
Proceedings of the 3rd International Joint Conference on Pat-
tern Recognition, 1976, pp. 71–75.

[214] Y. Sun, C. Babbs and E. Delp, A comparison of feature se-
lection methods for the detection of breast cancers in mam-
mograms: Adaptive sequential floating search vs. genetic al-
gorithm, in: 2005 IEEE Engineering in Medicine and Biology
27th Annual Conference, IEEE, 2006, pp. 6532–6535.

[215] D.F. Swayne, D. Cook and A. Buja, Xgobi: Interactive dy-
namic data visualization in the x window system, Journal
of Computational and Graphical Statistics 7(1) (1998), 113–
130.

[216] M. Templ, A. Kowarik and P. Filzmoser, Iterative stepwise re-
gression imputation using standard and robust methods, Com-
putational Statistics & Data Analysis 55(10) (2011), 2793–
2806. doi:10.1016/j.csda.2011.04.012.

[217] H.C. Thode Jr., Testing for Normality, Statistics: Textbooks
and Monographs, Vol. 164, 2002.

[218] I. Tomek, An experiment with the edited nearest-neighbor
rule, IEEE Transactions on Systems, Man, and Cybernetics 6
(1976), 448–452. doi:10.1109/TSMC.1976.4309523.

[219] P. Torres, C.H. Cruz and P.J. Patiño, Índices de calidad de
agua en fuentes superficiales utilizadas en la producción de
agua para consumo humano: Una revisión crítica, Revista In-
genierías Universidad de Medellín 8(15) (2009), 79–94.

[220] A. Valls, J. Pijuan, M. Schuhmacher, A. Passuello, M. Nadal
and J. Sierra, Preference assessment for the management of
sewage sludge application on agricultural soils, International
Journal of Multicriteria Decision Making 1(1) (2010), 4–24.
doi:10.1504/IJMCDM.2010.033684.

[221] C.J. Veenman and M.J. Reinders, The nearest subclass clas-
sifier: A compromise between the nearest mean and near-
est neighbor classifier, IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence 27(9) (2005), 1417–1429.
doi:10.1109/TPAMI.2005.187.

[222] A. Vellido, Missing data imputation through GTM as a mix-
ture of t-distributions, Neural Networks 19(10) (2006), 1624–
1635. doi:10.1016/j.neunet.2005.11.003.

[223] A. Vicente, Mineria de datos aplicada a la detección de fraude
con tarjetas de crédito, Master’s thesis, Degree on Statistics,
Universitat Politècnica de Catalunya, 2016.

[224] J. Villar, A. Otero, J. Otero and L. Sánchez, Taxime-
ter verification with GPS and soft computing techniques,
Soft Computing 14(4) (2010), 405–418. doi:10.1007/s00500-
009-0414-4.

[225] S.E. Wakefield, S.J. Elliott, D.C. Cole and J.D. Eyles, Envi-
ronmental risk and (re) action: Air quality, health, and civic
involvement in an urban industrial neighbourhood, Health
& Place 7(3) (2001), 163–177. doi:10.1016/S1353-8292(01)
00006-5.

[226] G.-R. Walther, E. Post, P. Convey, A. Menzel, C. Parme-
san, T.J. Beebee, J.-M. Fromentin, O. Hoegh-Guldberg and
F. Bairlein, Ecological responses to recent climate change,
Nature 416(6879) (2002), 389–395. doi:10.1038/416389a.

[227] G.M. Weiss and F. Provost, Learning when training data are
costly: The effect of class distribution on tree induction, Jour-
nal of Artificial Intelligence Research 19 (2003), 315–354.

[228] D. Wettschereck, D.W. Aha and T. Mohri, A review and em-
pirical evaluation of feature weighting methods for a class of
lazy learning algorithms, Artificial Intelligence Review 11(1–
5) (1997), 273–314. doi:10.1023/A:1006593614256.

http://dx.doi.org/10.1023/A:1014047731786
http://dx.doi.org/10.1016/S0031-3203(02)00119-X
http://dx.doi.org/10.1016/S0031-3203(02)00119-X
http://dx.doi.org/10.1109/TIT.1975.1055464
http://dx.doi.org/10.1007/3-540-36079-4_20
http://dx.doi.org/10.1093/biomet/63.3.581
http://dx.doi.org/10.1093/bioinformatics/btm344
http://dx.doi.org/10.1037/1082-989X.7.2.147
http://dx.doi.org/10.1111/0824-7935.00146
http://siasar.org/reportes/resumen_regional_sistema_distrito/resumen_regional_sistema_distrito.php
http://siasar.org/reportes/resumen_regional_sistema_distrito/resumen_regional_sistema_distrito.php
http://dx.doi.org/10.1016/S0167-8655(99)00083-5
http://dx.doi.org/10.1016/S0167-8655(99)00083-5
http://dx.doi.org/10.1145/7902.7906
http://dx.doi.org/10.1016/j.csda.2011.04.012
http://dx.doi.org/10.1109/TSMC.1976.4309523
http://dx.doi.org/10.1504/IJMCDM.2010.033684
http://dx.doi.org/10.1109/TPAMI.2005.187
http://dx.doi.org/10.1016/j.neunet.2005.11.003
http://dx.doi.org/10.1007/s00500-009-0414-4
http://dx.doi.org/10.1007/s00500-009-0414-4
http://dx.doi.org/10.1016/S1353-8292(01)00006-5
http://dx.doi.org/10.1016/S1353-8292(01)00006-5
http://dx.doi.org/10.1038/416389a
http://dx.doi.org/10.1023/A:1006593614256


AU
TH

O
R 

 C
O
PY

K. Gibert et al. / A survey on pre-processing techniques: Relevant issues in the context of environmental data mining 663

[229] D.L. Wilson, Asymptotic properties of nearest neighbor rules
using edited data, IEEE Transactions on Systems, Man, and
Cybernetics 3 (1972), 408–421. doi:10.1109/TSMC.1972.
4309137.

[230] D.R. Wilson and T.R. Martinez, Reduction techniques for
instance-based learning algorithms, Machine Learning 38(3)
(2000), 257–286. doi:10.1023/A:1007626913721.

[231] I.H. Witten, E. Frank and M.A. Hall, Data Mining: Practical
Machine Learning Tools and Techniques, Morgan Kaufmann
Publishers Inc., 2011.

[232] P.C. Wong, Visual data mining, IEEE Computer Graph-
ics and Applications 19(5) (1999), 20–21. doi:10.1109/
MCG.1999.788794.

[233] C.-F.J. Wu, Jackknife, bootstrap and other resampling meth-
ods in regression analysis, The Annals of Statistics 14(4)
(1986), 1261–1295. doi:10.1214/aos/1176350142.

[234] L. Xu, M.-Y. Chow and L.S. Taylor, Power distribution
fault cause identification with imbalanced data using the
data mining-based fuzzy classification e-algorithm, IEEE

Transactions on Power Systems 22(1) (2007), 164–171.
doi:10.1109/TPWRS.2006.888990.

[235] J. Yang and V.G. Honavar, Feature subset selection using a
genetic algorithm, IEEE Intelligent Systems 13(2) (1998), 44–
49. doi:10.1109/5254.671091.

[236] L.A. Zadeh, Discussion: Probability theory and fuzzy logic
are complementary rather than competitive, Technomet-
rics 37(3) (1995), 271–276. doi:10.1080/00401706.1995.
10484330.

[237] H. Zhang and G. Sun, Optimal reference subset selec-
tion for nearest neighbor classification by tabu search, Pat-
tern Recognition 35(7) (2002), 1481–1490. doi:10.1016/
S0031-3203(01)00137-6.

[238] Z. Zhao, R. Zhang, J. Cox, D. Duling and W. Sarle, Mas-
sively parallel feature selection: An approach based on vari-
ance preservation, Machine Learning 92(1) (2013), 195–220.
doi:10.1007/s10994-013-5373-4.

http://dx.doi.org/10.1109/TSMC.1972.4309137
http://dx.doi.org/10.1109/TSMC.1972.4309137
http://dx.doi.org/10.1023/A:1007626913721
http://dx.doi.org/10.1109/MCG.1999.788794
http://dx.doi.org/10.1109/MCG.1999.788794
http://dx.doi.org/10.1214/aos/1176350142
http://dx.doi.org/10.1109/TPWRS.2006.888990
http://dx.doi.org/10.1109/5254.671091
http://dx.doi.org/10.1080/00401706.1995.10484330
http://dx.doi.org/10.1080/00401706.1995.10484330
http://dx.doi.org/10.1016/S0031-3203(01)00137-6
http://dx.doi.org/10.1016/S0031-3203(01)00137-6
http://dx.doi.org/10.1007/s10994-013-5373-4

	Introduction
	Structure of the paper

	The main pre-processing issues
	Building the original data matrix
	Structure of original data matrix
	Introducing the data into the pre-processing tool

	Determining the working data matrix
	Determining the data matrix rows
	Determining the data matrix columns
	Practicum

	Data visualization
	Outliers and influential observations: Detection and treatment
	Dimensionality of outliers
	Causes of outliers
	Outlier detection
	Outlier treatment

	Error detection and treatment
	Missing data
	Types of missing data
	Missing data representation
	Detection
	Diagnoses
	Treatment

	Relevance or redundancy detection and dimensionality reduction
	Instance selection
	Filter instance selection methods
	Wrapper instance selection

	Relevance of variables and dimensionality reduction
	Feature weighting
	Feature selection
	Factorial methods and related


	Transformations
	Transform to improve data quality
	Transformations required to increase interpretability
	Transformations required to fit technical assumptions of data mining methods
	Transformations associated to imbalanced datasets
	Practicum


	Creating new variables
	Conclusions and challenges for pre-processing in environmental data mining
	References

