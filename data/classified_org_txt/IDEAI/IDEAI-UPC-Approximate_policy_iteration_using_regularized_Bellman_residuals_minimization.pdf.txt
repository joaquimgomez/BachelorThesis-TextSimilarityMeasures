











































January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

To appear in the Journal of Experimental & Theoretical Artificial Intelligence
Vol. 00, No. 00, Month 20XX, 1–16

Approximate Policy Iteration using

Regularized Bellman Residuals Minimization

G. Espositoa∗ and M. Martina

a Universitat Politecnica de Catalunya, Barcelona, Spain;

(v1.0 released October 2014)

In this paper we present an Approximate Policy Iteration (API) method called API−BRM�
using a very effective implementation of incremental Support Vector Regression (SVR) to
approximate the value function able to generalize in continuous (or large) space Reinforcement
Learning (RL) problems. RL ia a methodology able to solve complex and uncertain decision
problem usually modeled as Markov Decision Problem (MDP). API − BRM� is formalized
as a non-parametric regularization problem based on an outcome of the Bellman Residual
Minimization (BRM) which is able to minimize the variance of the problem. API−BRM� is
incremental and can be applied to RL using the on-line agent interaction framework. Based
on non-parametric SVR API −BRM� is able to find the global solution of the problem with
convergence guarantees to the optimal solution. A value function should be defined to find
the optimal policy specifying the total reward that an agent might expect in its current state
taking one action. Therefore the agent will use the value function to choose the action to take.
Some experimental evidence and performance for well known RL benchmarks are presented.

Keywords: Reinforcement Learning, Support Vector Machine, Approximate Policy
Iteration, Regularization, Regression

1. Introduction

In RL an autonomous agent interacting with the environment learns how to take correct
action for every situation in order to reach its goal. RL gives a method able to solve
difficult and uncertain sequential decision problems, which could be quite challenging
in real-world applications. RL problem is often modeled as a MDP, which has been
deeply studied in the literature. The main particularity of RL algorithms with respect to
other approaches to MDPs is that the RL agent learns optimal policies from experiences
without knowing the parameters of the MDP. Usually to find the optimal policy a value
function should be defined to specify the total reward an agent might expect at its current
state and taking one action. The agent will choose the action to take by using this value
function. However if one wants to handle problems with continuous or very large state
spaces, generalization should be used. Generalization property of RL algorithms is an
important factor to determine prediction performance in such cases.

Main problems using generalization in approximate RL are convergence guarantees
and quality of the solution. Parametric methods have been considered in the literature,
showing the advantage of fast and easy learning mechanisms. However, they present the
inherent problem that in some cases the solution of the problem might not be expressed
with the given architecture and the number of chosen parameters. Candidates for non

∗Corresponding author. Email: gesposit@lsi.upc.edu

1



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

parametric value function approximation are SVR known to have good properties over
the generalization ability. SVR being a convex optimization problem, do not suffer from
sub-optimality, finding always the global optimal solution to the approximation problem.
As non parametric learning method, they are also able to automatically adapt to the
complexity of the problem. Using non parametric methods for approximate RL allows
to choose the capacity of the approximation function space by means of a fixed kernel
function while the number supports can be determined as needed in order to find the
required solution.

The algorithm presented in this paper is an instance of API. Like in policy iteration, the
algorithm repeatedly computes an evaluation function of the policy of the previous step
and then uses this evaluation function to compute the next improved policy. In order
to avoid the need of learning a model, action value functions are computed, making
the policy improvement step simple, like in the Least-Squares Policy Iteration (LSPI)
algorithm of (Lagoudakis and Parr, 2003). LSPI relies on least squares temporal difference
learning (LSTD) while we build our algorithm using BRM. Major novelty of our algorithm
is based on the exact incremental SVR in the context of BRM with approximate policy
iteration. The idea of using Bellman residuals in policy iteration goes back to (Baird,
1995) who proposed it for computing approximate state value functions given the model
of a finite-state and action MDP using a quadratic loss function. Small Bellman errors
might yield a good approximation of the policy evaluation function, which in turn may
imply a good final performance. One major obstacle of using BRM when learning without
a model is that the sample based approximation using a least squares loss function based
on the data from a single trajectory of the behavior policy, is not an unbiased estimate
of the Bellman residual. Our algorithm using �−insensitive loss function might solve this
problem. As main condition for the convergence of our method we ask that the trajectory
should be sufficiently representative and rapidly mixing. We also require that the states
in the trajectory follow a stationary distribution. The mixing condition is essential for
efficient learning and in particular we use the exponential β−mixing condition.

2. Background and notation

For a space Ω with a σ-algebra σΩ define M(Ω) the set of all probability measure over
σΩ and B(Ω) the space of bounded measurable function w.r.t. σΩ and B(Ω, L) the space
of bounded measurable function with bound 0 < L <∞. Hence we may introduce

Definition 2.1: (Continuous State MDP) A continuous state and finite action dis-
counted MDP can be defined as a tuple (S,A, P, γ), with the following definitions for its
contents:

• S is a measurable state space where st ∈ S denotes the state the agent is in at time
t.
• A is a finite set of available actions where at ∈ A denotes the action the agent

performs at time t.
• P : S ×A→M(R× S) is a mapping that when evaluated at (s, a) ∈ S ×A gives

the distribution over R× S denoted as P (r, s
′
|s, a).

• Marginals of P can be defined as P(·|s, a) =
∫

R
P (dr, ·|s, a) denotes the probability

of ending up in state s
′

when performing action a in state s (transition probability)
• R(·|s, a) =

∫
S
P (·, ds

′
|s, a) is a reward function denoting the expected reward when

the agent transitions from state s to state s
′

after performing action a.
• The immediate expected reward is defined as r(s, a) =

∫
R
rR(dr|s, a) = E[r|s, a]

• γ ∈ [0, 1] is a discount factor.

2



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

At stage t an action at ∈ A is selected by the agent controlling the process and in response
the pair (rt, s

′

t) is drawn from the distribution P (r, s
′
|st, at) i.e (rt, s

′

t) ∼ P (r, s
′
|st, at)

where rt is the reward the agent receives and s
′

t the next MDP state. The procedure
continue leading to a random trajectory ξt = {s1, a1, r1, s2, a2, r2, ...} ∈ Ξ where Ξ denotes
the space of all possible trajectories.

For a MDP an agent consists mainly of an action selection policy such that at = π(st).
A stationary stochastic policy maps states to distributions over the action space πt : S →
M(A) with πt(a|s) denoting the probability that the agent will select the action a to
perform in state s at time t. We will use π(s) to refer to the probability distribution or the
probability mass function of the actions in state s. Stochastic policies are also called soft
when they do not commit to a single action per state. π(a|s) stands for the probability
that the soft policy chooses action a in state s. An �−greedy policy is a soft policy which
for some 0 ≤ � ≤ 1 picks deterministically a particular action with probability 1− � and
a uniformly random action with probability �. We will then use a ∼ π(·|s) to indicate
that action a is chosen according to the probability function in state s.

Definition 2.2: (Value function) For an agent following the policy π considering
the sequence of rewards {rt : t ≥ 1} when the MDP is started in the state ac-
tion (s1, a1) ∼ ν(s, a) ∈ M(S × A) the action value function Qπ is defined as
Qπ(s, a) = E{

∑∞
t=1 γ

t−1rt| s1 = s, a1 = a, π} The optimal action value Q∗(s, a) =
supπ Q

π(s, a) ∀ (s, a) ∈ S ×A. A policy π∗ is optimal whenever achieves the best value
in every state and a policy π = π̂(·, Q) is greedy w.r.t. an action value function Q if
∀ s ∈ S we choose π(s) = arg maxa∈A Q(s, a).

Definition 2.3: (Bellman Operator) The Bellman operator can be defined as T π :
B(S × A) → B(S × A) for action value function is defined as (T πQ)(s, a) = r(s, a) +
γ
∫
P(ds

′
|s, a)Q(s

′
, π(s

′
)) = E[r + γ

∑
a
′∈A π(a

′
|s
′
)Q(s

′
, a
′
)] where B(·) represents the

space of bounded measurable functions. Given a policy π a fixed point of the Bellman oper-
ator is the action value function Qπ = T πQπ. Bellman optimality operator can be defined
for action value function as (T ∗Q)(s, a) = r(s, a) + γ

∫
P(ds

′
|s, a) maxa‘∈AQ(s

′
, a‘)

Definition 2.4: (Reproducing Kernel Hilbert Spaces) Consider a subset of measurable
functions F :→ R and a subset of vector values measurable functions F |A| : S×A→ R|A|
such that F |A| = {(Q1, ..., Q|A|) : Qi ∈ F , i = 1, ..., |A|} called the hypothesis space H
which can be chosen as Reproducing Kernel Hilbert Space (RKHS) H : S × A → R
defined in S × A with the inner product 〈·, ·〉 and characterized by a symmetric positive
definite function κ : (S × A) × (S × A) → R called reproducing kernel, continuous in
S×A such that for each (s, a) ∈ S×A the following reproducing property holds: Q(s, a) =
〈Q(·), κ(·, s, a)〉H κ(·, s, a) ∈ H assuming as measurable function space F |A| = H. H is
the closure of the linear span of the set of functions Φspan = {Φ(s, a) = κ(·, s, a) (s, a) ∈
S × A} considering the map from Φ : S × A → CS×A which denotes the function that
assigns the value κ(st, at, s, a) to (st, at) ∈ S ×A.

The kernel function κ(·, ·) plays an important role in any kernel-based learning method
mapping any two elements from a space of input patterns to the real numbers taken the
state space of the MDP, and can be thought of as a similarity measure on the input
space. In the derivation of kernel methods, the kernel function arises naturally as an inner
product in a high-dimensional features space. Hence the kernel satisfy several important
properties of an inner product: it must be symmetric and positive semidefinite, meaning
that the associated Gram matrix Kij = κ(si, ai, sj , aj) must be positive semidefinite. A
kernel that satisfies these properties is said to be admissible. An important aspect of any
method solving RL problems is the way that data are collected and processed. The data

3



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

collection setting can be categorized as online or offline and the data processing method
can be categorized as batch or incremental. The online sampling setting is when the agent
chooses the action sequence at ∼ πt(·|st) and directly influences how the data stream is
generated Dn = {(s1, a1, s

′

1, r1), ..., (sn, an, s
′

n, rn)} which we may assume as a stationary
and in general non-i.i.d. process. The stochastic functions πt(·|s), whenever evaluated for
the states s

′
in Dn define the stochastic process Πn = {π1(·|s

′

1), ..., πn(·|s
′

n)}. Assuming
controlled mixing condition for the non-i.i.d. process, allows for the generalization of
strictly i.i.d. scenarios. We may define the ordered multisets {(s1, a1), ..., (sn, an)} and
{(r1, s

′

1), ..., (r1, s
′

1)} with at = πbt(st) and st ∼ νs(s) and νs(s) ∈ M(S) while (rt, s
′

t) ∼
P (r, s

′
|st, at). πb is a behavior stationary policy producing (st, at) ∼ ν(s, a) with ν(s, a) ∈

M(S ×A) the resulting state action distribution. The collected data Dn can be used to
define the empirical operators and can be thought of as the empirical approximation to
the true operators.

Definition 2.5: (Empirical Bellman operators) Given the policy π consider the

Dn data set the empirical Bellman operator is defined as (T̂
πQ)(st, at) = rt +

γ
∑

a
′∈A π(a

′
|s
′

t)Q(s
′

t, a
′
) while the empirical Bellman optimality operator T̂ ∗ : Zn → Rn

is defined as (T̂ ∗Q)(st, at) = rt + γmaxa′∈AQ(s
′

t, a
′
). This provide an unbiased estimate

of the Bellman operator where given the policy π for any fixed bounded measurable de-
terministic function Q : S ×A→ R it holds that E[T̂ πQ(st, at)|st, at] = T πQ(st, at) and
also E[T̂ ∗Q(st, at)|st, at] = T ∗Q(st, at) for 1 ≤ t ≤ n.

We may also use the symbol En[f ] =
1
n

∑n
t=1 f(zt) as a shorthand for the empirical

version of the expectation operator applied over a function f(z) using the data set Zn =
{z1, ..., zn}.

3. Approximate Policy Iteration using Bellman Residuals Minimization

Policy iteration is a method of discovering the optimal policy for any given MDP, pro-
viding an iterative procedure in the policies space. PI discovers the optimal policy by
generating a sequence of monotonically improving policies. Each iteration consists of two
phases: policy evaluation which computes the action value function Qk of the current
policy πk by solving the linear system of the Bellman equations and policy improvement
defining the improved greedy policy πk+1 over Q

πk through πk+1 = arg maxa∈AQk(s, a).
The value function Qk is typically chosen to be such that Qk ≈ T πkQk, i.e., it is an
approximate fixed point of T πk . The policy πk+1 is at least as good as πk if not bet-
ter. These two steps are repeated until there is no change in the policy in which case
the iteration has converged to the optimal policy, often in a surprisingly small number
of iterations. The guaranteed convergence of policy iteration to the optimal policy relies
heavily upon a tabular representation of the value function, exact solution of the Bellman
equations, and tabular representation of each policy. Exact representations and methods
are impractical for large state and action spaces. In such cases, approximation methods
are used. Approximations in the policy iteration framework can be introduced in the
representation of the value function or the policy. The crucial factor for a successful
approximate algorithm is the choice of the approximation architecture and this form of
policy iteration is known as Approximate Policy Iteration (API).

A way to implement API is using BRM and in this case API proceeds at iteration
k evaluating πk choosing Qk such that the Bellman residuals �

BR
k = |Qk − T

πkQk| to
be small (i.e. is the approximate fixed point of T πk). API calculates πk+1 = π̂(·, Qk)
producing the sequence Q0 → π1 → Q1 .... and for the sequence {Qk}K−1k=0 the Bellman
Residuals (BR) and the policy Approximation Error (AE) can be defined at each iteration

4



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

as �BRk = Qk − T
πkQk and �

AE
k = Qk −Q

πk . BRM minimizes the Bellman Error (BE) of
the Bellman residuals of Q given the distribution of the input data ν and using a given
loss function ` defined as

LBRM`(Q, π) =

∫
`(Q(s, a)− T πQ(s, a))dν(s, a)

Using the sample data setDn and the policy process Πn one tries to evaluate the empirical
estimate L̂BRM`(Q,Πn, Dn) = En[`(Q(st, at)−T̂ πQ(st, at))]. BRM problem can be solved
in the contest of an API method using a Reproducing Kernel Hilbert Space as hypothesis
space by taking linear combinations of the form Q̂(s, a) =

∑n
t=1 βtκ(st, at, s, a), βt ∈ R.

Accordingly the regularized BRM problem can be written as

Q̂ = arg min
Q∈H
{L̂BRM`(Q,Πn, Dn) + λ‖Q‖

2
H}

Standard BRM algorithm (Baird, 1995) uses a least squares loss function to approximate
the action value function over the data. Unfortunately this approach brings to a biased
estimate and to overcome this problem a common suggestion is to use uncorrelated, or
double sampling data sets.

4. Bellman Residuals Minimization with SVR

Hereafter we introduce our method aiming to solve the BRM problem using SVR. Con-
sider the Bellman residuals BR(s, a) = Q− T πQ = Qπr (s, a)− r(s, a) where the approx-
imating function is

Qπr (s, a) = Q(s, a)− γ
∫
P(ds

′
|s, a)

∑
a
′∈A π(a

′
|s
′
)Q(s

′
, a
′
)

= E[Q(s, a)− γ
∑
a
′∈A π(a

′
|s
′
)Q(s

′
, a
′
)|s, a] = E[ Q̂πr (s, a, s

′
)|s, a]

while r(s, a) = E[r̂|s, a] and the BRM using the �−insensitive loss function
`�(Q

π
r − r) = max(0, |Qπr − r| − �) can be written as LBRM�(Q, π) = E[`�(Qπr −

r)] = E[`�(Q − T πQ)]. Using the data sets Πn, Dn the empirical estimate be-
comes: L̂BRM�(Q,Πn, Dn) = En[`�(Q̂

π
r − r̂)] = En[`�(Q − T̂ πQ)] with Q̂πr (st, at, s

′

t) =
Q(st, at) − γ

∑
a
′
t∈A

πt(a
′

t|s
′

t)Q(s
′

t, a
′

t). Hence the BRM� optimization problem becomes

Q̂ = arg minQ∈H{ L̂BRM�(Q,Πn, Dn) + λ‖Q‖2H } where the regularization term uses the
norm in the Hilbert space H. BRM� shows a remarkable sparsity property in the solution
which essentially relies on the training support vectors. L̂BRM�(Q,Πn, Dn) is an unbiased
estimator of LBRM�(Q, π) as results evaluating the expectation over the empirical losses

E[L̂BRM�(Q,Πn, Dn)|Πn, Dn] = E[En[`�(Q̂πr−r̂)]|Πn, Dn] = E[En[`�(Q−T̂ πQ)]|Πn, Dn] ≥
E[ `�(Q

πr − r)] = LBRM�(Q, π) where we used the Jensen’s inequality `(E[ X]) ≤
E[ `(X)] holding for any convex function ` and Qπr (s, a) = E[Q

π
r (s, a, s

′
)|s, a] r(s, a) =

E[r̂|s, a] T πQ = E[T̂ πQ]. In practice the empirical estimate can be biased whenever
slacks are presents i.e. the errors on the regression function are above the fixed thresh-
old �. It is unbiased when the error is contained in the resolution tube of the SVR.
Nevertheless the choice of the SVR parameters C and � gives a way to control this effect.

5. API − BRM� Dual Batch Solution

Consider the subset of observed samples Dn and express the approximation of the value
function using a linear architecture as Q(s, a) = 〈Φ(s, a),w〉+ b where w = (w1, ..., wd)T

5



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

is the weight vector and Φ(s, a) = (φ1(s, a), ..., φd(s, a))
T the features vector of the point

(s, a) from which we may build the kernel function κ(st, at, s, a) = 〈Φ(st, at),Φ(s, a)〉. The
action value function belongs the Hilbert space Q ∈ H so it does the weight vector w ∈ H.
Using the Representer theorem we also know that the function in H can be expressed
as linear combination of the elements in the span Φspan = {Φ(s, a) = κ(·, s, a) (s, a) ∈
S × A} which can be expressed as Q(s, a) =

∑
t αtκ(s, a, st, at). Using the definition of

the Bellman operator T πQ the Bellman residuals at each training point for a fixed policy
π we may write: BR(st, at) = Q(st, at)− T π Q(st, at) = Qπr (st, at)− r(st, at)
= Q(st, at) − γ

∫
P(ds

′
|st, at) ·

∑
a
′∈A π(a

′
|s
′
) · Q(s

′
, a
′
) − r(st, at) and substituting the

functional form of Q yields BR(st, at) = 〈Φ(st, at),w〉−γ
∫
P(ds

′
|st, at) ·

∑
a
′∈A π(a

′
|s
′
) ·

〈Φ(s
′
, a
′
),w〉 + (1 − γ)b − r(st, at) expressing the Bellman residuals using the weight

w and the features mapping Φ(·). Here the policy and the MDP dynamic are not
included into the Hilbert space H. However an alternative way to express the Bell-
man residuals is trough the combination of the two terms is a Bellman feature map-
ping Ψπ(st, at) = Φ(st, at)− γ

∫
P(ds

′
|st, at)

∑
a
′∈A π(a

′
|s
′
)Φ(s

′
, a
′
) which takes into ac-

count the structure of the MDP dynamics. The Bellman residuals are now expressed as
BR(st, at) = 〈Ψπ(st, at),w〉 + (1 − γ)b − r(st, at) and with the Bellman feature vector
Ψπ(st, at) we may build the Bellman kernel κ̃

π(st, at, s, a) = 〈Ψπ(st, at),Ψπ(s, a)〉. The
function Qπr and weight vector w belong to the Bellman Hilbert space HΨπ . Using the
Representer Theorem the function in HΨπ can be expressed as linear combination of
the elements in the span Ψπspan = {Ψπ(s, a) = κ̃π(·, s, a) (s, a) ∈ S × A} which can be
expressed as Qπr (s, a) =

∑
t βtκ̃

π(s, a, st, at). With this choice the policy as well as the
MDP dynamic are directly incorporated into the Hilbert space HΨπ .

If we consider the empirical Bellman operator T̂ πQ the Bellman residuals can be writ-
ten as B̂R(st, at, s

′

t) = Q(st, at)− T̂ π Q(st, at) = Q̂πr (st, at, s
′

t)− r̂t
= Q(st, at) − γ

∑
a
′
t∈A

π(a
′

t|s
′

t) · Q(s
′

t, a
′

t) − r̂t and substituting the functional form of Q
yields hatBR(st, at, s

′

t) = 〈Φ(st, at),w〉−γ
∑

a
′
t∈A

π(a
′

t|s
′

t)·〈Φ(s
′

t, a
′

t),w〉+(1−γ)b−r̂t ex-
pressing the Bellman residuals using the weight w and the features mapping Φ(·). The em-
pirical Bellman features mapping are Ψ̂π(st, at, s

′

t) = Φ(st, at)−γ
∑

a
′
t∈A

π(a
′

t|s
′

t)Φ(s
′

t, a
′

t)

and the empirical Bellman residuals B̂R(st, at, s
′

t) = 〈Ψ̂π(st, at, s
′

t),w〉 + (1 − γ)b − r̂t
with the empirical Bellman feature vector Ψ̂π(st, at, s

′

t) we may build the Bellman kernel

κ̃π(st, at, s
′

t, s, a, s
′
) = 〈Ψ̂π(st, at, s

′

t), Ψ̂
π(s, a, s

′
)〉.

Using the Bellman kernel we may find the weighting vector w using the features
mapping Ψπ(s, a) and searching for a solution of the regression function Qπr (s, a) =
〈Ψπ(s, a),w〉+ b with Qπr ∈ HΨπ solving the SVR problem

min
w,b,ξ,ξ∗

1
2
‖w‖2HΨπ + C

∑n
t=1(ξt + ξ

∗
t ) (1)

s.t. r(st, at)− 〈Ψπ(st, at),w〉 − (1− γ)b ≤ �+ ξt
−r(st, at) + 〈Ψπ(st, at),w〉+ (1− γ)b ≤ �+ ξ∗t ξt, ξ∗t ≥ 0

Once the Bellman kernel κ̃π(st, at, s, a) and the rewards r(st, at) are provided it can be
solved in principle using any standard SVM package.

6. API − BRM� Computational Complexity

SVR are powerful tools, but their compute and storage requirements increase rapidly
with the number of training vectors. Solving SVR relies on Quadratic Programming
(QP) optimization, which is able to separate support vectors from the rest of the train-

6



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

ing data. For our SVR implementation test having an optimal solution involves O(n2)
dot products, while solving the QP problem directly involves inverting the kernel matrix,
which has complexity O(n3) where n is the size of the training set. Hence the computa-
tional complexity of a K-iteration API−BRM� algorithm is dominated by three factors:
first computing Gram matrix of the Bellman kernel κ̃π(zi, zj) for each pair zi = (si, ai),
zj = (sj , aj), second solving the regularized regression problem and third the number
of policy iteration steps used by the algorithm. Computing κ̃π for each pair involves
enumerating each successor state of both si and sj and evaluating the base kernel κ(·, ·)
for each pair of successor states. Empirically me may define the average branching factor
of a finite state MDP β̂ as the average number of possible successor states for any state
s ∈ S which in the limit of continuous state space can be assumed β̂ = 1. Equivalently
given the dataset Dn and an estimation of the transition kernel P

s
′

s,a, the branching factor

β̂ is the average number of terms P s
′

s,a that are non zero given (s, a). Assuming we have a
n samples data set with an MDP that empirically presents an average branching factor
β̂ each state, computing a single element of the Gram matrix requires O(β̂2) operations.
Since the Gram matrix has dimension n × n but is symmetric there are n(n + 1)/2
unique elements that must be computed. Therefore, the total number of operations to
compute the full Gram matrix is O(kernel) = O(β̂2n(n+ 1)/2). Once the Gram matrix
is constructed in principle its inverse must be evaluated. Since the Gram matrix is posi-
tive definite and symmetric, Cholesky decomposition might be used, resulting in a total
complexity of O(n3).

As a result, the total complexity of the BRM� algorithm is O(n
3 + β̂2n(n + 1)/2)

and with K policy iteration steps we assume a complexity O(K(n3 + β̂2n(n + 1)/2)).
This is clearly a pessimistic result and thanks to the sparsity property of SVR, assuming
an average number of support vectors nsv � n one may obtain a posterior complexity
O(K(n3sv+ β̂

2nsv(nsv+1)/2)). For the incremental implementation of API−BRM� if we
consider the worst case, to add a new sample we need O(n3)·O(kernel) when all the train-
ing samples are support vectors (Martin, 2002). On average the algorithm has complexity

O(n2) and for the incremental API −BRM� we may assume O(K(n2 + β̂2n(n+ 1)/2)))
as complexity bound with an optimist posterior bound O(K(n̂2sv + β̂

2n̂sv(n̂sv + 1)/2)))
with n̂sv the average number of support vectors in K policy iteration.

7. API − BRM� Algorithm Implementation

Speed of learning depends mostly on the number of support vectors, influencing significa-
tively the performance. This is the first reason to implement an incremental version of
the API−BRM� algorithm. Another reason comes from the fact that this version of the
algorithm can be easily implemented in an online setting whenever the agent may interact
with the environment. After each incremental step (or at least after some of them) which
allows to implement a policy evaluation updating the approximation of the value func-
tion, one might perform the policy improvement updating the policy. In fact, differently
from the offline case where only the final performance matters, in online learning the
performance should improve once every few transition samples. Policy iteration can take
this requirement into account by performing policy improvements once every few transi-
tion samples, before an accurate evaluation of the current policy can be completed. In the
practical implementation of online API − BRM�, by using the current behavior policy
the algorithm collects its own samples interacting with the system. As a consequence
some exploration has to be added to the policy which becomes soft. As already mention
an �−greedy policy is a soft policy which for some 0 ≤ � ≤ 1 picks deterministically a
particular action with probability 1− � and a uniformly random action with probability

7



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

Algorithm 1 Online API −BRM� with �−greedy exploration
Require: (κ, λ, γ,KP , �k function)

l← 0 initialize Q̂0(s, a) (π̂0)
solve initial SVR:
Q1 ← API −BRM�(Π0, D0, κ, λ) (policy evaluation)
store initial next state policy Π0
measure initial state s0
for all time step k > 0 do

update exploration factor �k
choose action: ak = {πk(·) w.p. 1− �k ∨ random action w.p. �k}
apply ak and measure next state sk+1 and reward rk+1
update training sample set:
Dk ← Dk−1 ∪ (sk, ak, rk+1, sk+1)
update next state policy Πk ← Πk−1 ∪ πk(·, sk+1)
solve incremental SVR:
Q̂k ← IncrementalAPI −BRM�(Πk, Dk, κ, λ)
if k = (l + 1)KP then

update policy πl(·) ← π̂(·, Q̂k−1)
end if
l← l + 1

end for
return

�. Hence policy improvements have to be implemented without waiting for the action
value function estimates to get close to their asymptotic values for the current policy.
Henceforth these estimates have to be updated continuously without reset after some
policy changes as this in practice corresponds to having multiple policies. In principle
one may assume that value function estimates remain similar for subsequent policies or
al least do not change too much. Another possibility would be to rebuild the action value
function estimates from the scratch before every update (some sort of purge for the SVR).
Unfortunately this alternative can be computationally costly and might not be necessary
in practice. The number of transitions between consecutive policy improvements is a
crucial parameter of the algorithm and should not be too large, to avoid potentially bad
policies from being used too long. Hereafter we make the assumption that the policy π
induces a stationary β−mixing process on the MDP with a stationary distribution ν. The
β−mixing process starts from an arbitrary initial distribution and following the current
policy, the state probability distribution rapidly tends to the stationary distribution of
the Markov chain. To guarantee the convergence of the online version of API − BRM�
we additionally require that the samples follow the stationary distribution over state-
action pairs induced by the policy considered ρ. Intuitively this means that the weight
of each state-action pair (s, a) is equal to the steady-state probability of this pair along
an infinitely-long trajectory generated with the policy π. Moreover assuming that the
distribution ρ is stationary the resulting Bellman equation has a unique solution as the
Bellman operator is a contraction and it admits one unique fixed point. Consider now the
incremental online API − BRM� algorithm, the performance guarantees rely on small
policy evaluation errors assuming that the policy is improved before an accurate value
function is available. In practice this means that the policy evaluation error can be very
large and this might affects the performance. Nevertheless, the algorithm works fine in
practice for several standard RL benchmarks.

8



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

8. API − BRM� Experiments

API−BRM� algorithm was implemented using a combination of Matlab and C routines
and was tested on the following standard RL benchmarks: inverted pendulum, bicycle
balancing and riding. The simulation is implemented using a generative model capable of
simulating the environment while the learning agent is represented by the API −BRM�
algorithm. The domains are standard benchmark in RL literature featuring continuous
state spaces and non-linear dynamics. Moreover in our experiments, we compare perfor-
mance of API − BRM� algorithm with other learning methods such as Q-learning or
the parametric linear approximation architecture of LSPI algorithm implementations for
offline (Lagoudakis and Parr, 2003) and online (Busoniu, Lazaric, Ghavamzadeh, Munos,
Babuka, and Schutter, 2012). To rank performance it is necessary to introduce some met-
rics measuring the quality of the solutions. In each benchmark a specific goal is foreseen
and a performance measure is represented by the fulfillment of a given task. For example
for the inverted pendulum, and the bicycle domains we may assess the quality of a policy
through its ability to avoid crashing during a certain period of time. To measure the
quality of a solution we can use the stationary policy it produces, compute the expected
return and say that the higher this expected return is, the better the RL algorithm per-
forms. This can be done defining a set of initial states S0 where we compute the average
expected return of the stationary policy chosen independently from the set of tuples Dn.
Such kind of metric is called as the score of a policy (see (Daniel, Pierre, and Luis, 2005)

for more details). Given the learned policy π̂ its score is defined by Scoreπ̂ =
∑
s0∈S0

R̂π̂(s0)

|S0|
where R̂π̂(s0) is the empirical estimate of R

π̂(s) = E[
∑n−1

t=0 γ
tr(st, π̂(st))|s0 = s] the av-

erage return. In order to evaluate the score one has to estimate the average empirical
return for every initial state s0 ∈ S0 by Monte-Carlo simulations. As we consider all the
benchmarks non deterministic the average of the score in more than 10 different simu-
lations. Another important aspect to keep in mind is the rather large flexibility of our
method which is based upon the generalization ability of SVR and the use of incremental-
ity. Generalization relies on the statistical properties of the structural risk minimization
of SVR and the use of a suitable kernel function. In all our experiments for any pair
of zi = (si, ai) and zj = (sj , aj) we use the RBF kernel κ(zi, zj) = e

− 1
2
(zi−zj)TΣ2(zi−zj)

where Σ is a diagonal matrix specifying the weight for any state-action vector compo-
nent. Using this kernel also allows to manage possible variant of the problems where the
action space may be considered continuous or eventually noisy. Even though we studied
the statistical properties using finite action spaces, in practice the algorithm may also
works fine using a continuous action space. A part from the matrix Σ we also have to
define the SVR parameters (C, �). We performed an grid search to find the appropriate
set of parameters (Σ, C, �) looking at the resulting performance of the learning system.
In fact, using different set of parameters might help finding near-optimal policies whose
performance can be measured using the score or their ability to reach the goal. Finally,
another important aspect which may affect the performance of the algorithm is repre-
sented by the way we collect data and therefore how we manage the compromise between
the need of exploration and exploitation of the learned policy. Thanks to the flexibility
of our method, we may run experiments using two different methods:
• Method-1 (online API − BRM�): some data are generated offline using a random
behavior policy which produces a set D0 of tuples i.i.d. using eventually a set of different
initial states S0 or a fixed one s0. (This step can be also avoided setting directly Q = 0 in
each state). This data set is only used to initialize the algorithm solving the BRM� and
providing an initial approximation of the value function. Hence API −BRM� algorithm
proceed incrementally adding new experiences and improving the policy any KP steps
(with KP a tunable parameter) using an �−greedy policy. The exploration partly relies

9



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

Parameter Description Value UM

g gravity constant 9.8 m/s2

m pole mass 2.0 Kg
M cart mass 8.0 Kg
l pole length 0.5 m
α 1/(m+M) 0.1 Kg−1

dt simulation step 0.1 s
r reward 0/-1
γ discount factor 0.95

Table 1. Parameters used in the simulation for the inverted pendulum control problem

on the initial data set D0 and partly depends on the way we manage the exploration �.
Generally one may foresee an exponential decay for the � factor starting from some value
�0 ≤ 1 and when no further exploration is required fix a minimum value �∞ = 0.1. We
assume that the process underlying the collected data Dn follows an unknown β−mixing
distribution.
• Method-2 (online-growth API − BRM�): another possibility consists to alternate
explorative samples using a random behavior policy with exploitative samples every
Ke steps (with Ke a tunable parameter) and using the �−greedy policy learned with
a small exploration � . This can be considered an online variant of the batch-growth
method. We assume that the process underlying the collected data Dn follow an unknown
β−mixing distribution. Algorithm 6 illustrates the online variants of API−BRM� using
an �−greedy exploration policy. The algorithm allows the definition of two parameters
which are not present into the offline version: the number of transition KP ∈ N0 be-
tween consecutive policy improvements and the exploration schedule Ke. Policy is fully
optimistic whenever KP = 1 and the policy is updated after every sample while while is
partially optimistic with 1 < KP ≤ Kmax where in experiments we choose Kmax = 10.
Extensive study about how this parameters affects the quality of the solution for the
online LSPI can be found in (Busoniu, Ernst, De Shutter, and Babuska, 2010) which in
principle might apply to our method. The exploration schedule can be controlled by the
parameter Ke and the decay factor �decay which should be chosen not too large while a
significant amount of exploration is necessary. However several alternatives are possible
and we also implemented an online variant of the batch-growth method. In this way
we perform learning alternating exploration trials using some exploration factor �f with
exploitation trials using a �0. In practice this method works well and allows to easily
found local optimal solution starting from a fixed initial state s0 . Nevertheless if we
need to find a global optimal valid for any initial state, it becomes necessary to explore
trough the set of initial states and Method 1 or Method 2 must be applied while learning
becomes slower.

9. The Inverted Pendulum Control Problem

For the inverted pendulum benchmark, the control problem consists in balancing at the
upright position a pendulum of unknown length and mass. This can be done by applying
a force on the cart where the pendulum cart is attached to (Wang, Tanaka, and Griffin,
1996). Due to its simplicity but still challenging control task, this benchmark is widely
used to test the performance of state of the art methods for function approximation in
RL. In this version of the problem we only have one degree of freedom which can be
obtained by fixing the pole to an axis of rotation. The state space S \ ST = {(θ, θ̇) ∈
R2} is continuous and consists of the vertical angle θ and the angular velocity θ̇ of the
inverted pendulum and a terminal state ST described later. Three actions are allowed
A = {−am, 0, am} where am = 50N and some uniform noise in σa ∈ [−10, 10] might be
added to the chosen action. The transitions are governed by the non-linear dynamics of

10



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

simulation time  10 sec simulation time 50 sec

simulation time 200 sec simulation time 1000 sec

Figure 1. Inverted pendulum: representative subsequences of policy found by online API−BRM� using Method-1
(Actions are discretized and only three grey levels show up)

0 100 200 300 400 500 600 700 800 900 1000
0

500

1000

1500

2000

2500

3000

Number of training episodes

S
te

ps

Best

Worst

0 100 200 300 400 500 600 700 800 900 1000
-0.6

-0.5

-0.4

-0.3

-0.2

-0.1

     0

Simulation time(s)

S
co

re

0 100 200 300 400 500 600 700 800 900 1000
-0.6

-0.5

-0.4

-0.3

-0.2

-0.1

0

Simulation time (s)

S
co

re

0 100 200 300 400 500 600 700 800 900 1000
0

500

1000

1500

2000

2500

3000

Number of training episodes

S
te

ps

Worst

Best

Figure 2. Inverted pendulum: average score for offline LSPI (left) and Q-learning with experience replay (right)

from adapted from (Lagoudakis and Parr, 2003)

the system as:

θ̈ =
g sin(θ)−αml(θ̇)2 sin(2θ)/2−α cos(θ)u

4/3l−mα cos(θ)2 (2)

where θ is the angular position, m the mass of the pole, l the length of the pole, M the mass
of the cart, u = a+σa the control action with noise consisting in the acceleration applied
to the cart, g the gravity constant. The parameters of the model used in the simulation
are reported in Table 1. The angular velocity θ̇ is restricted to [−4π, 4π]rads−1 using
saturation. The discrete-time dynamics is obtained by discretizing the time between t
and t + 1 chosen with dt = 0.1s. If θt+1 is such that |θt+1| > θm a terminal state
ST = {|θ| > θm} is reached where we fixed θm = π/2. The reward function r(st, at) is
defined as r(st, at) = −1 if |θ| > θm while is zero otherwise. The discount factor γ has been
chosen equal to 0.95. The dynamical system is integrated by using an Euler method with a
0.001s integration time step. To generate data samples we may consider episodes starting
from the same initial state s0 = (θ0, θ̇0) or using a random initial state and stopping when
the pole leaves the region represented by S\ST meaning enter in a terminal states ST . In
(Lagoudakis and Parr, 2003) an analysis of the same benchmark is reported comparing
performance with offline LSPI and Q-learning. In this case simulation runs for 1000s

11



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

0 200 400 600 800 1000
−0.1

−0.09

−0.08

−0.07

−0.06

−0.05

−0.04

−0.03

−0.02

−0.01

0

0.01
Policy score

Simulation time (s)
S

co
re

0 200 400 600 800 1000
0

50

100

150

200

250

300

Average balancing time

Simulation time (s)

B
a

la
n

ci
n

g
 t

im
e

 (
s)

Figure 3. Inverted pendulum: (left) average score of online API − BRM� with KP = 10 over a grid of initial
states; (right) average balancing time over the same grid of initial states using Method-1

0 5 10 15 20 25 30 35 40 45

−1

0

1

time (s)

th
et

a

0 5 10 15 20 25 30 35 40 45
−5

0

5

time (s)

th
et

a−
do

t

0 5 10 15 20 25 30 35 40 45
−50

0

50

time (s)

ac
tio

nFigure 4. Inverted pendulum: States and actions in representative subsequences of learning trials. Each trial lasts
30s max considered as the minimum balancing to reach. Using Method-2 (online growth) with a fixed initial state
S0 = (0, 0) API −BRM� learns a local optimal policy in a few episodes (20s of simulation time).

(10000 samples) separated in 1000 trials of max 1s (10 samples) stopping eventually when
reaching a terminal state. Offline LSPI uses a linear approximation architecture with a set
of 10 Basis Functions (BFs) for each one of the 3 actions, thus a total of 30 basis functions,
to approximate the value function. These 10 BFs included a constant term and 9 RBF
functions arranged in a 3 × 3 grid over the 2−dimensional state space with BFi(s) =
e−‖s−µi‖

2/(2σ2) where µi are the 9 points of the grid {−π/4, 0,+π/4} × {−1, 0,+1} and
σ = 1. Training samples were collected starting in a randomly perturbed state very close
to the equilibrium state s0 = (0, 0) and following a policy that selected actions uniformly
at random. Results are shown in Figure 2 showing the performance in terms of balancing
steps from the analysis detailed in (Lagoudakis and Parr, 2003). Each episode was allowed
to run for a maximum of 300s (3000 steps) of continuous balancing. A run that balanced
for this period of time was considered to be successful. The optimal policy found by LSPI
is good enough using the initial state s0 = (0, 0) while is much worse with other initial
states. In our simulation of the same benchmark using online API − BRM� we run for
1000s of simulated time collecting around 10000 samples. Run was split into separate
learning episodes initiated at random initial states and stopping when a terminal state
has been reached or otherwise after 30s (300 steps). Policy improvement were performed
once every KP = 10 steps (0.1s) using an �−greedy policy with �0 = 1 and reaching a
value of �∞ = 0.1 after 350s.

We also used an RBF kernel with parameters Σ = I3σ with σ = 0.5 and the regression
parameters where chosen as C = 10 and � = 0.01 selected using an grid search. Figure 1
shows a subsequence of policies found during representative run taken after simulation
times t = 10s, 50s, 200s, 1000s. Clearly the generalization ability of the SVR makes
possible to capture the structure of the approximated policy only after 50s of simulation
time which closely resembles the final policy obtained after 1000s of simulation time.

12



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

ϕ
h

d +w Fcen
CM

ω

Mg

x-axis

ψ

ψgoal

contact

θ

T

front wheel

back wheel - ground

goal

(xb,yb)

frame of the bike

center of goal (coord. = (xgoal ,ygoal))

(a) (b)

Figure 5. The bike control problem: Figure (a) represents the bicycle seen from behind where the thick line

represents the bicycle. The center of mass of the bicycle+cyclist CM with height h from the ground, ω the angle
from vertical to bicycle while φ represents the total angle of tilt of CM. Action d is agent displacement and w is

some noise to simulate imperfect balance. Figure (b) represents the bicycle seen from above. θ is the angle the

handlebars are displaced from normal, ψ the angle formed by the bicycle frame and the x axis and ψgoal the angle
between the bicycle frame and the line joining the back-wheel ground contact and the center of the goal. T is the

torque applied by the cyclist to the handlebars. (xb, yb) is the contact point of the back-wheel with the ground

(from (Daniel et al., 2005))

0 100 200 300 400 500

−0.3

−0.25

−0.2

−0.15

−0.1

−0.05

0

Policy score

Simulation time (s)

S
co

re

0 100 200 300 400 500
0

50

100

150

200

250

300

350

400

450

500

550
Average balancing time

Simulation time (s)

B
a

la
n

ci
n

g
 t

im
e

 (
s)

Figure 6. Bike balancing: performance of online API −BRM� with KP = 10 using Method-1

Figure 1 shows representative subsequences of policy found by online API−BRM� using
Method-1. Figure 3 shows the performance of the final policy found by online API −
BRM� along the online learning process. The performance was measured evaluating the
score over a grid of initial states simulating balancing up to 300s (3000 steps). Using
Q-learning requires state discretization which seriously influences the performance and
it cannot estimate the state action value properly until the state is visited which slows
the learning. On the contrary the generalization property of SVR our algorithm can
estimate state values of unvisited states reasonably using the experience gained with
the other states. Moreover being a non parametric regression it can easily adapt to
different situation while in general parametric approximation may work well but in a
specific contest without the possibility to eventually adapt to changes. In Figure 4 we
report states and actions in subsequences of learning trials. Each trial lasts 30s max
considered as the minimum balancing to reach. Using Method-2 (online growth) with a
fixed initial state S0 = (0, 0) an optimal local approximation can be found in less then
30s of simulation time. Finally the number of support vectors necessary to represents the
approximate action value function with the set of parameters used in the approximation
usually stays below 5% of the total number of collected samples which is also a indication
of the quality of the approximation.

13



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

10. The Bike Balancing Control Problem

We consider the control problems related to a bicycle (Randlov and Alstrom, 1998)
moving at constant speed on a horizontal plane (Figure 5). For the bicycle balancing the
agent has to learn how to balance the bicycle. The system dynamics is composed of seven
state variables S \ ST = {(ω, ω̇, θ, θ̇, ψ) ∈ R5 | ω ∈ [−ωm, ωm] θ ∈ [−θm, θm] ωm =
π
15
rad θm =

π
2.25

rad} plus a terminal state ST . Four states are related to the bicycle
itself and three to the position of the bicycle on the plane. The state variables related
to the bicycle are ω, ω̇ (the angle and radial speed from vertical to the bicycle), θ, θ̇ (the
angle and radial speed the handlebars are displaced from normal). If |ω| > ωm the bicycle
has fallen down reaching a terminal state ST . The state variables related to the position
of the bicycle on the plane are the coordinates (xb, yb) of the contact point of the back
tire with the horizontal plane and the angle Ψ formed by the bicycle with the x-axis.
The actions space A = {(u, T ) ∈ {−0.02, 0, 0.02}×{−2, 0, 2}} is composed of 9 elements
and depends on the the torque T applied to the handlebars and the displacement d
of the rider. The noise in the system is a uniformly distributed term σdt = [−0.02, 0.02]
added to action d. The system has a continuous time dynamics described by the following
differential equations described in (Randlov and Alstrom, 1998) Details of the dynamic
can be found in (Randlov and Alstrom, 1998) and the various parameter with meanings
are also reported in Figure 5, The dynamic holds valid if |ωt+1| ≤ ωm while if |ωt+1| > ωm
the bicycle is supposed to have fallen down reaching a terminal state ST . We suppose
that the state variables (xb, yb) cannot be observed. Since these two state variables do
not intervene in the dynamics of the other state variables nor in the reward functions
considered. Hence they may be considered no relevant variables which does not make
the control problem partially observable. The reward function for the bicycle balancing
is such that zero rewards are always observed, except when the bicycle has fallen down
and in that case the reward is equal to -1. The value of the discount factor γ has been
chosen for both problems equal to 0.98. The dynamical system is integrated by using
an Euler method with a 0.001s integration time step. To generate data samples we
may consider episodes starting from the same initial state corresponding to the bicycle
standing and going in straight line with s0 = (ω0, ω̇0, θ0, θ̇0, ψ0) = (0, 0, 0, 0,Ψ0) with a
fixed value of Ψ or chosen at random Ψ0 ∈ [−π, π] and stopping when the bicycle leaves
the region represented by S \ ST meaning a terminal state ST . In our simulation of this
benchmark using online API−BRM� we run for 500s of simulated time collecting around
50000 samples. Run was split into separate learning episodes initiated at random initial
states s0 = (0, 0, 0, 0,Ψ0) with Ψ0 ∈ [−π, π] and stopping when a terminal state has
been reached or otherwise after 1s (100 steps). Policy improvement were performed once
every KP = 10 steps (0.1s) using an �−greedy policy with �0 = 1 and reaching a value of
�∞ = 0.1 after 200s. We also used an RBF kernel with parameters Σ = I7σ with σ = 1.5
and the regression parameters where chosen as C = 10 and � = 0.01 selected using an grid
search. Figure 6 shows the performance of the final policy found by online API−BRM�
along the online learning process. The performance was measured evaluating the score
over a grid of initial states S0 = {(0, 0, 0, 0,Ψ0} with Ψ0 ∈ [−π, π]. In Figure 7 we report
states and actions in subsequences of learning trials. Each trial lasts 50s max (5000 steps)
considered as the minimum balancing to reach the goal. Using Method-2 (online growth)
with a fixed initial state S0 = (0, 0, 0, 0,Ψ0) an optimal local approximation can be found
in less then 50s of simulation time. In the lower part of Figure 7 we also show some of
the trajectories during the learning process as well as the final one. Finally the number of
support vectors necessary to represents the approximate action value function with the
set of parameters used in the approximation usually stays below 5% of the total number
of collected samples which is also a indication of the quality of the approximation.

14



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

A

B C

Figure 7. Bike balancing: (Upper A) States and actions in representative subsequences of learning trials. Each
trial lasts 50s max (5000 steps) considered sufficient reach the goal. Using Method-2 (online-growth) with small

perturbations of a fixed initial state S0 = (0, 0, 0, 0, π/2) API − BRM� may learn a local optimal policy in a
few episodes (50s of simulation time). (Lower) sketch of the trajectory (B zoom, C overall) in the time interval
(0, 500s) for the bicycle on the (xb, yb) plane controlled by the final policy of API −BRM�

11. Conclusions

We developed a model free BRM approach called API −BRM� able to find the optimal
policy in continuous state RL problems and studied practical implementation issues. In
particular, we demonstrated how the problem of finding the optimal policy minimizing
the Bellman residuals can be cast as a regression problem using SVR and an appropri-
ate RKHS. The main contribution of this work is the experimental analysis of a non
parametric approximation algorithm for the generalization problem in RL using policy
iteration and kernel methods. The algorithm eventually converges to the optimal policy
using β−mixing distributed data samples. Some interesting properties of API −BRM�
algorithm are: API −BRM� is quite efficient for problems where sampled experience is
sparse. The algorithm is based on approximate policy iteration, a very powerful frame-
work met with success mostly among planning problems. It also open new research direc-
tions for the use of kernel based approximate policy iteration in the context of learning.
API −BRM� is an API algorithm making a good use of function approximation implic-

15



January 28, 2015 Journal of Experimental & Theoretical Artificial Intelligence main

itly constructing an approximate model using kernels. API − BRM� uses incremental
SVR which allows for the estimation and approximation of state action value functions
in RL. The policy iteration can be done implicitly any time a new experience is obtained.
API −BRM� complexity strongly depends on the cost one has to pay in order to solve
the SVR which is essentially a quadratic problem optimization. SVR can be solved in
batch mode when the whole set of training sample are at disposal to the learning agents
or in incremental mode enabling the addition or removal of training samples effectively.
Finally it comes with a theoretical bound on his performance and statistical convergence
guarantee.

Acknowledgment

This work was partially supported by the FI-DGR programme of AGAUR
ECO/1551/2012.

References

Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
In Proceedings of the Twelfth International Conference on Machine Learning, pages 30–37.
Morgan Kaufmann, 1995.

L. Busoniu, D. Ernst, B. De Shutter, and R. Babuska. Online least-squares policy iteration for
reinforcement learning control. In US Baltimore, editor, In Proceedings of American Control
Conference ACC-10, pages 486–491, 2010.

Lucian Busoniu, Alessandro Lazaric, Mohammad Ghavamzadeh, Remi Munos, Robert Babuka,
and Bart Schutter. Least-squares methods for policy iteration. In Marco Wiering and Mar-
tijn Otterlo, editors, Reinforcement Learning, volume 12 of Adaptation, Learning, and Opti-
mization, pages 75–109. Springer Berlin Heidelberg, 2012. ISBN 978-3-642-27644-6. . URL
http://dx.doi.org/10.1007/978-3-642-27645-3_3.

Ernst Daniel, Geurts Pierre, and Whenkel Luis. Tree based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503–556, 2005.

Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine
Learning Research, 4:1107–1149, 2003. URL http://dblp.uni-trier.de/db/journals/
jmlr/jmlr4.html#LagoudakisP03.

Mario Martin. On-line support vector machine regression. In Proceedings of the 13th European
Conference on Machine Learning, ECML ’02, pages 282–294, London, UK, UK, 2002. Springer-
Verlag. ISBN 3-540-44036-4. URL http://dl.acm.org/citation.cfm?id=645329.650050.

Jette Randlov and Paul Alstrom. Learning to drive a bycicle using reinforcement learning an
shaping. In Proceeding of the fifth International Conference on Machine Learning, pages 463–
471, 1998.

H.O. Wang, K. Tanaka, and M.F. Griffin. An approach to fuzzy control of nonlinear systems:
stability and design issues. Fuzzy Systems, IEEE Transactions on, 4(1):14–23, Feb 1996.

16


