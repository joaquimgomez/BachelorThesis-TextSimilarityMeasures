









































tcds-acevedovalle-2699578-proof.pdf


IE
EE

 P
ro

of

IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 1

Autonomous Discovery of Motor Constraints in an
Intrinsically Motivated Vocal Learner

Juan Manuel Acevedo-Valle, Cecilio Angulo, and Clement Moulin-Frier

Abstract—This paper introduces new results on the modeling of1
early vocal development using artificial intelligent cognitive archi-2
tectures and a simulated vocal tract. The problem is addressed3
using intrinsically motivated learning algorithms for autonomous4
sensorimotor exploration, a kind of algorithm belonging to the5
active learning architectures family. The artificial agent is able6
to autonomously select goals to explore its own sensorimotor7
system in regions, where its competence to execute intended8
goals is improved. We propose to include a somatosensory system9
to provide a proprioceptive feedback signal to reinforce learn-10
ing through the autonomous discovery of motor constraints.11
Constraints are represented by a somatosensory model which12
is unknown beforehand to the learner. Both the sensorimotor13
and somatosensory system are modeled using Gaussian mixture14
models. We argue that using an architecture which includes a15
somatosensory model would reduce redundancy in the sensori-16
motor model and drive the learning process more efficiently than17
algorithms taking into account only auditory feedback. The role18
of this proposed system is to predict whether an undesired colli-19
sion within the vocal tract under a certain motor configuration20
is likely to occur. Thus, compromised motor configurations are21
rejected, guaranteeing that the agent is less prone to violate its22
own constraints.23

Index Terms—Active learning, early vocal development,24
Gaussian mixture models (GMMs), intrinsic motivations, sen-25
sorimotor exploration.26

I. INTRODUCTION27

IN RECENT years, there has been an increasing interest28 in using robots to perform daily life activities in the29
presence of humans. As robot–human interactions become30
common then human-like communication systems become31
more relevant to robotics. Speech is one of the most studied32
communication systems because it allows human-spoken lan-33
guage. However, as mentioned in [1], the idea that speech is a34
deeply encrypted “code” prevails among the speech specialists35

Manuscript received July 19, 2016; revised November 18, 2016 and
February 20, 2017; accepted April 14, 2017. This work was supported by
the PATRICIA Research Project through the Spanish Ministry of Economy
and Competitiveness under Grant TIN2012-38416-C03-01. The work of
J. M. Acevedo-Valle was supported by CONACYT under Grant 216872.
(Corresponding author: Juan Manuel Acevedo-Valle.)

J. M. Acevedo-Valle and C. Angulo are with the GREC Research
Group, Universitat Politècnica de Catalunya, 08028 Barcelona, Spain (e-mail:
juan.manuel.acevedo.valle@upc.edu).

C. Moulin-Frier was with Flowers team, Inria/ENSTA-Paristech,
33405 Bordeaux, France. He is now with the SPECS Laboratory, Universitat
Pompeu Fabra, 08018 Barcelona, Spain.

Color versions of one or more of the figures in this paper are available
online at http://ieeexplore.ieee.org.

Digital Object Identifier 10.1109/TCDS.2017.2699578

and cracking this code is still an unsolved problem. Some of 36
the mysteries about speech might be solved if we are able to 37
understand all the mechanisms underlying early speech acqui- 38
sition in children. Thus, this paper, provides new results to 39
contribute to the study of early speech development using 40
machines. 41

Developmental robotics is a relatively novel approach, it 42
aims at understanding and modeling the role of developmental 43
processes in the emergence of complex behaviors, including 44
social ones. Its goal is twofold, on the one hand it is used to 45
build more efficient cognitive machines applying developmen- 46
tal theories, and on the other hand it also provides insights into 47
human developmental mechanisms, especially during infancy. 48
A deeper understanding of these mechanisms would explain 49
how human beings develop from infancy to functional adults 50
capable of solving highly complex cognitive tasks [2]. 51

Autonomous robot design could notably benefit from 52
the available knowledge of biological science and self- 53
organization theories [3]. Deep understanding of the embod- 54
iment paradigm is paramount to integrate that knowledge 55
into robotics. This paradigm is also well represented by 56
the quote “understanding by building” [4]. It states that the 57
behavior of an agent is not only the result of a system 58
control structure, but also a result of complex interactions 59
with its ecological niche, its morphology, and its material 60
properties [3], [4]. 61

In this paper, language emergence is studied according 62
to behavioral and neurophysiological evidence, moreover the 63
role of motor constraints is especially considered. The main 64
assumption is that early vocal development can be studied 65
as a result of embodiment, self-organization, and emergence 66
mechanisms produced by human evolution. In general, studies 67
have shown that infants show preparedness to acquire natural 68
language. Motor, perceptual, social, and learning ability con- 69
straints, and their maturation during infant development play 70
a key role in the emergence of language [1]. 71

Equally important, machine learning techniques have 72
rapidly evolved, providing developmental robotics with 73
interesting approaches as active learning. In contrast to the 74
more usual passive learning algorithms, active learning data 75
are collected in order to minimize a given property of the 76
learning process, e.g., the uncertainty [5] or the prediction 77
error [6] of a model. This family of algorithms is of partic- 78
ular interest for developmental robotics. During sensorimotor 79
exploration they allow the agent to focus on parts of the sen- 80
sorimotor space in which exploration is expected to improve 81
the quality of the learned model [7]. 82

2379-8920 c© 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

mailto:juan.manuel.acevedo.valle@upc.edu
http://ieeexplore.ieee.org
http://www.ieee.org/publications_standards/publications/rights/index.html


IE
EE

 P
ro

of

2 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

The contribution of this paper is extending the study83
of early language development using intrinsically motivated84
exploration algorithms. Herein, we provide new simulation85
results showing the suitability of these algorithms in the self-86
exploration of sensorimotor vocal spaces. The theoretical basis87
of the probabilistic models used to represent knowledge is also88
provided. Furthermore, we propose an architecture that could89
be used to study the role of constraints during sensorimotor90
exploration in embodied agents. Finally, it is worth mentioning91
that the learning algorithm presented herein could be applied92
to any system subjected to constraints in order to improve93
learning progress.94

The remainder of this paper is organized as follows.95
Section II introduces related works. Section III highlights96
the role of intrinsic motivations and proprioceptive feedback97
in vocal development. The experiment setup is described98
in Section IV and results are presented and discussed in99
Section V. Finally, the conclusions are presented in Section VI.100

II. RELATED WORK101

This paper revisits and expands the investigation introduced102
in [8] and [9]. In [9], an intrinsically motivated exploration103
architecture was proposed for the study of the developmental104
stages emergent during the early vocal development of infants.105
For the experimentation the simulated ear-vocal tract model106
DIVA [10] was used. In spite of the relevance of its results,107
the motor constraints and the somatosensory system were108
neglected in [9]. However, morphological constraints play a109
key role in speech acquisition. Therefore, a new exploration110
algorithm proposed in [8] to incorporate motor constraints111
awareness using a somatosensory model. In the past, some112
studies have tried to explain the emergence of developmental113
stages during the vocal development, assuming their exis-114
tence, but those stages were bridged using hard-coding for115
experimentation [10]–[13].116

In [14], an approach for inverse kinematics learning in117
redundant systems was presented. It was demonstrated that118
goal babbling can be advantageous in learning in the early119
stages of development, as observed in developmental theo-120
ries. In parallel, [15] presented an intrinsically motivated goal121
exploration approach for the active learning of inverse mod-122
els. This approach was applied to the vocal sensorimotor space123
exploration in [9] and [16]. The algorithm considered in this124
paper extends intrinsically motivated exploration in the goal125
space to include motor constraints. Considering both motor126
and perceptual constraints during learning and exploration is127
crucial to design cognitive architectures for motor control.128

Among the efforts to model the acquisition of speech there129
is the DIVA model [10]. It aims to imitate the underlying130
neurophysiological mechanisms for speech acquisition and131
production. The cognitive architecture of the system is an132
artificial neural network. The model includes the premotor,133
motor, auditory and somatosensory cortical areas, and simu-134
lated ear-vocal tract system. In [10], the somatosensory model135
was effectively integrated into the acquisition and production136
of speech processes. It was not used as an element to integrate137
motor constraints but as an extra source of sensory-feedback.138

The ear-vocal tract component of the DIVA model is used in 139
this paper, as it was in [8] and [9]. 140

Finally, another interesting contribution was the active learn- 141
ing architecture presented in [17] which considered time con- 142
straints. This paper proposed a music performance imitation 143
scenario and implemented a learning architecture able to learn 144
a musical instrument model and a body capabilities model; the 145
architecture is also able to imitate a sequence of sound, while 146
simultaneously kinematic errors, due to the control architec- 147
ture, are corrected. Similar to [9], models employed in [17] 148
were based on Gaussian mixture models (GMMs). 149

III. EARLY VOCAL DEVELOPMENT IN MACHINES 150

Human speech production is one of the most complex motor 151
acts performed by any living being [18]. Producing a linguis- 152
tic message that can be understood by another human requires 153
coordinating many degrees of freedom in the respiratory, 154
laryngeal, and supraglottal articulatory system. 155

How infants acquire the complex ability to control speech 156
production and in general how they learn language remains a 157
matter of research [1]. It has been pointed out that strong 158
regularities can be observed in the structure of the vocal 159
development process independently of interindividual differ- 160
ences [1], [19]. In general, the infant first discovers how to 161
control phonation, then focuses on vocal variations of unartic- 162
ulated sounds and finally automatically discovers and focuses 163
on babbling with articulated proto-syllables. In [18], some 164
experiments suggested that goals of speech movements are 165
auditory in nature and maintenance of motor command maps 166
to auditory results is performed with auditory feedback. 167

It is important to inquire into the developmental assump- 168
tions considered in the experiments in [8] and [9], as this 169
paper is based on those experiments. Regarding the infant 170
development stages mentioned in [1], our experiments con- 171
sider the developmental stage known as canonical babbling 172
(CB) [20] and the beginning of language-specific speech pro- 173
duction [1]. Results suggest that during CB infants learn to 174
control their ear-vocal tract system based on auditory feed- 175
back. Nevertheless, when infants begin to babble they do it 176
regardless of the audibility of their vocalizations. CB could 177
be the result of a natural tendency of infants to move their 178
body parts rhythmically motivated by sensory feedback [20]. 179

Consistent with the theory, we assumed a simplified expla- 180
nation that the artificial agent is exploring its ear-vocal tract 181
system choosing auditory goals and evaluating the result. 182
Therefore, our cognitive architecture allows the agent to 183
explore regions, where the competence to produce intended 184
sounds is improved. However, we also endow the agent with 185
autonomous mechanisms to discover constraints in order to 186
drive the exploration. To accomplish that objective, previously 187
proposed active learning architectures and the proprioceptive 188
feedback concept are combined. 189

A. Intrinsically Motivated Exploration Architectures 190

Among the vast number of active learning architectures, 191
this paper considers the exploration architecture proposed 192



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 3

by [15]. This architecture reproduces the formalism of intrin-193
sic motivation inspired by psychological literature as proposed194
previously in [21] and [22]. Using goal babbling, intrinsi-195
cally motivated exploration aims to minimize the error of an196
agent to reach self-generated goals measured according to a197
competence function. This architecture allows artificial agents198
to efficiently and actively explore and generate maps from199
motor capacities to perceived results. Therefore, exploration200
occurs over regions in which agents perceive they are becom-201
ing more competent to reach self-generated goals. Intrinsically202
motivated exploration architectures were originally designed203
to actively learn inverse models of high-dimensional input–204
output spaces. This architecture was later extended by [9] to205
study self-organization in early vocal development stages in206
infants and robots.207

Intrinsically motivated learning algorithms have shown208
favorable results in previous experiments to learn sensorimotor209
coordination skills in redundant nonlinear high-dimensional210
mappings which share many mathematical properties with211
vocal spaces. Moulin-Frier et al. [9] used a simulated ear-212
vocal tract system to study the emergence of developmental213
stages implementing intrinsically motivated exploration. They214
argued that the development of the agent self-organizes into215
vocal developmental sequences. The results presented therein216
opened the door to a new approach in vocal development217
to be explored. This paper introduces a methodology which218
enhances intrinsically motivated architectures with constraint219
awareness.220

B. Proprioceptive Feedback221

Some of the most adopted theories of speech state that222
speech production is organized in terms of motor control sig-223
nals and their associated vocal tract configurations which has224
been corroborated by several experimental results [23], [24].225
Nevertheless, we adopt the simplified hypothesis that speech226
goals are defined acoustically and maintained by auditory227
feedback [18]. CB is a rhythmic behavior that, with some dif-228
ferences, emerges in both, normally developing infants and229
infants with hearing loss. When infants start to babble, they230
do it regardless of the audibility result (i.e., they produce audi-231
ble and voiceless vocalizations). However, evidence suggests232
that, around the onset of CB, infants learn to vocalize based233
on auditory feedback [1], [20].234

How the somatosensory system1 affects the ear-vocal tract235
exploration is an open question that was not previously236
approached in [9]. However, the relevance of the somatosen-237
sory system for speech has been shown in different exper-238
iments, for instance the results in deaf individuals suggest239
that somatosensory inputs related to movement play a more240
important role in speech production than what was thought241
before [25], [26]. Furthermore, the fact that CB also emerges242
in deaf infants suggests that somatosensory feedback must play243
a more relevant role during the prelinguistic vocal development244
in infants [27].245

1Strictly, the somatosensory system is also a sensorimotor system. In future
works, we will distinguish two sensorimotor systems: 1) the auditory-motor
system and 2) the somatosensory-motor system.

Fig. 1. Examples of articulatory configurations that produce collisions in the
DIVA vocal tract model.

In [28], a robotic device able to generate patterns of facial 246
skin deformation related to certain speech productions was 247
used. The results showed that when the facial skin is stretched 248
whilst subjects are listening to words, the sounds they hear are 249
altered. Thus, theory and results suggest that the somatosen- 250
sory system is involved in speech perception. Following 251
this hypothesis, improvements can made to the experiments 252
proposed in [9] by including a somatosensory system to endow 253
the learner with physical constraint awareness. 254

In [8], the foundations of a simplified architecture were 255
established allowing us to include physical constraints to the 256
learning process through a proprioceptive signal, similar to 257
the ability to feel pain in humans. The open source DIVA 258
model2 [10] provides a synthesizer that represents the human 259
vocal tract and ear systems. The DIVA model also includes 260
a somatosensory system, but in spite of it, there is a lack of 261
physical constraints in the DIVA vocal tract. The absence of 262
constraints allows the execution of motor commands that lead 263
to collisions or articulatory superpositions. Both circumstances 264
lead to no phonation and moreover, the latter is a contradictory 265
result since it lacks physical sense, as shown in Fig. 1. 266

To overcome the drawbacks caused by the lack of con- 267
straints, we introduce a somatosensory system. This new 268
element, not considered in [9], is based on an area function 269
which is a vector descriptor of the vocal tract shape. It con- 270
sists of a mechanism that evaluates if an exploratory motor 271
command produces a collision or superposition of articulatory 272
tissues, the system generates a proprioceptive signal. Using the 273
data generated with this mechanism, the agent builds a map 274
from motor commands to proprioceptive results. This map is 275
used to predict which motor commands may lead to undesired 276
collisions, so they may be rejected, forcing the agent to choose 277
a new auditory goal. In the next section, this mechanism is 278
explained in detail. 279

IV. PROPOSED ARCHITECTURE 280

The experimental architecture proposed in this paper to 281
study the early vocal development in machines is shown 282
in Fig. 2, where five elements interact. These elements are 283

2http://www.bu.edu/speechlab/software/diva-source-code/



IE
EE

 P
ro

of

4 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

Fig. 2. Experimental architecture. It is composed by five interacting modules,
two of them contained within the ear-vocal tract module (the sensorimotor
system and the somatosensory system).

introduced below and explained in detail in the coming284
sections.285

1) Sensorimotor system is a simulated ear-vocal tract. It286
corresponds to the physical properties of the embodied287
agent. For the present work the ear-vocal tract system288
of the DIVA model [10] is used.289

2) Somatosensory system is a perceptual mechanism that290
evaluates the shape of the vocal tract. It generates a291
proprioceptive feedback signal indicating if an undesired292
contact or collision is produced into the vocal tract.293

3) Sensorimotor model is a mathematical representation of294
the vocal tract-ear model. It endows the artificial agent295
to map motor commands to auditory effects using the296
data collected from the agent’s own vocalizations.297

4) Somatosensory model is a mathematical representation298
that maps motor configurations to their likely proprio-299
ceptive feedback to acquire self-awareness of its own300
physical constraints in order to avoid executing motor301
configurations that produce undesired behaviors.302

5) Interest model for auditory goals allows the agent to303
actively choose auditory goals in order to improve304
the quality of its sensorimotor model based on a cer-305
tain measure of competence. This model represents306
the core of the intrinsically motivated sensorimotor307
self-exploration.308

A. Sensorimotor System309

The DIVA vocal tract configuration is determined by the310
position of ten articulators and three phonation parameters.311
Along this paper, only seven articulators and two phonation312
parameters (voicing and glottal pressure) are considered [9].313
Articulators and voicing parameter motor dynamics are mod-314
eled as overdamped second order systems315

ẍ+ 2ζω0ẋ+ ω20(x− m) = 0 (1)316
with ζ = 1.01 and ω = (2π/0.8) representing the damping317
factor and the natural frequency, respectively. The duration of318
each vocal experiment in seconds is 0.8, whereas m and x rep-319
resents the desired articulator position (motor command) and320
the current articulator position, respectively. During each vocal321
experiment two different motor commands are introduced for322
each of the seven articulators and the two voicing parameters:323
one for the 0–250 ms window and another for the remain-324
ing time. Thus, each motor command is represented by an325

Fig. 3. Vocalization experiment structure. The upper plot shows the articu-
latory trajectories, from 0 to 250 ms, the commands for Art1, Art2, and Art3
are set to 2, 0, and 2, respectively, whereas the glottal pressure and voic-
ing are both set to 0.5. From 250 to 800 ms, the commands for Art1, Art2,
and Art3 are set to −3, 0, and 2, respectively, whereas the glottal pressure
and voicing are both set to 0.7. The remaining motor commands are set to
zero. The middle plot represents the speech sound wave signal. The bottom
plot shows the auditory trajectories. The dotted outlined boxes represent the
perception time windows from 250 to 400 ms and the second from 650 to
800 ms. The auditory output s are determined from the average of each tra-
jectories along each one of the time windows. Whereas the proprioceptive
feedback p is determined by the average value of min(af ).

18-D vector. The auditory output of a human vocalization can 326
be described by its formant frequencies. We consider the first 327
two formant frequencies, F1 and F2, along with an intonation 328
signal I. The intonation signal is 1 when phonation occurs 329
and 0 otherwise, two conditions are required for phonation 330
to occur: 1) the area function af of the vocal tract must be 331
positive elsewhere and 2) the voicing and pressure parameters 332
must be positive. The area function is a vector function that 333
describes the transversal shape of the vocal tract. 334

During the vocalization the auditory output of the system 335
is observed along two time windows, the first from 250 to 336
400 ms and the second from 650 to 800 ms. The value of each 337
auditory output is averaged for each time window, the result 338
is a 6-D output signal (two formants and the intonation, hence 339
three values, per each of the two time windows). In Fig. 3, 340
we reproduce the vocalization representation shown in [9]. To 341
be consistent with the co-articulated nature of speech, only 342
two perceptual windows are used [1]. However, since only two 343
portions of the vocalization are considered, a lot of information 344
is lost. For instance, it is shown in [23] the continuum of co- 345
articulated gestures. Therefore, future works should consider 346
studying the continuum of speech gestures and self-structuring 347
of vocalizations. 348

B. Somatosensory System 349

In Fig. 3, it is shown that the area function af is observed 350
during both perception time windows. The minimal value of 351
the area function min(af ) would be zero when the vocal tract 352
is closed at any point and negative values mean that some tis- 353
sues are overlapped, which does not have physical meaning. 354
However, in some cases it might be interpreted as the tongue 355
being bitten. In other cases it might represent high pressure 356
between the tongue and the palate, which might be interesting 357



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 5

to the learner in a realistic scenario, where motor constraints358
are not violated. In general, we made a strong assumption that359
any motor constraint violation over a threshold is uncomfort-360
able or painful. Hence, the average value of min(af ) in each361
perception time window is used to generate a proprioceptive362
feedback signal p: if the average of min(af ) is lower than a363
threshold for any perception window, then the configuration364
is evaluated as a undesired collision with p = 1, and p = 0365
otherwise.366

C. Sensorimotor Model367

GMMs are linear combinations of multivariate Gaussian368
distributions that represent clusters of data. They have369
been previously used to represent nonlinear redundant370
maps [17], [21], [29] in order to solve the inverse problem371
of inferring input motor commands from desired sensory out-372
puts. GMMs can be learned using an online variant of the373
expectation-maximization (EM) algorithm in order to learn374
incrementally from incoming data [30]. Here, the algorithms375
used to train GMMs are based on the open source tools3376
associated with [30], and modified according to our problem377
requirements. The three models in the experimental setup are378
probabilistic representations in the form of GMMs, obtained379
using data collected from experiments with the DIVA ear-vocal380
tract. A detailed explanation of the GMMs training is provided381
below.382

We assume that an n-dimensional input command space383
X ∈ Rn is mapped to an m-dimensional output space Y ∈ Rm,384
through a transform function y = f (x) + ε, where y ∈ Y ,385
x ∈ X and ε is random noise. When a dataset of couples386
(x, y) is available, the EM-algorithm is used to obtain a GMM387
which is defined by the parameters {πj, μj, �j}Kj=1, where πj,388
μj, and �j are, respectively, the prior probability, the distribu-389
tion centroid and the covariance matrix of the jth Gaussian, for390
j = 1, 2, . . . , K, being K the number of Gaussian components.391
From [30], Gaussian mixture regression (GMR) is applied to392
compute the conditional probability distribution P(X |y) in the393
input space X given a desired output y. Once it is computed,394
the value x∗ ∈ X is selected such that it maximizes P(X |y).395

To obtain the input x that maximizes the probability to pro-396
duce the output y, the GMR process first defines the partitioned397
vector z ∈ X × Y , where398

z =
(

x
y

)
. (2)399

For each Gaussian j in the GMM the partitions400

μj =
(

μxj
μ

y
j

)
and �j =

(
�xj �

xy
j

�yx �
y
j

)
(3)401

are considered to compute the conditional probability distribu-402
tion Pj(X |y) ∼ Nj(μ̂j, �̂j) in the input space X given a desired403
output y, where404

μ̂j = μyj +�yxj
(
�

x
j

)−1(
x− μxj

)
, �̂j = �yj +�yxj

(
�

x
j

)−1
�

xy
j .405

(4)406

3http://www.calinon.ch/sourcecodes.php

Considering that P(X |y) is at its maximum when x = x̂j = μ̂j, 407
then a natural selection for x in order to produce y is x̂j. But 408
we have K candidates for x, hence it is necessary to compute 409
the probability of the vector ẑj = [x̂j, y]T belonging to its 410
generator Gaussian as 411

P
(
ẑj
) = πj 1√

(2π)K |�j|
e
− 12

(
(ẑj−μj)T�−1j (ẑj−μj)

)
(5) 412

and finally the point z∗ = ẑj that maximizes P(ẑj) is selected as 413
the point that better fits the model. In other words, according 414
to our prior knowledge of f (x), z∗ ∈ f (x), we infer that the 415
output y is generated by x̂j. 416

Taking into account the above for the sensorimotor model, 417
an 18-D motor command space M, with m ∈ M, is defined 418
for the vocal tract articulatory configuration. A 6-D auditory 419
output space S, with s ∈ S, is also defined, the agent being able 420
to observe s according to s = f (m)+σ , where σ ∼ N(0, 0.01) 421
is Gaussian noise. The aim is to find a GMM that solves the 422
inverse problem m = f−1(sg), where sg is an auditory goal. 423

We define a GMM, GSM, to model the sensorimotor system, 424
with X = M and Y = S. Such a model allows computation 425
of the inverse model P(M | sg) using GMR. At the beginning 426
of the experiment, m is selected either, randomly or accord- 427
ing to the interest for initializing the inverse sensorimotor 428
model m ∼ f−1(sg) ∼ P(M | sg) around a specific region of 429
the sensorimotor space. After the initialization stage, the agent 430
starts to select new auditory goals, according to the interest 431
model explained below. In order to reduce memory storage 432
requirements, we consider a generative method for the train- 433
ing stage, which means that the model is trained using the last 434
NSM samples obtained from experimentation along with 435

Nold =
⌈

(1− α)NSM
α

⌉
samples (6) 436

generated using GSM, where α ∈ [0, 1] is the forgetting rate. 437

D. Interest Model for Auditory Goals 438

The interest model for auditory goals endows the learner the 439
ability to select goals that maximize the expected competence 440
progress in order to improve the quality of its sensorimotor 441
model, resulting in better control over it. It is derived from 442
the model proposed in [9]. The competence value for a goal 443
is defined by 444

c = e−|sg−s| (7) 445
where sg is the auditory goal and s is the actual auditory pro- 446
duction after executing a motor command m ∼ P(M | sg). To 447
construct the interest model, the auditory goal space is aug- 448
mented with two extra dimensions: 1) the competence c ∈ C 449
and 2) time tag t ∈ T . The number of vocalizations NIM con- 450
sidered to build the interest model is fixed. A GMM, GIM, with 451
KIM components will be computed from the 8-D data set with 452
NIM samples of the augmented goal space. To initialize this 453
model, some auditory results from the initialization of GSM 454
are selected as the first auditory goals sg. 455

Those Gaussian components in GIM that, according to the 456
covariance matrices �j, contain goals that will likely increase 457

http://www.calinon.ch/sourcecodes.php


IE
EE

 P
ro

of

6 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

the competence progressively are considered to build a prob-458
abilistic distribution P(S) over the auditory space. In order to459
build P(S), the components in GIM are weighted according to460
their time-competence covariance magnitudes. Thus, P(S) will461
prioritize goals in regions, where competence is expected to462
increase. Finally, a sample sg is drawn from P(S) for the next463
vocalization experiment. Model training is performed every464
time the agent has performed nIM experiments, using the last465
NIM vocalizations.466

E. Somatosensory Model467

For the somatosensory model we consider the 18-D motor468
command space M, with m ∈ M, and a new binary propriocep-469
tive output space P = {0, 1}, with p ∈ P. If a vocal production470
leads to undesired contacts, then p = 1, otherwise p = 0. A471
map g is assumed to exist such that p = g(m) and the agent472
can observe p for each vocal experiment. Thus, it is possible473
to find a GMM GSS, with X = M and Y = P, that allows474
computation of the probability distribution P(P |m) applying475
GMR, and determine when a motor command m is likely to476
lead to an undesired collision in the vocal tract.477

The inverse sensorimotor model GSM and the somatosensory478
model GSS are initialized together. When an auditory goal sg479
has been selected, m is computed using P(M | sg). Next, to480
predict the value of p, P(P |m) is used. If the prediction sug-481
gests that m will produce p = 1 then sg is rejected, otherwise482
sg and m are accepted. If sg is rejected, then GIM(S) is recom-483
puted without considering the Gaussian component in GIM that484
generated sg, this mechanism decreases the prior of the con-485
flicting Gaussian in GIM. The new GIM(S) is used to select a486
new goal sg, and the process is repeated until sg is accepted.487
During the agent’s life, the model GSS is trained when GSM is488
trained using the previously described generative mechanism.489

F. Self-Exploration Algorithm490

The self-exploration architecture with motor constraints491
self-awareness, first proposed in [8] for ear-vocal tract explo-492
ration, is an extended version of [9]. The algorithm associated493
with the cognitive architecture is shown in Algorithm 1. Our494
extended self-exploration algorithm with goal babbling and495
motor constraints self-awareness starts with the learner having496
no experience in vocalizing. Models GSM and GSS are initial-497
ized using random vocalizations with small values around the498
neutral position of the articulators. The neutral position of the499
pressure and voicing parameters are set to −0.25 to produce500
no phonation, whereas for the articulators it is considered 0,501
i.e., the rest position. Model GIM is also initialized.502

Then, in line 6 of Algorithm 1 the vocal learner agent selects503
a goal sg for the next experiment according to the probabilis-504
tic distribution P(S) and the motor command m is obtained505
using the inverse model GSM in line 7. The main feature of506
this algorithm, different from similar architectures, is that in507
line 8 P(P | m) provides a prediction for p that indicates if508
the selected motor command is likely to produce an undesired509
collision. From line 9 to 11, if p ≈ 1, the goal is rejected510
and the probabilistic distribution P(S) is updated, ignoring the511

Algorithm 1 Self-Exploration With Goal Babbling and Self-
Constraints Awareness

1: Initialize GSM and GSS
2: Initialize GIM and i← 1
3: while i in [1, 1e5] do
4: ptmp ← 1
5: while ptmp do
6: sg,i ← GIM(S)
7: mi ← GSM

(
M|sg,i

)
8: ptmp ← GSS(P|mi)
9: if ptmp then

10: update(GIM(S))
11: end if
12: end while
13: si ← f (mi)+ σ and pi ← g(mi)
14: ci ← e−|sg,i−si|
15: i← i+ 1
16: if i mod NSM = 0 then
17: train

(
GSM, m(i−NSM+1:i), s(i−NSM+1:i)

)
18: train

(
GSS, p(i−NSM+1:i), s(i−NSM+1:i)

)
19: end if
20: if i mod nIM = 0 then
21: train

(
GIM, sg,(i−NIM+1:i), c(i−NIM+1:i)

)
22: end if
23: end while

Gaussian component in GIM that generated sg and the algo- 512
rithm goes back to line 6. Otherwise, p ≈ 0, both, sg and m are 513
accepted. Next, the motor command is executed with the vocal 514
tract and the agent observes s and p in line 13. In line 14, the 515
learner evaluates the competence value c. It also checks if we 516
are at the end of a learning episode, so models GSM, GSS, and 517
GIM are updated in lines 17, 18, and 21, respectively. To pro- 518
vide objective evaluation elements, some experiments without 519
considering the somatosensory model for choosing goals are 520
also presented. In this later case, sg is always accepted, thus 521
line 4 is substituted with ptmp = 0 in Algorithm 1. 522

V. EXPERIMENTAL RESULTS 523

Eighteen independent simulations using Algorithm 1 were 524
run. All simulations consisted of half a million of vocaliza- 525
tions, including an initial vocalization set of 1000 random 526
samples. Nine different random seeds were considered to 527
generate the same number of motor command sets from a 528
uniform distribution. The limits for those motor commands 529
related to the vocal tract articulators were [−1, 1], whereas 530
for motor commands related to the phonation parameters were 531
[0, 0.7]. Each set was used twice to initialize simulations of 532
Algorithm 1, first without using the somatosensory model and 533
second with it. 534

Considering as a reference the parameters used for simula- 535
tions in [8] and [9], a few variations in their values were tested. 536
First, when values for KSM or KSS are increased the inference 537
error decreases slightly but the computation time grows con- 538
siderably. On the other hand if these values are decreased, the 539
inference error increases considerably. Second, if the training 540



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 7

Fig. 4. Mean competence evolution during simulations using Algorithm 1
for nine different initialization data sets. Moving average of 5000 samples are
considered to filter the results of each simulation. Results are shown in the
case of proprioceptive and nonproprioceptive agents.

steps are increased for the somatosensory model and the senso-541
rimotor model, then the training computational time increases542
as Nold in (6) increases proportionally, but no improvement543
is obtained in the inference error. However, if these values544
are decreased beyond the values used in [9], the mean infer-545
ence error increases. Third, when αSM and αSS are larger than546
0.1 the competence progress is slower. Finally, the parame-547
ters linked to the interest model allow a wider range of values548
to be chosen obtaining similar results. For KIM we observed549
that values greater than or equal to 12 worked similarly, but550
smaller values negatively impacted the competence progress.551
Thus, the main parameters for all the simulations were kept552
as in [8] and [9] as they performed better than other sim-553
ulations in terms of exploration results and simulation time.554
Summarizing, values were set to KSM = 28, NSM = 400,555
KSS = 28, KIM = 12, NIM = 4800, nIM = 12, and the con-556
tinuous sampling time used for the DIVA ear-vocal tract was557
ts = 10 ms. The forgetting rate parameter αSM for GSM starts558
from 0.1 and decreases logarithmically to 0.05 after half a559
million of vocalizations. On the other hand, αSS for GSS was560
chosen to be 0.05 through the whole simulation.4561

During the simulation, GSM and GSS are initialized as indi-562
cated in line 1 of Algorithm 1 with the initial motor command563
sets. Then, all the initial phonatory productions are used as564
auditory goals to initialize the interest model GIM as indicated565
in line 2 of Algorithm 1. In this stage, GSM is used to infer the566
motor commands that will likely produce the initial auditory567
goals. These commands are executed without considering the568
proprioceptive prediction p.569

A. On Competence and Contacts570

First of all, Fig. 4 represents the evolution of the compe-571
tence parameter c in (7) for self-generated auditory goals. To572
obtain this plot, first the result of each simulation is filtered573
using a 5000 samples moving average window. Next, simu-574
lations are divided into two general groups: 1) proprioceptive575
agents and 2) nonproprioceptive agents. Finally, the mean of576

4Supplementary downloadable material provided by the authors is available
at https://dx.doi.org/10.6084/m9.figshare.c.3676645.v1. After each experi-
ment, 20 random samples from the last 1000 vocalizations were drawn to
generate videos with audio. Videos of the experiments 1, 5, and 9 are provided.

(a)

(b)

Fig. 5. Algorithm 1 simulation results. (a) Mean percentage of vocalizations
producing undesired collisions considering all the simulated agents. Agents are
grouped by proprioceptive and nonproprioceptive. The results of each agent
are prefiltered considering a 5000 samples moving average. (b) Mean num-
ber of rejected goals using the proprioceptive prediction and considering all
the simulated proprioceptive agents. The results of each agent are prefiltered
considering a 5000 samples moving average.

TABLE I
RESULTS CONSIDERING ALL DATA FROM EXPLORATION

all the filtered results for each group is computed. The same 577
mechanism is considered to obtain the percentage of contacts 578
observed in Fig. 5(a). 579

Tables I and II show the volume of a convex hull cov- 580
ering the explored auditory region. They also display the 581
mean competence and the percentage of undesired contacts 582
at the end of the simulation. First, in Table I, descriptors are 583
computed considering all the vocalization during each simula- 584
tion. Second, in Table II, figures were computed considering 585

https://dx.doi.org/10.6084/m9.figshare.c.3676645.v1


IE
EE

 P
ro

of

8 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

TABLE II
RESULTS WITHOUT CONSIDERING VOCALIZATIONS

WITH UNDESIRED CONTACTS

vocalizations without undesired collisions. Convex hulls pro-586
vide an insight regarding the size of the explored regions in587
the auditory-space. They are computed considering formant588
frequency dimensions F11, F21, F12, and F22.589

In Fig. 4, results suggest that proprioceptive agents perform590
better than those which are not endowed with proprioception.591
We observe that at the beginning of the exploration the mean592
average competence is very similar for both groups. However,593
after the initialization the nonproprioceptive agents suffer an594
important decrement of the competence, which also coincides595
with a significant increment of the percentage of contacts in596
Fig. 5(a). Table I also confirms the expected results accord-597
ing to our hypothesis: using proprioceptive feedback drives598
artificial agents to produce significantly fewer undesired con-599
tacts and also increases the competence to reach self-generated600
auditory goals.601

Some observers might ask the reason of high competence602
values at the beginning of the simulations. We argue that it is603
an expected result as the competence computation begins when604
GIM is initialized with self-generated goals drawn from the ini-605
tial auditory productions of the agent. In other words, GSM and606
GSS models are initialized around a set of initial vocalizations607
and auditory productions. The initial auditory productions are608
selected as auditory goals, then motor commands are computed609
with a sensorimotor model that represents very well those ini-610
tialization samples. Later, as the agent explores the auditory611
space and it moves toward farther regions from those of initial-612
ization, the competence values might slightly decrease. This613
is due to the incremental learning of probabilistic models and614
depends on the values assigned to the forgetting rates αSM and615
αSS. If these forgetting rates are close to zero, then the agent616
is less prone to update its knowledge when new data are far617
from the current knowledge. On the other hand, if the forget-618
ting rates are high, then the agent will adapt its model to the619
new data very fast but also it will forget faster its previous620
knowledge since it is not reinforced.621

We also argue that one of the reasons the propriocep-622
tive agents perform better is the null competence produced623
by nonphonatory vocalizations. The nonproprioceptive agent624
produces much more undesired contacts (four times more625

Fig. 6. Mean percentage of vocalizations producing phonatory result along all
the simulations per each group, proprioceptive and nonproprioceptvie agents.
The percentage of phonatory auditory goal is also shown per each group.
Note: red and blue solid lines are overlapped.

undesired contacts on average) and, therefore, has more non- 626
phonatory vocalizations as corroborated in Fig. 6. Fig. 6 was 627
obtained using the same procedure than Fig. 4. It shows the 628
mean percentage of phonatory goals and actual phonatory 629
vocalizations considering all the simulations per each group 630
of artificial agents. In general, all the auditory goals through 631
the simulations are phonatory. The reason is that nonphonatory 632
goals become uninteresting very early in the artificial agent’s 633
life as they are very easy to be produced. Thus, all those 634
nonphonatory vocalizations which produce null competence 635
impact negatively the average competence. We also might 636
think about all those nonphonatory vocalizations as a waste 637
of energy during the exploration. Knowing which regions of 638
the motor space are leading to collisions might be a relevant 639
knowledge for the agent. However, as the nonproprioceptive 640
agents keep exploring conflicting regions, the proprioceptive 641
agents avoid exploitation of these regions due to their ability to 642
predict somatosensory results from a given motor command. 643

Additionally, discussion about the tradeoff between explo- 644
ration and exploitation can be detailed. We argue that pro- 645
prioceptive agents show a better performance with respect to 646
exploitation, as agents avoid exploring uninteresting regions 647
with high number of contacts. In other words, propriocep- 648
tion, and in general constraint awareness, contributes to the 649
agent finding regularities faster and then fosters specialization 650
in regions of the auditory space, where the agent competence 651
to reach self-generated goals is higher. It is worth mentioning 652
that we are aware that it is also important to include the social 653
factor in the learning development of the artificial agent, in 654
order to better understand the role of proprioception in social 655
learning. In social learning, exploration is not just driven by 656
the progress in competence and discovery of constraints, but 657
also by the relevance of auditory goals for socialization pur- 658
poses. These studies leading to more exploring behaviors is 659
left for future work. 660

Furthermore, Fig. 5(b) shows the mean number of goals 661
rejected by the proprioceptive mechanism, represented in 662
lines 5–12 of Algorithm 1. In this plot, we prefilter the results 663
for proprioceptive agents considering a 5000 samples mov- 664
ing average for visualization purposes. It can be observed in 665
Fig. 5(b) that in general the proprioceptive mechanism is more 666
active at the beginning of the simulations presumably due to 667
the quantity of contacts along the initial set of vocalizations. 668



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 9

Thus, we might deduce that proprioception prevents the agent669
from further exploration in regions that are producing unde-670
sired contacts especially in the early stages. In the next, we671
introduce some figures in order to show the implications over672
the shape of the explored auditory region when proprioception673
is considered.674

B. On Explored Regions675

Regarding the volume of the explored region, Table I676
indicates that the ratio of average volume of convex hulls677
described by the explored regions in the frequency space is678
0.51/0.48 between the nonproprioceptive and proprioceptive679
agents, whereas the ratio of the mean competence is 0.53/0.71.680
In other words, whereas proprioceptive agents explore a 5.88%681
tighter region than the nonproprioceptive, their performance682
is 25.35% better than the later ones. On the other hand,683
Table II considers only the vocalizations without undesired684
contacts. A shrinkage of the convex hulls is observed, the685
ratio of average volumes is, in this case, 0.40/0.39 while686
the mean competence ratio is 0.67/0.75. From these numbers687
we observe that, in general, the competence to vocalizations688
without undesired contacts is higher for both kinds of agents.689
However, regarding competence the proprioceptive agents still690
perform 11.94% better than the nonproprioceptive agents.691

Based on Table I, we selected three different initial sets692
given their simulation results in order to produce Figs. 7–9.693
First, we select initial set 1, as it performs better in terms694
of competence when proprioception is considered. Second, to695
contrast with initial set 1 we select initial set 5, as its proprio-696
ceptive agent performs the worst with regards to the percentage697
of undesired contacts. Finally, we select initial set 9, as its698
proprioceptive agent produced the largest convex hull volume.699

Figs. 7–9 show some projections of vocalizations distribu-700
tion maps of the auditory productions generated along the701
simulation. Points in the plots are colored according to the702
percentage of undesired contacts produced in its neighbor-703
hood. Specifically, three projections are shown for each of704
the selected sets of initial vocalizations. Projection F1,1F2,1705
represents the auditory fingerprint of the vocalizations in the706
first perceptual window. Projections F1,2F2,2 is similar to the707
first projection but for the second perceptual window. Finally,708
projection I1I2 represents the value of the intonation parameter709
in the first perceptual window against the same parameter in710
the second perceptual window.711

Distributions in Figs. 7–9 indicate that the intonation param-712
eter projection I1I2 is the most influenced sensory-output due713
to the proprioceptive feedback. Recall that I1 and I2 depend714
on the average audibility of the vocalization which is null in715
two cases: 1) when the voicing parameters are lower than zero716
or 2) when the area function of the vocal tract is nonpositive717
elsewhere. Therefore, keeping in mind the latter case, those718
vocalizations producing an intonation parameter (either I1 or719
I2) lower than one indicate that a contact has likely occurred.720
If a contact occurs, there are two possible results, the average721
of the minimum value of the area function might be nega-722
tive or not. If it is negative, then the contact is classified as an723
undesired contact and the proprioceptive signal takes the value724

Fig. 7. Projections of vocalizations distribution along simulations using initial
set 1 with Algorithm 1. Results for nonproprioceptive agent (left) and propri-
oceptive agent (right). Points are colored according to the total percentage of
undesired contacts in their neighborhood.

one. Thus, having both values lower than one at the same time 725
is even more likely to produce undesired contacts. That is the 726
reason proprioceptive agents explore less intensively the mid- 727
dle of the region in the intonation space. However, we argue 728
that in spite of the low density of vocalizations in that region, 729
proprioceptive agents succeed in finding more vocalizations 730
that produces nonconflicting articulatory configurations in that 731
region. For instance, looking at the projections in Fig. 9, the 732
proprioceptive agent almost covers all the intonation space 733
with low density of contacts. 734

Moreover, comparing proprioceptive and nonproprioceptive 735
agents in Figs. 7–9, we observe that the area of the explored 736
regions varies slightly due to the proprioceptive mechanism. 737
This fact is supported by Tables I and II. In general, in most 738
of the cases using the proprioceptive feedback results in a 739
slightly smaller explored region but this is not a conservative 740
fact. For instance, Table I indicates that the convex hull vol- 741
umes described in the auditory space by the experiments 2, 742
4, and 9 were larger when proprioception was considered. In 743
general, besides a certain degree of randomness due to our 744
probabilistic approach, we argue that there are three main ele- 745
ments that determine the shape of the explored region: 1) the 746
initial set of vocalizations; 2) evolution of competence; and 747
3) proprioception. 748

Regarding the initial set, our criteria to choose random 749
vocalizations close to the neutral positions produces rich sets 750
of phonatory vocalizations, either with contacts or without. 751



IE
EE

 P
ro

of

10 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

Fig. 8. Projections of vocalizations distribution along simulations using initial
set 5 with Algorithm 1. Results for nonproprioceptive agent (left) and propri-
oceptive agent (right). Points are colored according to the total percentage of
undesired contacts in their neighborhood.

As we are working with self-generated goals, the agent is752
expected to be very good at the beginning at reaching goals753
when auditory goals are close to the initialization region. In754
the case of nonproprioceptive agents, the only parameter that755
drives the exploration is the evolution of competence, which756
is why we observe plenty of areas with a huge amount of757
undesired contacts in the plots of nonproprioceptive agents758
with respect to the proprioceptive agents. Furthermore, we759
observe that proprioception might lead toward two different760
situations: 1) an unexplored auditory region or 2) explored761
region but with nonconflicting articulatory configurations. For762
instance, in Fig. 7, specifically in the projection F1,1F2,1, we763
observe that in general the nonproprioceptive agents produce764
a lot of undesired contacts over almost the whole explored765
region. On the contrary, the proprioceptive agent explores a766
smaller region over the same projection, however it achieves767
a considerably lower density of undesired contacts; results also768
supported by the convex hull volume displayed in Table I. In769
addition, projection F1,2F2,2 shows similar explored regions770
for both agents. Indeed, the proprioceptive agent explored a771
wider region in that projection and was capable of finding non-772
conflicting vocalizations for some of the regions, where the773
nonproprioceptive agent produces a lot of undesired contacts.774
We observe, in general, for all the agents in Figs. 7–9, that775
producing auditory results for the projection F1,2F2,2 close to776
the origin is hard without producing contacts.777

Fig. 9. Projections of vocalizations distribution along simulations using initial
set 9 with Algorithm 1. Results for nonproprioceptive agent (left) and propri-
oceptive agent (right). Points are colored according to the total percentage of
undesired contacts in their neighborhood.

In Fig. 8, corresponds to the agent with the worst results 778
using proprioception regarding the number of undesired con- 779
tacts, the projection F1,1F2,1 indicates that the proprioceptive 780
agent has explored a smaller region than the nonproprioceptive 781
agent. However, if we observe the boundaries of the explored 782
region with proprioception, they coincide with regions where 783
the nonproprioceptive agent produces a high amount of unde- 784
sired contacts. Thus, the proprioceptive mechanism does not 785
allow the proprioceptive agent to exploit those regions. In spite 786
of less exploration, we observe that in the explored region 787
where both agents intersect, the proprioceptive agent produces 788
less undesired contacts. Looking at Table II, the results for 789
the agent corresponding to Fig. 8 (experiment 5), we observe 790
that the explored regions with and without proprioception are 791
described by convex hulls with similar volume. This suggests 792
that the conflicting region explored by the nonpropriocep- 793
tive agent prevents the agent from exploiting regions without 794
undesired contacts. Thus, the agent achieve lower competence 795
values over the latter regions. On the other hand, the propri- 796
oceptive agent avoids conflicting regions, in consequence it 797
produces 15% less contacts and achieve a higher competence 798
average. 799

In addition, looking at the projection F1,1F2,1 Fig. 9, we 800
observe that the proprioceptive agent explores a larger region. 801
Moreover the density of contacts along the explored region is 802
significantly lower. This is also supported by the numerical 803
results in Tables I and II. On the other hand, in the projection 804



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 11

Fig. 10. Density distribution computed using Gaussian-kernels over all the
data obtained along the simulations considering the first principal component
with a variance contribution ratio of 0.68.

Fig. 11. Density distribution computed using Gaussian-kernels over the data,
excluding vocalizations with undesired contacts, obtained along the simula-
tions considering the first principal component with a variance contribution
ratio of 0.61.

F1,2F2,2 the exploration close to the origin of that projection805
is less intensive in the proprioceptive agent, which reinforces806
the previous observation of the difficulties to produce audi-807
tory results in that region without contacts, similar results are808
observed in Fig. 8. Future work also must focus in the study809
of what is happening in that region and how relevant it is to810
language, as well as modify the system accordingly.811

Finally, in order to observe the differences between the812
vocalization distributions obtained using the different explo-813
ration algorithms, we perform a sample density analysis over814
the formant frequency dimensions. In order to make the results815
easier to visualize we perform a principal component analysis816
(PCA) procedure. We consider analyzing the data twice, first,817
considering all the data collected along the exploration and818
second, without considering the vocalizations with undesired819
contacts. The PCA is done considering the dimensions F1,1,820
F2,1, F1,2, and F2,2, the data of all the 18 simulations are con-821
catenated and used to perform the PCA. The PCA considering822
all the samples is performed and the first component is kept,823
which contributes to the variance with a ratio of 0.61. A sec-824
ond PCA is performed considering only the non conflicting825
vocalizations, again only the first component is kept, since it826
contributes to the variance with a ratio of 0.68. Once PCA827

transforms 4-D data into 1-D data, kernel-distribution estima- 828
tion is performed using Gaussian-kernels according to [31] for 829
the proprioceptive and nonproprioceptive cases. 830

In Figs. 10 and 11, we can observe the density distributions 831
obtained separately with all the proprioceptive and the nonpro- 832
prioceptive agents. First in Fig. 10, the distribution considering 833
all the data obtained from all the experiments is shown. In gen- 834
eral, it is observed that the agents explored similar regions, 835
but with different intensity. In Fig. 11, we observe the distri- 836
bution of the first component obtained from the PCA when 837
only nonconflicting vocalizations are considered. In the latter 838
case it is observed that regarding the regions which are of 839
interest, in other words the regions where physical constraints 840
are not violated, both kinds of agents explore with a simi- 841
lar density shape, which means that even though both agents 842
explore similar interesting regions, the proprioceptive agents 843
achieve in general higher competence. 844

VI. CONCLUSION 845

An application of active learning techniques applied to the 846
study of vocal exploration considering motor constraints has 847
been introduced. It has been presented as an intrinsically moti- 848
vated sensorimotor self-exploration architecture with motor 849
constraints self-awareness. Constraints awareness is achieved 850
by providing a proprioceptive mechanism which endows an 851
artificial agent with the capacity to autonomously generate a 852
somatosensory model. This model is then used to predict the 853
consequences of a motor action and to avoid its execution if 854
it is expected to generate an undesired proprioceptive result. 855

The proprioceptive mechanism improved the quality of 856
learning according to a competence function. However, we 857
observe a tradeoff between exploration and exploitation, 858
predominantly nonproprioceptive agents achieve greater explo- 859
ration in the auditory space. In contrast, we observe a more 860
intensive exploitation in interesting regions driving to the 861
higher competence values achieved by proprioceptive agents. 862
In general, vocal-auditory spaces are high dimensional redun- 863
dant spaces, thus an auditory output may be produced by 864
different articulatory configurations. Some of these articula- 865
tory configurations may lead to undesired contacts. Hence, we 866
argue that sensorimotor redundancy is reduced when proprio- 867
ception is included in the system allowing the agent to focus on 868
exploitation of nonconflicting vocalizations. In consequence, 869
the sensorimotor model generated through the exploration does 870
not include conflicting regions, where constraint violations are 871
likely to happen. For that reason, sensorimotor models achieve 872
better fitting to the regions of interest where constraints are 873
met. In this way, we showed how sensorimotor exploration, 874
and in general sensorimotor knowledge, can be shaped by 875
constraints. 876

Regarding the advance toward vocal exploration, we have 877
showed the suitability of the presented architecture to learn 878
vocal spaces in interesting and less redundant regions as chil- 879
dren might do. However, in order to continue our research 880
on early vocal development, we must study in greater depth 881
the first period of vocalization development. A deeper anal- 882
ysis of the learning processes underlying the nonauditory 883



IE
EE

 P
ro

of

12 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

development related to mastication, deglutition, and crying884
from the cognitive and developmental perspectives should be885
completed in order to generate more complex somatosensory886
architectures. Finally, the next step of this paper should be887
directed toward the self-structuring of vocalization and social888
learning.889

REFERENCES890

[1] P. K. Kuhl, “Early language acquisition: Cracking the speech code,” Nat.891
Rev. Neurosci., vol. 5, no. 11, pp. 831–843, 2004.892

[2] M. Asada et al., “Cognitive developmental robotics: A survey,” IEEE893
Trans. Auton. Mental Develop., vol. 1, no. 1, pp. 12–34, May 2009.894

[3] R. Pfeifer, M. Lungarella, and F. Iida, “Self-organization, embodi-895
ment, and biologically inspired robotics,” Science, vol. 318, no. 5853,896
pp. 1088–1093, 2007.897

[4] R. Pfeifer and C. Scheier, Understanding Intelligence. Cambridge, U.K.:898
MIT Press, 1999.899

[5] D. A. Cohn, Z. Ghahramani, and M. I. Jordan, “Active learning with900
statistical models,” J. Artif. Intell. Res., vol. 4, no. 1, pp. 129–145, 1996.901

[6] S. Thrun, “Exploration in active learning,” in Handbook of Brain Science902
and Neural Networks, 1995, pp. 381–384.903

[7] C. Moulin-Frier and P.-Y. Oudeyer, “Exploration strategies in develop-904
mental robotics: A unified probabilistic framework,” in Proc. Int. Conf.905
Develop. Learn. (ICDL/Epirob), Osaka, Japan, 2013, pp. 1–6.906

[8] J. M. Acevedo-Valle, C. Angulo, N. Agell, and C. Moulin-Frier,907
“Proprioceptive feedback and intrinsic motivations in early-vocal devel-908
opment,” in Proc. 18th Int. Conf. Catalan Assoc. Artif. Intell., 2015,909
pp. 9–18.910

[9] C. Moulin-Frier, S. M. Nguyen, and P.-Y. Oudeyer, “Self-organization911
of early vocal development in infants and machines: The role of intrin-912
sic motivation,” Front. Psychol., vol. 4, pp. 1006–1025, Jan. 2014,913
doi: 10.3389/fpsyg.2013.01006.914

[10] F. H. Guenther, S. S. Ghosh, and J. A. Tourville, “Neural modeling915
and imaging of the cortical interactions underlying syllable production,”916
Brain Lang., vol. 96, no. 3, pp. 280–301, 2006.917

[11] A. S. Warlaumont, G. Westermann, E. H. Buder, and D. K. Oller,918
“Prespeech motor learning in a neural network using reinforcement,”919
Neural Netw., vol. 38, pp. 64–75, Feb. 2013.920

[12] B. J. Kröger, J. Kannampuzha, and C. Neuschaefer-Rube, “Towards a921
neurocomputational model of speech production and perception,” Speech922
Commun., vol. 51, no. 9, pp. 793–809, 2009.923

[13] I. S. Howard and P. Messum, “Modeling the development of pronun-924
ciation in infant speech acquisition,” Motor Control, vol. 15, no. 1,925
pp. 85–117, 2011.926

[14] M. Rolf, J. J. Steil, and M. Gienger, “Goal babbling permits direct927
learning of inverse kinematics,” IEEE Trans. Auton. Mental Develop.,928
vol. 2, no. 3, pp. 216–229, Sep. 2010.929

[15] A. Baranes and P.-Y. Oudeyer, “Active learning of inverse models with930
intrinsically motivated goal exploration in robots,” Robot. Auton. Syst.,931
vol. 61, no. 1, pp. 49–73, 2013.932

[16] C. Moulin-Frier and P.-Y. Oudeyer, “Learning how to reach various933
goals by autonomous interaction with the environment: Unification934
and comparison of exploration strategies,” in Proc. 1st Multidiscipl.935
Conf. Reinforcement Learn. Decis. Making (RLDM), Princeton,936
NJ, USA, Oct. 2014, Art. no. hal-00922537. [Online]. Available:937
https://hal.inria.fr/hal-00922537/document938

[17] A. Ribes, J. Cerquides, Y. Demiris, and R. Lopez de Mántaras, “Active 939
learning of object and body models with time constraints on a humanoid 940
robot,” IEEE Trans. Cogn. Develop. Syst., vol. 8, no. 1, pp. 26–41, 941
Mar. 2016, doi: 10.1109/TAMD.2015.2441375. 942

[18] J. Perkell et al., “The sensorimotor control of speech production,” 943
in Proc. 1st Int. Symp. Meas. Anal. Model. Human Functions, 2001, 944
pp. 359–365. 945

[19] D. K. Oller and R. E. Eilers, “The role of audition in infant babbling,” 946
Child Develop., vol. 59, no. 2, pp. 441–449, 1988. 947

[20] K. Ejiri, “Relationship between rhythmic behavior and canonical 948
babbling in infant vocal development,” Phonetica, vol. 55, no. 4, 949
pp. 226–237, 1998. 950

[21] P.-Y. Oudeyer, F. Kaplan, and V. V. Hafner, “Intrinsic motivation systems 951
for autonomous mental development,” IEEE Trans. Evol. Comput., 952
vol. 11, no. 2, pp. 265–286, Apr. 2007. 953

[22] J. Gottlieb, P.-Y. Oudeyer, M. Lopes, and A. Baranes, “Information- 954
seeking, curiosity, and attention: Computational and neural mecha- 955
nisms,” Trends Cogn. Sci., vol. 17, no. 11, pp. 585–593, 2013. 956

[23] B. Galantucci, C. A. Fowler, and M. T. Turvey, “The motor theory of 957
speech perception reviewed,” Psychonomic Bull. Rev., vol. 13, no. 3, 958
pp. 361–377, 2006. 959

[24] J.-L. Schwartz, A. Basirat, L. Ménard, and M. Sato, “The perception- 960
for-action-control theory (PACT): A perceptuo-motor theory of speech 961
perception,” J. Neurolinguist., vol. 25, no. 5, pp. 336–354, 2012. 962

[25] S. Tremblay, D. M. Shiller, and D. J. Ostry, “Somatosensory basis of 963
speech production,” Nature, vol. 423, no. 6942, pp. 866–869, 2003. 964

[26] S. M. Nasir and D. J. Ostry, “Speech motor learning in profoundly deaf 965
adults,” Nature Neurosci., vol. 11, no. 10, pp. 1217–1222, 2008. 966

[27] S. N. Iyer and D. K. Oller, “Prelinguistic vocal development in infants 967
with typical hearing and infants with severe-to-profound hearing loss,” 968
Volta Rev., vol. 108, no. 2, pp. 115–138, 2008. 969

[28] T. Ito, M. Tiede, and D. J. Ostry, “Somatosensory function in speech 970
perception,” Proc. Nat. Acad. Sci. USA, vol. 106, no. 4, pp. 1245–1248, 971
2009. 972

[29] C. Moulin-Frier and P.-Y. Oudeyer, “The role of intrinsic motivations in 973
learning sensorimotor vocal mappings: A developmental robotics study,” 974
in Proc. Interspeech, Lyon, France, 2013, pp. 1268–1272. 975

[30] S. Calinon, Robot Programming by Demonstration. Lausanne, 976
Switzerland: EPFL Press, 2009. 977

[31] D. W. Scott, Multivariate Density Estimation: Theory, Practice, and 978
Visualization. Hoboken, NJ, USA: Wiley, 2015. 979

Juan Manuel Acevedo-Valle, photograph and biography not available at the 980
time of publication. 981

Cecilio Angulo, photograph and biography not available at the time of 982
publication. 983

Clement Moulin-Frier, photograph and biography not available at the time 984
of publication. 985

https://hal.inria.fr/hal-00922537/document


IE
EE

 P
ro

of

IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 1

Autonomous Discovery of Motor Constraints in an
Intrinsically Motivated Vocal Learner

Juan Manuel Acevedo-Valle, Cecilio Angulo, and Clement Moulin-Frier

Abstract—This paper introduces new results on the modeling of1
early vocal development using artificial intelligent cognitive archi-2
tectures and a simulated vocal tract. The problem is addressed3
using intrinsically motivated learning algorithms for autonomous4
sensorimotor exploration, a kind of algorithm belonging to the5
active learning architectures family. The artificial agent is able6
to autonomously select goals to explore its own sensorimotor7
system in regions, where its competence to execute intended8
goals is improved. We propose to include a somatosensory system9
to provide a proprioceptive feedback signal to reinforce learn-10
ing through the autonomous discovery of motor constraints.11
Constraints are represented by a somatosensory model which12
is unknown beforehand to the learner. Both the sensorimotor13
and somatosensory system are modeled using Gaussian mixture14
models. We argue that using an architecture which includes a15
somatosensory model would reduce redundancy in the sensori-16
motor model and drive the learning process more efficiently than17
algorithms taking into account only auditory feedback. The role18
of this proposed system is to predict whether an undesired colli-19
sion within the vocal tract under a certain motor configuration20
is likely to occur. Thus, compromised motor configurations are21
rejected, guaranteeing that the agent is less prone to violate its22
own constraints.23

Index Terms—Active learning, early vocal development,24
Gaussian mixture models (GMMs), intrinsic motivations, sen-25
sorimotor exploration.26

I. INTRODUCTION27

IN RECENT years, there has been an increasing interest28 in using robots to perform daily life activities in the29
presence of humans. As robot–human interactions become30
common then human-like communication systems become31
more relevant to robotics. Speech is one of the most studied32
communication systems because it allows human-spoken lan-33
guage. However, as mentioned in [1], the idea that speech is a34
deeply encrypted “code” prevails among the speech specialists35

Manuscript received July 19, 2016; revised November 18, 2016 and
February 20, 2017; accepted April 14, 2017. This work was supported by
the PATRICIA Research Project through the Spanish Ministry of Economy
and Competitiveness under Grant TIN2012-38416-C03-01. The work of
J. M. Acevedo-Valle was supported by CONACYT under Grant 216872.
(Corresponding author: Juan Manuel Acevedo-Valle.)

J. M. Acevedo-Valle and C. Angulo are with the GREC Research
Group, Universitat Politècnica de Catalunya, 08028 Barcelona, Spain (e-mail:
juan.manuel.acevedo.valle@upc.edu).

C. Moulin-Frier was with Flowers team, Inria/ENSTA-Paristech,
33405 Bordeaux, France. He is now with the SPECS Laboratory, Universitat
Pompeu Fabra, 08018 Barcelona, Spain.

Color versions of one or more of the figures in this paper are available
online at http://ieeexplore.ieee.org.

Digital Object Identifier 10.1109/TCDS.2017.2699578

and cracking this code is still an unsolved problem. Some of 36
the mysteries about speech might be solved if we are able to 37
understand all the mechanisms underlying early speech acqui- 38
sition in children. Thus, this paper, provides new results to 39
contribute to the study of early speech development using 40
machines. 41

Developmental robotics is a relatively novel approach, it 42
aims at understanding and modeling the role of developmental 43
processes in the emergence of complex behaviors, including 44
social ones. Its goal is twofold, on the one hand it is used to 45
build more efficient cognitive machines applying developmen- 46
tal theories, and on the other hand it also provides insights into 47
human developmental mechanisms, especially during infancy. 48
A deeper understanding of these mechanisms would explain 49
how human beings develop from infancy to functional adults 50
capable of solving highly complex cognitive tasks [2]. 51

Autonomous robot design could notably benefit from 52
the available knowledge of biological science and self- 53
organization theories [3]. Deep understanding of the embod- 54
iment paradigm is paramount to integrate that knowledge 55
into robotics. This paradigm is also well represented by 56
the quote “understanding by building” [4]. It states that the 57
behavior of an agent is not only the result of a system 58
control structure, but also a result of complex interactions 59
with its ecological niche, its morphology, and its material 60
properties [3], [4]. 61

In this paper, language emergence is studied according 62
to behavioral and neurophysiological evidence, moreover the 63
role of motor constraints is especially considered. The main 64
assumption is that early vocal development can be studied 65
as a result of embodiment, self-organization, and emergence 66
mechanisms produced by human evolution. In general, studies 67
have shown that infants show preparedness to acquire natural 68
language. Motor, perceptual, social, and learning ability con- 69
straints, and their maturation during infant development play 70
a key role in the emergence of language [1]. 71

Equally important, machine learning techniques have 72
rapidly evolved, providing developmental robotics with 73
interesting approaches as active learning. In contrast to the 74
more usual passive learning algorithms, active learning data 75
are collected in order to minimize a given property of the 76
learning process, e.g., the uncertainty [5] or the prediction 77
error [6] of a model. This family of algorithms is of partic- 78
ular interest for developmental robotics. During sensorimotor 79
exploration they allow the agent to focus on parts of the sen- 80
sorimotor space in which exploration is expected to improve 81
the quality of the learned model [7]. 82

2379-8920 c© 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

mailto:juan.manuel.acevedo.valle@upc.edu
http://ieeexplore.ieee.org
http://www.ieee.org/publications_standards/publications/rights/index.html


IE
EE

 P
ro

of

2 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

The contribution of this paper is extending the study83
of early language development using intrinsically motivated84
exploration algorithms. Herein, we provide new simulation85
results showing the suitability of these algorithms in the self-86
exploration of sensorimotor vocal spaces. The theoretical basis87
of the probabilistic models used to represent knowledge is also88
provided. Furthermore, we propose an architecture that could89
be used to study the role of constraints during sensorimotor90
exploration in embodied agents. Finally, it is worth mentioning91
that the learning algorithm presented herein could be applied92
to any system subjected to constraints in order to improve93
learning progress.94

The remainder of this paper is organized as follows.95
Section II introduces related works. Section III highlights96
the role of intrinsic motivations and proprioceptive feedback97
in vocal development. The experiment setup is described98
in Section IV and results are presented and discussed in99
Section V. Finally, the conclusions are presented in Section VI.100

II. RELATED WORK101

This paper revisits and expands the investigation introduced102
in [8] and [9]. In [9], an intrinsically motivated exploration103
architecture was proposed for the study of the developmental104
stages emergent during the early vocal development of infants.105
For the experimentation the simulated ear-vocal tract model106
DIVA [10] was used. In spite of the relevance of its results,107
the motor constraints and the somatosensory system were108
neglected in [9]. However, morphological constraints play a109
key role in speech acquisition. Therefore, a new exploration110
algorithm proposed in [8] to incorporate motor constraints111
awareness using a somatosensory model. In the past, some112
studies have tried to explain the emergence of developmental113
stages during the vocal development, assuming their exis-114
tence, but those stages were bridged using hard-coding for115
experimentation [10]–[13].116

In [14], an approach for inverse kinematics learning in117
redundant systems was presented. It was demonstrated that118
goal babbling can be advantageous in learning in the early119
stages of development, as observed in developmental theo-120
ries. In parallel, [15] presented an intrinsically motivated goal121
exploration approach for the active learning of inverse mod-122
els. This approach was applied to the vocal sensorimotor space123
exploration in [9] and [16]. The algorithm considered in this124
paper extends intrinsically motivated exploration in the goal125
space to include motor constraints. Considering both motor126
and perceptual constraints during learning and exploration is127
crucial to design cognitive architectures for motor control.128

Among the efforts to model the acquisition of speech there129
is the DIVA model [10]. It aims to imitate the underlying130
neurophysiological mechanisms for speech acquisition and131
production. The cognitive architecture of the system is an132
artificial neural network. The model includes the premotor,133
motor, auditory and somatosensory cortical areas, and simu-134
lated ear-vocal tract system. In [10], the somatosensory model135
was effectively integrated into the acquisition and production136
of speech processes. It was not used as an element to integrate137
motor constraints but as an extra source of sensory-feedback.138

The ear-vocal tract component of the DIVA model is used in 139
this paper, as it was in [8] and [9]. 140

Finally, another interesting contribution was the active learn- 141
ing architecture presented in [17] which considered time con- 142
straints. This paper proposed a music performance imitation 143
scenario and implemented a learning architecture able to learn 144
a musical instrument model and a body capabilities model; the 145
architecture is also able to imitate a sequence of sound, while 146
simultaneously kinematic errors, due to the control architec- 147
ture, are corrected. Similar to [9], models employed in [17] 148
were based on Gaussian mixture models (GMMs). 149

III. EARLY VOCAL DEVELOPMENT IN MACHINES 150

Human speech production is one of the most complex motor 151
acts performed by any living being [18]. Producing a linguis- 152
tic message that can be understood by another human requires 153
coordinating many degrees of freedom in the respiratory, 154
laryngeal, and supraglottal articulatory system. 155

How infants acquire the complex ability to control speech 156
production and in general how they learn language remains a 157
matter of research [1]. It has been pointed out that strong 158
regularities can be observed in the structure of the vocal 159
development process independently of interindividual differ- 160
ences [1], [19]. In general, the infant first discovers how to 161
control phonation, then focuses on vocal variations of unartic- 162
ulated sounds and finally automatically discovers and focuses 163
on babbling with articulated proto-syllables. In [18], some 164
experiments suggested that goals of speech movements are 165
auditory in nature and maintenance of motor command maps 166
to auditory results is performed with auditory feedback. 167

It is important to inquire into the developmental assump- 168
tions considered in the experiments in [8] and [9], as this 169
paper is based on those experiments. Regarding the infant 170
development stages mentioned in [1], our experiments con- 171
sider the developmental stage known as canonical babbling 172
(CB) [20] and the beginning of language-specific speech pro- 173
duction [1]. Results suggest that during CB infants learn to 174
control their ear-vocal tract system based on auditory feed- 175
back. Nevertheless, when infants begin to babble they do it 176
regardless of the audibility of their vocalizations. CB could 177
be the result of a natural tendency of infants to move their 178
body parts rhythmically motivated by sensory feedback [20]. 179

Consistent with the theory, we assumed a simplified expla- 180
nation that the artificial agent is exploring its ear-vocal tract 181
system choosing auditory goals and evaluating the result. 182
Therefore, our cognitive architecture allows the agent to 183
explore regions, where the competence to produce intended 184
sounds is improved. However, we also endow the agent with 185
autonomous mechanisms to discover constraints in order to 186
drive the exploration. To accomplish that objective, previously 187
proposed active learning architectures and the proprioceptive 188
feedback concept are combined. 189

A. Intrinsically Motivated Exploration Architectures 190

Among the vast number of active learning architectures, 191
this paper considers the exploration architecture proposed 192



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 3

by [15]. This architecture reproduces the formalism of intrin-193
sic motivation inspired by psychological literature as proposed194
previously in [21] and [22]. Using goal babbling, intrinsi-195
cally motivated exploration aims to minimize the error of an196
agent to reach self-generated goals measured according to a197
competence function. This architecture allows artificial agents198
to efficiently and actively explore and generate maps from199
motor capacities to perceived results. Therefore, exploration200
occurs over regions in which agents perceive they are becom-201
ing more competent to reach self-generated goals. Intrinsically202
motivated exploration architectures were originally designed203
to actively learn inverse models of high-dimensional input–204
output spaces. This architecture was later extended by [9] to205
study self-organization in early vocal development stages in206
infants and robots.207

Intrinsically motivated learning algorithms have shown208
favorable results in previous experiments to learn sensorimotor209
coordination skills in redundant nonlinear high-dimensional210
mappings which share many mathematical properties with211
vocal spaces. Moulin-Frier et al. [9] used a simulated ear-212
vocal tract system to study the emergence of developmental213
stages implementing intrinsically motivated exploration. They214
argued that the development of the agent self-organizes into215
vocal developmental sequences. The results presented therein216
opened the door to a new approach in vocal development217
to be explored. This paper introduces a methodology which218
enhances intrinsically motivated architectures with constraint219
awareness.220

B. Proprioceptive Feedback221

Some of the most adopted theories of speech state that222
speech production is organized in terms of motor control sig-223
nals and their associated vocal tract configurations which has224
been corroborated by several experimental results [23], [24].225
Nevertheless, we adopt the simplified hypothesis that speech226
goals are defined acoustically and maintained by auditory227
feedback [18]. CB is a rhythmic behavior that, with some dif-228
ferences, emerges in both, normally developing infants and229
infants with hearing loss. When infants start to babble, they230
do it regardless of the audibility result (i.e., they produce audi-231
ble and voiceless vocalizations). However, evidence suggests232
that, around the onset of CB, infants learn to vocalize based233
on auditory feedback [1], [20].234

How the somatosensory system1 affects the ear-vocal tract235
exploration is an open question that was not previously236
approached in [9]. However, the relevance of the somatosen-237
sory system for speech has been shown in different exper-238
iments, for instance the results in deaf individuals suggest239
that somatosensory inputs related to movement play a more240
important role in speech production than what was thought241
before [25], [26]. Furthermore, the fact that CB also emerges242
in deaf infants suggests that somatosensory feedback must play243
a more relevant role during the prelinguistic vocal development244
in infants [27].245

1Strictly, the somatosensory system is also a sensorimotor system. In future
works, we will distinguish two sensorimotor systems: 1) the auditory-motor
system and 2) the somatosensory-motor system.

Fig. 1. Examples of articulatory configurations that produce collisions in the
DIVA vocal tract model.

In [28], a robotic device able to generate patterns of facial 246
skin deformation related to certain speech productions was 247
used. The results showed that when the facial skin is stretched 248
whilst subjects are listening to words, the sounds they hear are 249
altered. Thus, theory and results suggest that the somatosen- 250
sory system is involved in speech perception. Following 251
this hypothesis, improvements can made to the experiments 252
proposed in [9] by including a somatosensory system to endow 253
the learner with physical constraint awareness. 254

In [8], the foundations of a simplified architecture were 255
established allowing us to include physical constraints to the 256
learning process through a proprioceptive signal, similar to 257
the ability to feel pain in humans. The open source DIVA 258
model2 [10] provides a synthesizer that represents the human 259
vocal tract and ear systems. The DIVA model also includes 260
a somatosensory system, but in spite of it, there is a lack of 261
physical constraints in the DIVA vocal tract. The absence of 262
constraints allows the execution of motor commands that lead 263
to collisions or articulatory superpositions. Both circumstances 264
lead to no phonation and moreover, the latter is a contradictory 265
result since it lacks physical sense, as shown in Fig. 1. 266

To overcome the drawbacks caused by the lack of con- 267
straints, we introduce a somatosensory system. This new 268
element, not considered in [9], is based on an area function 269
which is a vector descriptor of the vocal tract shape. It con- 270
sists of a mechanism that evaluates if an exploratory motor 271
command produces a collision or superposition of articulatory 272
tissues, the system generates a proprioceptive signal. Using the 273
data generated with this mechanism, the agent builds a map 274
from motor commands to proprioceptive results. This map is 275
used to predict which motor commands may lead to undesired 276
collisions, so they may be rejected, forcing the agent to choose 277
a new auditory goal. In the next section, this mechanism is 278
explained in detail. 279

IV. PROPOSED ARCHITECTURE 280

The experimental architecture proposed in this paper to 281
study the early vocal development in machines is shown 282
in Fig. 2, where five elements interact. These elements are 283

2http://www.bu.edu/speechlab/software/diva-source-code/



IE
EE

 P
ro

of

4 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

Fig. 2. Experimental architecture. It is composed by five interacting modules,
two of them contained within the ear-vocal tract module (the sensorimotor
system and the somatosensory system).

introduced below and explained in detail in the coming284
sections.285

1) Sensorimotor system is a simulated ear-vocal tract. It286
corresponds to the physical properties of the embodied287
agent. For the present work the ear-vocal tract system288
of the DIVA model [10] is used.289

2) Somatosensory system is a perceptual mechanism that290
evaluates the shape of the vocal tract. It generates a291
proprioceptive feedback signal indicating if an undesired292
contact or collision is produced into the vocal tract.293

3) Sensorimotor model is a mathematical representation of294
the vocal tract-ear model. It endows the artificial agent295
to map motor commands to auditory effects using the296
data collected from the agent’s own vocalizations.297

4) Somatosensory model is a mathematical representation298
that maps motor configurations to their likely proprio-299
ceptive feedback to acquire self-awareness of its own300
physical constraints in order to avoid executing motor301
configurations that produce undesired behaviors.302

5) Interest model for auditory goals allows the agent to303
actively choose auditory goals in order to improve304
the quality of its sensorimotor model based on a cer-305
tain measure of competence. This model represents306
the core of the intrinsically motivated sensorimotor307
self-exploration.308

A. Sensorimotor System309

The DIVA vocal tract configuration is determined by the310
position of ten articulators and three phonation parameters.311
Along this paper, only seven articulators and two phonation312
parameters (voicing and glottal pressure) are considered [9].313
Articulators and voicing parameter motor dynamics are mod-314
eled as overdamped second order systems315

ẍ+ 2ζω0ẋ+ ω20(x− m) = 0 (1)316
with ζ = 1.01 and ω = (2π/0.8) representing the damping317
factor and the natural frequency, respectively. The duration of318
each vocal experiment in seconds is 0.8, whereas m and x rep-319
resents the desired articulator position (motor command) and320
the current articulator position, respectively. During each vocal321
experiment two different motor commands are introduced for322
each of the seven articulators and the two voicing parameters:323
one for the 0–250 ms window and another for the remain-324
ing time. Thus, each motor command is represented by an325

Fig. 3. Vocalization experiment structure. The upper plot shows the articu-
latory trajectories, from 0 to 250 ms, the commands for Art1, Art2, and Art3
are set to 2, 0, and 2, respectively, whereas the glottal pressure and voic-
ing are both set to 0.5. From 250 to 800 ms, the commands for Art1, Art2,
and Art3 are set to −3, 0, and 2, respectively, whereas the glottal pressure
and voicing are both set to 0.7. The remaining motor commands are set to
zero. The middle plot represents the speech sound wave signal. The bottom
plot shows the auditory trajectories. The dotted outlined boxes represent the
perception time windows from 250 to 400 ms and the second from 650 to
800 ms. The auditory output s are determined from the average of each tra-
jectories along each one of the time windows. Whereas the proprioceptive
feedback p is determined by the average value of min(af ).

18-D vector. The auditory output of a human vocalization can 326
be described by its formant frequencies. We consider the first 327
two formant frequencies, F1 and F2, along with an intonation 328
signal I. The intonation signal is 1 when phonation occurs 329
and 0 otherwise, two conditions are required for phonation 330
to occur: 1) the area function af of the vocal tract must be 331
positive elsewhere and 2) the voicing and pressure parameters 332
must be positive. The area function is a vector function that 333
describes the transversal shape of the vocal tract. 334

During the vocalization the auditory output of the system 335
is observed along two time windows, the first from 250 to 336
400 ms and the second from 650 to 800 ms. The value of each 337
auditory output is averaged for each time window, the result 338
is a 6-D output signal (two formants and the intonation, hence 339
three values, per each of the two time windows). In Fig. 3, 340
we reproduce the vocalization representation shown in [9]. To 341
be consistent with the co-articulated nature of speech, only 342
two perceptual windows are used [1]. However, since only two 343
portions of the vocalization are considered, a lot of information 344
is lost. For instance, it is shown in [23] the continuum of co- 345
articulated gestures. Therefore, future works should consider 346
studying the continuum of speech gestures and self-structuring 347
of vocalizations. 348

B. Somatosensory System 349

In Fig. 3, it is shown that the area function af is observed 350
during both perception time windows. The minimal value of 351
the area function min(af ) would be zero when the vocal tract 352
is closed at any point and negative values mean that some tis- 353
sues are overlapped, which does not have physical meaning. 354
However, in some cases it might be interpreted as the tongue 355
being bitten. In other cases it might represent high pressure 356
between the tongue and the palate, which might be interesting 357



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 5

to the learner in a realistic scenario, where motor constraints358
are not violated. In general, we made a strong assumption that359
any motor constraint violation over a threshold is uncomfort-360
able or painful. Hence, the average value of min(af ) in each361
perception time window is used to generate a proprioceptive362
feedback signal p: if the average of min(af ) is lower than a363
threshold for any perception window, then the configuration364
is evaluated as a undesired collision with p = 1, and p = 0365
otherwise.366

C. Sensorimotor Model367

GMMs are linear combinations of multivariate Gaussian368
distributions that represent clusters of data. They have369
been previously used to represent nonlinear redundant370
maps [17], [21], [29] in order to solve the inverse problem371
of inferring input motor commands from desired sensory out-372
puts. GMMs can be learned using an online variant of the373
expectation-maximization (EM) algorithm in order to learn374
incrementally from incoming data [30]. Here, the algorithms375
used to train GMMs are based on the open source tools3376
associated with [30], and modified according to our problem377
requirements. The three models in the experimental setup are378
probabilistic representations in the form of GMMs, obtained379
using data collected from experiments with the DIVA ear-vocal380
tract. A detailed explanation of the GMMs training is provided381
below.382

We assume that an n-dimensional input command space383
X ∈ Rn is mapped to an m-dimensional output space Y ∈ Rm,384
through a transform function y = f (x) + ε, where y ∈ Y ,385
x ∈ X and ε is random noise. When a dataset of couples386
(x, y) is available, the EM-algorithm is used to obtain a GMM387
which is defined by the parameters {πj, μj, �j}Kj=1, where πj,388
μj, and �j are, respectively, the prior probability, the distribu-389
tion centroid and the covariance matrix of the jth Gaussian, for390
j = 1, 2, . . . , K, being K the number of Gaussian components.391
From [30], Gaussian mixture regression (GMR) is applied to392
compute the conditional probability distribution P(X |y) in the393
input space X given a desired output y. Once it is computed,394
the value x∗ ∈ X is selected such that it maximizes P(X |y).395

To obtain the input x that maximizes the probability to pro-396
duce the output y, the GMR process first defines the partitioned397
vector z ∈ X × Y , where398

z =
(

x
y

)
. (2)399

For each Gaussian j in the GMM the partitions400

μj =
(

μxj
μ

y
j

)
and �j =

(
�xj �

xy
j

�yx �
y
j

)
(3)401

are considered to compute the conditional probability distribu-402
tion Pj(X |y) ∼ Nj(μ̂j, �̂j) in the input space X given a desired403
output y, where404

μ̂j = μyj +�yxj
(
�

x
j

)−1(
x− μxj

)
, �̂j = �yj +�yxj

(
�

x
j

)−1
�

xy
j .405

(4)406

3http://www.calinon.ch/sourcecodes.php

Considering that P(X |y) is at its maximum when x = x̂j = μ̂j, 407
then a natural selection for x in order to produce y is x̂j. But 408
we have K candidates for x, hence it is necessary to compute 409
the probability of the vector ẑj = [x̂j, y]T belonging to its 410
generator Gaussian as 411

P
(
ẑj
) = πj 1√

(2π)K |�j|
e
− 12

(
(ẑj−μj)T�−1j (ẑj−μj)

)
(5) 412

and finally the point z∗ = ẑj that maximizes P(ẑj) is selected as 413
the point that better fits the model. In other words, according 414
to our prior knowledge of f (x), z∗ ∈ f (x), we infer that the 415
output y is generated by x̂j. 416

Taking into account the above for the sensorimotor model, 417
an 18-D motor command space M, with m ∈ M, is defined 418
for the vocal tract articulatory configuration. A 6-D auditory 419
output space S, with s ∈ S, is also defined, the agent being able 420
to observe s according to s = f (m)+σ , where σ ∼ N(0, 0.01) 421
is Gaussian noise. The aim is to find a GMM that solves the 422
inverse problem m = f−1(sg), where sg is an auditory goal. 423

We define a GMM, GSM, to model the sensorimotor system, 424
with X = M and Y = S. Such a model allows computation 425
of the inverse model P(M | sg) using GMR. At the beginning 426
of the experiment, m is selected either, randomly or accord- 427
ing to the interest for initializing the inverse sensorimotor 428
model m ∼ f−1(sg) ∼ P(M | sg) around a specific region of 429
the sensorimotor space. After the initialization stage, the agent 430
starts to select new auditory goals, according to the interest 431
model explained below. In order to reduce memory storage 432
requirements, we consider a generative method for the train- 433
ing stage, which means that the model is trained using the last 434
NSM samples obtained from experimentation along with 435

Nold =
⌈

(1− α)NSM
α

⌉
samples (6) 436

generated using GSM, where α ∈ [0, 1] is the forgetting rate. 437

D. Interest Model for Auditory Goals 438

The interest model for auditory goals endows the learner the 439
ability to select goals that maximize the expected competence 440
progress in order to improve the quality of its sensorimotor 441
model, resulting in better control over it. It is derived from 442
the model proposed in [9]. The competence value for a goal 443
is defined by 444

c = e−|sg−s| (7) 445
where sg is the auditory goal and s is the actual auditory pro- 446
duction after executing a motor command m ∼ P(M | sg). To 447
construct the interest model, the auditory goal space is aug- 448
mented with two extra dimensions: 1) the competence c ∈ C 449
and 2) time tag t ∈ T . The number of vocalizations NIM con- 450
sidered to build the interest model is fixed. A GMM, GIM, with 451
KIM components will be computed from the 8-D data set with 452
NIM samples of the augmented goal space. To initialize this 453
model, some auditory results from the initialization of GSM 454
are selected as the first auditory goals sg. 455

Those Gaussian components in GIM that, according to the 456
covariance matrices �j, contain goals that will likely increase 457

http://www.calinon.ch/sourcecodes.php


IE
EE

 P
ro

of

6 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

the competence progressively are considered to build a prob-458
abilistic distribution P(S) over the auditory space. In order to459
build P(S), the components in GIM are weighted according to460
their time-competence covariance magnitudes. Thus, P(S) will461
prioritize goals in regions, where competence is expected to462
increase. Finally, a sample sg is drawn from P(S) for the next463
vocalization experiment. Model training is performed every464
time the agent has performed nIM experiments, using the last465
NIM vocalizations.466

E. Somatosensory Model467

For the somatosensory model we consider the 18-D motor468
command space M, with m ∈ M, and a new binary propriocep-469
tive output space P = {0, 1}, with p ∈ P. If a vocal production470
leads to undesired contacts, then p = 1, otherwise p = 0. A471
map g is assumed to exist such that p = g(m) and the agent472
can observe p for each vocal experiment. Thus, it is possible473
to find a GMM GSS, with X = M and Y = P, that allows474
computation of the probability distribution P(P |m) applying475
GMR, and determine when a motor command m is likely to476
lead to an undesired collision in the vocal tract.477

The inverse sensorimotor model GSM and the somatosensory478
model GSS are initialized together. When an auditory goal sg479
has been selected, m is computed using P(M | sg). Next, to480
predict the value of p, P(P |m) is used. If the prediction sug-481
gests that m will produce p = 1 then sg is rejected, otherwise482
sg and m are accepted. If sg is rejected, then GIM(S) is recom-483
puted without considering the Gaussian component in GIM that484
generated sg, this mechanism decreases the prior of the con-485
flicting Gaussian in GIM. The new GIM(S) is used to select a486
new goal sg, and the process is repeated until sg is accepted.487
During the agent’s life, the model GSS is trained when GSM is488
trained using the previously described generative mechanism.489

F. Self-Exploration Algorithm490

The self-exploration architecture with motor constraints491
self-awareness, first proposed in [8] for ear-vocal tract explo-492
ration, is an extended version of [9]. The algorithm associated493
with the cognitive architecture is shown in Algorithm 1. Our494
extended self-exploration algorithm with goal babbling and495
motor constraints self-awareness starts with the learner having496
no experience in vocalizing. Models GSM and GSS are initial-497
ized using random vocalizations with small values around the498
neutral position of the articulators. The neutral position of the499
pressure and voicing parameters are set to −0.25 to produce500
no phonation, whereas for the articulators it is considered 0,501
i.e., the rest position. Model GIM is also initialized.502

Then, in line 6 of Algorithm 1 the vocal learner agent selects503
a goal sg for the next experiment according to the probabilis-504
tic distribution P(S) and the motor command m is obtained505
using the inverse model GSM in line 7. The main feature of506
this algorithm, different from similar architectures, is that in507
line 8 P(P | m) provides a prediction for p that indicates if508
the selected motor command is likely to produce an undesired509
collision. From line 9 to 11, if p ≈ 1, the goal is rejected510
and the probabilistic distribution P(S) is updated, ignoring the511

Algorithm 1 Self-Exploration With Goal Babbling and Self-
Constraints Awareness

1: Initialize GSM and GSS
2: Initialize GIM and i← 1
3: while i in [1, 1e5] do
4: ptmp ← 1
5: while ptmp do
6: sg,i ← GIM(S)
7: mi ← GSM

(
M|sg,i

)
8: ptmp ← GSS(P|mi)
9: if ptmp then

10: update(GIM(S))
11: end if
12: end while
13: si ← f (mi)+ σ and pi ← g(mi)
14: ci ← e−|sg,i−si|
15: i← i+ 1
16: if i mod NSM = 0 then
17: train

(
GSM, m(i−NSM+1:i), s(i−NSM+1:i)

)
18: train

(
GSS, p(i−NSM+1:i), s(i−NSM+1:i)

)
19: end if
20: if i mod nIM = 0 then
21: train

(
GIM, sg,(i−NIM+1:i), c(i−NIM+1:i)

)
22: end if
23: end while

Gaussian component in GIM that generated sg and the algo- 512
rithm goes back to line 6. Otherwise, p ≈ 0, both, sg and m are 513
accepted. Next, the motor command is executed with the vocal 514
tract and the agent observes s and p in line 13. In line 14, the 515
learner evaluates the competence value c. It also checks if we 516
are at the end of a learning episode, so models GSM, GSS, and 517
GIM are updated in lines 17, 18, and 21, respectively. To pro- 518
vide objective evaluation elements, some experiments without 519
considering the somatosensory model for choosing goals are 520
also presented. In this later case, sg is always accepted, thus 521
line 4 is substituted with ptmp = 0 in Algorithm 1. 522

V. EXPERIMENTAL RESULTS 523

Eighteen independent simulations using Algorithm 1 were 524
run. All simulations consisted of half a million of vocaliza- 525
tions, including an initial vocalization set of 1000 random 526
samples. Nine different random seeds were considered to 527
generate the same number of motor command sets from a 528
uniform distribution. The limits for those motor commands 529
related to the vocal tract articulators were [−1, 1], whereas 530
for motor commands related to the phonation parameters were 531
[0, 0.7]. Each set was used twice to initialize simulations of 532
Algorithm 1, first without using the somatosensory model and 533
second with it. 534

Considering as a reference the parameters used for simula- 535
tions in [8] and [9], a few variations in their values were tested. 536
First, when values for KSM or KSS are increased the inference 537
error decreases slightly but the computation time grows con- 538
siderably. On the other hand if these values are decreased, the 539
inference error increases considerably. Second, if the training 540



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 7

Fig. 4. Mean competence evolution during simulations using Algorithm 1
for nine different initialization data sets. Moving average of 5000 samples are
considered to filter the results of each simulation. Results are shown in the
case of proprioceptive and nonproprioceptive agents.

steps are increased for the somatosensory model and the senso-541
rimotor model, then the training computational time increases542
as Nold in (6) increases proportionally, but no improvement543
is obtained in the inference error. However, if these values544
are decreased beyond the values used in [9], the mean infer-545
ence error increases. Third, when αSM and αSS are larger than546
0.1 the competence progress is slower. Finally, the parame-547
ters linked to the interest model allow a wider range of values548
to be chosen obtaining similar results. For KIM we observed549
that values greater than or equal to 12 worked similarly, but550
smaller values negatively impacted the competence progress.551
Thus, the main parameters for all the simulations were kept552
as in [8] and [9] as they performed better than other sim-553
ulations in terms of exploration results and simulation time.554
Summarizing, values were set to KSM = 28, NSM = 400,555
KSS = 28, KIM = 12, NIM = 4800, nIM = 12, and the con-556
tinuous sampling time used for the DIVA ear-vocal tract was557
ts = 10 ms. The forgetting rate parameter αSM for GSM starts558
from 0.1 and decreases logarithmically to 0.05 after half a559
million of vocalizations. On the other hand, αSS for GSS was560
chosen to be 0.05 through the whole simulation.4561

During the simulation, GSM and GSS are initialized as indi-562
cated in line 1 of Algorithm 1 with the initial motor command563
sets. Then, all the initial phonatory productions are used as564
auditory goals to initialize the interest model GIM as indicated565
in line 2 of Algorithm 1. In this stage, GSM is used to infer the566
motor commands that will likely produce the initial auditory567
goals. These commands are executed without considering the568
proprioceptive prediction p.569

A. On Competence and Contacts570

First of all, Fig. 4 represents the evolution of the compe-571
tence parameter c in (7) for self-generated auditory goals. To572
obtain this plot, first the result of each simulation is filtered573
using a 5000 samples moving average window. Next, simu-574
lations are divided into two general groups: 1) proprioceptive575
agents and 2) nonproprioceptive agents. Finally, the mean of576

4Supplementary downloadable material provided by the authors is available
at https://dx.doi.org/10.6084/m9.figshare.c.3676645.v1. After each experi-
ment, 20 random samples from the last 1000 vocalizations were drawn to
generate videos with audio. Videos of the experiments 1, 5, and 9 are provided.

(a)

(b)

Fig. 5. Algorithm 1 simulation results. (a) Mean percentage of vocalizations
producing undesired collisions considering all the simulated agents. Agents are
grouped by proprioceptive and nonproprioceptive. The results of each agent
are prefiltered considering a 5000 samples moving average. (b) Mean num-
ber of rejected goals using the proprioceptive prediction and considering all
the simulated proprioceptive agents. The results of each agent are prefiltered
considering a 5000 samples moving average.

TABLE I
RESULTS CONSIDERING ALL DATA FROM EXPLORATION

all the filtered results for each group is computed. The same 577
mechanism is considered to obtain the percentage of contacts 578
observed in Fig. 5(a). 579

Tables I and II show the volume of a convex hull cov- 580
ering the explored auditory region. They also display the 581
mean competence and the percentage of undesired contacts 582
at the end of the simulation. First, in Table I, descriptors are 583
computed considering all the vocalization during each simula- 584
tion. Second, in Table II, figures were computed considering 585

https://dx.doi.org/10.6084/m9.figshare.c.3676645.v1


IE
EE

 P
ro

of

8 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

TABLE II
RESULTS WITHOUT CONSIDERING VOCALIZATIONS

WITH UNDESIRED CONTACTS

vocalizations without undesired collisions. Convex hulls pro-586
vide an insight regarding the size of the explored regions in587
the auditory-space. They are computed considering formant588
frequency dimensions F11, F21, F12, and F22.589

In Fig. 4, results suggest that proprioceptive agents perform590
better than those which are not endowed with proprioception.591
We observe that at the beginning of the exploration the mean592
average competence is very similar for both groups. However,593
after the initialization the nonproprioceptive agents suffer an594
important decrement of the competence, which also coincides595
with a significant increment of the percentage of contacts in596
Fig. 5(a). Table I also confirms the expected results accord-597
ing to our hypothesis: using proprioceptive feedback drives598
artificial agents to produce significantly fewer undesired con-599
tacts and also increases the competence to reach self-generated600
auditory goals.601

Some observers might ask the reason of high competence602
values at the beginning of the simulations. We argue that it is603
an expected result as the competence computation begins when604
GIM is initialized with self-generated goals drawn from the ini-605
tial auditory productions of the agent. In other words, GSM and606
GSS models are initialized around a set of initial vocalizations607
and auditory productions. The initial auditory productions are608
selected as auditory goals, then motor commands are computed609
with a sensorimotor model that represents very well those ini-610
tialization samples. Later, as the agent explores the auditory611
space and it moves toward farther regions from those of initial-612
ization, the competence values might slightly decrease. This613
is due to the incremental learning of probabilistic models and614
depends on the values assigned to the forgetting rates αSM and615
αSS. If these forgetting rates are close to zero, then the agent616
is less prone to update its knowledge when new data are far617
from the current knowledge. On the other hand, if the forget-618
ting rates are high, then the agent will adapt its model to the619
new data very fast but also it will forget faster its previous620
knowledge since it is not reinforced.621

We also argue that one of the reasons the propriocep-622
tive agents perform better is the null competence produced623
by nonphonatory vocalizations. The nonproprioceptive agent624
produces much more undesired contacts (four times more625

Fig. 6. Mean percentage of vocalizations producing phonatory result along all
the simulations per each group, proprioceptive and nonproprioceptvie agents.
The percentage of phonatory auditory goal is also shown per each group.
Note: red and blue solid lines are overlapped.

undesired contacts on average) and, therefore, has more non- 626
phonatory vocalizations as corroborated in Fig. 6. Fig. 6 was 627
obtained using the same procedure than Fig. 4. It shows the 628
mean percentage of phonatory goals and actual phonatory 629
vocalizations considering all the simulations per each group 630
of artificial agents. In general, all the auditory goals through 631
the simulations are phonatory. The reason is that nonphonatory 632
goals become uninteresting very early in the artificial agent’s 633
life as they are very easy to be produced. Thus, all those 634
nonphonatory vocalizations which produce null competence 635
impact negatively the average competence. We also might 636
think about all those nonphonatory vocalizations as a waste 637
of energy during the exploration. Knowing which regions of 638
the motor space are leading to collisions might be a relevant 639
knowledge for the agent. However, as the nonproprioceptive 640
agents keep exploring conflicting regions, the proprioceptive 641
agents avoid exploitation of these regions due to their ability to 642
predict somatosensory results from a given motor command. 643

Additionally, discussion about the tradeoff between explo- 644
ration and exploitation can be detailed. We argue that pro- 645
prioceptive agents show a better performance with respect to 646
exploitation, as agents avoid exploring uninteresting regions 647
with high number of contacts. In other words, propriocep- 648
tion, and in general constraint awareness, contributes to the 649
agent finding regularities faster and then fosters specialization 650
in regions of the auditory space, where the agent competence 651
to reach self-generated goals is higher. It is worth mentioning 652
that we are aware that it is also important to include the social 653
factor in the learning development of the artificial agent, in 654
order to better understand the role of proprioception in social 655
learning. In social learning, exploration is not just driven by 656
the progress in competence and discovery of constraints, but 657
also by the relevance of auditory goals for socialization pur- 658
poses. These studies leading to more exploring behaviors is 659
left for future work. 660

Furthermore, Fig. 5(b) shows the mean number of goals 661
rejected by the proprioceptive mechanism, represented in 662
lines 5–12 of Algorithm 1. In this plot, we prefilter the results 663
for proprioceptive agents considering a 5000 samples mov- 664
ing average for visualization purposes. It can be observed in 665
Fig. 5(b) that in general the proprioceptive mechanism is more 666
active at the beginning of the simulations presumably due to 667
the quantity of contacts along the initial set of vocalizations. 668



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 9

Thus, we might deduce that proprioception prevents the agent669
from further exploration in regions that are producing unde-670
sired contacts especially in the early stages. In the next, we671
introduce some figures in order to show the implications over672
the shape of the explored auditory region when proprioception673
is considered.674

B. On Explored Regions675

Regarding the volume of the explored region, Table I676
indicates that the ratio of average volume of convex hulls677
described by the explored regions in the frequency space is678
0.51/0.48 between the nonproprioceptive and proprioceptive679
agents, whereas the ratio of the mean competence is 0.53/0.71.680
In other words, whereas proprioceptive agents explore a 5.88%681
tighter region than the nonproprioceptive, their performance682
is 25.35% better than the later ones. On the other hand,683
Table II considers only the vocalizations without undesired684
contacts. A shrinkage of the convex hulls is observed, the685
ratio of average volumes is, in this case, 0.40/0.39 while686
the mean competence ratio is 0.67/0.75. From these numbers687
we observe that, in general, the competence to vocalizations688
without undesired contacts is higher for both kinds of agents.689
However, regarding competence the proprioceptive agents still690
perform 11.94% better than the nonproprioceptive agents.691

Based on Table I, we selected three different initial sets692
given their simulation results in order to produce Figs. 7–9.693
First, we select initial set 1, as it performs better in terms694
of competence when proprioception is considered. Second, to695
contrast with initial set 1 we select initial set 5, as its proprio-696
ceptive agent performs the worst with regards to the percentage697
of undesired contacts. Finally, we select initial set 9, as its698
proprioceptive agent produced the largest convex hull volume.699

Figs. 7–9 show some projections of vocalizations distribu-700
tion maps of the auditory productions generated along the701
simulation. Points in the plots are colored according to the702
percentage of undesired contacts produced in its neighbor-703
hood. Specifically, three projections are shown for each of704
the selected sets of initial vocalizations. Projection F1,1F2,1705
represents the auditory fingerprint of the vocalizations in the706
first perceptual window. Projections F1,2F2,2 is similar to the707
first projection but for the second perceptual window. Finally,708
projection I1I2 represents the value of the intonation parameter709
in the first perceptual window against the same parameter in710
the second perceptual window.711

Distributions in Figs. 7–9 indicate that the intonation param-712
eter projection I1I2 is the most influenced sensory-output due713
to the proprioceptive feedback. Recall that I1 and I2 depend714
on the average audibility of the vocalization which is null in715
two cases: 1) when the voicing parameters are lower than zero716
or 2) when the area function of the vocal tract is nonpositive717
elsewhere. Therefore, keeping in mind the latter case, those718
vocalizations producing an intonation parameter (either I1 or719
I2) lower than one indicate that a contact has likely occurred.720
If a contact occurs, there are two possible results, the average721
of the minimum value of the area function might be nega-722
tive or not. If it is negative, then the contact is classified as an723
undesired contact and the proprioceptive signal takes the value724

Fig. 7. Projections of vocalizations distribution along simulations using initial
set 1 with Algorithm 1. Results for nonproprioceptive agent (left) and propri-
oceptive agent (right). Points are colored according to the total percentage of
undesired contacts in their neighborhood.

one. Thus, having both values lower than one at the same time 725
is even more likely to produce undesired contacts. That is the 726
reason proprioceptive agents explore less intensively the mid- 727
dle of the region in the intonation space. However, we argue 728
that in spite of the low density of vocalizations in that region, 729
proprioceptive agents succeed in finding more vocalizations 730
that produces nonconflicting articulatory configurations in that 731
region. For instance, looking at the projections in Fig. 9, the 732
proprioceptive agent almost covers all the intonation space 733
with low density of contacts. 734

Moreover, comparing proprioceptive and nonproprioceptive 735
agents in Figs. 7–9, we observe that the area of the explored 736
regions varies slightly due to the proprioceptive mechanism. 737
This fact is supported by Tables I and II. In general, in most 738
of the cases using the proprioceptive feedback results in a 739
slightly smaller explored region but this is not a conservative 740
fact. For instance, Table I indicates that the convex hull vol- 741
umes described in the auditory space by the experiments 2, 742
4, and 9 were larger when proprioception was considered. In 743
general, besides a certain degree of randomness due to our 744
probabilistic approach, we argue that there are three main ele- 745
ments that determine the shape of the explored region: 1) the 746
initial set of vocalizations; 2) evolution of competence; and 747
3) proprioception. 748

Regarding the initial set, our criteria to choose random 749
vocalizations close to the neutral positions produces rich sets 750
of phonatory vocalizations, either with contacts or without. 751



IE
EE

 P
ro

of

10 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

Fig. 8. Projections of vocalizations distribution along simulations using initial
set 5 with Algorithm 1. Results for nonproprioceptive agent (left) and propri-
oceptive agent (right). Points are colored according to the total percentage of
undesired contacts in their neighborhood.

As we are working with self-generated goals, the agent is752
expected to be very good at the beginning at reaching goals753
when auditory goals are close to the initialization region. In754
the case of nonproprioceptive agents, the only parameter that755
drives the exploration is the evolution of competence, which756
is why we observe plenty of areas with a huge amount of757
undesired contacts in the plots of nonproprioceptive agents758
with respect to the proprioceptive agents. Furthermore, we759
observe that proprioception might lead toward two different760
situations: 1) an unexplored auditory region or 2) explored761
region but with nonconflicting articulatory configurations. For762
instance, in Fig. 7, specifically in the projection F1,1F2,1, we763
observe that in general the nonproprioceptive agents produce764
a lot of undesired contacts over almost the whole explored765
region. On the contrary, the proprioceptive agent explores a766
smaller region over the same projection, however it achieves767
a considerably lower density of undesired contacts; results also768
supported by the convex hull volume displayed in Table I. In769
addition, projection F1,2F2,2 shows similar explored regions770
for both agents. Indeed, the proprioceptive agent explored a771
wider region in that projection and was capable of finding non-772
conflicting vocalizations for some of the regions, where the773
nonproprioceptive agent produces a lot of undesired contacts.774
We observe, in general, for all the agents in Figs. 7–9, that775
producing auditory results for the projection F1,2F2,2 close to776
the origin is hard without producing contacts.777

Fig. 9. Projections of vocalizations distribution along simulations using initial
set 9 with Algorithm 1. Results for nonproprioceptive agent (left) and propri-
oceptive agent (right). Points are colored according to the total percentage of
undesired contacts in their neighborhood.

In Fig. 8, corresponds to the agent with the worst results 778
using proprioception regarding the number of undesired con- 779
tacts, the projection F1,1F2,1 indicates that the proprioceptive 780
agent has explored a smaller region than the nonproprioceptive 781
agent. However, if we observe the boundaries of the explored 782
region with proprioception, they coincide with regions where 783
the nonproprioceptive agent produces a high amount of unde- 784
sired contacts. Thus, the proprioceptive mechanism does not 785
allow the proprioceptive agent to exploit those regions. In spite 786
of less exploration, we observe that in the explored region 787
where both agents intersect, the proprioceptive agent produces 788
less undesired contacts. Looking at Table II, the results for 789
the agent corresponding to Fig. 8 (experiment 5), we observe 790
that the explored regions with and without proprioception are 791
described by convex hulls with similar volume. This suggests 792
that the conflicting region explored by the nonpropriocep- 793
tive agent prevents the agent from exploiting regions without 794
undesired contacts. Thus, the agent achieve lower competence 795
values over the latter regions. On the other hand, the propri- 796
oceptive agent avoids conflicting regions, in consequence it 797
produces 15% less contacts and achieve a higher competence 798
average. 799

In addition, looking at the projection F1,1F2,1 Fig. 9, we 800
observe that the proprioceptive agent explores a larger region. 801
Moreover the density of contacts along the explored region is 802
significantly lower. This is also supported by the numerical 803
results in Tables I and II. On the other hand, in the projection 804



IE
EE

 P
ro

of

ACEVEDO-VALLE et al.: AUTONOMOUS DISCOVERY OF MOTOR CONSTRAINTS IN INTRINSICALLY MOTIVATED VOCAL LEARNER 11

Fig. 10. Density distribution computed using Gaussian-kernels over all the
data obtained along the simulations considering the first principal component
with a variance contribution ratio of 0.68.

Fig. 11. Density distribution computed using Gaussian-kernels over the data,
excluding vocalizations with undesired contacts, obtained along the simula-
tions considering the first principal component with a variance contribution
ratio of 0.61.

F1,2F2,2 the exploration close to the origin of that projection805
is less intensive in the proprioceptive agent, which reinforces806
the previous observation of the difficulties to produce audi-807
tory results in that region without contacts, similar results are808
observed in Fig. 8. Future work also must focus in the study809
of what is happening in that region and how relevant it is to810
language, as well as modify the system accordingly.811

Finally, in order to observe the differences between the812
vocalization distributions obtained using the different explo-813
ration algorithms, we perform a sample density analysis over814
the formant frequency dimensions. In order to make the results815
easier to visualize we perform a principal component analysis816
(PCA) procedure. We consider analyzing the data twice, first,817
considering all the data collected along the exploration and818
second, without considering the vocalizations with undesired819
contacts. The PCA is done considering the dimensions F1,1,820
F2,1, F1,2, and F2,2, the data of all the 18 simulations are con-821
catenated and used to perform the PCA. The PCA considering822
all the samples is performed and the first component is kept,823
which contributes to the variance with a ratio of 0.61. A sec-824
ond PCA is performed considering only the non conflicting825
vocalizations, again only the first component is kept, since it826
contributes to the variance with a ratio of 0.68. Once PCA827

transforms 4-D data into 1-D data, kernel-distribution estima- 828
tion is performed using Gaussian-kernels according to [31] for 829
the proprioceptive and nonproprioceptive cases. 830

In Figs. 10 and 11, we can observe the density distributions 831
obtained separately with all the proprioceptive and the nonpro- 832
prioceptive agents. First in Fig. 10, the distribution considering 833
all the data obtained from all the experiments is shown. In gen- 834
eral, it is observed that the agents explored similar regions, 835
but with different intensity. In Fig. 11, we observe the distri- 836
bution of the first component obtained from the PCA when 837
only nonconflicting vocalizations are considered. In the latter 838
case it is observed that regarding the regions which are of 839
interest, in other words the regions where physical constraints 840
are not violated, both kinds of agents explore with a simi- 841
lar density shape, which means that even though both agents 842
explore similar interesting regions, the proprioceptive agents 843
achieve in general higher competence. 844

VI. CONCLUSION 845

An application of active learning techniques applied to the 846
study of vocal exploration considering motor constraints has 847
been introduced. It has been presented as an intrinsically moti- 848
vated sensorimotor self-exploration architecture with motor 849
constraints self-awareness. Constraints awareness is achieved 850
by providing a proprioceptive mechanism which endows an 851
artificial agent with the capacity to autonomously generate a 852
somatosensory model. This model is then used to predict the 853
consequences of a motor action and to avoid its execution if 854
it is expected to generate an undesired proprioceptive result. 855

The proprioceptive mechanism improved the quality of 856
learning according to a competence function. However, we 857
observe a tradeoff between exploration and exploitation, 858
predominantly nonproprioceptive agents achieve greater explo- 859
ration in the auditory space. In contrast, we observe a more 860
intensive exploitation in interesting regions driving to the 861
higher competence values achieved by proprioceptive agents. 862
In general, vocal-auditory spaces are high dimensional redun- 863
dant spaces, thus an auditory output may be produced by 864
different articulatory configurations. Some of these articula- 865
tory configurations may lead to undesired contacts. Hence, we 866
argue that sensorimotor redundancy is reduced when proprio- 867
ception is included in the system allowing the agent to focus on 868
exploitation of nonconflicting vocalizations. In consequence, 869
the sensorimotor model generated through the exploration does 870
not include conflicting regions, where constraint violations are 871
likely to happen. For that reason, sensorimotor models achieve 872
better fitting to the regions of interest where constraints are 873
met. In this way, we showed how sensorimotor exploration, 874
and in general sensorimotor knowledge, can be shaped by 875
constraints. 876

Regarding the advance toward vocal exploration, we have 877
showed the suitability of the presented architecture to learn 878
vocal spaces in interesting and less redundant regions as chil- 879
dren might do. However, in order to continue our research 880
on early vocal development, we must study in greater depth 881
the first period of vocalization development. A deeper anal- 882
ysis of the learning processes underlying the nonauditory 883



IE
EE

 P
ro

of

12 IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS

development related to mastication, deglutition, and crying884
from the cognitive and developmental perspectives should be885
completed in order to generate more complex somatosensory886
architectures. Finally, the next step of this paper should be887
directed toward the self-structuring of vocalization and social888
learning.889

REFERENCES890

[1] P. K. Kuhl, “Early language acquisition: Cracking the speech code,” Nat.891
Rev. Neurosci., vol. 5, no. 11, pp. 831–843, 2004.892

[2] M. Asada et al., “Cognitive developmental robotics: A survey,” IEEE893
Trans. Auton. Mental Develop., vol. 1, no. 1, pp. 12–34, May 2009.894

[3] R. Pfeifer, M. Lungarella, and F. Iida, “Self-organization, embodi-895
ment, and biologically inspired robotics,” Science, vol. 318, no. 5853,896
pp. 1088–1093, 2007.897

[4] R. Pfeifer and C. Scheier, Understanding Intelligence. Cambridge, U.K.:898
MIT Press, 1999.899

[5] D. A. Cohn, Z. Ghahramani, and M. I. Jordan, “Active learning with900
statistical models,” J. Artif. Intell. Res., vol. 4, no. 1, pp. 129–145, 1996.901

[6] S. Thrun, “Exploration in active learning,” in Handbook of Brain Science902
and Neural Networks, 1995, pp. 381–384.903

[7] C. Moulin-Frier and P.-Y. Oudeyer, “Exploration strategies in develop-904
mental robotics: A unified probabilistic framework,” in Proc. Int. Conf.905
Develop. Learn. (ICDL/Epirob), Osaka, Japan, 2013, pp. 1–6.906

[8] J. M. Acevedo-Valle, C. Angulo, N. Agell, and C. Moulin-Frier,907
“Proprioceptive feedback and intrinsic motivations in early-vocal devel-908
opment,” in Proc. 18th Int. Conf. Catalan Assoc. Artif. Intell., 2015,909
pp. 9–18.910

[9] C. Moulin-Frier, S. M. Nguyen, and P.-Y. Oudeyer, “Self-organization911
of early vocal development in infants and machines: The role of intrin-912
sic motivation,” Front. Psychol., vol. 4, pp. 1006–1025, Jan. 2014,913
doi: 10.3389/fpsyg.2013.01006.914

[10] F. H. Guenther, S. S. Ghosh, and J. A. Tourville, “Neural modeling915
and imaging of the cortical interactions underlying syllable production,”916
Brain Lang., vol. 96, no. 3, pp. 280–301, 2006.917

[11] A. S. Warlaumont, G. Westermann, E. H. Buder, and D. K. Oller,918
“Prespeech motor learning in a neural network using reinforcement,”919
Neural Netw., vol. 38, pp. 64–75, Feb. 2013.920

[12] B. J. Kröger, J. Kannampuzha, and C. Neuschaefer-Rube, “Towards a921
neurocomputational model of speech production and perception,” Speech922
Commun., vol. 51, no. 9, pp. 793–809, 2009.923

[13] I. S. Howard and P. Messum, “Modeling the development of pronun-924
ciation in infant speech acquisition,” Motor Control, vol. 15, no. 1,925
pp. 85–117, 2011.926

[14] M. Rolf, J. J. Steil, and M. Gienger, “Goal babbling permits direct927
learning of inverse kinematics,” IEEE Trans. Auton. Mental Develop.,928
vol. 2, no. 3, pp. 216–229, Sep. 2010.929

[15] A. Baranes and P.-Y. Oudeyer, “Active learning of inverse models with930
intrinsically motivated goal exploration in robots,” Robot. Auton. Syst.,931
vol. 61, no. 1, pp. 49–73, 2013.932

[16] C. Moulin-Frier and P.-Y. Oudeyer, “Learning how to reach various933
goals by autonomous interaction with the environment: Unification934
and comparison of exploration strategies,” in Proc. 1st Multidiscipl.935
Conf. Reinforcement Learn. Decis. Making (RLDM), Princeton,936
NJ, USA, Oct. 2014, Art. no. hal-00922537. [Online]. Available:937
https://hal.inria.fr/hal-00922537/document938

[17] A. Ribes, J. Cerquides, Y. Demiris, and R. Lopez de Mántaras, “Active 939
learning of object and body models with time constraints on a humanoid 940
robot,” IEEE Trans. Cogn. Develop. Syst., vol. 8, no. 1, pp. 26–41, 941
Mar. 2016, doi: 10.1109/TAMD.2015.2441375. 942

[18] J. Perkell et al., “The sensorimotor control of speech production,” 943
in Proc. 1st Int. Symp. Meas. Anal. Model. Human Functions, 2001, 944
pp. 359–365. 945

[19] D. K. Oller and R. E. Eilers, “The role of audition in infant babbling,” 946
Child Develop., vol. 59, no. 2, pp. 441–449, 1988. 947

[20] K. Ejiri, “Relationship between rhythmic behavior and canonical 948
babbling in infant vocal development,” Phonetica, vol. 55, no. 4, 949
pp. 226–237, 1998. 950

[21] P.-Y. Oudeyer, F. Kaplan, and V. V. Hafner, “Intrinsic motivation systems 951
for autonomous mental development,” IEEE Trans. Evol. Comput., 952
vol. 11, no. 2, pp. 265–286, Apr. 2007. 953

[22] J. Gottlieb, P.-Y. Oudeyer, M. Lopes, and A. Baranes, “Information- 954
seeking, curiosity, and attention: Computational and neural mecha- 955
nisms,” Trends Cogn. Sci., vol. 17, no. 11, pp. 585–593, 2013. 956

[23] B. Galantucci, C. A. Fowler, and M. T. Turvey, “The motor theory of 957
speech perception reviewed,” Psychonomic Bull. Rev., vol. 13, no. 3, 958
pp. 361–377, 2006. 959

[24] J.-L. Schwartz, A. Basirat, L. Ménard, and M. Sato, “The perception- 960
for-action-control theory (PACT): A perceptuo-motor theory of speech 961
perception,” J. Neurolinguist., vol. 25, no. 5, pp. 336–354, 2012. 962

[25] S. Tremblay, D. M. Shiller, and D. J. Ostry, “Somatosensory basis of 963
speech production,” Nature, vol. 423, no. 6942, pp. 866–869, 2003. 964

[26] S. M. Nasir and D. J. Ostry, “Speech motor learning in profoundly deaf 965
adults,” Nature Neurosci., vol. 11, no. 10, pp. 1217–1222, 2008. 966

[27] S. N. Iyer and D. K. Oller, “Prelinguistic vocal development in infants 967
with typical hearing and infants with severe-to-profound hearing loss,” 968
Volta Rev., vol. 108, no. 2, pp. 115–138, 2008. 969

[28] T. Ito, M. Tiede, and D. J. Ostry, “Somatosensory function in speech 970
perception,” Proc. Nat. Acad. Sci. USA, vol. 106, no. 4, pp. 1245–1248, 971
2009. 972

[29] C. Moulin-Frier and P.-Y. Oudeyer, “The role of intrinsic motivations in 973
learning sensorimotor vocal mappings: A developmental robotics study,” 974
in Proc. Interspeech, Lyon, France, 2013, pp. 1268–1272. 975

[30] S. Calinon, Robot Programming by Demonstration. Lausanne, 976
Switzerland: EPFL Press, 2009. 977

[31] D. W. Scott, Multivariate Density Estimation: Theory, Practice, and 978
Visualization. Hoboken, NJ, USA: Wiley, 2015. 979

Juan Manuel Acevedo-Valle, photograph and biography not available at the 980
time of publication. 981

Cecilio Angulo, photograph and biography not available at the time of 982
publication. 983

Clement Moulin-Frier, photograph and biography not available at the time 984
of publication. 985

https://hal.inria.fr/hal-00922537/document

