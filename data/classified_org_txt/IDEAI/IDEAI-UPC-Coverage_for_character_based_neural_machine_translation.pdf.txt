












































Coverage for Character Based Neural Machine
Translation

Técnicas de Cobertura y Caracteres integrados en la
Traducción Automática Basada en Aprendizaje Profundo

M.Bashir Kazimi Marta R. Costa-jussà
TALP Research Center

Universitat Politècnica de Catalunya, Barcelona
mohammad.bashir.kazimi@est.fib.upc.edu

marta.ruiz@upc.edu

Abstract: In recent years, Neural Machine Translation (NMT) has achieved state-
of-the art performance in translating from a language; source language, to another;
target language. However, many of the proposed methods use word embedding
techniques to represent a sentence in the source or target language. Character em-
bedding techniques for this task has been suggested to represent the words in a
sentence better. Moreover, recent NMT models use attention mechanism where the
most relevant words in a source sentence are used to generate a target word. The
problem with this approach is that while some words are translated multiple times,
some other words are not translated. To address this problem, coverage model has
been integrated into NMT to keep track of already-translated words and focus on
the untranslated ones. In this research, we present a new architecture in which we
use character embedding for representing the source and target languages, and also
use coverage model to make certain that all words are translated. Experiments were
performed to compare our model with coverage and character model and the results
show that our model performs better than the other two models.
Keywords: Machine Learning, Deep Learning, Natural Language Processing, Neu-
ral Machine Translation

Resumen: En los últimos años, la traducción automática basada en el aprendizaje
profundo ha conseguido resultados estado del arte. Sin embargo, muchos de los
métodos propuestos utilizan espacios de palabras embebidos para representar una
oración en el idioma de origen y destino y esto genera muchos problemas a nivel de
cobertura de vocabulario. Avances recientes en la traducción automática basada en
aprendizaje profundo incluyen la utilización de caracteres que permite reducir las
palabras fuera de vocabulario. Por otro lado, la mayoŕıa de algoritmos de traducción
automática basada en aprendizaje profundo usan mecanismos de atención donde las
palabras más relevantes en de la oración fuente se utilizan para generar la traducción
destino. El problema con este enfoque es que mientras algunas palabras se traducen
varias veces, algunas otras palabras no se traducen. Para abordar este problema,
usamos el modelo de cobertura que realiza un seguimiento de las palabras ya tra-
ducidas y se centra en las no traducidas. En este trabajo, presentamos una nueva
arquitectura en la que utilizamos la incorporación de caracteres para representar el
lenguaje origen, y también usamos el modelo de cobertura para asegurarnos que la
frase origen se traduce en su totalidad. Presentamos experimentos para comparar
nuestro modelo que integra el modelo de cobertura y modelo de caracteres. Los
resultados muestran que nuestro modelo se comporta mejor que los otros dos mod-
elos.
Palabras clave: Aprendizaje automático, Aprendizaje profundo, Procesado del
Lenguaje Natural, Traducción Automática



1 Introduction

Machine Translation (MT) is the task of us-
ing a software to translate a text from one
language to another. Many of the natural
languages in the world are quite complex due
to the fact that a word could have different
meanings based on the context it is used in,
and it could also be used in different gram-
matical categories (e.g. match as a noun or
as a verb). Therefore, the main challenge in
MT is the fact that for a correct translation
of a word, it is required that many different
factors be considered; the grammatical struc-
ture, the context, the preceding and succeed-
ing words.

Over the years, researchers have devel-
oped different methods in order to reduce
the amount of manual work and human in-
tervention, and increase the amount of auto-
matic work, and machine dependent transla-
tion. One of the main methods in MT is Sta-
tistical Machine Translation (SMT) which is
a data-driven approach and produces trans-
lation based on probabilities between the
source and target language. The goal is to
maximize the conditional probability p(y|x)
of a target sentence y given the equivalent
source sentence x based on a set of pre-
designed features (Koehn, 2009).

NMT is the most recent approach in Ma-
chine Translation which is purely based on a
large neural network that is trained to learn
and translate text from a source to a tar-
get language. Unlike SMT, it does not re-
quire pre-designed feature functions and can
be trained fully based on training data (Lu-
ong and Manning, 2015). NMT has attracted
the attention of many researchers in the re-
cent years. The use of neural networks for
translation by Baidu (Zhongjun, 2015), the
attention from Google’s NMT system (Wu et
al., 2016), Facebook’s Automatic Text Trans-
lation, and many other industries has given
the urge for research in NMT a push.

In this research, we study the state of the
art in NMT, and propose a novel approach by
combining two of the most recent models in
NMT; coverage (Tu et al., 2016) and charac-
ter model (Costa-jussà and Fonollosa, 2016),
in the hopes to achieve state of the art results.
The rest of the paper has been organized as
follows. Section 2 studies the related work in
NMT, section 3 explains the proposed model
in this study and points out the contribution
of the research, section 4 explains the exper-

iments performed and the results obtained,
and finally section 5 summarizes the thesis
and points out possible future research.

2 Related Work

NMT has achieved state of the art results in
MT, and the first NMT models used the Re-
current Neural Network (RNN) Encoder De-
coder architecture (Sutskever, Vinyals, and
Le, 2014; Cho et al., 2014). In this ap-
proach, the input sentence is encoded by the
encoder into a fixed-length vector hT using
a recurrent neural network (RNN), and the
fixed-length vector is decoded by the decoder;
another RNN, to generate the output sen-
tence. Word-embedding (Mandelbaum and
Shalev, 2016) has been used for representa-
tion of the source and target words. One of
the main issues in the simple RNN Encoder
Decoder models is that the encoded vector
is of a fixed length, and it cannot represent
long sentences completely. To address this
issue, attention model has been introduced
to the simple RNN Encoder Decoder model
(Bahdanau, Cho, and Bengio, 2014). At-
tention model uses a bi-directional recurrent
neural network to store the information into
memory cells instead of a fixed-length vector.
Then a small neural network called attention
mechanism uses the input information in the
memory cells and the information on the pre-
viously translated words by the decoder in or-
der to focus on the most relevant input words
for the translation of a specific output word.

In the models mentioned above, word em-
bedding has been used for word representa-
tions. While it performs well, it limits the
NMT model to a fixed-size vocabulary. Since
the models are trained using a large set of
vocabularies, and vocabulary is always lim-
ited, the models face problems with rare and
out-of-vocabulary (OOV) words (Yang et al.,
2016; Lee, Cho, and Hofmann, 2016). Many
of the words could have various morphologi-
cal forms, and could have affixes, and word-
embedding models would not be able to dis-
tinguish a word it has been trained with if an
affix is added to it or a different morphologi-
cal form of the word is used (Chung, Cho, and
Bengio, 2016). To address these problems, it
has been proposed to use character embed-
ding rather than word embedding, resulting
into fully character-level NMT system (Lee,
Cho, and Hofmann, 2016), character based
NMT models that use character embedding



only for source language (Costa-jussà and
Fonollosa, 2016; Kim et al., 2015), and
character-level decoders that use character
embedding for the target language (Chung,
Cho, and Bengio, 2016). Two additional ad-
vantages of character embedding for NMT
are its usability for multilingual translation,
which is the result of its ability to identify
shared morphological structures among lan-
guages, and also the fact that as opposed to
word embedding models, no text segmenta-
tion is required, which enables the system to
learn the mapping from a sequence of charac-
ters to an overall meaning representation au-
tomatically (Lee, Cho, and Hofmann, 2016).
It has been proved that character NMT mod-
els produce improved performance over the
attention model (Costa-jussà and Fonollosa,
2016; Yang et al., 2016; Lee, Cho, and Hof-
mann, 2016; Chung, Cho, and Bengio, 2016).

Another issue with the models mentioned
earlier; specifically in the case of the atten-
tion model, is that they do not track the
translation history and hence, some words
are translated many times while some other
words are not translated at all or translated
falsely. To address this problem, different
models of coverage have been proposed to
track translation history, avoid translating
words multiple times and focus on words that
are not yet translated (Tu et al., 2016; Mi
et al., 2016). The authors claim to have
achieved better results as compared to the
attention based model.

3 Coverage for Character Based
Neural Machine Translation

3.1 Contribution

While researchers have based their models
on the RNN Encoder Decoder (Sutskever,
Vinyals, and Le, 2014; Cho et al., 2014) and
the attention model (Bahdanau, Cho, and
Bengio, 2014), to produce character models
(Costa-jussà and Fonollosa, 2016; Yang et al.,
2016; Kim et al., 2015; Lee, Cho, and Hof-
mann, 2016) and coverage models (Tu et al.,
2016; Mi et al., 2016) and have achieved state
of the art results, both the models address
one of the two issues in the earlier models sep-
arately. The character model addresses the
problem of rare, OOV words, and words with
various morphological structures, and uses
character embedding rather than word em-
bedding, and the coverage model addresses
the problem where some words are trans-

lated multiple times while some of the rest are
never or falsely translated. In this research,
we propose to jointly address the two im-
portant problems in traditional NMT models
and introduce coverage to character model to
achieve state of the art results in NMT. The
character embedding has only been used for
the source words, and the target words still
uses word embedding.

3.2 Architecture of the Proposed
NMT Model

The backbone of the proposed architecture
is still the the attention model proposed by
Bahdanau et al. (Bahdanau, Cho, and Ben-
gio, 2014) with the word embedding in the
input language replaced by the character
embedding as proposed by Costa-jussà and
Fonollosa (Costa-jussà and Fonollosa, 2016).
Thus, first of all, the encoder computes the

input sentence summary ht = [
−→
h t;
←−
h t] which

is the concatenation of
−→
h t and

←−
h t for t =

1, 2, ...., T .
−→
h t and

←−
h t are the hidden states

for the forward and backward RNN encoder
reading the information from the input sen-
tence in the forward and reverse order, re-
spectively. The hidden states are calculated
as follows.

−→
h t =

−→
f (xt,

−→
h t−1) (1)

←−
h t =

←−
f (xt,

←−
h t−1) (2)

where
−→
h t−1 and

←−
h t−1 denote the previous

hidden states for the forward and backward
RNN,

−→
f and

←−
f are recurrent activation

functions, and xt is the embedding represen-
tation for the t-th input word. In the atten-
tion model, xt is the simple word embedding
representation of the word in the source lan-
guage, but in our case, xt is the character
embedding calculated as proposed by Costa-
jussà and Fonollosa (Costa-jussà and Fonol-
losa, 2016) and explained as follows.

First of all, each source word k is repre-
sented with a matrix Ck which is a sequence
of vectors representing the character embed-
ding for each character in the source word k.
Then, n convolution filters H of length w,
with w ranging between 1 to 7, are applied
to Ck in order to obtain a feature map fk for
the source word k as follows.

fk[i] = tanh(〈Ck[∗, i : i+w−1], H〉+b) (3)

where b is the bias and i is the i-th element in
the feature map. For each convolution filter



H, the output with the maximum value is
selected by a max pooling layer in order to
capture the most important feature.

ykH = max
i
fk[i] (4)

The concatenation of these output values
for the n convolution filters H; yk =
[ykH1, y

k
H2, ...., y

k
Hn], is the representation for

the source word k. Addition of two highway
network layers has been proved to give a bet-
ter representation of the source words (Kim
et al., 2015). A layer of the highway network
performs as follows.

xt = t� g(WHyk + bH) + (1− t)� yk (5)

where g is a nonlinear function, t =
σ(WTy

k + bT ) is the transform gate, (1 - t)
is the carry gate, and xt is the character em-
bedding that is used in equations 1 and 2.

The decoder then generates a summary
zT ′of the target sentence as follows.

zt′ = f(zt′−1, yt′−1, st′) (6)

where st′ is the representation for the source
words calculated as follows.

st′ =
T∑
t=1

αt′tht (7)

where ht is calculated by the encoder as ex-
plained earlier, and αt′t is computed as fol-
lows.

αt′t =
exp(et′t)∑T

k=1 exp(et′k)
(8)

and
et′t = a(zt′−1, ht, Ct′−1t) (9)

is called the attention mechanism or the
alignment model which scores how relevant
the input word at position t is to the output
word at position t′, Ct′−1t is the previous cov-
erage and coverage model proposed by Tu et
al. (Tu et al., 2016) is calculated as follows.

Ct′t = f(Ct′−1t, αt′t, ht, zt′−1) (10)

Then, the output sentence is generated by
computing the conditional distribution over
all possible translation.

log p(y|x) =
∑

p(yt′ |y<t′ , x) (11)

where y and x are the output and input sen-
tences, respectively, and yt′ is the t

′-th word

in the sentence y. Each conditional proba-
bility term p(yt′ |y<t′ , x) is computed using a
feed forward neural network as follows.

p(yt′ |y<t′ , x) = softmax(g(yt′−1, zt′ , st′))
(12)

where g is a nonlinear function, zt′ is the de-
coding state from equation 6, and st′ is the
context vector from equation 7

The overall architecture of the proposed
model is illustrated in figures 1, 2, 3. Figure
1 illustrates the character based word embed-
ding model which takes as input the embed-
dings for each character in the source word xt,
and outputs a final word level representation
of it. The output is then fed to the encoder;
depicted in figure 2 which outputs a context
vector s′t based on the attention mechanism
and coverage model. The context vector s′t is
then fed to the decoder illustrated in figure 3
which generates a target translation.

Highway Network

Highway Network

E c h t

xt

Character Embeddings

Multiple 
convolution 
of different 
lengths

Max pooling layer 
to select the 
maximum output 
for each filter 

Two layers of 
highway 
network

Figure 1: Character based word embedding

Echt dicke kiste

Character Based Word Embedding System

x1 x2 x3

h1 h2 h3

+

c1 c2 c3

Attention Layer

∑αt = 1

st’

Coverage

Bidirectional 
RNN

Figure 2: Encoder with coverage & alignment

4 Experiments

In order to evaluate the performance of our
model, experiments on the same data set has



Z2Z1

Echt dicke kiste

Character Based Word Embedding System

x1 x2 x3

The Encoder + Attention Layer

s1 s2

Awesome
y1

sauce
y2

Figure 3: The decoder

been performed using the character model by
Costa-jussà and Fonollosa (Costa-jussà and
Fonollosa, 2016), the coverage model by Tu
et al.(Tu et al., 2016), and finally the pro-
posed model in this study; coverage for char-
acter model. This section has been divided
into two subsections. Subsection 4.1 explains
the data set used and the preprocessing per-
formed on the data, and subsection 4.2 elab-
orates on the evaluation method and the re-
sults obtained.

4.1 Data

The data set used for this experiment is
kindly provided by Costa-jussà (Costa-jussà,
2017) and includes a subset of a larger data
set which includes a set of paper edition
over 10 years of a bilingual Catalan news-
paper , El Periodico, in addition to a cor-
pus of medical domain provided by Universal-
Doctor project1. As a preprocessing task,
the data set has been tokenized and a dic-
tionary of 10 thousand most frequent words
have been prepared for training the system.
Detailed information about the data set is
listed in table 1

Language Set # of Sentences # of Words # of Vocabs

Ca
Train
Dev
Test

83.5k
1k
1k

2.9M
27.7k
27k

83.5k
6.9k
6.7k

Es
Train
Dev
Test

100k
1k
1k

2.7M
25k
24.9k

90k
7k
7k

Table 1: Spanish-Catalan Dataset Statistics

1http://www.universaldoctor.com/

Model BLEU Score
Character 53.30
Coverage 53.76
Character+Coverage 54.87

Table 2: BLEU Scores for the NMT Models

4.2 Evaluation and Results

To evaluate the model, the BLEU (BiLingual
Evaluation Understudy) evaluation method
proposed by Papineni et al.(Papineni et al.,
2002) has been used. The main idea behind
this evaluation method is that the closer to
a human translation the machine translation
is, the better the model performs. The result
of the experiments performed on the data set
mentioned in section 4.1 have been listed in
table 2.

As observed in table 2, the proposed
model outperforms the other models and
achieves state of the art performance. The
main motivation for this study is to try to ad-
dress two main issues in the attention model.
First, the attention model uses word embed-
ding for language representation, and thus it
suffers from the rare, OOV word problems,
and problems with identifying different mor-
phemes added to a word. The second issue
is that even though the attention model fo-
cuses on most relevant part of the input sen-
tence in order to translate and generate an
output sentence, it does not keep track of
already-translated words, which leads to mul-
tiple translation of some words while the rest
are never or falsely translated. The two is-
sues were individually tackled with charac-
ters models (Costa-jussà and Fonollosa, 2016;
Yang et al., 2016; Lee, Cho, and Hofmann,
2016; Kim et al., 2015), and coverage mod-
els (Tu et al., 2016; Mi et al., 2016), respec-
tively. In this research, we tried to improve
the state of the art and introduce coverage
for character model in NMT. The experiment
performed on the data set shown in table
1 clearly shows that our model outperforms
earlier models, as shown in table 2. To under-
stand the contribution of our proposed model
and see how the combination of character and
coverage model compliments the two models
and sometimes performs better than both of
the models, we list in table 3 some manual
analysis on sample translations by the mod-
els tested.

martaruiz
Highlight

martaruiz
Cross-Out

martaruiz
Inserted Text
We use a subset of a large corpus extracted of the paper edition of a bilingual Catalan newspaper, El Periódico (Costa-juss`a et al., 2014). The Spanish-Catalan corpus is partially available via ELDA (Evaluations and Language Resources Distribution Agency) in catalog number ELRA-W0053. 

reference:
Marta R. Costa-juss`a, March Poch, Jose´ A.R. Fonollosa, Mireia Farr´us, and Jose´ B. Mari˜no. 2014. A large Spanish-Catalan parallel corpus release for Machine Translation. Computing and Informatics Journal, 33

martaruiz
Cross-Out

martaruiz
Sticky Note
you should not repeat surname two times... it should be "by Tu et al. (2016)" the same for the previous... and several other references in the paper!

martaruiz
Cross-Out

martaruiz
Inserted Text
words has 


martaruiz
Cross-Out

martaruiz
Inserted Text

martaruiz
Cross-Out

martaruiz
Cross-Out

martaruiz
Inserted Text
has

martaruiz
Sticky Note
no need to list references here... this should go once in the related work, and it is enough!

martaruiz
Sticky Note
Examples show that our model is capable of keeping the best translation from character-based (example X) and coverage-based (example Y) baseline systems and add new improvements (exemple XXX)

(you may write better ;-), but this is the idea...

martaruiz
Cross-Out

martaruiz
Cross-Out



1

Src
Tgt
Ch
Cov
Ch+Cov

dos regidors es presenten als comicis.
dos concejales se pre-sentan alos comicios.
dos ediles se presentan en los comicios.
dos concejales sepresentan a los comicios.
dos concejales se presentan a los comicios.

2

Src

Tgt

Ch

Cov

Ch+Cov

la falta de públic l ’ ha condemnat a mort en una zona clau de l ’ oci
barcelońı que , pel que es veu , té més poder de convocatòria.
la falta de público lo ha condenado a muerte en una zona clave del
ocio barcelonés que , por lo que se ve , tiene más poder de convocatoria.
Palma alguna de público le ha condenado a muerte en una zona clave del
ocio barcelonés que , por lo que se ve , tiene más poder de convocatoria.
a falta de público al ha condenado a muerte en una zona clave del
ocio barcelonés que , por lo que se ve , tiene mejor de convocatoria.
la falta de público le ha condenado a muerte en una zona clave del ocio
barcelonés que , por el que se ve , tiene además poder de convocatoria.a.

3

Src
Tgt
Ch
Cov
Ch+Cov

Una firma austŕıaca va voler vendre sang amb sida a l ’ Àsia..
Una firma austriaca quiso vender sangre con sida en Asia.
Una firma UNK quiso vender sangre con sida en Asia.
Una seguidores UNK quiso vender sangre con sida en Asia.
Unafirma UNK quiso vender sangre consida en Asia

4

Src
Tgt
Ch
Cov
Ch+Cov

com a conseqüència de la progressiva reducció dels marges.
como consecuencia de la progresiva reducción de los márgenes.
a consecuencia de la UNK reducción de los márgenes.
a consecuencia de la UNK reducción de los márgenes.
como consecuencia de la progresiva reducción de los márgenes.

5

Src

Tgt

Ch

Cov

Ch+Cov

... requereix un esforç que involucri “ departaments de Turisme , Joventut
i Educació , i també de coordinació en l ’ àmbit europeu ” ...
... requiere un esfuerzo que involucre “ a departamentos de Turismo ,
Juventud y Educación , y también de coordinación a nivel europeo ...
... requiere un esfuerzo que UNK “ departamentos de Turismo , Joventut
y Educación , y que tiene que UNK en el ámbito europeo ...
... requiere un esfuerzo que UNK “ departamentos de Turismo , Joventut
y Educación , y que tiene que UNK en el ámbito europeo ...
... requiere un esfuerzo que UNK “ departamentos de Turismo , Juventud
y Educación , y también de coordinación en el ámbito europeo ” ...

Table 3: Manual Analysis. Src and Tgt represent Source and Target sentences, Ch, Cov, and
Ch+Cov represent translation by Character, Coverage, and the proposed moedl, respectively.
In example 1 and 2, the proposed model behaves like the coverage model, in example 3, it
behaves like the character model, and examples 4 and 5, it performs better than both of the
other models.

5 Summary

The recent model; attention, proposed by
Bahdanau et al.(Bahdanau, Cho, and Ben-
gio, 2014) tackles the problem of fixed-
length encoding vector in the RNN En-
coder Decoder model used by Sutskever et
al.(Sutskever, Vinyals, and Le, 2014) and
Cho et al. (Cho et al., 2014). It gives NMT
the ability to be able to translate sentences
of any length. It faces two main problems;
the rare, and OOV words problem along with
problems with different possible morphemes
for a single word, and the problem of over-

translation and under-translation. The char-
acter models which use character embedding
(Costa-jussà and Fonollosa, 2016; Kim et al.,
2015; Yang et al., 2016; Lee, Cho, and Hof-
mann, 2016) and the coverage models, which
keep track of translation history (Tu et al.,
2016; Mi et al., 2016) have individually ad-
dressed both the issues, respectively.

In this research, coverage has been intro-
duced to the character model which aims to
address the main issues mentioned earlier al-
together, and improve the state of the art
in NMT. The corpus shown in table 1 has

martaruiz
Cross-Out

martaruiz
Inserted Text
model

martaruiz
Cross-Out

martaruiz
Inserted Text
Una firma

martaruiz
Sticky Note
you should mark in bold the correct translations

martaruiz
Sticky Note
encoder-decoder

martaruiz
Sticky Note
italics? why?

martaruiz
Sticky Note
please, do not add the list of references!!!

martaruiz
Cross-Out

martaruiz
Cross-Out

martaruiz
Sticky Note
why italics?



been experimented and the results have been
listed in table 2. It is clearly observed that
the model in this study outperforms the pre-
vious models and achieves state of the art
performance in NMT.

As in the case of character model, the
character embedding has been used only for
the source language, and the target language
is still limited to word embedding. further re-
search is required in order to study how char-
acter embedding added for the target lan-
guage impacts the performance of the model,
and it is left to investigate more factors af-
fecting the performance of NMT systems.

Acknowledgements

This work is supported by Ministerio de
Economı́a y Competitividad and Fondo Eu-
ropeo de Desarrollo Regional, through con-
tract TEC2015-69266-P (MINECO/FEDER,
UE) and the postdoctoral senior grant
Ramón y Cajal.

References

Bahdanau, D., K. Cho, and Y. Bengio.
2014. Neural machine translation by
jointly learning to align and translate.
CoRR, abs/1409.0473.

Cho, K., B. Van Merriënboer, C. Gulcehre,
D. Bahdanau, F. Bougares, H. Schwenk,
and Y. Bengio. 2014. Learning phrase
representations using rnn encoder-decoder
for statistical machine translation. arXiv
preprint arXiv:1406.1078.

Chung, J., K. Cho, and Y. Bengio. 2016.
A character-level decoder without explicit
segmentation for neural machine transla-
tion. CoRR, abs/1603.06147.

Costa-jussà, M. R. 2017. Why catalan-
spanish neural machine translation? anal-
ysis, comparison and combination with
standard rule and phrase-based technolo-
gies. In A: Workshop on NLP for Similar
Languages, Varieties and Dialects. ”Pro-
ceedings of the Fourth Workshop on NLP
for Similar Languages, Varieties and Di-
alects (VarDial)”, pages 55–62.

Costa-jussà, M. R. and J. A. R. Fonollosa.
2016. Character-based neural machine
translation. CoRR, abs/1603.00810.

Kim, Y., Y. Jernite, D. Sontag, and A. M.
Rush. 2015. Character-aware neu-

ral language models. arXiv preprint
arXiv:1508.06615.

Koehn, P. 2009. Statistical machine transla-
tion. Cambridge University Press.

Lee, J., K. Cho, and T. Hofmann.
2016. Fully character-level neural machine
translation without explicit segmentation.
CoRR, abs/1610.03017.

Luong, M.-T. and C. D. Manning. 2015.
Stanford neural machine translation sys-
tems for spoken language domains.

Mandelbaum, A. and A. Shalev. 2016. Word
embeddings and their use in sentence clas-
sification tasks. CoRR, abs/1610.08229.

Mi, H., B. Sankaran, Z. Wang, and A. It-
tycheriah. 2016. A coverage embed-
ding model for neural machine translation.
CoRR, abs/1605.03148.

Papineni, K., S. Roukos, T. Ward, and W.-
J. Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation.
In Proceedings of the 40th annual meeting
on association for computational linguis-
tics, pages 311–318. Association for Com-
putational Linguistics.

Sutskever, I., O. Vinyals, and Q. V. Le. 2014.
Sequence to sequence learning with neural
networks. In Advances in neural informa-
tion processing systems, pages 3104–3112.

Tu, Z., Z. Lu, Y. Liu, X. Liu, and H. Li. 2016.
Coverage-based neural machine transla-
tion. CoRR, abs/1601.04811.

Wu, Y., M. Schuster, Z. Chen, Q. V. Le,
M. Norouzi, W. Macherey, M. Krikun,
Y. Cao, Q. Gao, K. Macherey, et al. 2016.
Google’s neural machine translation sys-
tem: Bridging the gap between human
and machine translation. arXiv preprint
arXiv:1609.08144.

Yang, Z., W. Chen, F. Wang, and B. Xu.
2016. A character-aware encoder for neu-
ral machine translation. In COLING.

Zhongjun, H. 2015. Baidu translate: re-
search and products. ACL-IJCNLP 2015,
page 61.




